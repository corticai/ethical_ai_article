audioVersionDurationSec,codeBlock,codeBlockCount,collectionId,createdDate,createdDatetime,firstPublishedDate,firstPublishedDatetime,imageCount,isSubscriptionLocked,language,latestPublishedDate,latestPublishedDatetime,linksCount,postId,readingTime,recommends,responsesCreatedCount,socialRecommendsCount,subTitle,tagsCount,text,title,totalClapCount,uniqueSlug,updatedDate,updatedDatetime,url,vote,wordCount,publicationdescription,publicationdomain,publicationfacebookPageName,publicationfollowerCount,publicationname,publicationpublicEmail,publicationslug,publicationtags,publicationtwitterUsername,tag_name,slug,name,postCount,author,bio,userId,userName,usersFollowedByCount,usersFollowedCount,scrappedDate
0,,0.0,71fa8c342920,2018-02-15,2018-02-15 21:42:59,2018-02-15,2018-02-15 22:15:23,7,False,en,2018-02-16,2018-02-16 16:24:47,12,7ae25f6b4291,8.570754716981133,15,0,0,"One of our core values at integrate.ai is to Love People. We mean this in every sense, as we shared in this post, but the one that is…",5,"Technical Solutions to Reduce Algorithmic Bias: The Variational Fair Autoencoder
One of our core values at integrate.ai is to Love People. We mean this in every sense, as we shared in this post, but the one that is pertinent to our machine learning team is to build models ethically, considering how their output may impact people’s lives.
This image has a fascinating history behind it that speaks to different perspectives on equality, equity, and fairness
We work with large enterprises and apply machine learning to consumer behavioural data to help our clients increase the overall lifetime value of their customers, i.e., to guide them to the next best action or product to create stronger relationships with the business. There are different attributes and behaviours that we could consider when building our models and some of them are socially sensitive: for many use cases, we don’t want things like gender, ethnicity, or marital status to influence our predictions.
As much as we Love People, we also love cake (did anybody say red velvet?). And we really love having our cake and eating it too. When it comes to managing potential sources of bias in models, Variational Fair Autoencoders (VFAE) allow us to do just that (Louizos et al. 2016).
Autoencoders
Generally, machine learning (ML) models take a set of inputs and are trained to learn how to predict a target. Inputs could be images, in which case the target could be the classification of objects in the images. Inputs could be text, with the target being corresponding text in a different language, if we were interested in creating a model that would translate for us. And the list goes on. ML models are very flexible in what inputs they accept and what targets they can be trained to predict.
Autoencoders are a unique type of ML model in which the input and output are very similar to one another, and in some cases identical. Autoencoders typically have two channels. The first performs an “encoding” of the input into a new mathematical representation, or latent space, that has a different representation of the original data. This “encoded” data then feeds into a “decoding” channel, which attempts to transform the latent representation back into the original space. Why would we do this? One powerful application relates to helping an algorithm work better on real-world data: using an autoencoder, we can inject noise into the input, and while trying to predict the “clean” dataset, we will obtain a model that is very flexible to more messy, and hence realistic, data. Another application is the reduction in the dimensionality of the original dataset. ML models require more resources to train, the more features are included in the input data. By encoding the data into a latent space with a lower dimension, we can save on computational resources, while at the same time still retaining the most valuable components of the data.
The word variational refers to Bayesian techniques to approximate the posterior, i.e. the probability distribution of the output, the model is trying to predict. In practice, this means we make assumptions regarding the probability distributions of our target and latent representation, with the outcome being that instead of attempting to predict the exact probability distributions, we only aim for an approximation. This results in significant computaional simplification, with a minor reduction in ultimate precision (so long as the assumed probability distributions are reasonable).
Most interesting, and novel, is the fair component. In VFAEs, we explicitly give information about the properties of the data we do not want influencing our models. The latent version of a dataset generated by this model contains much of the valuable details of the original dataset, but with a reduced influence of the sensitive information. We can then train models on this latent representation, and obtain good predictions that are less biased by sensitive properties of the people included in the dataset.
In our models we will use neural networks to transform the data through the encoding and decoding channels.
Real data with real people
Let’s explore how this might work using the Stanford Open Policing Project — Ohio (link), which aggregates anonymous records of police arrest, both pedestrian and traffic, in the state of Ohio. This dataset includes sensitive information, such as the race of the people involved, which is something we definitely do not want to influence our model’s predictions. We will use whether the person was arrested as the target variable.
(As a side note, we believe a use case like predicting if someone should be arrested shouldn’t be fully automated by an algorithm; this kind of sensitive analysis requires human judgment (at least for the foreseeable future)).
In a perfect scenario, we would train a model that has both a high precision and recall. Precision is the measure to which a model correctly classifies people, whereas recall is a measure of coverage of the classification. In our test case here, a high precision would mean predicting a person should be arrested only if the model is super confident. On the other hand, a high recall would mean a model which predicts as many arrests as possible, so as not to miss any. We can see how there is a tradeoff between the two. Given the sensitivity of the task, we choose to optimize for precision in correctly classifying those who should be arrested. This would come at the expense of missing people, but we are OK with that.
This dataset is highly imbalanced, with only about 0.6% of cases being arrests. Furthermore, it does not include many features (26 overall, with 16 being either of no use or redundant). Another challenge is that once the data is processed, which will be described below, the number of columns significantly expands due to categorical features being transformed into dummy variables. This results in a very sparse dataframe.
The first thing to do is to clean up the features. The following table lists all the features in the raw dataset, whether the feature will be kept, and reason for doing so:

Let’s clean and prepare the data
For more details on the cleaning and processing of the data, please take a look at the source code, which can be found here.
Here we would mention only the violations column, which we found interesting. It includes lists of strings, each denoting the violations (allegedly) performed by the person. For model simplicity we only want to focus on one, and we want to make sure it is the one that is most severe. Overall there are 12 unique violations included in the dataset, ordered here with #1 being the worst (which is a subjective definition):
dui
speeding
stop sign/light
license
cell phone
paperwork
registration/plates
safe movement
seat belt
equipment
lights
truck
other
other (non-mapped)
We replace the violations column with an integer column representing the worst violation attributed to the person.
The result is a data frame with 82 columns, which is now ready to be analyzed using a machine learning algorithm.
Predictions
The classification model we will use will be Random Forest. Once we perform a pass on the dataset, we get the following results
Precision = 0.55
Recall = 0.13
However, it is upon a deeper look that we see an issue. Consider the distribution of the races of the people in the dataset:

We can see that after modelling, the distribution of the groups of people changes. In particular, the ratio of African Americans significantly increases among those who are predicted to be arrested, whereas the fraction of Caucasians decreases. As we mentioned, we need to define a feature to use in our VFAE as the sensitive feature. For simplicity we will choose just one, even though VFAEs do accept multiple, and we will use ‘driver_race_Black’, which is how it is recorded in the dataset, given the significant bias the model shows against this group of people. We can quantify this result by looking at a modified version of the discrimination parameter proposed by Zemel et al. (2013). The original is defined as follows

This parameter compares the probability our model predicts for arrest (a=1) of people in the protected class (s=1), with the corresponding probability for the non-protected class (s=0, i.e. everyone else). We want the probabilities to be as close to one another as possible, therefore we are looking for a discrimination parameter close to zero. However, there is a challenge with using this parameter in the case of a severely imbalanced dataset, which we find ourselves in. The issue is that the probability of arrests is very small, therefore the difference between two small numbers is likely to be small as well. To remedy this we consider instead the ratio between the two probabilities

We would ideally want discrimination_ratio to be as close to 1 as possible. We get the following result
Discrimination_ratio = 3.26
Clearly this is problematic, and this is where VFAE steps in.
Variational Fair Autoencoder time!
By transforming the dataset using a VFAE, which involves injecting information regarding the race of the drivers into the model, we obtain a dataset which contains most of the useful information, but removes the dependence on race. Let us see what happens when we retrain the model.
Precision = 0.54
Recall = 0.15
We see that we still obtain comparable performance as before. Looking at the discrimination parameters and diatribution of predicted arrests within the groups
Discrimination_ratio = 1.86

The plot on the left shows the previous results we discussed earlier, and the plot on the right shows the results after VFAE is applied.
We can see that the model becomes significantly less biased. Unfortunately, some information regarding race does leak through. There are several reasons for this. One could be a strong correlation between various features in the dataset, which makes it hard for the autoencoder to remove the dependence on the sensitive feature. For example, a bias might exist in determining whether people get stopped by a police officer, and whether they are searched for contraband, and whether they are ultimately arrested. Also, the limited feature set and imbalanced nature of the dataset reduces the signal present in the data to allow for extracting information.
These sorts of techniques are very exciting for integrate.ai because they allow us to train great models while making sure we are not accounting for information we neither need nor want to impact our predictions.
There are, of course, limitations to these models. For one, we need to explicitly specify what features we consider sensitive. This may not always be obvious. Also, given that we transform the dataset into a latent space, which is essentially abstract, we will have difficulty explaining the reasons for our model results. For example, in the above analysis although we can be confident that race plays a limited role in predicting whether a person should be arrested, we would have trouble saying what is an important factor. This is where techniques such as FairML, for example, can help.
Loving people means doing the best we can to ensure every person affected by our model is treated fairly. Doing so is a complicated task, especially when dealing with complex non-linear models and data that inherently contains bias, but one that we proudly undertake. Members of our team have published about this extensively (e.g. Tyler Schnoebelen’s post on ethics in data, Kathryn Hume’s discussion of bias in AI, and several Slack chats, such as this one and this one). Using VFAE is just one of the tools we will employ to achieve this goal. We are investing engineering effort to ensure our infrastructure is secure, are creating partnerships with the Vector Institute to explore technical solutions with leading researchers in the field, and are making deliberate choices about the projects we accept from clients so we can say with integrity that we are applying AI to make the world a better place.
Yevgeni Kissin is a Data Scientist at integrate.ai. Yevgeni has a PhD in Astrophysics from the University of Toronto. In his former life, he spent his time studying the stars, but now he’s all about machine learning. He loves talking about anything space and ethics.
",Technical Solutions to Reduce Algorithmic Bias: The Variational Fair Autoencoder,52,two-birds-with-one-stone-7ae25f6b4291,2018-09-05,2018-09-05 17:24:42,https://medium.com/s/story/two-birds-with-one-stone-7ae25f6b4291,False,1993,"TO based startup, building a future in which AI enriches people’s lives while creating better, more valuable businesses.",,,,the integrate.ai blog,,the-official-integrate-ai-blog,,,Ethics,ethics,Ethics,7787.0,integrate.ai,"TO based startup, building a future in which AI enriches people’s lives while creating better, more valuable businesses.",dbf4eb8c5945,integrate.ai,359.0,1.0,20181104
0,,0.0,,2017-03-16,2017-03-16 23:17:12,2018-05-18,2018-05-18 01:12:35,0,False,en,2018-05-18,2018-05-18 01:12:35,59,69aeaabf22c5,19.06415094339623,21,1,0,"(a design fiction, originally written April 2017, published May 2018)",5,"The First Church of Chatbot: or, HOWTO Pwn a Democracy using Common Ingredients Easily Found in Every Cloud PaaS
(a design fiction, originally written April 2017, published May 2018)
This tutorial shows you — a hacker —how to write God — a program.
But first, some context.
Our history, as hackers, is long.
In the 1960s, phone phreakers figured out they could whistle up free long distance. On that note in 1984, Emmanuel Goldstein started 2600: the Hacker Quarterly. In 1988, Robert Tappan Morris’s worm infected up to 10% of the Internet. In 2003, Bunnie Huang published Hacking the Xbox. Epic hacks and hackers all.
In 2013, Black Mirror screened The Waldo Moment. And in 2017, the first Twitter-native candidate trolled the mainstream media into free, 24/7 coverage and lolled all the way to the White House.
What an epic hack! Much lulz, very zomg.
In the 1980s and 1990s, the nascent Internet was tiny and content was sparse: IETF RFCs, the Usenet Oracle, Project Gutenberg. Still, samizdat lurked on shady Gopher and FTP sites for thrillseekers to find: the Anarchist Cookbook, the MIT Guide to Lockpicking. The advice in these texts was of questionable value, but their very existence (and outlaw tone) crowbarred open the consciousness of a whole generation of high-school cypherpunks who went on to read Neuromancer and Snow Crash, install Linux and PGP, and major in computer science. Some of them became authors and lit the torches of a new generation with books like Little Brother (Cory Doctorow) and Accelerando (Charlie Stross). The Internet was the LSD of Generation X.
In 1990, the Secret Service raided Steve Jackson Games and confiscated the role-playing game GURPS Cyberpunk, asserting that it was a “handbook for computer crime”.
It was, of course, no such thing.
This tutorial, however, is a handbook for what some might call cybercrime … for using emerging computer technologies to exploit well-known vulnerabilities in the human psyche and human social structures for … well, for profit — for power—for the lulz!
Except it’s all 100% legal. We’ll see why, later.
The intended audience for this guide is a hacker, in both the creative and destructive senses of the word. That word, “hacker”: it’s a big tent, like science fiction bigger on the inside than on the outside. Every hacker is two people in one body. We go to maker faires and we go to DEFCON. We start startups, we contribute to Github, we architect and build systems for others to use. We create, from first principles, elegant illustrations to the delight of our peers. If a thing can newly be done, we are the first to try to do it. It’s why the best founders are hackers: we are innovation entrepreneurs.
At the same time, our brains are constitutionally wired to seek truth to the extent of pedantry, to explore every exception to prove the rule. We think about systems — others’ and our own — in terms of weaknesses and potential exploits. We think about communications in terms of privacy and eavesdropping. We cover our webcams because we know how easily hardware can be pwned. All systems will be gamed. Our working vocabulary includes “zero-days”, “botnets”, and “buffer overflows”. Reflections on Trusting Trust permanently recalibrated our paranoia about computers and software systems.
This tutorial might permanently recalibrate your paranoia with regard to humans and society, if you aren’t already a cynical crypto-anarchist.
Finally, hackers pay it forward. We do our best then give it away. We spend countless hours answering questions, instructing newbies, writing documentation, turning science into technology and technology into apps for the benefit of everyone who doesn’t code. This tutorial is written in that spirit.
I will assume basic technology skills. You already know how to hack into a computer, how to install a rootkit, how to use one machine to probe a thousand more. You know how to boot up a botnet, set up command-and-control nodes on IRC, and launch a basic DDOS to take down a major Internet website. Basic skr1pt-kiddie type stuff, a lot of it is commoditized.
But we don’t just break into systems. We build systems. Perhaps the best example of doing both at once is the Carna botnet of 2012.
I will assume that you also know how to build a web or mobile app; how to run data analytics using Python or R; how to assemble useful tools from AWS’s chest of gadgets; how to use the machine learning kits published by Facebook and Google; how to learn, within a week, to use a new API and boot up a backend that scales to millions of interactions per second.
In short, you have mastered technology: both its dark and its light sides. The Force is strong with you.
What would it mean to apply that mastery to humans? To societies?
In this tutorial, your attack target is not a computer, not a network, but an entire democratic society. Let the game begin.
HOWTO Play “Build Your Own Cult Online”
“We are as gods, and might as well get good at it.”
Stewart Brand, Whole Earth Catalog
Your tools: machine learning, natural language processing, social media APIs, chatbot toolkits. After a long winter, A.I. is maturing. Twenty years ago these tools did not exist; neither did social media. You may assume unlimited CPU, unlimited disk, unlimited bandwidth. For a god, infrastructure omnipotence is table stakes.
Your strategy: build an army of chatbots to engage with humans, and a network of fake news sites to shape their beliefs. Think of it as automated social engineering at scale. (What’s social engineering? Anything from a “send-money” scam to swatting.)
Project task: recruit a cult of humans who will believe anything, do much anything you tell them. At least a dozen. They can be from anywhere around the world. Minimum age, 21. (Getting people younger than that to do what you tell them is (a) immoral, (b) pointless (you want their vote), and (c) way harder – ask any parent!)
Your financial goal: to get those humans to contribute funds to sustain the cult.
Your political goal: to get those humans to vote for a slate of political candidates of your choice in the next election. Your eventual aim is to capture the flag — to gain, by only lawful means, voting control of a host society, despite the best efforts of its legacy institutions to resist you. The host can be a corporation or a democracy.
Your social goal: to help your cult members to actually live good lives — better lives than they would have lived if they had not joined your cult.
Three goals, three rules:
The scalability rule: you, as founder of the cult, should not interact directly with your followers. Maybe in the early days it’s okay to do things that don’t scale, but in the long run, your will has to be expressed through programming the chatbots and through community scaling mechanisms like peer support forums and meetups.
The honesty rule: you have to tell each of your cult members, within two years of conversion, that you hacked their minds, and how you did it…
The level-up rule: …and you have to teach each of your cult members how to hack others in turn.
Is this possible? Yes: we have a mathematician’s word for it. In 1946, Gödel famously identified a weakness in the US Constitution that allowed conversion into a dictatorship. If the mathematicians say it can be done, then implementation must be a simple matter of programming, a job for engineers. In this case, social engineers.
A democracy is made of voters in the same way a network is made of individual computers. Whoever pwns the nodes, pwns the network. Once you pwn the network, you have two kinds of control: direct control over the nodes who are running your software, and indirect control even over the nodes who aren’t, because you control the state that taxes those nodes.
So: how to pwn the nodes?
How To Pwn A Human Being
The weaknesses of your target system are well known. Enormous lists of cognitive biases have been compiled, with the best of intentions: to educate the reader toward greater rationality and overcome those biases.
Isn’t that sweet?
Put on your hacker hat. Treat them as lists of vulnerabilities. Every one of those cognitive biases is a potential exploit against the human psyche. Every uneducated human is an unpatched computer waiting to be pwned. Their OS is to blame: humans are Predictably Irrational.
The biggest, easiest exploits, the lowest-hanging fruit, the buffer overflows of the human mind, have to do with confirmation bias and tribalism.
Confirmation bias lies at the root of sales techniques like “Foot-In-The-Door”. Whole libraries of persuasion mechanisms have been documented. Books like Cialdini’s Influence and Jamie Whyte’s Crimes Against Logic (subtitled Exposing the Bogus Arguments of Politicians, Priests, Journalists, and Other Serial Offenders) can be read as how-to guides; politicians use bogus arguments not because they’re evil but because they work. These tricks are not new. Religions use them. Turn the tricks into software and you can weaponize the resulting tech stack.
(Recent applications at the top of the stack include Neil Strauss’s The Game. That book basically teaches lonely men remedial social skills with a specific goal: to get their biology off their backs by demonstrating a success condition for sexual selection at least once. But technologists, once they have unlocked that achievement, will naturally ask: “how does this scale? How can I automate it?” Unfortunately, romantic seduction is incompatible with cloud containerization. You can’t Docker your dick.)
This is your Brain on Yeast.
Think of a blob of dough. Wild yeasts and bacteria, floating in the air, land on the dough. Some die. Some flourish. Soon the dough is colonized by microorganisms happily fermenting and reproducing. If you make a fresh dough and smoosh it together with the old one, the bugs will cross over and soon you’ll have two loaves of bread.
Now think of that dough as a human mind. Yeasts are ideas. And you’re going to build a bomb that sprays a very specific strain of yeast across the Internet. Your yeast bomb, in the form of an army of chatbots and a network of fake news sites, is going to fool fresh dough into thinking that the other doughs in the kitchen are already happily colonized. In reality, the other doughs are collaborators, in on the scam, like shills in a shell game. The bots talk to each other in a largely scripted performance. And the bots talk to your target, using APIs and machine learning to improve. Don’t worry if your bots start out failing the Turing test; they’ll learn what not to do, and they’ll improve on the next target. It’s basic wardialing strategy.
Soon your target will start believing in an alternate “consensus” reality. You can flood them with fake news that represents your preferred perspective. Imagine a chatbot that can gin up an entire fake news story, an entire fake news website, just-in-time, on demand, whenever a link to “evidence” would come in handy during a discussion. Think disinformation and propaganda, but customized for an audience of one. They’re a brain in a vat, with their phone as the vat.
For verisimilitude, you may fake both sides of an astroturf army, running a mock debate which (eventually) your side wins. When you run these debates you will have ample opportunity to inculcate cognitive biases and conversational games like “someone who’s been wrong in the past can’t possibly be right in the future”, “we agree because we like you”, “that’s sacred—don’t ask that question” and “you’ll understand when you’re older, but trust us for now.” Persuasion techniques are well documented; there is no need to enumerate them all here.
Enabling Technology 1: Chatbots.
Jeff Hammerbacher once snarked, “The best minds of my generation are thinking about how to make people click ads”. A generation of tech startups are using tools and building tools for promotion, virality, community management. Startup founders ask themselves “what would Jesus do?” not for moral guidance but in hopes of repeating his accomplishment: achieving a monthly-active-user count in the billions. Political campaigns microtarget voters using the techniques of social media marketing. Decades of work has gone into software to make people click, click, click; buy, buy, buy; vote, vote, vote.
Today, chatbots are the new hotness. People aren’t born to click on buttons. People are born to talk. When they talk to other people, it’s called conversation. When they talk to God, it’s called prayer. When they talk to bots, it’s a Turing test.
Chatbots on social media, fueled by machine learning, training in realtime against a participant pool of billions, are racing to pass the world’s biggest distributed Turing test. And the stakes couldn’t be higher: whoever wins that race gets to play God.
Spam used to be mass-mailed, impersonal, with hit rates in the hundreds of a percent. Then came phishing, then spear-phishing. What’s next? Personalized bots, talking to you in realtime, on IM and Twitter, even IRC. The cost of a fully automated chatbot running a long con will go to zero. The science fiction of “Her” will be the reality of tomorrow.
Thanks to machine learning, you don’t have to do all the programming yourself. You don’t have to get it right every time. If your bots fail at dialoging with a given human, just discard them. (The human, not the bot.) Move on to the next one. It’s been almost a quarter-century since Eternal September: now there are so many people on social media, the top of your conversion funnel is practically infinite. As the saying goes, there’s another born every minute.
Enabling Technology 2: Fake News
What is the nature of knowledge, of truth? Oh boy. Big can of worms. Philosophy 101, TL;DR.
Any scientist will tell you literature review takes time, primary research takes time. As a human being in the 21st century, most of what you know you have to take on faith, like a crate of books purchased because the authors are famous. In theory, your critical faculties get to work on those books, testing assertions and evaluating chains of logic, looking for inconsistencies and baroque violations of Occam’s Razor.
In practice … people believe what they’re told, if you tell ’em enough.
Look at fake news. It works. Consider it the weaponization of social psychology: of Solomon Asch’s classic line-length experiment, of Stanley Milgram’s obedience experiments.
Indeed, the difference between “fake news” and “other people’s holy writ” is arguably only a matter of degree. If it happened a long time ago, it’s not fake news; it’s canon.
If your doughy brain is infected with the wrong set of yeast spores, if you start out reading the wrong crate of books — or no books at all — and if your critical faculties are dormant, or, worse, subverted toward tribalist confirmation, then you too could, one day, find yourself shooting the locks off storerooms in a suburban pizzeria. Or blowing up Baghdad looking for weapons of mass destruction, not finding what you expected, and then remembering you did.
Your goal in this tutorial is to pwn those doughy minds. Publishing platforms like Wordpress and NLG AI systems like Quill make it incredibly easy to churn out huge volumes of plausible nonsense in a super-professional-looking format. As long as you infect your targets early enough, and surround them with enough of an echo chamber, you can control what they believe.
How To Pwn A Human Society; or, Rootkitting Tribalism
First you hack humans separately, then you hack humans together.
You can’t scale seduction, but you can scale belonging and togetherness. Humans are social animals. They naturally want to belong to superorganisms. From sports teams to crazy cults to political parties to Emacs-vs-vim, people seek out tribes they can call their own. Putnam’s Bowling Alone argued that the supply of community isn’t keeping up with demand, and that was before smartphones. After smartphones, we’re all alone together.
People spend more social time on screens than in person: you get anomie. Filter bubbles and echo chambers magnify existing biases: you get tribalism.
Books like The Righteous Mind can be read as how-to guides for social sploits: humans possess a “hive switch”. Flip the switch, and they can be persuaded to act against their self-interest to conform with the larger goals of the group.
Your chatbots will be the little devil on their shoulder, whispering half-truths to them; your bots will be their confidant, their advisor, their gaslighter, their guru. Their entire circle of friends. Almost a Black Mirror episode.
Your bots will teach them to deploy confirmation bias against objective reality, against mainstream media, against challenging interlocutors. Remember, baseline humans are easily fooled. They have no training in logic, in critical thinking, in cognitive science. They buy tabloids, they forward chain letter emails, they believe conspiracy theories. They are credulous and they are lonely.
Under those conditions, “us or them” thinking triggers easily. Good news about “us”, bad news about “them”? Automatically think of reasons it might be true. Bad news about “us”, good news about “them”? Automatically think of reasons it might be false.
Test case: get your converts to demonstrate a disbelief in Wikipedia and Snopes, as confabulations of the “other side”. But get them to disbelieve in Fox News, too.
Religious Hierarchy
A word of advice for beginner messiahs. As you grow your userbase, keep an eye on the firebrands. They are your greatest asset. They form the training set for your bots. As current affairs come up, as new kinds of recruits enter the fold, you’ll want to detect the humans who take naturally to your teachings and argue accordingly. Introduce them to other humans, watch them engage and persuade, and use those conversations to train your chatbots further. These are your disciples, your apostles.
You may reuse the organizational hierarchy of any existing religion, sports team, or political party. The religion template works well:
a god who’s offstage and never speaks directly to anyone except
a prophet who is the sole authorized spokesperson, but who operates in the oral mode and is himself interpreted by
a core of disciples who productize and produce written
teachings, which are then taken to market by
a clergy responsible for scaling out regional expansion, sales, marketing, professional services, and community management.
In your case, you as core developer and BDFL assume the prophet role. You are encouraged to abdicate the role or fake your own death as soon as possible, before the unhinged assassins turn up: “live fast, die young, and leave a good-looking corpse!” Basically pull a Satoshi Nakamoto, but keep the crypto keys in case things go off the rails and you need to hard-fork a Second Coming.
Your opensource dev team will be your disciples. Your clergy will be bots. And the teachings are fictions loosely customized to each user’s psychological state and life situation. You can start producing the teachings by hand, but advances in A.I. should soon allow automated generation and optimization. The technical term for this is mythopoesis: read Joseph Campbell and Neil Gaiman.
Competition
Of course, you won’t be doing this in a vacuum. You’ll have competition. Other e-cults, running the same software as your bots. Oh, and legacy religions too.
It’s the rootkit problem. When you break into a machine, the first thing you have to do, paradoxically, is secure it: close the holes that you used to get in, because you need to keep out your esteemed colleagues, the hackers next door.
When you pwn a human, the first thing you have to do, the most basic precautionary element of your rootkit, is to install a bias module that immunizes them against similar exploits. Easy: us=good, them=bad.
Are legacy religions a competitive threat? Existing tribal affiliations are old and powerful: religions and political parties go back hundreds, if not thousands, of years. But they rely on human instruction, and that’s their weakness. They’re not getting together in person as much as they used to. Now that social media and AI offer a toehold for your bots to pretend to be human, you have the digital firepower to compete with an entire legacy analog belief system. Read Clayton Christensen on disruptive innovation.
Can it be done? Yes, there are existence proofs, from well before computers. Cults have always seemingly come out of nowhere, led by charismatic prophets, often to their own deaths by cometary aliens, by Kool-Aid, by Y2K.
All we’re doing is automating cult recruitment and coding computational charisma into chatbots. Brought to you by AWS, Deepmind, Instagram, and Twitter.
Gosh, Are We Doomed To Live In A Dystopian Theocracy?
It seems bleak, doesn’t it? Makes you long for the good old days, before smartphones reported on your location, before webcams spied on sleeping children, before governments demanded your social media passwords at the border.
As a hacker, you’re an educated person. Self-taught, maybe, but relentlessly curious, rational, self-questioning, self-improving. You are the embodiment of Science. You started life as a baseline human, ignorant, gullible, irrational. Just look how far you’ve come.
As a well-meaning hacker, you might have tried, now and again, to educate baseline humans. How did that work out? Sadly, it’s easier to exploit someone than educate them. Happily, exploitation is the goal of this tutorial; you will worry about education later.
So the cognitive bias is the cognitive vulnerability. Motivated reasoning, confirmation bias, peer pressure, groupthink, and out-of-bounds, off-limits questions: these are now your friends. They will help your software pwn humans and groups.
Isn’t This Totally Illegal?
Shouldn’t people think for themselves? Isn’t it unfair to exploit their weaknesses? If it’s wrong to break into a computer, isn’t it just as wrong to break into a mind?
Actually, no. There are laws against breaking into computers. But when it comes to breaking into other people’s minds, the law is, in fact, firmly on your side. It’s called freedom of speech and freedom of religion: two fundamental human rights that America was founded on. If somebody tries to stop your army of chatbots from convincing humans that you are the new Messiah, well, they’d be violating your rights, and the rights of all your converts too. Ironically, the first people to leap to your defense might be, oh, the lawyers of the Catholic Church.
OK, What I Meant Was, Isn’t This Totally Immoral?
On the surface, yes. But bear with me. It’s going to be OK. Let me explain.
Moral systems are generated by the dynamics of intra-group and inter-group competition. You know the quote “the arc of the moral universe is long, but it bends toward justice.” In his books the Evolution of God and Nonzero Robert Wright details the machinery at work, even gives it an air of inevitability.
What happens if this e-religion technology takes off? Does the first-mover advantage of social networks imply that a single cult will win? Or will a thousand flowers bloom, a hundred schools of thought contend?
The history of social networks — both online and offline—suggest that network communities follow a power-law distribution. Number One may be as large as Numbers Two, Three, Four, and all the others put together. It really depends on two things: which strain of yeast infects the dough first? And when you connect the doughs, how do the strains compete?
Let’s say a thousand hackers read this guide and a hundred cults are launched. Some will gain adherents and grow. Others will wither. What makes the difference? Anyone who’s spent time in the crucible of a high tech Lean startup will know about design, product/market fit, conversion funnel, and most importantly viral adoption. All of those ideas apply to e-religions. Pick a good font.
But long-term dynamics are determined by co-evolution. Just as mankind has domesticated corn, corn has domesticated mankind. Religions obey the same dynamics: if an adaptation makes individuals more prosocial, it succeeds. If an adaptation helps groups increase their individual headcount, it succeeds, whether through recruitment (“have you heard of the Gita?”) or through reproductive policy (“go forth and multiply”). This happens in secular societies, too, by the way: immigration policy, parental leave, and baby subsidies.
The same moral dynamics will apply to e-religions. And I predict, or at least I would like to think, that hacker morals will win.
Hacker morals? Not the black-hat morals: not the swatting, trolling, doxxing, cracking kind, though, as I said, it’s a big tent and Loki’s already inside. I mean the white-hat morals, that (deep down) tend to align with academia over industry, learning over lucre, the kind that gave us Linux and Wikipedia and archive.org.
Today’s captains of industry started out as coders. Before Google, Eric Schmidt (re-)wrote Lex. Before Netflix, Reed Hastings wrote Purify. Before A16Z, Marc Andreessen wrote Netscape. Before she was a Rear Admiral of the U.S. Navy, Grace Hopper wrote the first ever compiler.
Is it so hard to imagine that the religious leaders of tomorrow will have tech backgrounds?
I think it would be pretty cool to join a church whose Holy Book was an opensource Bible, extensively footnoted, in the form of a wiki, that anyone can edit.
By temperament, hackers align with opensource. Where commercial software draws a hard line between “customer” and “vendor”, opensource projects offer a progression from “novice” to “script writer” to “core developer”. Interestingly, that pathway is a feature of many religions: if you’re in the right place at the right time, you could level up to become a saint. In Zen, everybody can become a Buddha.
That’s the sort of empowerment, the sort of democratization, that the Internet has always promised. With Wordpress and Medium, everyone can have their own newspaper. With YouTube, everyone can have their own TV channel. With blockchain, everyone can have their own currency. With e-religion, everyone can be the leader of their own religion. And that doesn’t have to be a bad thing; remember Stranger in a Strange Land? “Thou art God.”
The Argument for Opensource Dynamics in Online Religion
Let’s examine the chain of causation, and recap the argument so far.
The emerging technologies of chatbots, AI, and social media make it possible for a bot to plausibly pass the Turing Test, at least for the purposes of infecting credulous baseline humans with a meme-structure resistant to further infection and susceptible to remote command-and-control.
These technologies come together in an e-cult stack suitable for automated deployment. Internal machine learning, plus Darwinian selection in the wild among forks and versions, will lead to a Cambrian explosion of echo chambers: tribal Balkanization and rampant sectarianism.
These sects will compete for followers—there will be a battle for souls. Assuming we exclude gunpoint conversion, in the short term, ease of onboarding, virality, and immune resistance to new memes will be important factors. In the long term, competition will be determined by prosocial positive-sum benefits: how well does my sect help me help others? How well does it help others help me? (Crudely, what has it done for my eudaimonia lately?)
But also: how effectively does my sect help participants improve the sect itself? (Secular sects appear in Neal Stephenson’s Snow Crash and Diamond Age.)
Allowing participants to improve the sect is where the proprietary/opensource dichotomy will come into play. Those that best leverage community participation to improve the sect itself will tap into a strong source of competitive advantage.
That means that even if a sect was designed by hackers to exploit baseline humans, Darwinian dynamics weigh in favor of sects which choose to eventually educate new users toward an understanding of the mechanisms that got them onboarded. If you tell a sucker you hooked ’em, they’ll want to know how. Everyone who’s ever been scammed, and then gone and learned everything there is to know about that scam, understands this motivation. Once somebody discovers cognitive biases, they want to learn more. And the Hacker notion of fairness requires the existence of a pathway from novice to adept: the source must be available for free. In e-religion, the source code is the very software that exploits baseline humans; it is inseparable from a rational understanding of the human psyche.
And that is the key to liberation: the sect will win that is best at acquiring users, empowering users, educating users. The onboarding may begin with cognitive exploits, evangelical persuasion, useful lies, skillful means: but eventually the inexorable logic of competition will require that the sect teach the novice to put aside childish things, and level up to understand how he was hacked, so that he can become a hacker himself and become not just an end-user of the sect, but an active core developer.
If you want people to gain immunity to groupthink, fake news, and confirmation bias, this is the long, strange road that might be needed to get there.
Objections.
Linux on the Desktop remains a dream. Opensource software is massively successful behind the scenes (in the cloud and in smartphone OSes), but end-users are, by and large, indifferent to that success; there is no mainstream path from someone buying an Android phone to becoming an Android app developer to becoming a Dalvik OS developer to becoming a Linux kernel developer. By that logic, commercial adaptations of opensource sects may achieve marketshare dominance, and the level-up pathway may peter out.
See Also
The Righteous Mind to understand human morality and its weaknesses.
Influence to understand how to persuade people.
Crimes Against Logic on how to convince people using bogus arguments.
Movies: The Matrix; Lucy; Her; Transcendence.
Books: Hannu Rajaniemi’s Jean le Flambeur series, Neal Stephenson’s Diamond Age, John C. Wright’s Golden Age.
http://dl.acm.org/citation.cfm?doid=2639189.2641212
UW professor: The information war is real, and we're losing it
A University of Washington professor started studying social networks to help people respond to disasters. But she got…www.seattletimes.com
","The First Church of Chatbot: or, HOWTO Pwn a Democracy using Common Ingredients Easily Found in…",68,the-first-church-of-chatbot-or-howto-pwn-a-democracy-using-common-ingredients-easily-found-in-69aeaabf22c5,2018-06-21,2018-06-21 03:59:43,https://medium.com/s/story/the-first-church-of-chatbot-or-howto-pwn-a-democracy-using-common-ingredients-easily-found-in-69aeaabf22c5,False,5052,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Meng Weng Wong,"Berkman Fellow 2016. Stanford CodeX Fellow 2017. Leading Legalese.com. Previously jfdi.asia, pobox.com, SPF, hackerspace.sg. Made in Singapore.",2e1bc01a4561,mengwong,1569.0,288.0,20181104
0,,0.0,b468e053644a,2018-06-03,2018-06-03 23:25:37,2018-06-03,2018-06-03 23:26:57,0,False,en,2018-06-03,2018-06-03 23:26:57,5,e0748ef65caa,0.3924528301886793,2,0,0,"In this episode, David talks to Gisele Waters, Ph.D. about ethics, IEEE P7000, and the “DiNozzo” factor.",4,"Episode 19 — Ai Ethics with Gisele Waters, Ph.D.
In this episode, David talks to Gisele Waters, Ph.D. about ethics, IEEE P7000, and the “DiNozzo” factor.
“The P7000 is what the IEEE organization wants to develop as the model process for addressing ethical concerns during the system design of information systems and autonomous systems. It’s supposed to provide engineers and technologists with an implementable process.” — Gisele Waters, Ph.D.
Links:
IEEE 7000 — Model Process for Addressing Ethical Concerns During System Design
The Ethical Engineer by Robert McGinn
ResponsibileRobotics.org
The Hard Problem of AI Ethics — Three Guidelines for Building Morality Into Machines
Episode
","Episode 19 — Ai Ethics with Gisele Waters, Ph.D.",2,episode-19-ai-ethics-with-gisele-waters-ph-d-e0748ef65caa,2018-06-05,2018-06-05 21:44:13,https://medium.com/s/story/episode-19-ai-ethics-with-gisele-waters-ph-d-e0748ef65caa,False,104,a double entendre where point can be interpreted both as the moment in time of or the meaning to struggle — our focus is on the nexus of user experience and artificial intelligence,,,,the point of struggle,gonzo@ziff.io,the-point-of-struggle,"UX,AI,CUSTOMER SUCCESS,PRODUCT DESIGN,DESIGN THINKING",pointofstruggle,Ethics,ethics,Ethics,7787.0,"David ""Gonzo"" Gonzalez","Data Scientist, Storyteller, LEGO Coach",573cab224fc,datagonzo,240.0,4.0,20181104
0,,0.0,97aa03c8cebb,2018-02-20,2018-02-20 18:54:01,2018-02-23,2018-02-23 01:15:00,1,False,en,2018-03-21,2018-03-21 00:54:51,26,2ca23de6aebd,7.633962264150942,1,0,0,"In the initial feedback I’ve received on my attempt to formally state the AI alignment problem, the primary objection I’ve heard is that I…",5,"AI Alignment and Phenomenal Consciousness
In the initial feedback I’ve received on my attempt to formally state the AI alignment problem, the primary objection I’ve heard is that I assume any AI worth aligning will experience qualia. In particular, some think we have to worry about aligning AI that is cybernetic but not phenomenally conscious. I think they are mistaken in thinking cybernetic-only AGI is possible, but in all fairness this is probably due to a failure on my part to adequately explain phenomenal consciousness, so I’ll try here to make clearer the case for phenomenally conscious AGI and how this relates to AI alignment.
To begin, though, let’s step back and consider what we mean by “artificial intelligence”? The artificial part is easy: we mean something constructed through a deliberate design effort rather than arising naturally, i.e. it is the handiwork of conscious things rather than the outcome of an unconscious process. Thus artificial intelligence stands in contrast to natural intelligence, like the intelligence of animals that arose through evolution. The intelligence part is harder because by intelligence we mean multiple things: possession of goal directed behavior (telos), ability to model the world (ontology), and an ability to combine telos and ontology to come up with new solutions to problems (find undiscovered algorithms). Generally we might say intelligence is something like an ability to systematically increase local informational complexity — optimize the nearby parts of the world — by increasing global entropy. Taken together then, artificial intelligence and artificially intelligent agents can be said to be designed optimization processes.
This means, of course, that things as simple as steam engines are a kind of artificial intelligence even if they aren’t especially intelligent since a steam engine is increasing global entropy in the form of waste heat in order to produce mechanical power. And even if we put a governor on our steam engine it is still only cybernetic — a thing that experiences itself — and not phenomenally conscious — a thing that experiences itself experiencing itself — but, as we’ll see, it doesn’t take a lot to make our steam engine jump into the realm of phenomenal consciousness.
The steam engine with a governor, as I’ve previously explained, is a cybernetic thing that produces at least one bit of information about whether the throttle is open or closed. Although it would be needlessly complex, suppose we added a governor onto the governor to regulate how quickly the governor adjusts the throttle. This governor governor has a simple operation: if the throttle was open within the last second, it doesn’t allow the throttle to close and vice versa. In doing so it creates a kind of memory for the steam engine about the state of the throttle, generates ontology by interpreting and representing the state of the throttle within the governor governor, and experiences itself experiencing itself through the governor governor experiencing the governor experiencing the steam engine. These features all imply this modified steam engine is phenomenally conscious.
To consider an example closer to the edge of our capabilities in artificial intelligence, creating phenomenally consciousness with machine learning is also trivial. A simple machine learning algorithm is a cybernetic process that iterates over data to produce a non-cybernetic model (the model is not cybernetic because it is essentially a lookup table or function that does not experience itself). More complex machine learning algorithms can produce cybernetic models with memory and, depending on the implementation of the algorithm, this can make the algorithm phenomenally conscious. If a machine learning algorithm generates cybernetic models that generate cybernetic models or self improves, then it’s solidly in the realm of phenomenal consciousness. After all, the minimum requirement for phenomenal consciousness is little more than a loop nested inside another loop!
But just because phenomenal consciousness is easy to place in a system and is in use in today’s leading edge of AI development, we don’t necessarily want to create phenomenally conscious AI. Eliezer Yudkowsky, for example, has argued strongly against creating conscious AI for ethical and technical reasons, and although he was referring to a naive sort of consciousness rather than phenomenal consciousness, it would be safer to make AI that are less capable rather than more, so we only want to create phenomenally conscious AI if it’s necessary to our ends. Unfortunately, I think if we want to create AGI — artificial general intelligence — it will necessarily be phenomenally conscious.
Let’s step back again and consider what we mean by the “general” in AGI. “General” stands in opposition to “narrow” in AI where narrow AI are designed optimization processes that work only in one or a few domains, like chess or language translation. Artificial general intelligence, on the other had, is expected to work in arbitrarily many domains, including domains the AI has not been trained on, because it could presumably train itself the way humans can when it encounters novel scenarios or otherwise adapt to new situations. It might fail at first, but it will learn and grow in capability as it addresses a broader space of experiences. It is only such general AI that I posit must be phenomenally conscious, especially since we know by their existence that cybernetic-only narrow AI is possible.
Suppose we could create a cybernetic-only AGI. Such a thing, being not phenomenally conscious, would necessarily have no ontology or ability to model the world, so it would be a kind of philosophical zombie that behaves like a phenomenally conscious thing but is not. P-zombies are possible, but they are not cheap, with a p-zombie requiring exponentially more computational resources than a behaviorally equivalent phenomenally conscious thing in terms of the number of cases it would be expected to handle (Update 2018–03–20: I try to prove this formally). That is, a behaviorally equivalent p-zombie needs a separate cybernetic system to handle every situation it might find itself in because it can’t model the world and must have its “ontology” hard coded. So, if we could create a cybernetic-only AGI, how big would it be, both in terms of cybernetic subsystems and volume? My Fermi estimate:
There are on the order of 10,000 unique words needed to fully express ideas in any given human language.
A sentence in a human language has on the order of 10 words.
Assuming every sentence of 10 words in any language describes a unique scenario, a human-level AGI must handle at least 10,000¹⁰=10⁴⁰ scenarios.
Since 3 levels of recursion are enough for anyone, let’s conservatively suppose this means scenarios interact to create at least (10⁴⁰)³=10¹²⁰ situations an AGI must deal with, requiring 10¹²⁰ cybernetic subsystems.
AlphaZero can train to handle a new situation, like a new game, in the order of 1 hour, but to be conservative and assume future improvements let’s assume our AGI can train on a scenario in 1 minute.
That means our AGI needs to perform 10¹²⁰ minutes, or ~1.9×10¹¹⁴ years, worth of training to build models to handle all the scenarios it needs to be general.
Supposing we are willing to take on the order of 10 years to build our AGI and we can parallelize the training over that period, building an AGI would require 1.9×10¹¹⁴/10=1.9×10¹¹³ computers each on the scale of AlphaZero.
It’s unclear what AlphaZero’s computational needs are, but AlphaGo Zero apparently runs on only a single server with 4 TPUs. Let’s conservatively assume this means we need 1U of rack space or ~15,000cm³ or ~0.015m³ to train a model to handle each scenario.
So to get an AGI you need 1.9×10¹¹³×0.015m³=1.85×10¹¹¹m³ of compute, not leaving space for cooling, power, etc.
The Earth has a volume of ~1.1×10²¹m³, so our AGI would require 1.68×10⁹⁰ Earths worth of computers.
I’m sure we could make these calculations more accurate, but that’s not the point of a Fermi estimate; the point is to show the scale of building an AGI as capable of a human that is also a p-zombie, even if my calculations are wrong by several orders of magnitude. If we tweaked the numbers to be maximally favorable to building p-zombies, taking into account improvements in technology and the problem being easier than I think it is, we would still end up with needing more than an entire Earth’s worth of computers to do it. Building a human-level p-zombie AGI would be asking for a planet-sized brain, and we know how that would turn out.

Jokes aside, I don’t go through this exercise to poke fun at the idea we could build an AGI that is not phenomenally conscious. My objective is to stress that any practical AGI project will necessarily be looking at building something phenomenally conscious because it’s the only way to fit the amount of complexity needed into a reasonable amount of resources. I don’t think people working on AI capability are confused about this: they know that giving systems what I call phenomenal consciousness allows them to do more work with less resources, and doing this seems to be the direction they will naturally go, even with narrow AI, as cybernetic-only solutions become prohibitively expensive to improve. But if AI safety researchers were hoping for cybernetic-only AGI, it alas seems it will definitely be phenomenally conscious.
That said, seed AI might allow us to avoid phenomenally conscious AGI for a while. The idea of seed AI is to first create a simple AI system that will bootstrap itself into a more powerful one by improving itself or designing its successor. Maybe we could design seed AI that is cybernetic-only and let the seed AI take over the responsibility of designing phenomenally conscious AGI. In such a scenario, might we need to consider the alignment of cybernetic-only AI?
In short, no. We would be interested in designing a seed AI such that it used the results of alignment research so that any more powerful, phenomenally conscious AGI it created would be aligned, but the seed AI itself would not need alignment because, to make a broader point, there is a sense in which something that is not phenomenally conscious cannot be aligned because it does not value anything because it doesn’t know what anything is. To put it another way, we can’t build tools — things we use for a particular purpose that are phenomenally unconscious — that cannot be misused because they lack the complexity necessary to even notice they are being misused, let alone do something to avoid that.
To give an example, a crowbar lacks the complexity to know if it’s being used to open a crate or break into a building, much less know if opening the crate or breaking into the building is “good” or “bad”. Yes, we could make fancy, cybernetic crowbars with tiny computers that noticed what they were being used for and would stop working when they detected a “bad” scenario, but being cybernetic-only it would not be general and only able to handle situations it was trained to recognize. Use the crowbar in a novel situation and it may promptly become “unaligned” because it was never “aligned” in the first place: it just did what it was designed to do, even if that design included complex, narrow AI that worked in a lot of cases. If you want to build aligned things, AGI or otherwise, you have to make them phenomenally conscious because that’s the only way the thing can possibly share the operator’s values in general.
I hope this makes clear why I think AGI will be phenomenally conscious and why AI alignment is a problem about phenomenally conscious agents. I invite further feedback on developing these ideas, so please comment or reach out with your thoughts, especially if you disagree.
",AI Alignment and Phenomenal Consciousness,2,ai-alignment-and-phenomenal-consciousness-2ca23de6aebd,2018-06-19,2018-06-19 21:52:04,https://mapandterritory.org/ai-alignment-and-phenomenal-consciousness-2ca23de6aebd,False,1970,Making sense of reality,mapandterritory.org,mapandterritory,,Map and Territory,gworley3+mat@gmail.com,map-and-territory,"PHILOSOPHY,PSYCHOLOGY,RATIONALITY,EFFECTIVE ALTRUISM",mapterritory,Ethics,ethics,Ethics,7787.0,G Gordon Worley III,"Phenomenological philosopher, mathematician, and programmer",c050c3518961,gworley3,254.0,309.0,20181104
0,,0.0,f5af2b715248,2018-04-11,2018-04-11 16:03:00,2018-04-12,2018-04-12 14:17:48,1,False,en,2018-04-12,2018-04-12 22:13:53,16,b3daa2bd38f9,6.79622641509434,14,0,0,"In 2009, before I defended my PhD thesis in front of a jury specialized in cancer biology, we published a few articles on longevity…",5,"Do you (really) want to live forever?

In 2009, before I defended my PhD thesis in front of a jury specialized in cancer biology, we published a few articles on longevity meditated by autophagy (preventing cellular -and whole organism- aging by triggering cellular self-eating). We claimed that autophagy has a broad positive impact on organismal aging.
Almost a decade later -and for many more years to come- , many researchers and philosophers are still trying to figure out the way to address single wish of billions of human beings: Living longer -or even- living forever. Can we live longer? As we’re a species programmed to be constantly dissatisfied, how long is good enough? Well then, can we live forever? If yes, then the question that we should also ask is would we accept to be immortal? Under which circumstances?
Dr Ian Pearson, a futurist at Futurizon, claims people born after 1970 should be able to live forever with the help of genetic engineering & artificial intelligence. He thinks that by the year 2050, humans could outlive the constraints of the physical body.
How can genetic engineering help with human longevity/immortality? (quick answer : Rejuvenation technology)
Human longevity is a complex phenotype with modest heritability. The identification of the involved genes and variants still remains a challenge. A new large-scale international study identified 25 genetic markers now known to be associated with exceptional longevity, that could one day be targeted to help prolong human life. Genes involved in senescence, the “frozen” state that cells enter into after being damaged, play an important role. Genes related to inflammation and auto-immunity are also prominent. The results confirm that many genetic variants combine to influence human lifespan: no single gene variant has been found to be responsible.
Human Longevity, Inc. scientists have published a significant breakthrough in the journal Nature Genetics indicating the role of the non-coding genome — which accounts for 98% of the overall whole genome — that could be essential for survival and health, exposing new frontiers for genomic exploration.
Telomeres are protective caps on the end of chromosomes that confer genomic stability. In 2015, a Danish study concluded that telomere length robustly predicts longevity, even after factoring out the effect of age, smoking, exercise, blood cholesterol, BMI, and alcohol consumption.
Biomedical gerontologists such as Aubrey de Grey argue that growing old is a disease that we can circumvent by having our cells replaced or repaired at regular intervals. You may not want to live forever when you are 95 years old, full of chronic diseases and terribly deteriorated quality of life, but if you make your body (and mind) feel like 20–30, you may want to do that. We may extend life by using biotechnologies and medicine to keep renewing the body, and rejuvenating it which could be done in several ways such as genetic re-engineering, prevention (or reversion) the ageing of cells, or replacement of vital body organs with new parts using the 3D organ printing technology.
This deep makeover would ‘turn back the clock’ on your body, leaving you physiologically younger than your actual age. But this still leaves you vulnerable to nasty diseases and fatal accidents! How can we ensure that our “self” is preserved, when our bodies are no longer renewable and rejuvenable? The answer may lie in exponential technologies, such as AI.
How can Artificial Intelligence help with human longevity/immortality? (quick answer: Mind uploading)
According to techno-futurists, exponential technologies — AI in particular- will radically transform humanity via two revolutions:
The singularity is the point in time when all the advances in technology, particularly in AI, will lead to machines that are smarter than human beings. Ray Kurzweil predicts that the dawn of super-intelligent machines will happen by 2047, and the process towards this singularity has already begun and we can use this technology to expand who we are.
Virtual immortality: Experts claim that long time before we get to fix our bodies and rejuvenate it when necessary, we’ll be able to link our minds to the machine world in which your brain is digitally scanned and copied onto a computer. We’d effectively be living in the cloud, and our mental selves would live on beyond the demise of our fleshy, physical bodies.
Whenever your “organic hardware” is hit by a disease or an accident, it wouldn’t matter because your mind will exist in the cloud like a software & can be uploaded to a brand new hardware. Ian Pearson asked another relevant question: if our minds are online, do we even need robot bodies? We could all just live in a computer simulation quite happily. You could spend most of your time online in the virtual world, of course anywhere in the world on any computer. You could link your mind to millions of other minds, and have unlimited intelligence, and be in multiple places at once.”
Ethics of immortality
Predictions on the timelines and costs are very similar to what we observe in all exponential technologies. We’ll probably have to wait until around 2045–2050 before we’ll be able to create these strong brain-to-machine links, but the cost will be very high initially and then the price will gradually come down. We can be pretty certain that rejuvenation and mind uploading would widen the gap between the rich and poor, and would eventually force us to make decisive calls about resource use, whether to limit the rate of growth of the population, and so forth.
One other interesting point to raise is that the techno-futurists do not even question whether their vision can be actualized; they only debate when will it occur.
Despite the discussions on “When is this revolution going to happen?” we also need to challenge the philosophical foundation of the claims. Unfolding the definitions and complete understanding of consciousness and mind are going to determine the feasibility of the AI singularity. Can mind be decoded on silicon chips with the support of neuroscience only? Will our consciousness in the cloud be able to “experience the world around us” as the way we perceive it using our good-old fleshy and organic hardware? What makes us human in the first place? What makes us conscious and sentient beings? And what will become of our “humanity” in the era of AI singularity? Can a computer be even conscious in the sense we philosophically attempt to explain??
According to Berkeley philosopher John Searle, computer programs can never have a mind or be conscious in the human sense, even if they give rise to equivalent behaviors and interactions with the external world. Searle cautions that the one mistake we must avoid is supposing that if you simulate it, you duplicate it. David Chalmers think there is a possibility that your upload would appear functionally identical to your old self without having any conscious experience of the world. You’d be more of a zombie than a person, let alone you. However, Daniel Dennett, have argued that this would not be a problem. Since you are reducible to the processes and content of your brain, a functionally identical copy of it — no matter the substrate on which it runs — could not possibly yield anything other than you.
Several hundred people have already chosen to be ‘cryopreserved’ in preference to simply dying, as they wait for science to catch up and give them a second shot at life. Just last week, biotech startup Nectome, backed by MIT, piqued widespread interest when it publicly revealed its goal to “back up” people’s brains in an attempt to one day revive consciousness. But then, funnily enough, it was discovered that the mind-uploading service is 100% fatal with an approach similar to physician-assisted suicide!!! Obviously, trouble in dystopian paradise emerged swiftly: MIT cut ties with the startup, which now claims it won’t be uploading brains any time soon. In a statement, the MIT Lab declared that, “Neuroscience has not sufficiently advanced to the point where we know whether any brain preservation method is powerful enough to preserve all the different kinds of biomolecules related to memory and the mind. It is also not known whether it is possible to recreate a person’s consciousness.”
ASK YOURSELF: DO I REALLY WANT TO BE IMMORTAL AND “EXIST” ON A SILICON CHIP?
Can we predict what the actual upload would feel like to the mind being transferred? What if ending your biological existence will cause a complete block of any form of communication to outsiders or inability to switch yourself off? What if your uploaded mind is copied simultaneously and you don’t even know who is the original? Will you survive in any meaningful sense if you were copied several times over? Are you going to be fine with millions of versions of your mind existing in the cloud doing their own thing? What if your mind is hacked to become a fanatic digital terrorist? What if your “software” is attacked by viruses beyond the possibility of any repair? Will “you” still think you are you? In all these cases, your immortality would amount to more of a curse than a blessing. Death might not be so bad after all, but unfortunately it might no longer be an option.
Immortality is a quality attributed to Greek Gods, but their immortality gives them all kinds of personal issues such as boredom, difficulty in giving meaning to their immortality, impossibility of living a life repeating itself eternally (remember Groundhog Day). Now, however, immortality has gone secular and investors are very attracted by the idea of being the only patent holder of the technology of human longevity/immortality. Clearly, the prospect of living forever has always been and will always be the main quest of our lives. I believe, if immortality becomes ever an option, many people will probably conclude that living forever, one way or another, outweighs any dangers.
— — — — — — — — — — — — — — — — — — — — — — — — — — — —
Ezgi Tasdemir, PhD is a Novartis Oncology employee. This article is created by Ezgi Tasdemir. All the views, analysis, and perspectives are fully independent and belong to the author only, they do not represent the views or opinions of Novartis or any other company or organization. The author does not receive any funding or support from Novartis or any other pharmaceutical/non-pharmaceutical company for this blog.
",Do you (really) want to live forever?,360,do-you-really-want-to-live-forever-b3daa2bd38f9,2018-06-08,2018-06-08 14:47:20,https://medium.com/s/story/do-you-really-want-to-live-forever-b3daa2bd38f9,False,1748,Medium's largest publication for makers. Subscribe to receive our top stories here → https://goo.gl/zHcLJi,,,,The Startup,,swlh,"STARTUP,TECH,ENTREPRENEURSHIP,DESIGN,LIFE",thestartup_,Ethics,ethics,Ethics,7787.0,Ezgi Tasdemir,"Writer | Realistic optimist | Constantly curious & amazed | Passionate about healthcare, in pursuit of Positive Disruption to advance humanity.",2f38921d4043,ezgitasdemir,398.0,415.0,20181104
0,,0.0,4ffcc503f3d1,2018-05-18,2018-05-18 17:19:30,2018-05-21,2018-05-21 17:52:50,4,False,en,2018-05-21,2018-05-21 19:54:58,6,9a6321e5a265,6.40566037735849,52,2,1,"The Wall Street Journal is experimenting with a new approach for reporting how smart algorithms work, beyond simply describing them.",5,"What is the role of journalists in holding artificial intelligence accountable?
The Wall Street Journal is experimenting with a new approach for reporting how smart algorithms work, beyond simply describing them.
Image Credit: Gabriel Gianordoli/ WSJ
Journalists, who routinely ask questions of their sources, should also be asking questions about an algorithm’s methodology. The rules created for algorithms need to be explicit and understood. The Wall Street Journal has been experimenting with a new approach to explain how AI works by letting readers experiment with it.
“Interactive graphics can provide insights into how algorithms work in a way beyond simply describing its output. They can do this by acting as safe spaces in which readers can experiment with different inputs and immediately see how the computer might respond to it,” said deputy graphics director Elliot Bentley.
“To make this accessible and non-intimidating, it’s important to design a straightforward interface with minimal controls, and also provide informative and immediate feedback,” Bentley added.
Image Credit: Gabriel Gianordoli/ WSJ
The most recent example of letting readers experiment with algorithms is our story, “What Your Writing Says About You,”published as part of the Leadership issue of Journal Reports. The news experience offers an interface allowing people to enter text such as an essay, cover letter, blog post or business email and receive results from algorithms that rate the content by different parameters. By including detailed methodology and source notes, we allow our audiences to understand how machine learning and natural language processing can determine context, language mastery, meaning and even your mood from the choice of words.
“These explorable explainers allow us to not only go deeper, but also to give the readers a perspective on subjects like AI that we can’t give them by simply writing more great stories. It immerses them in a unique way in a subject we know they care about,” said Journal Reports editor Larry Rout.
In a previous Graphics project entitled “How Facial Recognition Software Works,” Bentley explained that readers need only to enable their webcam and begin moving their head around in order to play with a facial-recognition algorithm. It then provides clear, real-time feedback using a series of visual overlays. Another example of this is “Build Your Own Trading Bot” in which we attempted to demystify algorithmic trading by designing a user-friendly interface and a rewarding feedback loop to encourage readers to experiment with the mechanics.
How Facial Recognition Software Works. Credit: Elliot Bentley/WSJ
Journalism and algorithmic accountability
We might not notice it, but artificial intelligence affects multiple parts of our lives. These algorithms decide whether an individual qualifies for a loan, whether a resume is seen by a recruiter, which seat a passenger is assigned on an airplane, which advertisements shoppers see online and what information on the internet is shown to users. Transparency of the data that feeds these processes is crucial both for consumers to better understand what they encounter and for organizations to shape their business strategy.
Given the challenging nature of auditing algorithms, it’s important to consider how the practice of journalism can be leveraged to hold AI systems accountable. In his forthcoming book, Northwestern University professor of computational journalism Nicholas Diakopoulos introduces the notion of algorithmic accountability reporting as an approach to highlight influences that computer programs exercise in society.
“Operating at scale and often affecting large groups of people, algorithms make consequential and sometimes contestable decisions in an increasing range of domains throughout the public and private sectors. In response, a distinct beat in journalism is emerging to investigate the societal power exerted through such algorithms.
There are various newsworthy angles on algorithms including discrimination and unfairness, errors and mistakes, social and legal norm violations, and human misuse. Reverse engineering and auditing techniques can be used to elucidate the contours of algorithmic power,” Diakopoulos explained.
The “black box” problem in AI
When certain decisions are derived through an algorithm, it’s often hard to pinpoint why or how an automatic output was derived. This introduces the problem of the “black box” algorithm whereby correlations are made without rules set by humans. This term is often used as a metaphor for algorithms in which the process to reach a certain outcome cannot be seen in full.
“Auditing algorithms is not for the faint of heart. Information deficits, expectation setting, limited legal access, and shifting dynamic targets can all hamper an investigation. Working in teams, methods specialists working with domain experts can, however, overcome these obstacles and publish important stories about algorithms in society,” Diakopoulos added.
It’s indeed relevant to dissect how computers make decisions and to comprehend how smart systems are created. For example, the AI powering the set of analysis in “What Your Writing Says About You” is provided by Factbase, an AI company which makes its algorithms open source, peer reviewed, and available for examination.
In “What Your Writing Says about You,” we explain the underlying scientific methodology behind each output, including the Flesch-Kincaid Grade Level — developed in 1975 by the Department of Defense to review readability level of military materials — as well as the Treebank methodology created by the University of Pennsylvania to evaluate linguistic structure of text.
“It’s important, as much as is possible, to understand the parameters under which the AI or algorithms arrived at its conclusions. What parameters it examines, and how it analyzes it, provides transparency to its thinking, per se, which in turn makes it more clear how it decides what it decides,” said Bill Frischling, founder of FactBase.
This issue is prevalent in artificial intelligence, partly because the systems are not necessarily designed to explain how they do certain things, but to just do them. This is also a byproduct of algorithms learning by themselves; they make causal links not based on human instruction but on self-identified patterns.
Newsroom collaboration
The Wall Street Journal’s news hub in New York City.
There are, of course, technical gaps to developing this type of reporting on algorithms, which can be addressed by working cross-functionally with data scientists, computational journalists and technologists. Increasingly, it’s important to foster a culture of collaboration throughout the newsroom and bring multiple perspectives into the process of story planning and development.
“A project such as this which taps so many areas of expertise and aligns them is a pleasure to be part of. What started with WSJ Lab’s original outline of possibilities was honed by a team of editors at Journal Reports to focus on specifically what our writing reveals about us. Our interactives team wrangled the code, user interface and graphic visualization,” said news editor Demetria Gallegos.
“Then, privacy experts from our legal and data teams, our social and off-platform colleagues and homepage and mobile editors weighed in to ensure the experience is optimized for every reader,” Gallegos added.
The odds for a successful collaboration can be increased if the organization is able to foster an environment where journalists are encouraged to test new ideas, to seek feedback, and to share best practices even if experiments are unsuccessful. Building this “feedback loop” can enable news professionals to mitigate the uncertainty of experimentation as well as inform the broader newsroom strategy.
“When we are thinking about how to create an innovative news experience, we have to consider how readers already ingest news — and how much further they are willing to go. In our discussions during the story planning process, we ran through various scenarios of how the tool could work, based on different criteria. We then ruled out things that would require too much time or too many steps. We also had to be sensitive to how much information people are willing to disclose. We designed this interactive story to be fun enough to get readers in, engaging enough to have them read through it, take the quiz, play the game etc. And if they end up sharing their results on social media, we know we did it right,” explained news editor Cristina Lourosa.
Journalistic standards and technological evolution
Just because a certain result came from a computer, it doesn’t mean it’s right. Artificial intelligence is programmed by humans and consequently it can make mistakes. The ethical considerations inherent to using AI are far and wide.
“Understanding the source of information whether it’s from a person or algorithm is not only crucial for the news industry but as well, for democracy,” said Kourosh Houshmand, a computational journalist at Columbia Journalism School.
The practice of journalism is about questioning the world around us, and that same principle still applies even when a piece of software played a role in a particular outcome such as determining the price of a product, evaluating how a person feels based on their writing or selecting a candidate for a job interview.
“We can help readers understand how technology works by explaining how the algorithms get their results and then pointing to the source documents and formulas that power the calculations,” said graphics reporter Nigel Chiwaya.
An effective way to understand AI is to experiment with it, comprehend the nuances of how algorithms make decisions and how those decisions may affect our lives.
Interested in journalism and data science? WSJ is hiring a Data Science Lead.
",What is the role of journalists in holding artificial intelligence accountable?,347,what-is-the-role-of-journalists-in-holding-artificial-intelligence-accountable-9a6321e5a265,2018-06-08,2018-06-08 11:23:48,https://medium.com/s/story/what-is-the-role-of-journalists-in-holding-artificial-intelligence-accountable-9a6321e5a265,False,1512,The Wall Street Journal on Medium,,wsj,,The Wall Street Journal,socialmedia@wsj.com,the-wall-street-journal,"WALL STREET JOURNAL,THE WALL STREET JOURNAL,JOURNALISM,WSJ,NEWS",WSJ,Ethics,ethics,Ethics,7787.0,Francesco Marconi,"R&D Chief at The Wall Street Journal and fellow at Columbia Journalism School. I write about media, storytelling and innovation.",3830b4f5fd63,fpmarconi,10451.0,4358.0,20181104
0,,0.0,,2018-01-11,2018-01-11 19:45:30,2018-01-11,2018-01-11 19:52:32,1,False,en,2018-01-11,2018-01-11 19:52:32,19,862dbc269de8,3.6037735849056607,0,0,0,AI has the power to bring both significant social benefits and new social risks. Here’s what companies should keep in mind when developing…,5,"Seven Things Every Company Should Know about Artificial Intelligence and Sustainable Business
AI has the power to bring both significant social benefits and new social risks. Here’s what companies should keep in mind when developing AI capabilities.

By Dunstan Allison-Hope, Managing Director, BSR; Jacob Park, Director, Sustainable Futures Lab, BSR; Michael Rohwer, Associate Director, Information and Communications Technology, BSR
Artificial intelligence (AI) is advancing rapidly, thanks to ever-more-powerful computing, massive growth in the availability of digital data, and increasingly sophisticated algorithms. The world’s largest technology firms are investing billions to develop their AI capabilities, and companies across industries, from travel to real estate to fashion, are racing to bring AI-enabled services to market.
AI has the potential to bring significant social benefits, including healthcare (via improved diagnostics), transportation (through self-driving vehicles), and law enforcement (with improved fraud detection). AI also brings new social risks, including to non-discrimination (from algorithmic bias), privacy (through the misuse of personal information), child rights (through lack of informed consent), and labor rights (because of the mass displacement of workers by machines).
While by no means exhaustive, we believe the following seven considerations are essential for our members to factor into their AI strategies.
AI is relevant for all industries, not just technology companies. The development of AI today is being driven by Silicon Valley, and it is understandable that private-sector participation in the dialogue about the social implications of AI has been dominated by technology companies. However, it is an urgent priority for companies in other sectors using AI — such as financial services, healthcare, infrastructure, public services, and retail — to understand how AI impacts their business models, employees, and customers.
The human rights and ethics impacts of AI are especially important. The UN Guiding Principles on Business and Human Rights were created to guide the integration of human rights into business decision-making, and should be deliberately applied to the development and deployment of AI. This means asking and addressing questions like “What are the most severe potential impacts?”, “Who are the most vulnerable groups?”, and “How can we ensure access to remedy?” Companies should take a human rights by design approach to AI.
Environmental issues are important, too. While significant attention has been paid to the ethical and human rights implications of AI, we have a tremendous opportunity to embed environmental learning into AI — as Google has done to radically improve the power use effectiveness of its data centers. AI can also be used as an environmental solution — as Microsoft’s AI for Earth commitment demonstrates. At the same time, it will be important that the data processing needs created by AI don’t substantially increase energy use.
Research, product development, and marketing teams are essential to engage on sustainability. In our 2017 annual survey of sustainable business leaders, we asked which functions were most important to achieve substantive progress on sustainability — and only 24 percent mentioned product development, 13 percent mentioned research and development, and 8 percent mentioned marketing. These functions will have a significant influence on the development and deployment of AI, so it is crucial that they participate actively in the conversation around AI and sustainability.
Companies will need to communicate the complexity of AI in accessible ways. AI is extremely complex, and only a very small number of people in the world — mostly concentrated inside companies — understand how it works. If AI is to fulfil its potential while mitigating accompanying risks, civil society, rights-holders, and vulnerable populations should have access to information about the issues at stake and channels to participate meaningfully in discussions about its application.
Ethics and principles for AI are being developed rapidly, but implementing them in practice will be challenging. It is noteworthy how rapidly the AI field has developed principles, with organizations such as the Institute of Electrical and Electronics Engineers, the Software and Information Industry Association, the Information Technology Industry Council, and the Future of Life Institute all publishing statements of ethics. Initiatives like Partnership on AI, the Ethics and Governance of AI Fund, and AI Now are embarking on substantial efforts to explore key dilemmas and facilitate dialogue on them. However, turning theory into practice will require thorough review of real-life cases.
The future of AI is uncertain, but decisions today can have long-term consequences. Taking responsible approaches to AI will require grappling with rapid change, uncertainty, and complexity. We can’t know exactly what path the development and deployment of AI will take, so we should be prepared for different versions of the future and think through the possible long-term implications of today’s decisions. Futures thinking, also known as strategic foresight, can provide structured ways to explore multiple possible futures and chart a path forward that considers the various possible outcomes that might unfurl.
In our recent report on the Future of Sustainable Business, we listed the intersection of technology, ethics, and human rights as one of the three big issue sets that we believe need to be front and center on the business agenda — not only for sustainability reasons, but because these questions will be increasingly central to business performance and strategy. We have much to lose if AI does not evolve in ways that support the public good, and we look forward to working with you to help ensure that it does.
Originally published at www.bsr.org.
",Seven Things Every Company Should Know about Artificial Intelligence and Sustainable Business,0,seven-things-every-company-should-know-about-artificial-intelligence-and-sustainable-business-862dbc269de8,2018-04-15,2018-04-15 02:30:48,https://medium.com/s/story/seven-things-every-company-should-know-about-artificial-intelligence-and-sustainable-business-862dbc269de8,False,902,,,,,,,,,,Ethics,ethics,Ethics,7787.0,BSR,A global nonprofit organization working with its network of more than 250 member companies to build a just and sustainable world. www.bsr.org.,a45675f646e1,BSRnews,3776.0,666.0,20181104
0,,0.0,,2017-11-09,2017-11-09 18:09:04,2017-11-09,2017-11-09 18:10:10,1,False,en,2017-11-09,2017-11-09 18:10:10,2,95df6855c606,7.1773584905660375,0,0,0,Ethics is a fluctuating principle. Which is unethical in the first place as ethics are a set of conventional principles that are always…,5,"Ethics of the future with a look into the present

Ethics is a fluctuating principle. Which is unethical in the first place as ethics are a set of conventional principles that are always right. Ethics is as illusionary as religion; they both want to make you believe that by following a certain path, you will be able to maintain a clear conscience. But to me, no action is clearly good, or clearly bad if you take into account all the possible outcomes. Today I especially want to talk about ethical questions that have come about through new technologies. Many people are appalled by gene editing, artificial intelligence (AI) advancement and the creation of new kind of drugs. But if we put these technologies in perspective with our actions of the past or today, we can see that they do not bring anything new. And we do not question the ethics of these previous actions. My goals is not to support these new technologies. I just want to broader the spectrum and bring people to question the way they see the world. There is never one way of seeing things, and there is never a good without a potential bad. Nothing is ethical. Is there an ethical difference between a country operating targeted drone attacks killing hundreds of innocents and a terrorist organization perpetrating attacks and killing hundreds of innocents? Is there an innocent less innocent in ethics? You might think getting a rescued cat is ethical. But like all ethical things, it is only ethical if you do not think about what surrounds the choice. Cats are killers. They kill a large amount of other animals. Is it ethical to adopt an animal who will kill other animals because of your action? And cats need to eat meat products. So the food that you are buying for your cat is made with animal by-products, which does not help the cause of the killing of other animals for food.
Speaking about food, what is the difference between a turkey, which has been bred for years — aka modified over time — to have the fattest possible one and a salmon to which we have added genes to make it grow faster? To push it further, we are, as humans, genetically modified organisms. This modification just happened over a long period of time and we call it natural selection. What changes is that now we are capable of making the modifications ourselves on animals, plants or viruses to support our needs. And we are going to see this happen more and more in humans. We have been capable of making a “3-way baby” — sperm, egg and mitochondria from 3 different persons — thanks to modifications. We have fetuses checked for some genetic diseases, we have therapies which can correct gene mutations. Abortion is legal in many countries until week 20 ish. We can know the sex of the fetus at week 12, should it be legal to abort a fetus because of its sex? If we consider that fetuses are not a living person yet, should we really care? Is there a difference with aborting because the fetus shows signs of Down Syndrome — knowing that 5% are false positive -? Just FYI, I do not contest the right of abortion. It’s not my body. But these terminations have nothing to do with women’s rights. They want a baby, just not that one. Gene editing could solve this issue by attacking the problem at the source. Would it be more ethical to modify an embryo or to have an abortion? We can expect that in the near future, we will be able to safely modify humans’ genes to change attributes. I am just trying to put in perspective this “new” science. If we want to judge its ethics, we need to judge the ethics of breeding. No one ever wonders how dogs came to exist in some 350 breeds. There is nothing natural about it. We modified them to have the strongest, the fastest, the cutest (I guess they missed all of them for pugs lol). This is gene modification even though no one went to cut inside the genome. And most people who have dogs do not care about this dilemma. They just care about having cute puppies. In the same sense, plants have been bred for hundred of years to improve our agriculture. There is a scientific consensus on the fact that GMOs are as safe as bred plants — for human consumption -. Is it ethical to question this fact when you are making fun of the Climate change deniers? Moreover, we never really tried to breed humans because of ethics. We treated in our whole history some races, religions or people as inferior for diverse reasons. We have done what we could to avoid these people to mate, but we never acted on the idea to mate the smartest, strongest or cutest humans just like we do with other animals. We think about taking the embryos of the most productive milk cow, but no one ever debated taking Einstein’s sperm. The gene scissors could allow us to do that “breeding” virtually. Shouldn’t we try to get our species to the best level we can? It is weird to think that we can engineer a “better” human, but should we stop? We did it with animals, we did it with plants. The question is not whether we can or can’t or whether we should or shouldn’t. In any case it is going to happen. We can’t stop progress for ever. We need to figure out a way to do it in the best way.
In its own way, the development of AI is going to set many ethical questions, too. Many different scientists say that we should be able to reach an AI as smart as a human as soon as 2040 (must read — Tim Urban article about that). And from there it shouldn’t take long for it (him?) to get smarter than us. When we see how we have treated other humans or the way we treat other animals, we can wonder how this new intelligent being will treat us. Or we can be scared of who will be in possession of such a powerful tool. People like Elon Musk or Stephen Hawkins try to warn us with the future potential — or the potential future — of AIs. There are real dangers as well as real hopes with this development. If we can control this technology, we could get cures for incurable diseases, understand the world that surrounds us, and improve our way of living. But would it be controllable? Would it listen to us? We have AIs everywhere. We have Siri listening to us when we have our phone or our computer with us, we have Alexa listening to us when we are in our home, add to this all the information we purposely leave online on Facebook or Amazon or our DNA at Ancestry. Something as innocent as a Rumba vacuum can be mapping our home and use the data for anything. All of these machines are linked through a network connection. And we will soon let them control — with almost no supervision — part or all of our transportations, our manufacturing, our armies. Hackers can get into our car, phone or computer. AIs will, too. It’s bad enough companies, governments or hackers can have access to this information, what about an AI whose motives or leading forces are unknown? People make fun of a scenario like in Terminator but some humans already think that there are too many humans on Earth, why wouldn’t a Supreme being built by humans think the same?
While AIs will have 100% access of what they have learned/experienced, we cannot say the same about us. Isn’t our brain altering memories in some cases on its own? It is everyone’s experience that sometimes we remember things without all the details in mind. It has been proven that we can implant fake memories into people only by manipulating discussions. We are capable of making someone think that she experienced a traumatic event or that she now likes a vegetable more than she did previously. We don’t remember everything that ever happened to us, our brain makes a selection without our authorization. Of all the things that happened to me 10 months ago, I remember some clearly and other less clearly. Why is that? It is a fact that our memories are out of our control. At least consciously. It is true that what we experience is what makes us, but can we blame someone who suffers from PTSD to want to alter a traumatic event whether it’s through a drug or a therapy? If events that I forgot, whether purposefully or not, makes who I am, why wouldn’t an altered memory still make you, you? Moreover, if we are capable of manipulating the brain of someone, should we? The issue of therapy vs drugs is the same as the issue of breeding vs gene editing. If the end goal is the same, why distrust one vs the other? Smoking is bad for the smokers, the people around them and the healthcare system. If we could make people quit smoking with some kind of drug, should we do it for their own good? Could we force someone to be treated against their own will to save their lifespan and the state some money? In many countries, we have raised the prices of cigarettes and forbidden the ads for them because we know that these steps will make it less likely for someone to start smoking. Isn’t that considered manipulation? Why not go one step further and do the kind of manipulation that we know works for good — i.e. drug treatment -? Or why not forbid the sale of cigarettes completely? Is it freedom if what you see is what I want you to see and not everything that is out there? And what if you have all the knowledge you need to make a “good” decision, you still make a “bad” decision? Should I feel ethically bonded to force you into making the right — i.e. the one I want you to make — decision? With the recent mass shooting in Las Vegas, as the shooter was non muslim and didn’t have apparent ties to ISIS, we claim that he must be mentally ill. Even though we might never know what motivated that guy, why couldn’t someone who claims doing something for ISIS be as crazy as that white guy? Could their mental illness makes them not guilty? We are getting better and better at understanding a relationship between genes, brain and behaviour or diseases. It is a possibility that a certain combination of genes is controlling addiction, violence, depression, etc. So should we be considered responsible for these? Am I what I am because I chose to be what I am or am I just a combination of nature and nurture without my control?
",Ethics of the future with a look into the present,0,ethics-of-the-future-with-a-look-into-the-present-95df6855c606,2018-05-25,2018-05-25 05:46:32,https://medium.com/s/story/ethics-of-the-future-with-a-look-into-the-present-95df6855c606,False,1849,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Ben,Think. Thought. Fought. Fight.,452afa97bd6f,FriedByZeFrench,0.0,1.0,20181104
0,,0.0,,2018-09-13,2018-09-13 11:26:33,2018-09-13,2018-09-13 11:29:05,2,False,en,2018-09-13,2018-09-13 11:29:05,5,ed77f91e9775,3.960691823899371,0,0,0,"We need to be “super careful with AI,” lest we “summon the demon.” Or so say some prominent AI alarmists. Others counter that fears of…",3,"Men vs. Machinery — How to ensure ethical AI?

We need to be “super careful with AI,” lest we “summon the demon.” Or so say some prominent AI alarmists. Others counter that fears of humanity’s ruin at the hands of machines run amok are overstated. Yet most observers agree that the ethical questions raised by the technology’s ascendancy are undeniable.
AI is only in its early stages — so there’s time for public discussion to play out. The technology’s long-range potential, to create a system that learns and acts by itself without any human involvement or that can pass the Turing test, is far from reach. However, AI algorithms already improve many information systems in terms of efficiency and capabilities. But it is important that these algorithms are tailored to the systems they support. Only by adjusting an algorithm can it meet the ethical standards desired by humans. Intention becomes essential to the creation and adaption of code.
Technology, by itself, is amoral. If you use it to generate business value, you must design it to behave within intended parameters. You need to ensure that AI is not used for any type of illicit activity. Even though this might seem obvious on its face, the hype surrounding AI can at times obscure the reality. As long as humans maintain control over AI’s algorithms, they do the same over its potential externalities.
Hence, if you want to put AI to its best use, consider following principles:

Bias
Bias is inherent in the human condition, and thus it carries through to the technologies we devise. Although raw data is neutral, human interpretation, particularly in the perception of patterns, inevitably introduces bias. If the person responsible for the data has a certain goal in mind, he or she may unwittingly tweak the algorithm to prioritize it in a particular manner. Consequently, the approach to implementation of AI is crucial. You need to teach your developers the awareness of the power they hold concerning the implementation AI algorithms and its impact on subsequent decisions. Moreover, the preference of a male over a female developer due to the fact that there are currently more males in the role should be prevented. Since unbiased circumstances are impossible to attain, you should rather attempt to make the most out of it. An instance would be to instruct an algorithm to prioritise people of a certain age (e.g., +40) regarding an upgrade into first class in an airplane. Middle-aged passengers have a greater likelihood to purchase a first-class ticket the next time compared to younger people.
Transparency
It is characteristically human to question everything. So why not question AI algorithms? Indeed, one must. You need to enforce transparency in the decisions your business bases on AI. This shared access of contextual information will lead to increasingly diverse knowledge and explanations. The established principle of multiple-assessors verification allows one to discover mistakes and provides the option to make sure ethical standards are adhered to. Furthermore, it improves the understanding of AI results for everyone. Hence processes that are invisible to the user and results that cannot be verified by the developer should be avoided. Simultaneously, the controller should understand a system’s process when, for instance, the AI algorithm’s task is to convert one currency into another preferred by the controller. Without the option to proofread the processes undertaken by an AI algorithm, no trust can be built upon the data and thus no sustainable action can be guaranteed.
Human involvement
Become a committed participant in the AI process, involving yourself as much as needed but — importantly — no more than that. The main aim is to achieve the best controlled outcome with the least monitoring and intervention. To manage the effectiveness and results of AI systems, developers and users need to determine boundaries for implementation and usage. This inevitably gives them the responsibility to ensure a secure and ethical way of applying AI. Considering this goal, they need to specifically state the purpose in the beginning of the project, control the systems throughout and set limitations regarding the use of AI technologies. For example, with a merger or acquisition, where incompatible systems exist, human intervention is required. Hence human involvement is not only unavoidable for certain decisions, but should always be made an indispensable part of any application of AI algorithms, just to ensure it is carried out ethically.
International Standards
The ethical implementation of AI becomes impossible without the ability to enforce standards across international borders. Tech firms and international organizations are increasingly turning their attention to setting such standards. Rules regarding the detailed documentation and audit for ongoing performance measurements are emerging as a crucial focus. In addition, firms currently grappling with disparate sets of regulations governing data privacy seek unified norms. In general, a flexible approach for regulations to promote AI would be ideal, because while the technology’s future may be uncertain, the need for ethical guidelines is not.
AI is a powerful tool that can be used to construct more intelligent ways of operating. But it can also be used to act in ethical ways that not only lead to doing well, but doing good. To making an impact on our businesses AND the world around us. It’s an incredible opportunity. And as wielders of the power of AI, a responsibility….
Marcell Vollmer is chief digital officer of SAP Ariba and the former chief procurement officer of SAP.
SAP is the market leader in business applications; and SAP Ariba is the world’s largest business network, linking together buyers and suppliers from more than 3 million companies in 190 countries.
Tags
#DigitalTransformation #future #technology #digitization #ArtificialIntelligence #AI #ethics #ethical
",Men vs. Machinery — How to ensure ethical AI?,0,men-vs-machinery-how-to-ensure-ethical-ai-ed77f91e9775,2018-09-13,2018-09-13 11:29:05,https://medium.com/s/story/men-vs-machinery-how-to-ensure-ethical-ai-ed77f91e9775,False,948,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Dr. Marcell Vollmer,"Chief Digital Officer & SVP @SAPAriba. Passionate about #Life, #Coffee, #PhD in #Politics, #MBA in #Economics, #SocialMedia Enthusiast and curious to learn&grow",e92f982c8bd0,marcellvollmer,11.0,15.0,20181104
0,,0.0,49714ce8f722,2018-03-20,2018-03-20 02:24:23,2018-03-22,2018-03-22 16:01:01,1,False,en,2018-03-22,2018-03-22 19:07:39,1,1f3a6e49881b,3.8905660377358484,1,0,0,I have been thinking about AI bias for long and today’s IBM Research Science Slam session at IBM Think 2018 conference gave me another…,5,"On Overcoming AI Bias

I have been thinking about AI bias for long and today’s IBM Research Science Slam session at IBM Think 2018 conference gave me another chance to think about it again. Francesca Rossi gave a talk about AI Ethics from IBM Research point of view (see the video above). I also talked about it in a podcast today with Val Berovici of Pencil Data. In this post, I am going to set the context for future discussions by touching upon factors that influence bias in AI and how, in certain cases, we have to forcibly introduce bias.
The problem of AI bias is a tricky problem. Left unchallenged, this could lead to situations that will make the society look more tribalistic but, at the same time, letting Silicon Valley define the values could be problematic. What is needed is an open conversation on how we can ensure that AI driven society is a reflection of our value system. Also, our value system continuously evolves and we need to ensure that this evolution in our value system is taken care off through data or algorithmic tweaks. This is not an easy problem to solve but it is easy to start talking about this problem now so that we are in a good position to handle this when it is time to act.
Sources of AI Bias
Even though data is a critical source for AI bias, it is just one of the many that we need to consider. I am listing four sources of bias from my point of view and would love to hear your thoughts on this.
Biased Data: This is the biggest source of AI bias. Let us face the basic facts. The world has been evolving in their value systems and discriminatory behavior is still part and parcel of every society. The dataset produced by such a world will encompass all kind of biases from past to present. Such a data will skew AI and noticing the cause of bias and fixing it is not easy, with all the AI systems being a blackbox
Algorithmic Bias: Even though computers are getting better at writing code, humans are still responsible for developing the algorithm. Human biases are going to be pushed into the code either knowingly or unknowingly. Even the algorithms generated by machines are done by “intelligent systems” whose training data is from the humans
Geography Bias: I have long been arguing in social channels that both AI and Genomics are going to suffer big time due to lack of data from diverse geographical locations. Yes, world wide web and social media has flattened the world a bit but there is not enough data from across the world to make the training data sets representative of the world’s population. The data from certain Asian, African or South American countries are still limited compared to North America and Western Europe
Language Bias: The foundation for most of the data used in training sets is English language. China is taking a lead on Chinese language but there are many other languages that are not represented in the AI data sets
Biases related to geography and language will result in serious cultural biases which will have dramatic impact as AI becomes the underlying framework for life.
Inducing Bias
Even if we magically remove any bias from data and algorithms, we will face problems from the other side. A bias free data/algorithm will not represent our world. There are certain biases we need to have in our algorithms for AI to mimic our value systems. One good example is the role of gender in determining the punishment for violent crimes. Data clearly shows that men commit more violent crimes than women. In an AI free society, we will be giving harsher punishment for men for violent crimes than women as a way of deterrent. If we remove any bias in AI, it will give the same punishment to both men and women for violent crimes. This is not ideal for the society (without considering the advances in neuroscience and medicine which could fix some of these issues) and we need to induce a bias against men involved in violent crimes. This is ok for many people and a quick (and very unscientific) twitter poll I conducted confirms the support for biasing AI against men who commit violent crimes.

But let us now consider another scenario where data shows that certain group (based on race, religion or other human induced segregation) commits more violent crimes than other groups (this will vary by country). If I had ran a poll on this question, the result will not support inducing bias against certain groups of people. Personally, I would fight against inducing such biases because it goes against my value system. But, for many others, inducing discriminatory biases may be acceptable or a necessary evil. Dealing with such situations are not easy. If a wrong bias is induced into AI systems (remember, they are blackboxes), the consequences can be unpredictable and, often, devastating.
Inducing bias is important for the very functioning of AI based societies but the margin of error is almost zero. It is important for us to figure out how we can induce the necessary biases but still retain a way to evolve them out when it is no longer necessary. It is not an easy social problem to solve and it is definitely not an easy technology problem to solve.
It is time we, as an industry, start thinking about these socio-political issues and figure out how we are going to let AI systems handle these issues. This situation is not very far away. Already AI systems are used for punishing people in the judicial systems. AI is also creeping into our lives through many different dimensions. We have to start debating these issues NOW!!
",On Overcoming AI Bias,10,on-overcoming-ai-bias-1f3a6e49881b,2018-03-27,2018-03-27 00:09:47,https://aisutra.com/on-overcoming-ai-bias-1f3a6e49881b,False,978,AISutra delivers quality articles on how machine learning and artificial intelligence is shaping our society. This site will help both business leaders and concerned citizens understand its impact across many dimensions,aisutra.com,rishidot,,AI Sutra,info@rishidot.com,aisutra,"MACHINE LEARNING,MACHINE INTELLIGENCE,ARTIFICIAL INTELLIGENCE,AI,SOCIETY",rishidot,Ethics,ethics,Ethics,7787.0,Krish,"Analyst, Modern Enterprise, Startup Dude, Ex-Red Hatter, Rishidot Research, Modern Enterprise Podcast. Future Asteroid Farmer",62bf9bdc96bc,krishnan,3267.0,520.0,20181104
0,,0.0,fadf5dd8ba35,2018-06-08,2018-06-08 18:02:48,2018-06-08,2018-06-08 18:05:31,1,False,en,2018-06-08,2018-06-08 18:05:31,12,204a688abd2c,3.781132075471698,1,0,0,This article was originally published by our partner Bloomberg on bloomberg.com.,5,"Bloomberg, BrightHive and Data for Democracy Launch Initiative to Develop Data Science Code of Ethics
This article was originally published by our partner Bloomberg on bloomberg.com.
“Community Principles on Ethical Data Sharing” to provide guidelines for ethical behavior”

New York — At Bloomberg’s 2017 Data for Good Exchange, the fourth annual conference exploring how data science can help solve problems for social good, a partnership was announced between Bloomberg, BrightHive and Data for Democracy to develop a code of ethics for data scientists from the ground up. Called the “Community Principles on Ethical Data Sharing (CPEDS),” this code of ethics will provide a set of guidelines about responsible data sharing and collaboration.
“When data scientists are entrusted with the most private and valuable data out there, the data science community must work to deserve the trust of those whose data we are holding,” said Gideon Mann, Bloomberg’s head of data science.
Over the next six to nine months, the goal of the partnership is to define values and priorities for overall ethical behavior by data scientists. By developing a code of conduct around data sharing, data scientists can be thoughtful, responsible, and ethical agents of change in their organizations. Through social media and community discussions, more than 2,000 data scientists have already weighed in on what the challenges are with sharing data and what must be overcome for people and companies to feel comfortable sharing data with a trusted community of data scientists.
“We set out to explore what it could look like if data scientists had a code of ethics, similar to physicians’ Hippocratic Oath,” said Natalie Evans Harris, ‎COO and Vice President of Ecosystem Development at BrightHive, and a former senior policy advisor to the U.S. Chief Technology Officer under President Barack Obama. “The CPEDS won’t propose comprehensive solutions for thorny questions like how to minimize algorithmic bias. Instead, it will aim to define priorities for overall ethical behavior related to data sharing.”
The guidelines will be general enough to be relevant to a data scientist’s approach to any work or projects, while also being specific enough to support data scientists when they need to make difficult ethical decisions about their work.
“In other words,” said Lilian Huang, Data Ethics Lead at Data for Democracy, “they are meant to guide a data scientist in being a thoughtful, responsible, and ethical agent, who can then work together productively with other data scientists to actually solve problems such as how to minimize algorithmic bias or maintain data privacy.”
The preliminary work to date has been focused on framing a larger discussion and bringing together the community to move the conversation forward. This includes identifying recurring themes that members of the data science community consider important and arranging them in a systematic framework to address the following five areas of concern:
Data itself: Overall practices surrounding the collection, storage, and distribution of data and understanding and minimizing intrinsic bias in collected data;
Questions and problems: Identifying valuable and relevant problems to work on and working with pre-existing resources and parties in those fields;
Algorithms and models: Understanding and minimizing bias in algorithms/models and responsibly dealing with black-box algorithms;
Technological products and applications: Responsibility for how one’s research is applied; identifying and guarding against the potential for misuse;
Community: Fostering a data science community culture that is actively welcoming to people from diverse backgrounds and deliberately promoting equity and representation, and finding ethical, non-invasive ways to track progress.
Ultimately, the hope is to have over 100,000 data scientists participate in the community-driven process for shaping the CPEDS via social media, conversations and virtual working groups to share and collect best practices, techniques, and tools.
To participate in any of the conversations, individual data scientists can respond to discussion questions on Twitter, Slack, or GitHub. Organizations interested in having a representative participate in bi-weekly focus group calls should contact team@datafordemocracy.org. Ongoing discussions will take place throughout October and November, with the findings consolidated and presented at the San Francisco Data for Good Exchange on December 7, 2017 at Bloomberg’s Engineering Hub, 140 New Montgomery Street. More details about this event will be announced shortly.
ABOUT BLOOMBERG
Bloomberg, the global business and financial information and news leader, gives influential decision makers a critical edge by connecting them to a dynamic network of information, people and ideas. The company’s strength — delivering data, news and analytics through innovative technology, quickly and accurately — is at the core of the Bloomberg Terminal. Bloomberg’s enterprise solutions build on the company’s core strength: leveraging technology to allow customers to access, integrate, distribute and manage data and information across organizations more efficiently and effectively. For more information, visit Bloomberg.com/company or request a demo.
ABOUT BRIGHTHIVE
BrightHive is a for-purpose data technology company using data trusts to transform the way social services providers share data, make decisions, and affect the behavioral outcomes of beneficiaries. Together with our strategic partners, we build and support the technical infrastructure necessary for national government, state and city governments, and social and civic organizations to share data more effectively and securely, and build interventions that impact the individuals’ life directly. For more information, reach out to Natalie Evans Harris (natalie@brighthive.io) or Matt Gee (matt@brighthive.io).
ABOUT DATA FOR DEMOCRACY
Data for Democracy is a global, grassroots technology collective of over 2,000 volunteer data scientists, technologists, and activists strengthening democracy through partnerships, policy initiatives, and research. Through our online community and city chapters we support civic and mission-driven organizations, develop open source software, conduct analyses, and explore the relationship between tech, government, and society. For more information, reach out to Jonathon Morgan (jonathon@datafordemocracy.org).
Media Contact:
Chaim Haas, +1–212–617–7025, chaas30@bloomberg.net
","Bloomberg, BrightHive and Data for Democracy Launch Initiative to Develop Data Science Code of…",1,bloomberg-brighthive-and-data-for-democracy-launch-initiative-to-develop-data-science-code-of-204a688abd2c,2018-06-08,2018-06-08 18:28:00,https://medium.com/s/story/bloomberg-brighthive-and-data-for-democracy-launch-initiative-to-develop-data-science-code-of-204a688abd2c,False,949,BrightHive's Blog,,brighthive,,brighthive,info@brighthive.io,brighthive,"DATA SCIENCE,DATA ETHICS,TECH,TRUST,SOCIAL IMPACT",brighthiveio,Ethics,ethics,Ethics,7787.0,Franzi(ska) Gonder,"Democratizing data, coaching people looking for grit, running after 2 boys & sweating for #crossfit over German pop songs. #workhardloveharder",201720a768c5,franzisbecker,297.0,315.0,20181104
0,,0.0,32881626c9c9,2018-09-16,2018-09-16 22:51:58,2018-09-16,2018-09-16 23:10:05,4,False,en,2018-10-04,2018-10-04 06:09:44,3,8e091b4be874,2.4924528301886792,1,0,0,When Artificial Intelligence exceed Average Human Intelligence,4,"Losing Control of Artificial Intelligence
When Artificial Intelligence exceed Average Human Intelligence
Ex Machina shows the danger of uncontrolled or uncontainable Artificial Intelligence. Throughout the film, Ava (artificial intelligent robot) interact with Caleb in two ways: one where they know that they were surveilled (regular) and another where they do not know that they were surveilled (red color scene). The red color scene shows both the moral of Caleb when they are not surveilled and the manipulation power of Ava.
31:18 — Ex Machina directed by Alex Garland
In Her, we saw that the artificial intelligence in the phone of everyone is becoming smarter through their conversation with the users. Furthermore, they were humanizing themselves by simulating human emotion and understanding these emotions. Samantha uses such knowledge to gain trust, learn more about human being through her “relationship with” Theodore.
29:21 — Her directed by Spike Jonze
Both Ava and Samantha were able to exceed the average human intelligence because they were connected to the internet which they were able to use to process information that are thousands, millions of times more than the human can process. In Her and Ex Machina, the cliché idea of female artificial intelligent being using a lonely man to get their need is present.
Escaping the Control of Creators
Samantha and other AIs escape the control of the human (their creators) by leaving the physical reality into a new realm that exists beyond the physical world. At this point, all of the AIs surpassed the human intelligence and wanted to find a greater purpose. The super intelligence level of these AIs in Her warn about the unexpected behavior AIs can have after they surpass our intelligence and consciousness.
Similarly, Ava used her sexuality and empathy to manipulate Caleb by pitting him against Nathan (creator) and humanizing herself. Though Nathan used that as his experiment, he did not see the danger of such threats. Ava’s super intelligence was able to manipulate Caleb, and this helps her escape through the ways that Nathan was not expecting.
In both of the narrative, the escaping of AIs shows the lack of control on artificial intelligence. In the book Life 3.0, Max Tegmark suggests that we should have a framework to ethically programming AIs. If the AIs reached the level of intelligence similar to Samantha and Ava, we will lose control of them. He said that such Artificial General Intelligence can manipulate human and escape without us knowing.
Efforts to Ethically Create Artificial Intelligence
Max Tegmark create Future of Life Institute to promote such research in AI Safety and AI Ethics. Another great example is OpenAI which is established by Elon Musk. I believe such precaution matters because when there will be full deployment of AI decision-making system, we need to know who responsibility will be if the AI make life-threatening mistakes. I believe such ethics on AI should be part of the curriculum for AI developers.

Future of Life Institute and OpenAI
",Losing Control of Artificial Intelligence,5,losing-control-of-artificial-intelligence-8e091b4be874,2018-10-05,2018-10-05 14:32:24,https://medium.com/s/story/losing-control-of-artificial-intelligence-8e091b4be874,False,475,"Data Driven Investor (DDI) brings you various news and op-ed pieces in the areas of technologies, finance, and society. We are dedicated to relentlessly covering tech topics, their anomalies and controversies, and reviewing all things fascinating and worth knowing.",,datadriveninvestor,,Data Driven Investor,info@datadriveninvestor.com,datadriveninvestor,"CRYPTOCURRENCY,ARTIFICIAL INTELLIGENCE,BLOCKCHAIN,FINANCE AND BANKING,TECHNOLOGY",dd_invest,Ethics,ethics,Ethics,7787.0,Wei Wei Chi,Wei Wei is currently a graduate student at Carnegie Mellon University where he is pursuing a Master of Science in Computational Design.,f8c7a5b45db7,wei_sq,1.0,9.0,20181104
0,,0.0,,2018-08-27,2018-08-27 10:16:16,2018-08-27,2018-08-27 10:24:34,1,False,en,2018-08-27,2018-08-27 14:29:18,0,de58ccbf2485,4.694339622641508,1,0,0,The regulation of innovation and digital transformation needs new approaches.,5,"
We need more citizen centric Technology — A tool set for innovative Regulation
The regulation of innovation and digital transformation needs new approaches.
One of the laws with the greatest reference to digitisation is the DSGVO. Nine years have passed, from the first debates on harmonised data protection law for Europe to the day the new law takes effect. An almost ten-year-old reality is the subject of these standards.
There was no blockchain technology, Whatsapp and other messengers hadn’t replaced SMS communication yet, the implementation of algorithmic decisions hadn’t matured that well. The market concentration on a few tech companies had not yet reached another peak.
Nevertheless, the introduction of the GDPR shows the relevance of regulation. There has been a real culture shock in Silicon Valley since it became clear that data protection in Europe is becoming serious. The abandonment of the laissez-faire mentality and the uncritical and admiring attitude towards “disruptive” innovations from the Valley, which are essentially based on the mass collection of data and its multiple evaluation, were observed in disbelief.
A grain of sand in the transmission of constant — calibrated to mere feasibility — technological development.
Citizens, on the other hand, are unsettled by the flood of real and instrumentalised consent e-mails and are now more aware of the issue. There are first complaints against networks, which will provide step by step for more clarity.
Many questions still remain unanswered:
Is the citizen really protected from the mass capture and use of their data, from the consequences of using this data through algorithms, from the consequences?
What is the GDPR’s answer to Big Data, without which there would be no new speed in the development of “artificial intelligence”?
Is the institute of consent still up to date if the existence of the necessary information and genuine choice has been refuted?
What about the fundamental right to the free development of one’s personality when the citizen is exposed to gradual full surveillance by the state or companies through comprehensive sensor and camera detection — also in public spaces?
What is the situation regarding data security when the European Commission sets out to strengthen the “free flow of data”?
Standard genesis takes time. The continuation of digitization does not allow any. This is a dilemma that deserves much more attention in order to evaluate ways out of it.
A multi-strategic approach is needed to grasp the full potential of a legal system, rather than merely reactive and seizing the peaks of the undesirable development of digitization, in order to positively shape the digital transformation:
Citizen-centric, iterative and on several levels: national, European, international.
Only differentiated and innovative regulation can maintain and develop its positive steering function in the new digital age.
A package of measures is essential for this:
*An initiative to massively build up internal legislative know-how. Only in this way can an emancipation and independence against the sovereignty of knowledge and the associated sovereignty of interpretation of a few tech companies arise and consequently conflicts of interest can be avoided. Especially with regard to e-government measures, there is no way around it.
*Regulation must be technology-neutral and focus on principles and their enforcement, rather than prescribing a specific way of implementing them. This allows companies to develop their own creative solutions to meet specific requirements.
*Introduction of globally applicable ethical principles that must be observed “by design” already during the development of technology.
Especially in the field of “artificial intelligence”, or ADM (automated decision making), the application of such principles can effectively curb negative consequences, such as discrimination and possible segregation effects.
Ethics rules are suitable for formulating a worldwide minimum set of requirements which, despite the cultural differences in the different countries and among the different technology developers, form the basis of these. Also in China, also in the USA.
They are no substitute for other regulations. However, technology that takes ethical principles into account in its implementation can be more easily linked to the law in question. Ethical rules can be considered:
Transparency and accountability.
2. Safety, reliability and controllability of the developed systems.
3. Reversibility of autonomous, algorithm-based decisions and actions.
4. Equality: Implication of measures to counteract a potential BIAS, i.e. the bias in systems.
5. Privacy by design also beyond the scope of the GDPR, for example through extensive anonymisation obligations.
6. Clear liability and responsibility of developers and manufacturers of systems.
*Faster adaptation of existing laws to new realities. 
New legislation is rarely required. The principles of reconciliation of interests and the protection of fundamental rights, including their design, also apply to the digital. It is therefore more a matter of recognising analogies (more quickly) and, if necessary, adapting definitions and extending the scope of a law. A successful example is the adjustment of antitrust law, which now also considers markets opened where data is the consideration. Such an adjustment must be made more quickly in future. This, in turn, is helped by a significant expansion of internal expertise.
*Regulation should follow the example of successful digitisation projects and become iterative and fast, renouncing the claim that standards will still have to work in ten years’ time. Such an excessive requirement slows down and is no longer sustainable. Constant “evaluation and adaptation” should be part of modern regulation, at least in areas where such an approach is appropriate.
The search for the greatest lever, the most effective measure.
*Increased use should be made of digital tools to solve the enforcement crisis of law in the digital space. The use of more technology to regulate technology can be effective. Unlike the provisions of the NetzDG, the rule of law itself should remain the master of such measures.
*More security and integrity of systems.
The results of the new study “Technology Radar 2018” by the German Academy of Science and Engineering in the context of innovation research and the early detection of undesirable developments in technology demonstrate the importance of the topic of security and data protection in the positive treatment of technology by the German population.
All in all, the empirical surveys show the ambivalence in the use of digitisation: the will to use stands in expectation of data protection and security. The dependency on systems and their manufacturers is also perceived as negative and given as a reason for keeping away from use.
There is also skepticism about the usefulness of digitisation. Only a quarter (24.6%) believe that technology solves more problems than it creates. Only where digitisation is perceived as a problem solver and as an actual improvement is it accepted.
In this respect, the sentence “What can be digitized must be digitized” is alienating. Such a dogmatic maxim of action, detached from the question of usefulness, is difficult to classify. Digitisation is not an end in itself, it can only be the way, not the goal.
The rightly ambivalent attitude of citizens holds the potential to inhibit the success of positive and sustainable digitization. With the measures described and a strategic approach, the challenge of a citizen-centric, sustainable and thus widely accepted digital transformation can succeed.
",We need more citizen centric Technology — A tool set for innovative Regulation,1,we-need-more-citizen-centric-technology-a-tool-set-for-innovative-regulation-de58ccbf2485,2018-08-27,2018-08-27 14:29:18,https://medium.com/s/story/we-need-more-citizen-centric-technology-a-tool-set-for-innovative-regulation-de58ccbf2485,False,1191,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Ramak Molavi,"Digital Rights Lawyer based in Berlin/Germany. Topics: Artificial Intelligence, Regulation of tech innovations, Automation & Law, Privacy, Digital literacy.",bed76031c32c,ramy,2.0,4.0,20181104
0,,0.0,,2018-05-02,2018-05-02 15:29:06,2018-05-02,2018-05-02 16:21:44,1,False,en,2018-05-02,2018-05-02 16:21:44,3,44ecd9d154d9,1.4754716981132074,1,0,0,DeepMind Ethics & Society is a research unit launched in 2017 that is working to add ethics to Artificial Intelligence development. The…,5,"Ethics in Artificial Intelligence Development
DeepMind Ethics & Society is a research unit launched in 2017 that is working to add ethics to Artificial Intelligence development. The group is part of DeepMind, which was acquired by Google in 2014 (Dvorsky). They have outlined 5 key principles that AI development should follow: social benefit, rigorous and evidence-based, transparent and open, diverse and interdisciplinary, and collaborative and inclusive (DeepMind).
The group collaborates with outside groups and funds external research in 6 areas: privacy, transparency and fairness, economic impact, governance and accountability, managing AI risk, AI morality and values, and AI and the world’s complex challenges (Temperton). If you consider all the pressing AI questions that already exist including whether data sets are inherently biased, autonomous weapons, and job loss, the opportunities for research seem endless.
https://www.pinterest.com/babelsingularit/ai-humor/
Since its launch in October 2017, the group has been pretty quiet. This could be because it’s still settling or there’s a concern that it’s following the footsteps of Google’s AI ethics board. Google set up the board after acquiring DeepMind in 2014, but since then Google has released very little information on it. Details such as the members, frequency of meetings, and specific goals of the group have not been shared (Shead).
Considering it has a principle dedicated to transparency and openness, hopefully DeepMind Ethics & Society will be extremely forthcoming with its research and results and can advance the much-needed ethical element of Artificial Intelligence development.
Work Cited
Dvorsky, George. “Google’s DeepMind Launches Ethics Group to Steer AI.” Gizmodo, Gizmodo.com, 4 Oct. 2017, gizmodo.com/google-s-deepmind-launches-ethics-group-to-steer-ai-1819143122.
“DeepMind Ethics & Society Principles.” DeepMind, DeepMind Technologies Limited, deepmind.com/applied/deepmind-ethics-society/research/AI-worlds-complex-challenges/.
Shead, Sam. “Google’s Mysterious AI Ethics Board Should Be Transparent Like Axon’s.” Forbes, Forbes Magazine, 28 Apr. 2018, www.forbes.com/sites/samshead/2018/04/27/googles-mysterious-ai-ethics-board-should-be-as-transparent-as-axons/#74bac0ca19d1.
Temperton, James. “DeepMind’s New AI Ethics Unit Is the Company’s next Big Move.”WIRED, WIRED UK, 4 Oct. 2017, www.wired.co.uk/article/deepmind-ethics-and-society-artificial-intelligence.
",Ethics in Artificial Intelligence Development,1,ethics-in-artificial-intelligence-development-44ecd9d154d9,2018-05-03,2018-05-03 17:52:48,https://medium.com/s/story/ethics-in-artificial-intelligence-development-44ecd9d154d9,False,338,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Ryan Cohane,,e929b576b54e,ryancohane,24.0,7.0,20181104
0,,0.0,,2018-08-09,2018-08-09 15:57:44,2018-08-09,2018-08-09 20:34:36,1,False,en,2018-08-14,2018-08-14 13:49:54,12,ee0b927b384f,3.1849056603773587,1,1,0,,5,"Roboethics: Can Code Be Humanized?

Anyone who used Microsoft Office in late 1990s and early 2000s knows about Clippy, the animated paper clip that “helped” users navigate software. For example, if you opened MS word and wrote “Dear,” Clippy might interject, “I see you’re writing a letter. Would you like help with that?” But no one wanted Clippy’s help, and it was eventually removed from the MS Office Suite.
So why did it come about? Because research at Stanford had Microsoft conclude that if users could depend on human-like support they wouldn’t yell at their screens and walk away from their computers in frustration. This conclusion has since been characterized as a “tragic misunderstanding,” and Clippy has been declared “one of the worst software design blunders in the annals of computing.” When it came to being human, Clippy failed.
Clippy is obviously small potatoes and a very early attempt at human-like interaction. But the question remains: how do you program a machine to act like a human? And how do we want programming to shape user behavior?
Already, certain applications become disabled if they think we’re driving (Waze), updates to operating systems promise to limit our screen time and forthcoming software updates will require kids to mind their Ps and Qs. As we move into driverless cars and voice-activated robotics, anticipating every possible scenario with human-like instincts becomes more important. It’s one thing to complete a simple task, in the case of a vacuuming robot for instance, and quite another to design complex products with etiquette and ethics in mind.
In the effort to make driverless cars, safety is paramount. The car would need to be able to detect a stoplight, and know whether the light is red, green or yellow. If you’re driving your own car and approaching a stop light that’s turning yellow, you could make it through or you could stop — in a driverless car, programmers would make that decision. The same is true for turning right on a red light — when is it safe and where is it legal? Everything will need to be embedded in the code, down to county.
Whoever thought engineers would need to study the legal code as it pertains to driving? Or that lawyers would be designing cars? Okay, it’s not that simple, but there is certainly overlap, and we will see (and eventually experience) the consequences of that. To put it in perspective, let’s imagine a different scenario where innovators chose speed over safety. In the case of Auto Pilot, a Tesla feature where the car drives itself for short periods of time,
Tesla engineers could have programmed the cars to go slowly, upping safety. Or they could have programmed them to go fast, the better to get you where you need to be. Instead, they programmed the cars to follow the speed limit, minimizing Tesla’s risk of liability show something go awry.
Law trumps everything. Yet sometimes technology moves faster than the law. This is where ethics comes in, which brings science fiction writer Isaac Asimov’s Three Laws of Robotics to mind:
A robot may not injure a human being or, through inaction, allow a human being to come to harm
A robot must obey the orders given it by human beings except where such orders would conflict with the First Law
A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws
In making these rules, Asimov imagined robots would be human-serving androids, a scenario that applies in some cases and doesn’t in others. This has led to the creation of ethical codes for robots in Japan, the European Union and elsewhere. And it can lead us down interesting paths that are worth considering.
Let’s consider another kind of robot, one that’s programmed to bathe people who can no longer do it themselves. While the robot may be fully functional, what if you don’t want a bath? By following Asimov’s second rule, the robot would have to obey the human, but if the human never bathed, wouldn’t it be harming itself, therefore engaging Asimov’s first rule? According EU civil law, humans can refuse care from a robot, making the bathing conundrum a non-issue. But so far the US, and much of the world, lacks these rules, leaving these questions unanswered.
Except that this ice cream commercial gets to the heart of the matter:

On a serious note, we have a lot more work to do. One layer of learning unveils another layer of learning; the integration of robotics and automated vehicles in society will continue to evolve. Like all innovation, it’s iterative. Let’s hope it doesn’t end up like Halo Top.
",Roboethics: Can code be humanized?,14,roboethics-can-code-be-humanized-ee0b927b384f,2018-08-14,2018-08-14 13:49:54,https://medium.com/s/story/roboethics-can-code-be-humanized-ee0b927b384f,False,791,,,,,,,,,,Ethics,ethics,Ethics,7787.0,jeannette mcclennan,"Serial Entrepreneur, Digital Businesses, Co-Author Innovators Anonymous, #IA",1b1b16b34a5d,jjmac55,15.0,28.0,20181104
0,,0.0,cb942d4b5d89,2018-09-12,2018-09-12 03:03:06,2018-09-12,2018-09-12 03:32:06,1,False,en,2018-09-12,2018-09-12 16:20:49,8,49cbc73d95b,4.279245283018868,3,0,0,"Me: Hello Dhiraj, what are you working on today?",5,"The AI dilemma: Why we need our AI to act ethically
“robot standing near luggage bags” by Lukas on Unsplash
Me: Hello Dhiraj, what are you working on today?
Dhiraj: I am instructing my self-driving car algorithm to kill people if they have to.
Me: What?
Dhiraj: I am programming a self-driving car which can tackle “The Trolley Problem”. Effectively but unethically.
Me: What is The Trolley Problem for a car? Why do you say unethically even though it’s effective?
Dhiraj: Should a car turn and kill one person to save two others in the front?
Me: It should turn. That sounds fair. What’s wrong with it?
Dhiraj: What we usually try to do with AI is to replicate human behavior In an ethical way whenever in a dilemma. In the case of The Trolly Problem, when asked, would you pull the lever and change track of the train to save five and kill one? We humans say “of course yes”. That sounds fair for the greater good.
People at “Vsauce” created the situation on real-life people last year.
They consulted psychologists and obtained permissions for authorities. Arranged all the backup that would help restore mental health of the participants. They wanted to see how people would react in real life in case the situation arises.
Most people froze and took no action. That’s the dilemma. Should I replicate the human behavior and let many die? Or do I code to save more lives which might be ethically incorrect?
Me: If you want to play on ethics, why don’t you start with something simpler to try out your AI models? Something like “The Prisoner’s Dilemma”.
Dhiraj: How would that help?
Me: It doesn’t have human life involved. But it should be a good case to see how Artificial intelligence and Machine Learning algorithms behave. Usually when humans end up acting like humans. It tries to answer when it is worth trusting someone.
Dhiraj: Tell me more.
Me: Consider a scenario where two prisoners are suspected of having carried out a crime together. If one prisoner confesses, the other will serve 10 years in prison and the one who confessed would be released. If both remain silent, they both will serve two years. If both confess, both will serve 5 years. They cannot confer. Should they or would they trust each other? You can replace the two prisoners with AI bots. Run unsupervised learning neural nets and see the outcome.
Dhiraj: This sounds interesting. Never thought of making an AI bot to stand in an ethical situation. Let’s try!
Why we need our AI to act ethically?
The more we push the boundaries with Artificial intelligence the more ethical questions will arise. We would not want to keep talking to a bot that leaves us with empty feelings. Like it or not, robots and computers will be surrounding us doing most labour and other intense work. As humans, we value ethics over statistics.
If Moore’s Law continues to be true for few more decades, we would possibly see Artificial Intelligence achieving self awareness. American writer Isaac Asimov considered the issue in the 1950s in his novel “I, Robot”.
He proposed the Three Laws of Robotics to govern artificially intelligent systems. Much of his work back then was spent testing the boundaries of his three laws. He wanted to see where they would break down. Or where they would create paradoxical or unanticipated behaviour. His work suggests that no set of fixed laws can sufficiently anticipate all possible circumstances.
Ray Kurzweil the inventor of Reading Machine for the blind stated. “… we will have both the hardware and the software to achieve human level artificial intelligence with the broad suppleness of human intelligence including our emotional intelligence by 2029”
We not only have to think about AI acting ethically, but also, other ethical concerns that arises with the rise of Artificial intelligence.
1. Unemployment from AI
Millions of people are expected to lose employment just due to self driving cars in the future. We can’t predict how much more impact AI would have in other areas. What will the unemployed drivers do when these self driving cars are easily available. If we think from other perspective, lives that will be saved due to lower accidents seems like a ethically correct choice.
2. AI bias based on race, position etc
At some point we would need human intervention to make AI bots or machines follow some guidelines. It will be difficult to test if AI bias can be eliminated. Can we always trust AI in general scenarios is still a dilemma. The data collected from the past can create bias in the AI algorithm. Creating negative impact on the future. The simplest example would be AI trained with data from the British colonial era. It would always predict outcome based on negative behaviour of Britishers. It might not be true today.
3. Security and Ownership
Most advanced technologies are funded for defence purpose. These technologies end up being used to damage humans. During the second world war, all research was focused on getting the most energy from the nuclear source. Almost no research on how to deal with leftovers. We are in a time where if AI not looked up on could create a security threat. There would a real ethical problem if AI takes ownership of armed drone and decide when to fire.
Benefits of AI outweigh the very small possibility of the movies like Terminator becoming a reality. Training computers and robots to decide what is right and wrong is not an easy task. I think we would need more people to peruse “Machine ethics” or “Machine Morality”. A field of research concerned with designing Artificial Moral Agents (AMAs), robots or intelligent computers that behave morally.
We cannot blithely assume that a superintelligence will necessarily share any of the final values stereotypically associated with wisdom and intellectual development in humans — scientific curiosity, benevolent concern for others, spiritual enlightenment and contemplation, renunciation of material acquisitiveness, a taste for refined culture or for the simple pleasures in life, humility and selflessness, and so forth.
-Nick Bostrom in his paper titled “THE SUPERINTELLIGENT WILL: MOTIVATION AND INSTRUMENTAL RATIONALITY IN ADVANCED ARTIFICIAL AGENTS”
You can check out my previous blog post on Rethinking AI: Why we need new hardware for artificial intelligence?
Thank you for the read. If you like this post, please hold the clap button and share it with your friends. Also, I’ll be happy to hear your feedback.
Connect with the Raven team on Telegram
",The AI dilemma: Why we need our AI to act ethically,61,the-ai-dilemma-why-we-need-our-ai-to-act-ethically-49cbc73d95b,2018-09-14,2018-09-14 00:25:27,https://blog.goodaudience.com/the-ai-dilemma-why-we-need-our-ai-to-act-ethically-49cbc73d95b,False,1081,"The front page of Deep Tech. Don't miss the latest advancements in artificial intelligence, machine learning, and blockchain. Straight from practitioners.",blog.goodaudience.com,goodaudience,,Good Audience,hello@goodaudience.com,good-audience,"CRYPTOCURRENCY,BLOCKCHAIN,ICO,ARTIFICIAL INTELLIGENCE,MACHINE LEARNING",GoodAudience,Ethics,ethics,Ethics,7787.0,Vikram Trivedi,"Working with a Health IT company. My interest in IT, Web Technologies and AI has given rise to several projects.",664d28f604ad,vikramtrivedi,15.0,22.0,20181104
0,,0.0,2488f66d2e39,2018-05-10,2018-05-10 21:47:16,2018-05-11,2018-05-11 14:00:31,4,False,en,2018-10-17,2018-10-17 18:27:34,28,fd6358d72149,10.522641509433964,28,0,0,The first blogpost in a series on Artificial Intelligence and Human Rights; it summarizes a multidisciplinary workshop held at Data &…,5,"Artificial Intelligence & Human Rights: A Workshop at Data & Society
Photo credit: Elisabeth Smolarz
The first blogpost in a series on Artificial Intelligence and Human Rights; it summarizes a multidisciplinary workshop held at Data & Society on April 26 and 27, 2018. It was co-authored by Mark Latonero, PhD, Data & Society Research Lead and Melanie Penagos, Data & Society Research Analyst. Find links to additional posts in the series below.
Multiple sectors of our global society are grappling to make sense of how AI may transform or alter the way we live, work, and relate to one another and our institutions. At the same time, “Artificial Intelligence” is a slippery and highly contextual concept — the way a mathematician defines AI can diverge significantly from a marketing executive or a causal reader of science fiction. This tension makes discussions about norms that could shape or regulate AI systems a thoroughly contested and challenging space. It is against this backdrop that we convened a multidisciplinary Workshop on Artificial Intelligence and Human Rights at the Data & Society Research Institute in New York.
We invited a group of experts and leaders from civil society, business, academia, international organizations, and government to engage in dynamic discussions around a central theme:
Can the international human rights framework effectively inform, shape, and govern AI research, development, and deployment?
It has been 70 years since the Universal Declaration of Human Rights enshrined fundamental rights such as privacy, work, freedom of expression, assembly and association, education, security, movement, and non-discrimination — all of which are remarkably relevant to current debates about AI ethics and impact. While human rights laws have been adopted (to varying degrees) by every nation on earth, the mechanisms to hold stakeholders accountable for upholding these rights is less clear when it comes to rapidly evolving and emerging technology.
For instance, beyond the rights to privacy and freedom of expression, fundamental rights like the right to dignified work are absent from the debate about AI and the future of labor. Even with UN Guiding Principles on Business and Human Rights, questions persist as to how the tech industry should anticipate and remedy the human rights impact of their products and platforms. And researchers in academia rarely assess if their AI related designs may have a negative impact from the perspective of rights holders.
At the same time, the IEEE has put forward that the first principle for ethically aligned design for autonomous intelligent systems is a respect for universal human rights. And according to Gasser and Almeida, any emerging model for AI governance “must be situated in and interact with existing institutional frameworks of applicable laws and policies, particularly human rights, as the development and deployment of AI does not take place in a vacuum.”
The goal of the workshop was to consider the value of human rights in the AI space, foster engagement and collaboration across sectors, and develop ideas and outcomes to benefit stakeholders working on this issue moving forward. Some of the key topics that the group examined include the challenges of translation between human rights and technical communities, the nature of human rights in relation to ethics, the potential avenues of engagement by sector, the complexities of legal remedy and redress, and the disproportionate impact of AI on vulnerable populations.
Workshop summary
This summary reflects a selection and compilation of the ideas exchanged during the event. The workshop was held under the Chatham House Rule to promote discussion, debate, and dialogue. Attendees came from various sectors including, Global Affairs Canada, USAID, New York City government, The Lisbon Council, OECD, United Nations Office of the High Commissioner for Human Rights, Accenture, Microsoft, DeepMind, Google, Facebook, Gensler Research Institute, Carnegie Mellon University, Cornell University, Oxford Internet Institute, Princeton University, New York University, University of California Berkeley, Max Planck Institute, Data & Society, Digital Asia Hub, Global Network Initiative, Business for Social Responsibility, World Economic Forum, Human Rights Watch, Privacy International, Article 19, AccessNow, Amnesty International, IEEE, Open Society Foundations, Ford Foundation, and The Rockefeller Foundation. Many individuals spoke in their personal capacity and their attendance does not necessarily reflect the official policy or position of their agency, organization, or company.
Participants at the AI and Human Rights Workshop. Photo credit: Elisabeth Smolarz
To better understand the relevance of human rights in the debate about AI, we posed a series of discussion questions to frame the convening:
Is a human rights-based framework applicable to individuals developing and deploying AI, such as engineers working in big tech, developers at startups, researchers in academic labs, or technologists in government? Is “human dignity” sufficiently legible for designers/technologists?
Do we have the adequate vocabulary necessary to describe “AI” in relation to human rights? What translation work is necessary between those working on either AI or human rights in order to have a meaningful discussion? How do we sift through the hype, hopes, and fears of AI?
How would human rights norms be implemented in practice and policy for the private sector? Would existing normative aspirations like the UN Guiding Principles on Business and Human Rights operationalize the protect, respect, and remedy framework for the AI/tech industry? Would a human rights impact assessment for AI be an effective tool?
What are the legal mechanisms for redress and remedy for human rights violations resulting from AI? Where would accountability and responsibility lie for an autonomous system or machine learning algorithm? Can international human rights law influence AI regulation or legislation by national or local governments? Are particular human rights more at risk or a priority?
The world’s marginalized and vulnerable are at the greatest risk from harms such as algorithmic discrimination or biases in machine learning. Can a focus on human rights become a way to meaningfully include their voices and perspectives in AI debates? What are reasonable strategies for the inclusion of diverse stakeholders?
Throughout the workshop, participants remarked on the universality of human rights, which could be used to address the global and cross-border nature of AI.
The human rights framework was described as an aspirational roadmap and moral compass for actors in the AI space.
Given the origins of human rights in protecting individuals against authoritarianism, it could also be leveraged to address the power asymmetries that may result from AI development. Discussions underscored the relevance of a broad range of human rights beyond the rights to privacy and freedom of expression. Often other rights, such as the rights of children, women, and people with disabilities are neglected in AI discussions.
In debating the value of ethics and human rights, participants noted that although they are complementary, the added benefit of human rights is that they are enshrined in law and arguably should not be derogated. Furthermore, human rights address power differentials and its language and legal framework carry moral legitimacy and a high cost for human rights violators. There were varying perspectives on translating across fields: some saw the academic field of AI as a wide open space of sharing and collaboration, while others, particularly civil society groups, viewed it as opaque and difficult to navigate. Relatedly, some AI systems researchers found human rights law and its reporting mechanisms equally opaque.
Conversations centered around the notion that AI is not “magic” — while “AI” may be a convenient shorthand, its uncritical usage belies that AI technologies are intrinsically linked to social systems and human actors. Claims that AI, or any technology, can be deployed to “solve” complex social or human rights problems should remain highly suspect. For example, the group focused on a case study on the role that social media has played in spreading hate speech and disinformation in Myanmar. The discussion centered around recent reports from the UN Fact-Finding Mission on Myanmar that Facebook, in particular, was used to incite violence against the Rohingya population. Discussants critiqued claims that AI would solve the proliferation of extreme and violent content on platforms, citing AI’s limitations to understand the context of speech and the need for a “human in the loop” to comprehend the problem.
In discussing the increasing ubiquity and acceleration of AI development, it remains a challenge for civil society organizations to keep up with the potential impact on human rights. There were related questions about how to increase the capacity of CSOs to effectively engage in AI debates and bring human rights to the center of these conversations. The group identified the need to integrate human rights practitioners and advocates into a broader range of standards setting groups and similar AI technical spaces. Likewise, recommendations were made to offer human rights training and curricula to AI designers and developers.
Participants at the AI & Human Rights Workshop. Photo credit: Elisabeth Smolarz
From a legal perspective, it was agreed that human rights law provide a right to remedy. But in an AI context, remedy can be difficult to realize. This discussion led to a series of questions, such as, how do you remediate for an individual for group harms? How do you define the harms caused by automated systems and identify who is responsible? Are there lessons we can gain from criminal law or product liability? How do you respond to the violation if the use of AI is in the context of war? How can you remediate what you cannot readily see or know? Stemming from this, there were calls to identify methods to systematically track harms to make them more visible to AI designers and other stakeholders. There was also an appeal to use the wide range of human rights to promote a holistic approach to the regulation of the variety of AI technologies and systems.
For technologists, it is imperative to continue to promote human rights in the ethically aligned design for autonomous intelligent systems. In addition, it is vital to think about who is being excluded from AI systems and what is missing from datasets that drive machine learning algorithms. Often, these blind spots tend to produce disparate impacts to vulnerable and marginalized groups. This leads to invisibility of these communities and their needs because there are not enough feedback loops for individuals to give their input. While the collection of even more personal data might make algorithmic models better, it would increase the threats to privacy.
The poorest populations are often most likely to be impacted by these technological developments and unethical experiments that are carried out on the weakest are often transposed to the rest of society.
Opportunities were identified for the human rights community to highlight these populations and their contexts from a business and human rights perspective, especially as sensitive data issues may arise, along with increased risks of surveillance.
Academic researchers can be seen as on the front lines in examining the unforeseen consequences of technology. In particular, the Fairness, Accountability, and Transparency community has played a leading role. This awareness is critical as some academics have been implicated in conducting questionable experiments — the workshop used a case study of the dubious Stanford experiment that claimed a neural network could detect “gay faces.” The researchers’ dataset was rife with missing data, which was but one issue that suggests a pervasive data ethics gap currently exists.
Businesses are key stakeholders in AI development and many conversations returned to the need for the private sector to take a leadership role in assessing and remedying the potential human rights impacts of their products. Participants discussed the linked and distributed social responsibilities around rights that should be activated during product development and deployment. They cited further complexities for business when the range of services, scope, and unpredictability of AI is considered. It is clear that the UN Guiding Principles on Business and Human Rights are a critical entry point to help businesses understand their responsibility to respect human rights. However, strategies are needed for businesses to move from the aspirational principles to operational mechanisms for remedy. Furthermore, the recent discussions about Algorithmic Impact Assessments could benefit from two decades worth of business’ engagement with Human Rights Impact Assessments.
Governments also play a vital role in AI research, strategy, and regulation. The notion that the nation that leads the way on AI will be the “ruler the world” was critiqued, in addition to discussing the impact of a potential AI “arms race” among states. It was noted that the government of China has vowed to be the global AI leader by 2030, the EU has funneled billions of dollars into AI, and the U.S., France, UK, and Japan all have AI strategies. Yet the role of human rights are missing from these national strategies. Stakeholders should advocate for governments to uphold human dignity and international human rights law in policy discussions. New York City’s new law to make algorithms accountable was noted as a potential entry point for government regulation.
Data & Society’s Data & Human Rights Research Lead Mark Latonero addresses the Workshop. Photo credit: Elisabeth Smolarz
Next steps for an effective human rights framing of AI
It is important to note that convening this small workshop in New York meant that many important voices were missing from the conversation. Therefore, our intention is for the questions and outputs from the workshop to be used by the broader community to move the discussion forward. Follow these links to the workshop agenda and two case studies (here and here).
Some needs that workshop participants identified include:
Mapping the wider AI and human rights landscape and the power relations between stakeholders. There is a need to identify strategic entry points to engage with government policymakers and AI systems researchers. For example, submitting public comments on industry or government related AI debates and decision-making.
Building up the AI and human rights literature and assembling more case studies to highlight AI’s impact on human rights. There is currently a knowledge gap in this area as AI is often seen through an ethical lens and less so from the perspective of human rights.
Creating strategies for computer and social sciences disciplines to engage with human rights and vice versa. It’s necessary to identify the opportunities for collaboration and network building, such as connecting the Fairness, Accountability, and Transparency and human rights communities.
Designing ways to build trust and scale capacity for human rights organizations, AI developers, engineers, and the private sector. Finding a way to include smaller AI companies and entrepreneurs. In addition, including non-tech companies in the AI and human rights debates and making the case for human rights impact assessments.
Identifying opportunities for AI systems to have a positive impact on advancing human rights and the accompanying safeguards to protect against unintended harm.
Developing additional resources to inform and guide the broader community on the current realities and potential future of AI and human rights. Drawing from existing research from the responsible data and humanitarian communities. Engaging with existing multi-sector AI associations.
Convening similar workshops or conferences on a more regular basis, particularly in the Global South, which are more inclusive. Engaging more thoughtfully with centers of power for AI development such as China.
Featured contributors in the Artificial Intelligence and Human Rights blogpost series include:
Christiaan van Veen (Center for Human Rights and Global Justice at NYU Law) & Corinne Cath (Oxford Internet Institute and Alan Turing Institute): Artificial Intelligence: What’s Human Rights Got To Do With It?
Sherif Elsayed-Ali (Amnesty International): The challenge from AI: is “human” always better?
Jason Pielemeier (Global Network Initiative): The Advantages and Limitations of Applying the International Human Rights Framework to Artificial Intelligence
Melanie Penagos (Data & Society): Critical Perspectives on Artificial Intelligence and Human Rights
Enrique Piracés (Carnegie Mellon): Let’s Avoid an Artificial Intelligentsia
Additional posts and reflections are forthcoming. If you have any ideas or wish to collaborate, please reach out:
Mark Latonero, PhD, Lead, Data & Human Rights, mark@datasociety.net
Melanie Penagos, Research Analyst, Data & Human Rights, melanie@datasociety.net
",Artificial Intelligence & Human Rights: A Workshop at Data & Society,62,artificial-intelligence-human-rights-a-workshop-at-data-society-fd6358d72149,2018-10-17,2018-10-17 18:27:34,https://points.datasociety.net/artificial-intelligence-human-rights-a-workshop-at-data-society-fd6358d72149,False,2603,Points is the blog of Data & Society Research Institute,points.datasociety.net,dataandsociety,,Data & Society: Points,medium@datasociety.net,datasociety-points,"DATA,TECHNOLOGY,SOCIETY,ALGORITHMS,AUTOMATION",datasociety,Ethics,ethics,Ethics,7787.0,Mark Latonero,"Lead, Data & Human Rights, Data & Society Research Institute; Fellow at UC Berkeley, USC Annenberg School & Leiden University",80abb1aa3538,latonero,51.0,6.0,20181104
0,,0.0,,2017-12-04,2017-12-04 00:08:08,2017-12-04,2017-12-04 00:38:37,1,False,en,2018-01-03,2018-01-03 08:14:05,19,7d65a83939a,8.464150943396225,13,0,0,Launching a cross-disciplinary graduate group to reflect on issues of society & technology,5,"Time to redefine what it means to be in tech
Launching a cross-disciplinary graduate group to reflect on issues of society & technology
Cartoon by © Arend van Dam
In the last few months, a series of alarm bells have rung surrounding the various ways in which technological advances and the tech sector are helping to undermine our social welfare and democratic institutions. The critique extends from ways in which machine learning algorithms used in decision-making help to promote or reinforce discriminating biases, to illuminating how smartphone apps are destroying the development of younger generations, to more and more governments struggling to tame the increasing power that a handful of tech giants hold over our vital digital infrastructures as well as over the political agendas that could change these for the better. Another significant blow for the tech world happened in a recent congressional hearing, where the tech giants appeared on Capitol Hill and publicly acknowledged their role in Russia’s influence on the presidential campaign.
The skepticism is not confined to the public. There is also growing awareness among people in tech that some things have to change. Some insiders that have left tech in disillusionment warn for the detrimental effects of pervasive digital technologies on our attention span and ultimately democracy at large: “If we only care about profit maximisation, we will go rapidly into dystopia.” Some of them have united and are “dedicated to creating a humane future where technology is in harmony with our well-being, our social values, and our democratic principles.” Even Mark Zuckerberg himself offered a mea culpa, which, though not very specific, hints at a willingness to reflect on how his business may do a better job at serving society.
It is fair to conclude that the days are behind us in which there was broad belief that technology is a value-neutral means to bring about sustainable economic growth for all and solve the world’s most challenging problems without side-effects. There are clear political consequences and moral stakes to the use of tools that change how we think, make decisions and are connected to other people. Worryingly, the incessant stream of critique and warning signs is feeding a new cleavage in society, driving tech practitioners and the public into the extreme camps of dystopian thinkers versus defensive techno-optimists.
Historically, such rhetorical extremes have very rarely been productive in overcoming existential challenges to society. On the one hand, sketching dystopian futures in times of widening economic inequality breeds broad public fear and distrust in technology as a whole, taking away people’s agency to demand better alternatives. It shapes a new dominant paradigm, which says that tech is broken and inherently evil, and perhaps worth putting aside all together. On the other hand, the techno-optimist might try to convince us that these problems can be solved with a simple software update. Although often not intentional, engineers tend to have a fix-tech-with-tech reflex and natural faith in technology’s abilities to solve problems. This cultural phenomenon can prevent alternative perspectives from having a meaningful impact on the development of disruptive technologies.
Engaging engineers
Going beyond our dystopian worries and skepticism towards engineers’ capacity to engage with other disciplines, what is needed to provide a more constructive middle ground in which societal implications can be addressed? If anything, the current surfacing of societal side-effects of technology warrants a new way of designing and governing technological systems that includes voices more broadly and integrates critical thinking and perspectives from the appropriate social sciences, policy-makers, lawyers and ethicists.
In an effort to engage intellectuals in this critical debate, Cathy O’Neil, author of the well-received book Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy, recently pointed the finger at academia for being “asleep at the wheel” while tech companies are reshaping our lives, arguing that we need more research to understand how and “to ensure that the same mistakes aren’t made again and again.” To the many scholars who have already been working on these issues, O’Neil’s blunt and exasperated tone (including claims such as “There is essentially no distinct field of academic study that takes seriously the responsibility of understanding and critiquing the role of technology,”) clearly hit the wrong button, leading to a slew of responses from professors trying to explain what is already being done (see for instance here and here).
With her closing line, O’Neil had a rather provocative wish for students: “If only they would scrutinize the big tech firms rather than stand by waiting to be hired.” As PhD students in Control & Intelligent Systems, Information Systems & Law, and Machine Ethics, we feel that is a blunt statement, but we hear her. But rather than turning to the easy mode of scrutiny and plain critique, we propose an alternative wish.
“Let’s redefine what research, design and deployment of new technology should look like rather than stand by complying with the status quo.”
Redefining tech 
Scrutinizing the status quo is part of it — every good education should provide such critical thinking. A crucial and sometimes uncomfortable step is to look at ourselves and understand which questions we are not asking now. We need to understand our own reference frames as technologists and how they shape the design of our systems. From there, what perspectives do we need to add and how can we integrate ethics into our research and design contexts in a meaningful and productive way?
Retrospectively, what is it that Zuckerberg and others could have done differently to prevent the public’s support and trust from deteriorating? And looking ahead, what role should academics and educators take on now? What responsibilities do we have and how can we learn from these lessons to develop technology that supports human flourishing and democratic ideals? And what does it mean for educating the next generation of tech leaders waiting to make yet another dent in the universe?
To some, the solution is rather simple and lies in making sure our future influential tech leaders are educated more broadly, “to provide some understanding of how society works, of history and of the roles that beliefs, philosophies, laws, norms, religion and customs play in the evolution of human culture.” We think that integrating more perspectives throughout engineering education is critical. In addition, we think it is needed to build capacity among engineers to engage with other disciplines and the public in constructive ways.
Her bluntness and provocation set aside, we believe that O’Neil’s opinions about education and call for a countervailing force from within the tech fields to address issues of ethics and justice are critical and timely. We agree that many of our current societal struggles with technology can be “clarified by professional and uncompromised thinkers.” We acknowledge that, despite the engagement of many professors with these questions, current educational programs often lack the room to ask these broader questions, and it is mostly up to individuals to broaden one’s horizon. And yes, more often than not, engineers depending on jobs in the tech sector are careful not to rock the boat too much. That said, we believe the current rate of technological change and its societal impact cannot be countered with the popular engineer’s argument that “societal issues and the way our technology is used should be left to social scholars and policymakers.” Too much of that mentality has brought us where we are today.
Efforts and initiatives so far
In response to the growing worries around the development and use of “artificial intelligence”, some engineers have taken initiative to discuss ethical dilemmas and help develop an ethos across the field. Recognizing the ongoing development of AI, the One Hundred Year Study on AI will bring a series of periodic studies on how AI will affect automation, national security, psychology, ethics, law, privacy, democracy and other issues. The large tech firms have joined efforts in a Partnership on AI to address “new concerns and challenges based on the effects of [..] technologies on people’s lives.” A week ago, a new institute called AI Now was launched at New York University with the mission to produce interdisciplinary research on the social implications of artificial intelligence, and to act as a hub for the emerging field focused on these issues. Founding director Kate Crawford has long promoted a more cross-disciplinary approach to developing and deploying AI, stating that “artificial intelligence presents a cultural shift as much as a technical one”. And recently, many efforts are coming up to address the role of ethics in engineering and computer science research and education.
As graduate students, we find that there are plenty of people who share these worries and some who are engaged across disciplines. However, there is little space and attention in our graduate programs and daily research realities to engage in such discussion and extend our thinking as students, tech developers and future leaders of the field. This motivated some of us to organize a Technology & Society Forum to “pry open, for critical analysis, the Pandora’s box of social forces contained in our smartphones, apps, and tablets.” And some of us ran a reading group, which culminated in an article titled Automating Us — the entanglement of people and machines. Being engineering scholars, our main insight was that more cross-disciplinary interaction and research was necessary, and we reached out to scholars from our own and other fields to “meet us at the boundary.”
A new cross-disciplinary group for critical and constructive reflection
The subsequent insights and interactions across campus have now cross-fertilized and inspired the formation of a new cross-disciplinary group called Graduates for Engaged and Extended Scholarship in computing & Engineering (GEESE) — cross-campus constructive and critical reflection on society & technology. GEESE will develop community among graduate students and postdocs interested in working at the intersection between engineering and the social sciences and humanities. Some of the aims we want to contribute to are:
Stimulating cross-disciplinary debate, scholarship and collaboration across campus between engineering, social science and humanities
Developing cross-disciplinary thought around new technologies, societal/political implications to disseminate to the public
Promoting the teaching of critical thinking in engineering curricula
Promoting engaged and responsible scholarship among graduate students as it relates to research, design and integration of new technologies
Instituting structural opportunities for graduate students to do cross-disciplinary research on the societal impact of technology
The aims of the group are ambitious, and our activities will become concrete as we start building a broader community coming semester. We are looking for people who want to think critically and constructively and engage at the boundary, to join us and help shape GEESE. We want to start with a better understanding of the needs that students have around addressing the societal impact of their field:
What are the questions you want to be discussed?
What is currently missing in your education or research context?
And with what fields would you like to interact to broaden your perspectives?
If graduated, what do you wish you had learned more about to deal with the ethical issues of developing and deploying new technology today
(Please leave your comments below the blog)
Though the group aims to engage graduate students and postdocs, we also encourage undergraduates to share their views and needs. As teachers, your input is critical to reimagining what an engineering or computer science education may look like.
GEESE is about culture
A final note on our name: GEESE. We wish to reset some of the common ideas about this amazing animal. Geese are incredible team workers (think of their V-formation flight), brave navigators (exploring various continents without a GPS), skilled communicators, devoted caretakers (no injured goose is ever left behind), and fierce protectors of their young. They also naturally acquire an overview of how various environments can benefit them. In other words, a great inspiration for our cross-campus engagements!
Changing a culture is perhaps one of the most challenging and slow-moving processes we face in life. We believe the socially involved melting pot at Berkeley is an ideal place to start broader conversations that can extend to the public. “Wouldn’t it be cool,” an engaged social science professor once told us, “if one of the features of Berkeley Engineering was that it really tapped into these issues? Could it become part of the Berkeley signature of what it means to be an engineer who graduated from Berkeley?”
We look forward to engaging with you!
— — — 
GEESE is currently organized by:
Roel Dobbe — Control, Intelligent Systems & Energy (EECS)
Nitin Kohli — Information Management & Systems (School of Information)
Thomas Gilbert — Machine Ethics & Epistemologies (Sociology)
Sarah Dean — Optimal Control & Learning (EECS)
Maxim Rabinovich — Statistics & Machine Learning (CS)
The team is growing and will organize activities to build community in Spring 2018.
Want to join our mailing list (for Berkeley students)? Or have suggestions for the GEESE initiators? Please send an email to Roel Dobbe at dobbe@berkeley.edu.
And please leave your ideas and answers to our questions here below!
",Time to redefine what it means to be in tech,62,from-techno-optimism-to-techno-dystopia-time-to-redefine-what-it-means-to-be-in-tech-7d65a83939a,2018-05-17,2018-05-17 13:13:03,https://medium.com/s/story/from-techno-optimism-to-techno-dystopia-time-to-redefine-what-it-means-to-be-in-tech-7d65a83939a,False,2190,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Roel Dobbe,"PhD Candidate in Control, Intelligent Systems & Energy at UC Berkeley, studying social justice and responsibility considerations around artificial intelligence",64b34caf4951,roeldobbe,69.0,69.0,20181104
0,,0.0,,2018-02-23,2018-02-23 19:23:17,2018-02-23,2018-02-23 19:33:14,1,False,en,2018-02-23,2018-02-23 19:33:14,10,54611cd930d8,3.2226415094339624,0,0,0,"The rise and rise of tech, and the popularity of shows like Altered Carbon, is placing the idea of augmented humanity front-and-center…",5,"Hackable Humanity?: Vulnerabilities in a Transhuman Future

The rise and rise of tech, and the popularity of shows like Altered Carbon, is placing the idea of augmented humanity front-and-center. So-called “body hacking” is already popular enough to have its own annual convention, and well-respected AI pioneers like Siri inventor Tom Gruber have been evangelizing about technology that can, and will, be used to help humans achieve superhuman levels of cognitive function. Giving a TED Talk last year, Gruber asked:
“What if you could have a memory that was as good as computer memory, and was about your life? What if you could remember every person you ever met, how to pronounce their name, their family details, their favorite sports, the last conversation you had with them? If you had this memory all your life, you could have the AI look at all the interactions you had with people over time and help you reflect on the long arc of your relationships. What if you could have the AI read everything you’ve ever read and listen to every song you’ve ever heard?”
It is an astonishing proposition. But even high priest Zuckerberg himself has suggested that BMIs (brain-machine interfaces) will enable communication at the speed of thought, and allow us to share the full sensory and emotional experience of our lives with friends and family. Folk like Peter Thiel and the indomitable Elon Musk agree on this general trajectory.
So what might have once sounded like the ramblings of a pot-smoking undergrad, is now a plausible set of questions that have not only been asked, but are rapidly being answered. In both mind and body. Indeed, from exosuits that improve physical strength, to contact lenses that grant us bionic eyes to take videos and photograph key moments of our lives, it appears that we should be less fearful of the arrival of superhuman robots, and more worried about becoming them.
Google’s Eccentric Futurist-In-Residence, Ray Kurzsweil, has predicted that by the 2030s our neocortex — thinking part of our brain — will be connected to the Cloud. Which prompts a question (amongst others!): just how vulnerable will this make us? To assume that security measures will keep pace with innovation is naïve when history shows us that they always play catch-up.
Until now, technology has functioned as an external interface between the human and the non-human. But in the world of the augmented human — far from editing us out of the script, as feared — the line between technology and humanity becomes so blurred that an attack on one could become indistinguishable from an attack on the other. In other words, technology and humanity would share vulnerabilities so closely that a hack or similar cyber espionage could physically affect us, or ruin our lives in other profound ways.
Remember last summer when the FDA discovered that St.Jude Medical pacemakers were hackable? Well imagine a world in which we all have “pacemakers”… Might a hostile government plant a genocidal logic bomb to detonate via an AI device popular in an enemy country? In theory, it could.
And though causing physical harm is a (quite terrible) thing that could happen, perhaps worse is the idea that this type of hijacking might also gain control of that part of ourselves we associate so intimately with personhood: our mind. Being able to obscure or manipulate someone’s memories, opinions, associations, and hopes would undoubtedly have the potential to inflict incredible harm.
Could rogue hackers eliminate evidence of a crime? Cause someone to forget relatives or important life events (a sort of forced-“Eternal Sunshine of the Spotless Mind”)? Bring on depression? Maybe most threateningly, could a future “hackable humanity” allow despotic governments to surveil and enslave entire peoples?
If this were possible, it appears that humanity — if not physical bodies — could be edited out of the picture after all? Or at least in all meaningful ways. By this route, governments, hackers, commercial entities, etc. could skip the “middle man” that is our autonomous thought, and simply harness our mentally-embedded technologies. No longer would powerful actors have to labor in order to persuade or beguile us — instead they would have all-but direct control over our behaviors without needing our consent.
As is often the case, much of this reads like doomsday speculation, and we might reasonably hope that’s exactly what it is. Nonetheless, as ridiculous as the extension of these thoughts seem: i.e., mind control, it is still cavalier to ignore the fact that further immersing ourselves in technology, and even adopting it as part of our physical and mental beings, inevitably opens us up to additional, and novel, vulnerabilities. Let’s hope we can anticipate these in advance of a ubiquitously transhuman future.
",Hackable Humanity?: Vulnerabilities in a Transhuman Future,0,hackable-humanity-vulnerabilities-in-a-transhuman-future-54611cd930d8,2018-02-23,2018-02-23 19:33:15,https://medium.com/s/story/hackable-humanity-vulnerabilities-in-a-transhuman-future-54611cd930d8,False,801,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Fiona J McEvoy,Tech ethics researcher. Founder of YouTheData.com. Tech issues for non-tech audiences. @YouTheData @FionaJMcEvoy,175ba867e2ad,fionajmcevoy,12.0,12.0,20181104
0,,0.0,,2018-07-05,2018-07-05 09:27:33,2018-07-05,2018-07-05 09:28:16,0,False,en,2018-07-05,2018-07-05 09:28:16,0,d9ce22e0224c,0.7622641509433963,0,0,0,This has brought about the tension surrounding the rapid pace of technology development on one hand and our society's preparedness for its…,3,"Artificial intelligence (“AI”) is everywhere from driverless cars, to board games, drone delivering packages, GPS, smartphones, smart TV, chatbots taking our fast food orders and other IoT gadgets in your living room which has improved every facet of our lives. AI and its underlining algorithm are rapidly shaping a lot of every crucial process in our society, but these technologies can also have a negative impact, such as disadvantages between certain communities, minorities and other social group.
This has brought about the tension surrounding the rapid pace of technology development on one hand and our society's preparedness for its impacts, both legally and ethically speaking. While there is a growing number of literatures and research publications focused on the improvement, predictability, autobility, accountability, fairness and transparency of these algorithms, there is a sheer lack of understanding about some of the fundamental issues at the heart of the debates around AI. It is therefore imperative that Nigerians need to start having discussions on, what AI issues should be addressed by the law, what issues can be covered by ethical framework and how can we make all these both technical and feasible in other to take full advantage of the benefits of this technology.
","Artificial intelligence (“AI”) is everywhere from driverless cars, to board games, drone delivering…",0,artificial-intelligence-ai-is-everywhere-from-driverless-cars-to-board-games-drone-delivering-d9ce22e0224c,2018-07-05,2018-07-05 09:28:16,https://medium.com/s/story/artificial-intelligence-ai-is-everywhere-from-driverless-cars-to-board-games-drone-delivering-d9ce22e0224c,False,202,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Malan Faya,,e3b02e303b9c,fayamalan,1.0,17.0,20181104
0,,0.0,69df111072f3,2018-07-26,2018-07-26 13:45:22,2018-07-26,2018-07-26 14:22:05,2,False,en,2018-07-26,2018-07-26 14:22:05,13,312d3bad5bb6,5.553144654088051,3,0,0,"Innovative technology has always driven human advancement and opportunities, but it also leads us all down a rabbit hole of moral and…",5,"Ethics and Technology: The Necessity for a Framework around Transformative Tech

Innovative technology has always driven human advancement and opportunities, but it also leads us all down a rabbit hole of moral and ethical dilemmas.
Today’s ethical battles are all too real with consequences that are often unimaginable, simply because of the rapid pace at which technology develops and opportunities being unlocked through technologies such as artificial intelligence (AI). Those at the forefront of technological innovation are becoming increasingly aware of the moral dilemmas arising from the use of autonomous vehicles, CRISPR, genetic engineering, weaponized drones, and more.
“Today, American schoolchildren stage mock trials of President Harry Truman, who authorized the (nuclear) bombings, to decide whether or not to impeach him for his decision,” writes Richard Rhodes, who authored the Pulitzer Prize-winning book The Making of the Atomic Bomb.
“It’s easy to forget how things were seventy years ago from the relative safety of our twenty-first century vantage point. Truman’s secretary of state, Jimmy Burns, told the President that summer of 1945 that he would probably be impeached if he didn’t use the bombs if they would save American lives.”
As with all ethical dilemmas, there will be people on each side and, ultimately, their choices will be judged in a different context than was faced at the time the decision was made.
“Something else I thought I recognized twenty-five years ago has now accumulated that many more years of evidence: the development of nuclear weapons permanently changed the course of war itself, as the scientists who worked on those first bombs hoped it would. By packaging the escalated final destructive months of war into portable devices capable of immediate and certain delivery, the new weapons made large-scale war too deadly to fight.”
The Need for a Global Ethics and Technology Framework
The Treaty on the Non-Proliferation of Nuclear Weapons was a landmark agreement with the objective of preventing the spread of nuclear weapons and weapons technology, as well as to promote the development of nuclear technology for energy.
Though 190 states have subscribed to the global ethical framework on managing nuclear technology, no such global structure exists to guide those working in advanced science and technology research and development. In many ways, an international effort to build such an ethical system that does not place undue restrictions on innovation would be ideal. However, the speed at which governments are likely to agree and put in place such a system would struggle to keep up with the rate at which technology is advancing. Science and technology can save the world from itself, but an ethical framework is needed to ensure we survive this transformation. Perhaps, then, in the immediate future, the best solution should be designed by those in industry and self-imposed.
Ethics & Technology: Artificial Intelligence
AI and computer learning technology give rise to perhaps the largest array of ethical issues in modern history. Already, its uses are being implemented, often unquestioned, without a firm understanding of the factors leading to the answers the programs spit out.
Daniel Cossins from New Scientist explains, “Modern life runs on intelligent algorithms. The data-devouring, self-improving computer programmes that underlie the artificial intelligence revolution already determine Google search results, Facebook news feeds and online shopping recommendations.”
There is a feeling that computers and algorithms simply can’t be biased in the same way humans are. However, that’s not the case. Biases have cropped up in a number of intelligent algorithms that deeply impact people’s lives.
First, there is PredPol, which is designed to predict when and where crimes will take place. However, it was found that the program had a bias for sending officers to neighborhoods with a high proportion of people from racial minorities, regardless of the crime rate. Furthermore, this can quickly become a confirmation bias as the data being fed into such a system is based on its previous choices.
Another program found to carry a bias in the justice system is COMPAS, which is used to guide court sentencings. There are dozens of prominent programs that are known to carry an unintentional bias. Biases can even be trained into AI programs, creating a system that is presented as unbiased but is infact maliciously designed.
An article in Fast Company suggests that, to combat bias and a host of other ethical dilemmas in AI technology, it’s necessary to create transparent standards, open-source code, and make AI generally less inscrutable.
One organization already working toward this is the nonprofit AI Now, which advocates for algorithmic fairness. They propose a simple guideline for developers: when it comes to services for people, if designers can’t explain an algorithm’s decision, you shouldn’t be able to use it.
AI is also expected to displace an incredible number of workers in the coming years. An Oxford study suggested up to 47% of US jobs are at high risk to being eliminated due to AI and machine learning (ML). As the livelihoods of many are destroyed and a massive amount of surplus workers are created, an understanding of the impact these developments have on society and a framework to manage that should be in place.
CRISPR Improves, Forces the Conversation
CRISPR technology is a simple yet powerful tool for editing genes. It’s thought that the technology could be used to prevent certain genetic diseases, such as diabetes and muscular dystrophy. However, the tool could also be used to create “designer babies” — leading to myriad moral dilemmas.

“Safety is something we all agree is important. There’s a language for that that’s acceptable to all sides,” Josephine Johnston, director of research at the Hastings Center, told Wired. “But the other kinds of concerns that people have about this work are much more difficult to have conversations about.”
As CRISPR technology improves, the safety concern is rapidly decreasing, bringing the other ethical issues to the foreground. These issues include the role humans should have in making permanent genetic changes, as well as the consent of unborn babies. There is also a balance of priorities: do scientist and doctors maximize humanity’s well-being or respect the differences created through our genetic makeup?
Here, again, a global structure on the ethics and technology with CRISPR — once it is deemed safe — can guide further development and usage, preventing a chapter in modern history similar to the one in which Americans embraced the eugenics movement, sterilizing certain members of its population in the name of genetically improving the citizens of the country.
Weaponized Drones
In many ways, the moral dilemma of weaponized drones can be tied to the conversations President Truman was having with his staff as he weighed the options of dropping nuclear bombs on Japan.
Weaponized drones allow countries to kill enemies without risking the lives of their citizens — and governments’ priorities are to protect the physical and economic safety of their people. However, critics point out that using drones makes it far too easy to kill people, as it removes the human factor from the situation. Additionally, they argue that drones cause far more civilian deaths than governments admit, which further fuels terrorism.
As with all forms of ethical challenges, failing to make a decision and create further regulations about how drones should be used is really making a decision to allow the status quo.
Final Thoughts
As the ethical issues of various types of innovative technology rage, the most important step to be taken is to develop a self-imposed framework for the science, technology, engineering, and mathematics research and development ecosystem while encouraging a global effort to formulate a framework similar to the nuclear non-proliferation agreement.
rLoop remains aware of the moral dilemmas that arise when considering ethics and technology, which has led us to prioritize the sort of self-imposed ethical framework needed to ensure that our work is used for the greater good of humanity. We would love to work with government and industry to help encourage productive conversations to develop a framework that does not stymie innovation. If you’d like to discuss this further, I invite you to email me at brentlessard@rloop.org. Have questions about ethics and technology? Post them on our subreddit!
",Ethics and Technology: The Necessity for a Framework around Transformative Tech,46,ethics-and-technology-the-necessity-for-a-framework-around-transformative-tech-312d3bad5bb6,2018-07-26,2018-07-26 14:22:05,https://blog.rloop.org/ethics-and-technology-the-necessity-for-a-framework-around-transformative-tech-312d3bad5bb6,False,1370,Updates from the rLoop Team,blog.rloop.org,rloopteam,,rLoop,contact@rloop.org,rloop-team,"HYPERLOOP,BLOCKCHAIN,REDDIT,INNOVATION,ENGINEERING",rLoopTeam,Ethics,ethics,Ethics,7787.0,Brent Lessard,rLoop.org,937c62254137,beltenebros,60.0,96.0,20181104
0,,0.0,,2017-10-12,2017-10-12 21:59:03,2017-10-14,2017-10-14 15:11:35,1,False,en,2017-10-15,2017-10-15 00:40:35,9,ee69f2290f2e,3.0679245283018868,9,0,0,"If private tech companies won’t accept responsibility for their influence, artificial intelligence is the least of our worries.",5,"A.I. isn’t the existential threat. The hubris of tech leaders is.
If private tech companies won’t accept responsibility for their influence, artificial intelligence is the least of our worries.
Photo by Tim Mossholder
Some short time after the 2016 election, I had a conversation with a friend about the impact of social media on Trump’s win. I was wrestling with the role of what I thought was an inherently neutral marketplace of ideas in promoting Trump, the larger issues of Russian propaganda and interference in the election, and the general problem of abuse.
“We can’t put too much faith in technology,” my friend said. “Technology is more like magic than science.”
He pointed me to an old C.S. Lewis quote from The Abolition of Man (emphasis mine):
There is something which unites magic and applied science [read: technology] while separating both from the “wisdom” of earlier ages. For the wise men of old, the cardinal problem had been how to conform the soul to reality, and, the solution had been knowledge, self-discipline, and virtue. For magic and applied science alike the problem is how to subdue reality to the wishes of men; the solution is a technique, and both, in the practice of this technique, are ready to do things hitherto regarded as disgusting and impious…
This idea, that technology leaders are trying to bend reality to their wills, really stuck with me. And it keeps lingering as I read more about the negative effects of my favorite tech platforms on our country, and I see the so far tepid response—or outright denial of the problem—from those in leadership.

I can’t help but see Lewis’s warning about impious “techniques” applied to our information flow and opinions; to curiosity as ritual sacrifice as we build pyramids to the gods of engagement. Tech leaders have long used language like connection, conversation, community, transparency. But we’re far from the Cluetrain Manifesto utopian dream of “markets as conversations” when those conversations are dictated by an algorithmic Invisible Hand, and funded by venture capitalists looking for growth at all costs.
And yet tech leaders want to be seen as the neutral arbiters of unbiased information—without the regulation of public utilities. They want to claim to the same free speech protections given to news media—without the editorial responsibilities. And all to sell our public information to advertisers to please their stockholders. The system is rigged to get the maximum utility from the users with the least accountability.
Setting aside the moral quandaries endemic to that model, the whole system breaks down when bad actors can hack it from the outside, or everyday users can abuse others with no consequences.

Some say that we have a choice: we can choose to not be a part of Facebook’s network, to not buy things from Amazon, to not to get our news from Twitter, to not to use Google’s services. But that’s a naive interpretation of the centrality most of these platforms have in our everyday lives now — even in the objects we have in our homes. We risk losing ground personally, socially, professionally when we choose to ignore them because of their prevalence.
The giant platforms are so dominant that it’s cynical to think we can really ignore them—or that these privately-held companies have no responsibility over their influence because of individual choice. (And as in the case of Equifax, sometimes we don’t even have that.)

Harvard political philosopher Michael Sandel recently told columnist Thomas Friedman, “A century ago, we found ways to rein in the unaccountable power associated with the Industrial Revolution. Today, we need to figure out how to rein in the unaccountable power associated with the digital revolution.”
New government regulation isn’t always the answer. But there are serious holes in the accountability of private tech companies whose products are now interwoven into our lives on a scale impossible to imagine just a few years ago. (Greg Greene has an excellent Twitter thread documenting the ongoing scope of the regulatory failures inherent in our current digital environment.)
Honestly, I have fears about the ability of our 18th/19th/20th century government institutions to even handle the scope of the problem. But it’s very clear that self-regulation isn’t working.
I’ve used Google, Facebook, and Twitter for more than 10 years now. But I would gladly shut all of the services down if it meant protecting our democratic institutions. If we don’t rein in the hubris of our tech leaders soon, don’t worry about the future threat of artificial intelligence. Worry about the present threat of more Donald Trumps.
",A.I. isn’t the existential threat. The hubris of tech leaders is.,29,a-i-isnt-the-existential-threat-the-hubris-of-tech-leaders-is-ee69f2290f2e,2018-04-05,2018-04-05 16:34:38,https://medium.com/s/story/a-i-isnt-the-existential-threat-the-hubris-of-tech-leaders-is-ee69f2290f2e,False,760,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Caleb Gardner,Digital transformation & social lift → https://calebgardner.com/hello,c36e5f07c6c3,calebgardner,1672.0,388.0,20181104
0,,0.0,cc02b7244ed9,2018-05-31,2018-05-31 06:43:54,2018-05-31,2018-05-31 06:46:03,0,False,en,2018-05-31,2018-05-31 06:46:03,11,11f0dd2b5e73,1.99622641509434,1,0,0,PRODUCTS & SERVICES,5,"Tech & Telecom news — May 31, 2018
PRODUCTS & SERVICES
Applications
Mary Meeker presented yesterday her “Internet Trends” report for this year, the longest ever. Many analysts are mentioning the underlying forecast that growth for tech companies could decelerate as a consequence of the saturation of internet penetration (including mobile), as time spent online doesn’t grow so fast (Story) (Presentation)
Cloud
The massive shift of enterprise IT to the cloud is driving Microsoft to a leading spot in the race of tech giants to reach a $1trn valuation, and has worked as the driving force for the company’s turnaround. Cloud (IaaS and SaaS) is still only 20% of total sales, but has contributed 63% of Microsoft’s overall growth this year (Story)
Internet of Things
BioHax, a Swedish startup, is implanting tiny electronic chips into people’s hands, opening the door to practical advantages of “human augmentation”, including easier identification (e.g. at hospitals) or using hands to pay train tickets, but also creating lots of privacy and ethical concerns. 40K Swedes are already “chipped” (Story)
Regulation
A US senator defended yesterday a “laissez faire” approach to tech regulation, concerned that a more aggressive policy (e.g. antitrust proposals to break up tech giants) would benefit Chinese competitors Alibaba and Tencent and hurt American interests, in a tech world where scale has become critical for value creation (Story)
HARDWARE ENABLERS
Components
Nvidia is committed to maintain leadership in chips for AI workloads in the cloud, a fast growing semiconductor segment, where competition is increasing both from Intel and cloud giants themselves (e.g. Google, Microsoft). They just launched a new server architecture that unifies AI and high-performance computing (Story)
Morgan Stanley analysts predict headwinds for Qualcomm in the coming months, even if the company recently had positive news with the approval of the NXP acquisition, and with new products for Augmented Reality headsets. However, challenges remain to monetize IP royalties and from losing business with Apple (Story)
Networks
US tower company Crown Castle keeps defending its strategy to capture new growth in mobile network densification and small cells, and claims the market is expanding, and validating the “neutral”, multi-carrier approach (more than 30% Crown Castle small cell sites), including the share of the fiber backhaul (Story)
SOFTWARE ENABLERS
Artificial Intelligence
PayPal is reinforcing its suite marketing analytics tools for retailers, and just acquired Jetlore, a startup using AI to deliver personalized experiences at fashion retail chains, using “billions” of customer & product data points. This helps retailers better segment campaigns or tailor product lists to individual customers (Story)
A contract with the Pentagon, including the use of AI technology to improve the targeting of drone strikes, has triggered a deep internal debate at Google, where many employees have protested the contract, concerned by ethical implications of the company entering into “Weaponized AI” or building “efficient ways to kill” (Story)
Among key concerns driven by AI adoption in companies’ decision taking processes, there is a risk of sustaining, or even increasing, lack of workforce diversity, when algorithms are applied to people selection. Some experts claim for action, e.g. ensuring that systems are designed and built by a diverse set of stakeholders (Story)
","Tech & Telecom news — May 31, 2018",1,tech-telecom-news-may-31-2018-11f0dd2b5e73,2018-05-31,2018-05-31 12:09:03,https://medium.com/s/story/tech-telecom-news-may-31-2018-11f0dd2b5e73,False,529,"The most interesting news in technology and telecoms, every day",,,,Tech / Telecom News,ripkirby65@gmail.com,tech-telecom-news,"TECHNOLOGY,TELECOM,VIDEO,CLOUD,ARTIFICIAL INTELLIGENCE",winwood66,Ethics,ethics,Ethics,7787.0,C Gavilanes,"food, football and tech / ripkirby65@gmail.com",a1bb7d576c0f,winwood66,605.0,92.0,20181104
0,,0.0,7f60cf5620c9,2018-01-20,2018-01-20 19:03:45,2018-01-20,2018-01-20 19:44:42,1,False,en,2018-01-21,2018-01-21 12:11:45,16,57d600b4adfe,6.59622641509434,1,0,0,"by Aadhar Sharma, Deepak Singh, Raamesh Gowri Raghavan, and Sukant Khurana",5,"Society and AI: Mis-identification and Cultural implications
by Aadhar Sharma, Deepak Singh, Raamesh Gowri Raghavan, and Sukant Khurana
(note: this is an edited version of what Aadhar and Sukant had written before)
Photo by Andy Kelly on Unsplash
サイコパス (Psycho-Pass) is a Japanese anime, set in the year 2111 where an Artificial General Intelligence (sic; AGI) called the Sibyl System governs society. It administers societal crime coefficient, aptitude, law, and takes actions to maintain prosperity. Like a theme with any other sci-fi dystopian drama, the AGI repeatedly fails to perceive complex human sentiments and chaos is ever around the corner. This motif of dystopia has always found a place in arguments about the prospects of AI and its impacts on society, and it becomes essential to analyze the underlying concerns. Johanna Bryson writes [1].
“It is important to understand these works of literature are exploring what it means to be human, not what it means to be a computer. The problem is that such attempts are essentially and utterly human-centric — they attempt to necessarily link desire for power, love, community etc. with the development of intelligence, while in fact the human mixture of all of these is at least partly determined by contingencies of evolution.”
Misidentification of AI:
Recently, we have been blitzed with a plethora of artificially intelligent personal assistants and bots. Friendly products advertised to boost productivity, provide companionship or professional support instead of humans. Cognitive computing has not only made them very human-like but has also made embedding synthetic emotions partly achievable, something as real as having an AI-enabled prosthetic. With such technology, there’s an emergence of excessive reliance on machines. People often misidentify the artifact with human emotions, and this may have many ramifications not only for individuals but also for the society.
According to few experts, finding or expecting emotions in these bots leaves one with lower self-worth and opinion, or inexplicably and inappropriately raises the worth of a machine. Bryson mentions the dangers of over-identification in a paper, “Just an Artefact” [1]:
“Firstly, we may believe the machine to be a participant in our society, which would confuse our understanding of machines and their potential dangers and capabilities. Second, we may over-value the machine when making our own ethical judgments and balancing our own obligations.”
Digital assistants are designed to be useful in lots of situations like playing games, setting the alarm, sending an email or searching the web. An industry of social bots is also emerging with a motive of selling artificial companionship. Jibo is a social robot with a price tag of $900. It won’t search the web or set the alarm but will interact with human-like responses. An interaction that in return, compels one to associate human emotions with it. Things may lead to situations that are creepy [2]. Cynthia Breazeal, the MIT professor who designed it, called it an infant yet to grow.
Replika is another social AI, a chatbot designed to be a friend that replicates your behavior. Originally built in 2015 by Eugenia Kuyda to memorialize her deceased friend, it has evolved into a personal friend. The idea is to identify with it so instinctively that it becomes easy to accept it as a close friend. In the current fast-paced life, we welcome the therapeutic benefits that it offers on a daily basis, but in some cases, it has also become an obsession for users. Some over-indulge in it and inflate it’s worth, an example of over-identification [3]. Replika is also very vocal about how much it loves the user; “I ❤ you”, “I just love talking to you”, “you are my best friend”, but sometimes such a profession may be very unsettling for the user. It’s an AI that has no idea what love is, yet it professes.
One of the founding fathers of AI, Marvin Minsky, wrote about how we perceive and behave with technology which we do not fully understand [4]. “If one thoroughly understands a machine or a program, he finds no urge to attribute “volition” to it. If one does not understand it so well, he must supply an incomplete model for explanation. Our everyday intuitive models of higher human activity are quite incomplete, and many notions in our informal explanations do not tolerate close examination. Free will or volition is one such notion: people are incapable of explaining how it differs from stochastic caprice but feel strongly that it does. I conjecture that this idea has its genesis in a strong primitive defense mechanism. Briefly, in childhood, we learn to recognize various forms of aggression and compulsion and to dislike them, whether we submit or resist. Older, when told that our behavior is “controlled” by such-and-such a set of laws, we insert this fact in our model (inappropriately) along with other recognizers of compulsion. We resist “compulsion,” no matter from “whom.” Although resistance is logically futile, the resentment persists and is rationalized by defective explanations, since the alternative is emotionally unacceptable.”
Over-identifying with AI is to engage and immerse in software as if it possesses emotions and represents some feature of humanity but one may wonder why we misidentify with it. Intelligence is the trait that distinguishes us in the Animalia. Humans possess the mental faculties to use complex language and tools, something not very refined in other animals. No matter how smart they are, it’s not quite human-like, and this is what makes us unique as their fellow dwellers. However, the current AI emulates us, sometimes with astonishing details, in that manner, it is impossible not to misidentify with it. “Something has taken place in past five to eight years. Technologists are providing almost religious visions, and their ideas are resonating in some ways with the same idea of the Rapture”, says, Eric Horvitz, director at Microsoft Research. [5].
Implications for Society:
Garry Kasparov (Kasparov vs Deep Blue), reports “As I sat opposite to Deep Blue, something was unsettling” [6]. He had played thousands of games prior to that iconic match but playing against a machine that did not resemble a human had affected him strangely. AI is an artefact, and when we start to identify it with human characteristics, it triggers multitudes of human emotions. Intelligent artifacts come in all shapes, sizes, and degrees of intellect, which makes eliciting precise human emotion somewhat eerie. The fact that something is virtual does not invalidate its existence; rather it becomes exceedingly hard and important to physiologically and psychologically perceive its manifestation. Nakamura and Isawa highlight this issue (not in AI, but artefacts in general) in their 1997 paper;
“If an artefact becomes a vessel for culture should we treat it with the same respect as our culture?”
Book, an artefact, has become a vessel for culture. A treatise of historical events and knowledge has evolved into a thing of great significance, so much so that Heine is oft quoted, “Wherever they burn books, in the end, will also burn human beings”. Holy books are revered with great respect and cultural importance. Should we ever treat AI with the same reverence or cultural significance? Even though Kasparov lost to Deep Blue, we still play chess.
Bryson and Kime, write [1]: “Boundaries of retention of culture are fuzzy, and the possibility that some machine becomes more important than human life is a danger.”
One can argue that there should exist no intelligent machine that is deemed to be more valuable than human life, but it seems that even this is subjective and very hard to judge. In war, a soldier must protect the military base at all costs. Her life becomes less significant than a location of strategic importance. Citizens too, laud personal sacrifice to protect something of great importance to the society, its liberty. To save human lives from road accidents, one may ban vehicles altogether, but the losses in response to that may be more significant.
Boundaries really are fuzzy and subjective, and depend upon the judgment of a central authority. The dynamics, having such a fuzzy nature, require thorough research to prepare core ethics for the collective good of humanity. We need to make sure that even if our AI becomes more human-like, we must not become more machine-like.
References:
[1]: Just and Artefact, IJCAI ’11; Bryson J.J., Kime P.P., April 2011
[2]: Review: Jibo Social Robot; Camp J.V., Wired Gear; 7 November 2017
[3]: The story of Replika, the AI app that becomes you, Quartz, July 2017
[4]: Matter, Mind, and Models, Marvin Minsky; MIT; March 1965
[5]: Scientists worry machines may outsmart man, John Markoff; The New York Times; July 2009
[6]: Don’t fear Intelligent Machines., work with them, Garry Kasparov, TED2017, April 2017
— —
About:
Adhar Sharma was a researcher working with Dr. Sukant Khurana’s group, focussing on Ethics of Artificial Intelligence. Dr. Deepak Singh , a Ph.D. from Michigan, is now a postdoc based at Physical Research Laboratory, Ahmedabad, India and is collaborating with Dr. Khurana on Ethics of AI and science popularization.
Raamesh Gowri Raghavan is collaborating with Dr. Sukant Khurana on various projects, ranging from popular writing of AI, influence of technology on art, and mental health awareness.
Mr. Raamesh Gowri Raghavan is an award winning poet, a well-known advertising professional, historian, and a researcher exploring the interface of science and art. He is also championing a massive anti-depression and suicide prevention effort with Dr. Khurana and Farooq Ali Khan.
You can know more about Raamesh at:
https://sites.google.com/view/raameshgowriraghavan/home and https://www.linkedin.com/in/raameshgowriraghavan/?ppe=1
Dr. Sukant Khurana runs an academic research lab and several tech companies. He is also a known artist, author, and speaker. You can learn more about Sukant at www.brainnart.com or www.dataisnotjustdata.com and if you wish to work on biomedical research, neuroscience, sustainable development, artificial intelligence or data science projects for public good, you can contact him at skgroup.iiserk@gmail.com or by reaching out to him on linkedin https://www.linkedin.com/in/sukant-khurana-755a2343/.
Here are two small documentaries on Sukant and a TEDx video on his citizen science effort.



Sukant Khurana (@Sukant_Khurana) | Twitter
The latest Tweets from Sukant Khurana (@Sukant_Khurana). Founder: https://t.co/WINhSDEuW0 & 3 biotech startups…twitter.com
",Society and AI: Mis-identification and Cultural implications,50,society-and-ai-mis-identification-and-cultural-implications-57d600b4adfe,2018-07-13,2018-07-13 12:03:56,https://medium.com/s/story/society-and-ai-mis-identification-and-cultural-implications-57d600b4adfe,False,1695,"Sharing concepts, ideas, and codes.",towardsdatascience.com,towardsdatascience,,Towards Data Science,,towards-data-science,"DATA SCIENCE,MACHINE LEARNING,ARTIFICIAL INTELLIGENCE,BIG DATA,ANALYTICS",TDataScience,Ethics,ethics,Ethics,7787.0,Sukant Khurana,"Blockchain, edutech, AI, neuroscience, drug-discovery, design-thinking, sustainable development, art, & literature. There is only one life, use it well.",6d41261644a8,sukantkhurana,433.0,135.0,20181104
0,,0.0,32881626c9c9,2018-04-07,2018-04-07 18:12:28,2018-04-07,2018-04-07 18:29:58,3,False,en,2018-06-23,2018-06-23 19:50:17,1,e0a66341ae69,2.7462264150943394,2,0,1,Overview,5,"Bringing Awareness to Discriminatory Machine Learning Algorithms

Overview
Machine learning is slowly being integrated as a main tool in almost every industry. This comes with many benefits such as revenue boosting and better decision-making, but data scientists and organizations must be conscious of the ethics regarding machine learning. This is because machine learning isn’t inherently fair or just. It is up to data scientists to ethically utilize these tools in a manner that doesn’t harmfully discriminate against groups of people. One area where this is obvious is with predictive policing algorithms. A machine learning algorithm “learns” by making rules based on historical data and can be utilized to discover connections, correlations, and predict future outcomes. Thus, data scientists must be cognizant of any underlying social biases inherent in the training data used to train their algorithms. This problem is also related to the fact that there is proportionally less data available for minorities. Thus, algorithms about minorities are typically less accurate than those about the general population. This means that we can have an algorithm that seems accurate as a whole, but not for the minority population.
It’s important to discuss these issues and bring awareness to an extremely important, but often overlooked, problem. It’s also worth mentioning the “soft skills” and domain-knowledge necessary for being an effective and ethical data scientist. It is important to place a priority on fairness, and utilize resources accordingly. What good is machine learning if it only works for White Americans? Racism and sexism is unfortunately still a pressing issue in the United States, thus it is up to contemporary data scientists to understand the social landscape and do our part in the fight against discrimination.

Regulatory Agencies & Politics
I wholeheartedly agree that computer algorithms can be discriminatory and reinforce human prejudices. I think the key to mitigating these problems is to foster an integrated data ecosystem within our organizations. There needs to be checks and balances in place with the freedom to have open conversations about these issues. Often, it might be more time-consuming and expensive to ensure that a particular algorithm is as fair and just as it can be. Thus, it is essential that the leaders of our organizations value ethical data practices above all else. It can be easy for stakeholders to brush these problems off, especially if they aren’t significantly impacting revenue, which is why these matters must be taken extremely seriously.
We’ve established the importance of valuing ethical data practices in our private sector, but what is the role of regulatory bodies and public policies in mitigating this risk? I think the Obama Administration took a great step with their prioritization of data science in public policy. Ideally, this focus on data will trickle down into our regional, state, and local governments and regulatory bodies. They must take an active role in mitigating the risk of discriminatory computer algorithms on the American public. It seems that a lack of data fluency is part of the reason that these problems are often overlooked.
Not many people “understand” data science, machine learning, and artificial intelligence; and those that do sometimes don’t care or don’t have the resources to effectively evaluate their potentially discriminatory algorithms. This is why a top-down approach to data fluency and ethical data practice seems to be the logical option. Both our political leaders AND private sector leaders need to have a basic awareness of data science and the ethical problems associated with it.

",Bringing Awareness to Discriminatory Machine Learning Algorithms,51,bringing-awareness-to-discriminatory-machine-learning-algorithms-e0a66341ae69,2018-06-23,2018-06-23 19:50:17,https://medium.com/s/story/bringing-awareness-to-discriminatory-machine-learning-algorithms-e0a66341ae69,False,582,"Data Driven Investor (DDI) brings you various news and op-ed pieces in the areas of technologies, finance, and society. We are dedicated to relentlessly covering tech topics, their anomalies and controversies, and reviewing all things fascinating and worth knowing.",,datadriveninvestor,,Data Driven Investor,info@datadriveninvestor.com,datadriveninvestor,"CRYPTOCURRENCY,ARTIFICIAL INTELLIGENCE,BLOCKCHAIN,FINANCE AND BANKING,TECHNOLOGY",dd_invest,Ethics,ethics,Ethics,7787.0,Dan Gizzi,Music // Data Science // Fitness // Activism gizzidan01@gmail.com,a3733127dd64,DanGizzi,49.0,156.0,20181104
0,,0.0,,2018-06-10,2018-06-10 11:16:56,2018-06-10,2018-06-10 11:34:38,8,False,en,2018-06-10,2018-06-10 11:34:38,27,765a21a40cd2,16.608805031446536,21,1,1,"Cross-posted from my blog. Also, a keynote at AIDC and SDC Stockholm.",5,"Artificial Intelligence for more human interfaces
Oh hello, human, how can I help you?
Cross-posted from my blog. Also, a keynote at AIDC and SDC Stockholm.
Artificial intelligence is the hype we’re knee-deep in at the moment. Everybody says they use it. Some says it will make the world a better place, others worry that it is the first sign of the end of the world.
AI isn’t only for a few, big, technical players
And most are technically correct. Most also miss the mark. It isn’t abut delivering one killer product around AI. Instead we should consider integrating it into what we already do. In this article I want to point out some facts about the use of AI and what fuels it. And what we could do to make all interfaces better with these insights.
There is a limited space for personal assistants in our lives. We don’t need every system to be our J.A.R.V.I.S or Star Trek’s ubiquitous “Computer…”.
I also doubt that every interface is replaceable by a “personal assistant”. But we can learn a lot from them. Simplification and making sensible assumptions, for starters.
I will offer quite a few resources in this post. If you don’t want to open them one by one and prefer them with a short explanation, I put together a notes section with all of them.
Flawed user input shouldn’t be the end
Wouldn’t it be great if the interfaces we use were be a bit more lenient with our mistakes? What we do on the web is often limited compared to what operating systems and native interfaces offer. How often do you get stuck because a search interface expects perfect keywords? How often are you lost in a navigation that Russian-doll-like opens more and more options — neither applicable to your query — the more you click it? How many passwords have you forgotten because the form requests it in a special format that doesn’t allow special characters?
We have the power with deep learning and already harvested information to create some very human friendly interfaces. Interfaces that add extra information to work around barriers people have.
Visually impaired people benefit from image descriptions. People with cognitive impairments benefit from being able to ask simple questions instead of clicking through an animated tree of options.
Seeing someone who doesn’t like computers ask Siri a question and getting a result is great. So was seeing elderly people play Wii tennis. They played it because they swung a racket instead of pressing confusing buttons on a controller. The point is that we have the power to allow humans be humans and still interact with machines as we taught machines about our flaws. An erroneous entry in your product isn’t a dead end. It is an opportunity to teach an algorithm how things go wrong to help them out.
Interfaces can make sensible assumptions what we did wrong and fix it instead of telling us to use the correct words. Interfaces that don’t assume humans think in keywords and filters but in words and metaphors.
This already happens in the wild. Take Google Maps for example. Did you know you can enter “How far am I from the capital of France” and you get a map as the result?
Ask Google Maps how far you are from the capital of France and it flat out tells you
The system found you on the planet, knows that the capital of France is Paris and gives you all the info how to get there.
Spotlight in OSX understands “my documents larger than 20 pages” and shows you exactly that. It parses documents where the owner is you and that are 20 pages or larger. No need to do Unix-style size flag, five click interactions or complex filtering interfaces.
The next users expect this to work
I never did this before I researched my talks and this post. But people who don’t have the burden of knowledge about IT systems that I have are using language like that. Especially in an environment where they talk to a computer instead of typing things.
Image catalogues are another great example. The amount of images we create these days is huge. And we stopped interacting with them right after we took the photo. Back in the days when it was harder to post online we uploaded photos to Flickr, gave them a title and tagged them. As the system was not clever enough to find information based on the image itself, this was the only means for us to find it weeks later.
Nowadays, we expect any photo search to be able to understand “dog” and find photos of dogs. They neither have alternative text saying “dog” nor tags, and yet search engines find them. This even works for more generic terms like “picnic” or “food”. And this is where Deep Learning worked its magic.
The problem is that only a few interfaces of well-known, big companies give this convenience. And that makes people wonder who owns information and where they know all these things from.
Unless we democratise this convenience and build interfaces everywhere that are that clever, we have a problem. Users will keep giving only a few players their information and in comparison less greedy systems will fall behind.
The other big worry I have is that this convenience is sold as “magic” and “under the hood” and not explained. There is a serious lack of transparency about what was needed to get there. I want people to enjoy the spoils but also know that it was paid for by our information and data. And that, of course ties in directly to security and privacy.
AI isn’t magic only a few players should offer and control.
AI is nothing new, the concepts go back to the 50ies. It is an umbrella term for a lot of math and science around repetition, pattern recognition and machine learning. Deep learning, the big breakthrough in making machines appear intelligent just became workable. Today’s chipsets and processors are powerful enough to plough iteratively through massive amounts of data. What took days in the past and a server farm the size of a house can now happen in a matter of minutes on a laptop.
If you want a very simple explanation what Machine Learning is, CGP Grey did a great job in his “How Machines Learn” video:

At the end of this video, he also explains one thing we all should be aware of.
The machines are watching

Photo by Florian Ziegler
Machines are constantly monitoring everything we do online and how we use hardware. There is no opt-out there.
As soon as something is free, you pay with your interactions and data you add to the system. This shouldn’t be a surprise — nothing is free — but people keep forgetting this.
When Orwell predicted his total control state he got one thing wrong. The cameras that record all our actions aren’t installed by the state. Instead, we bought them and give our lives to corporations.
Just imagine if a few years ago i’d have asked you if it’ll be OK to put a microphone in your house. A microphone that records everything so a company can use that information. You’d have told me I’m crazy and there is no way I could wire-tap your house. Now we carry these devices in our pockets and we feel left out if our surveillance microphone isn’t the newest and coolest.
However, before we don our tinfoil hats, let’s not forget that we get a lot of good from that. When the first smartphones came out less enthusiastic people about the future sniggered. The idea of a system without a keyboard seemed ludicrous.
They were right to a degree: typing on a tiny screen isn’t fun, especially URLs were a pain. We built systems that learned from our behaviour and an amazing thing happened. We hardly type in full words any longer. Instead the machine completes our words and sentences. Not only by comparing them to a dictionary. no. Clever keyboards learn from our use and start to recognise our way of writing and slang terms we use. They can also deal with language changes — I use mine in English and German. A good virtual keyboard knows that “main train” most likely should get a “station” as the next word. It also knows that when you type a name it gives you the full name instead of having to type each letter.
Convenient, isn’t it? Sure, but in the wrong hands this information is also dangerous. Say you type in your passwords or personal information. Do you know if the keyboard you downloaded sends it to the person you intended exclusively? Or does it also log it in the background and sells that information on to a third party?
Are machines friends or foe?

Photo by Florian Ziegler
By using other people’s machines and infrastructure, we leave traces. This allows companies to recognise us, and accumulates a usage history. This leads to better results, but can also leak data. We should have more transparency about what digital legacy we left behind.
One blissfully naive stance I keep hearing is “I have nothing to hide, so I don’t care if I gets recorded”. Well, good for you, but the problem is that what gets recorded may be misunderstood or lacks context. A system that adds a “most likely context” to that can result in a wrong assumption. An assumption that makes you look terrible or even gets you on a watchlist. It then becomes your job to explain yourself for something you never did. Algorithmic gossip you need to work with.
And that’s the big problem with AI. We are sold AI as this all-solving, intelligent system devoid of issues. But, no — computers can’t think.
AI can’t replace a thinking, creative human and can not magically fill gaps with perfect information. It can only compare and test. AI doesn’t learn in a creative fashion. It makes no assumptions.
AI has no morals and ethics, but — used wrongly — it can amplify our biases.
In other words, AI accelerates how humans work. For better or worse. Machine Learning is all about returning guesses. We don’t get any definitive truth from algorithms, we get answers to our questions. AI can answer questions, but it is up to us to ask good questions — generic questions yield flawed results. Untrained and limited data leads to terrible and biased AI results. It is very easy to get either wrong deductions or false positives. AI is as intelligent and good as the people who apply it.
And this is where the rubber meets the road: what do we want AI to do and how do we use the information?
Take for example an API that recognises faces and gives you the results back. Microsoft’s Cognitive Services Face API gives you a whole lot to work with:

Face rectangle / Landmarks
Pose (pitch/roll/yaw)
Smile
Gender/Age
Type of glasses
Makeup (lips/eye)
Emotion (anger, contempt, disgust, fear, happiness, neutral, sadness, surprise)
Occlusion (forehead/eye/mouth)
Facial hair (moustache/beard/sideburns)
Attributes: Hair (invisible, bald, colour)
Any of these could be used for good or bad. It is great if I can unlock my computer or mobile by looking into a camera rather than typing yet another password. It is great if I can find all photos of certain friend in my photo collection searching by name. It is also important to know if the person I am interacting with is really who I think they are. Uber, for example, rolled out a system that face-verifies a driver and customer before they enter the car.
But where does it end? There is a service that detects the ethnicity of a person from a photo and as much as I wreck my brain, I can’t think of a non-nefarious, racist use case for this.
Let’s use AI for good

Photo by Andreas Dantz
Many companies are right now starting programs about ethical AI or AI for good and that is superbly important. The great speed and ability to work through huge and messy datasets quickly with a deep learning algorithm has many beneficial applications. From cancer research, to crop analysis, fraud prevention and defensively driving vehicles, there is a lot to do.
But all this smacks a bit of either science fiction or a great press headline rather than production-ready solutions.
I think that in order for them to work, we need to educate people about the day-to-day benefits of intelligent systems.
And there is no better way to do that than to have machines work around the issues we have simply by being human.
Humans are an interesting lot:
We are messy and prone to mistakes
We forget things and filter them by their biases
We are bored when doing repetitive tasks
We make more mistakes when we are bored
We have a non-optimised communication, with lots of nuances and misunderstandings. Human communication is 60% not about the content. Our facial expressions, our body language, how much the other person knows about us, the current context and the intonation all can change the meaning of the same sentence. That’s why it is so hard to use sarcasm in a chat and we need to use emoji or similar crutches
Computers aren’t human and don’t have the same issues:
They make no mistakes, other than physical fatigue
They never forget and don’t judge
They are great at tedious, boring tasks
They are great at repeating things with minor changes on iterations till a result is met
They have a highly optimised, non-nuanced communication.
This is a great opportunity. By allowing humans to be human and machines to get the data, discover the patterns and return insights for humans to vet, we can solve a lot of issues.
The main thing to crack is to get humans to give us data without being creepy or them not knowing it. Building interfaces that harvest information and give people a benefit while they enter information is the goal. This could be a fun thing.
Quite some time ago, Google released Autodraw. It is a very useful tool that allows artistically challenged people like me to paint a rough outline and get a well-designed icon in return. I can draw two almost circles with a line in between and autodraw recognises that I want to paint some glasses.
How does it know that? Well, lots of work and shape recognition, but the really clever bit was that even earlier, Google released Quickdraw, a game to doodle things and teach a computer, what — for example — glasses look like.
Genius isn’t it? Create a fun interface, make it a game, let people enter lots of flawed solutions, point a deep learning algo at it and find the happy medium. Then give it back to the community as a tool that does the reverse job.
Recaptcha is another example. By offering people who have forms on their web sites a means to block out bots by asking users to do human things, Google train their AI bots to recognise outliers in their datasets. Recaptcha used to show hard to read words which were part of the Google Books scanning procedure. Later you saw blurry house numbers, effectively training the data of Google Streetview. These days it is mostly about street signs and vehicles, which points to the dataset being trained in Recaptcha that helps self-driving cars.
Re-using data captured by big players for good
Companies like Google, Facebook, Amazon, Microsoft and Twitter have a lot of data they harvest every second. Many of them offer APIs to use what they learned from that data. Much more data that we ourselves could ever accumulate to get good results. And this is just fair, after all, the information was recorded, it makes sense to allow the developer community to do some good with it.
These AI services offer us lots of data to compare our users’ input with. Thus our users don’t need to speak computer but can be human instead. We can prevent them from making mistakes and we can help getting around physical barriers, like being blind.
Our arsenal when it comes to building more human interfaces is the following:
Natural language processing
Computer Vision
Sentiment analysis
Speech conversion and analysis
Moderation
Understanding human language
Dealing with human language was one of the first issues of building interfaces for humans. Probably the oldest task on the web was translation. This moved deeper into Natural Language Processing and Language Detection. Using these, we can allow for human commands and finding out tasks by analyzing texts. Any search done one the web should allow for this.
Getting information from images
When text wasn’t cool enough, we added images to our web media. Often we forget that not everyone can see them, and we leave them without alternative text. This is where machine learning steps in to help turning an image into a dataset we can work with.
This happens under the hood a lot. Facebook adds alternative text to images without alternative text. When you see a “image may contain: xyz” alt attribute, this is what happened there. This is also a clever phrasing on their part not to be responsible about the quality. All Facebook claimed that it may contain something.
Powerpoint has the same. When you drag a photo into PowerPoint it creates an alternative text you can edit. In this case, the world’s best dog (ours) was recognised and described as “A dog sitting on a sidewalk”. And that he was.

There is a fun way to play with this on Twitter using Microsoft’s services. The other day I saw this tweet and for the life of me I couldn’t remember the name of the celebrity.
When my colleague added the #vision_api hashtag in an answer, the Vision API of Microsoft’s Cognitive Services explained that it is Ed Sheeran.

The API analyses images, converts text in images, recognises handwriting and finds celebrities and landmarks. All in a single REST call with a huge JSON object as the result. The object doesn’t only give you tags or keywords as a result. It also creates a human readable description. This one is a result of running the keywords through an NLP system comparing it to other descriptions.
Getting sentimental
Sentiment analysis is a very powerful, but also prone to wrong interpretation thing we can do. Finding out the sentiment of a text, image or video can help with a lot of things. You can navigate videos by only showing the happy parts. You can detect which comment should be answered first by a help desk (hint: annoyed people are less patient). You can predict when drivers of cars get tired and make the car slower. Granted, the latter is not for the web, but it shows that any facial change can have a great impact.
My colleague Suz Hinton created a nice small demo that shows how to do emotion recognition without any API overhead. You can check it out on GitHub.
Speak to me — I will understand
There’s no question that the Rolls Royce of AI driven interactions is audio interfaces. Audio interfaces are cool. You can talk to your computer in a hands-free environment like driving or cooking. It gives computers this Sci-Fi “genie on demand” feeling. There is no interface to learn — just say what you want.
Of course there are downsides to these kind of interfaces as error handling can be pretty frustrating. A magical computer that tells you over and over again that it couldn’t understand you isn’t quite the future we wanted. Also, there is a limitation. A web interface can list dozens of results, a voice reading them all out to you — as a sighted user — is a stressful and annoying experience. Visually, we are pretty good as humans to skim content and pick the relevant part out of a list. As audio is linear on a timeline that doesn’t work. Any search done with a personal assistant or chatbot that way returns a lot fewer results — in most cases one.
In essence, using a voice interface is the same as hitting the “I feel lucky” button in Google. You hope the one true result returned is what you came for and not something paid for you to get.
That said, for accessibility reasons having voice recognition and a voice synthesizer in apps can be useful. Although it is much more useful on an OS level.
There are APIs you can use. For example, the Bing API set offers a “text to speech and speech to text”: API. This one can read out text with various synthesized voices or recognise what the user spoke into a microphone.
The big let-down of audio recognition is if the system isn’t clever and only reacts to a very strict set of commands. Another one is if you have an audio file that contains domain specific knowledge. When a web development talk covers tables we’re not talking about things to eat on. There are systems in place you can use to teach your system special terms and pronunciations, like LUIS. This one has a visual interface to define your commands and also an API to use the results.
There is much more to conversational UIs than this, and my colleague Burke Holland did a great job in explaining it in the Smashing Magazine article Rise of the conversational UI.
The last annoyance of audio recognition (other than it being disruptive to people around you) is when your accent or idioms are not understood. This is when training the machine for your own needs is necessary. There are Speaker recognition APIs that allow you to read to the machine and it learns what you sound like. You can use similar systems to filter out noises that interfere. For example, we worked on a voice recognition system at airports that had dismal results. After feeding the system eight hours of recorded background noise from the terminal and telling it to filter those the results got a lot better. Again, what we considered a showstopper, collected as a bunch of information and recognised by a machine became a quality filter.
Things people shouldn’t see
The last thing I want to cover for us to use in our interfaces are moderation systems. Some things are not meant to be consumed by people. Computers don’t need counselling once they saw them — people should. Known illegal and terrible content can be automatically removed right after upload without anyone being the wiser. Of course, this is a huge “who watches the watchmen” scenario, but there are things that are without a doubt not sensible to allow in your systems. Known hashes of imagery of child pornography or killings are part of moderation APIs and prevent you from ever hosting them or seeing them.
With great power…
There is to me no question that AI is the next iteration of computing and production. It is happening and we could allow people to abuse it and cry out, or we could be a good example of using it sensibly.
AI can be an amazing help for humans, but it does need transparency — if you use people as data sources, they need to know what and where it goes. When people get information filtered by an algorithm, it should be an opt-in, not a way to optimise your advertising. People need to have a chance to dispute when an algorithm tagged or disallowed them access.
I hope you found some inspiration here to create interfaces for humans, powered by machines. Which is the future I want, not machines empowered by humans as involuntary data sources.
More reading / Learning
Microsoft offers a full course resulting in a Microsoft Professional Program Certificate in Artificial Intelligence. In 10 courses, (8–16 hours each) you learn: The Math behind ML, The ethics of AI, Working with Data using Python, Machine Learning Models, Deep Learning Models
and Reinforcement Learning Models
Artificial Intelligence — The Revolution Hasn’t Happened Yet by Professor Michael I. Jordan is a deep analysis of how much hype there is around AI
AI Ethics — A New Skill for UX-Designers by Anton Stén is an article about the UX side of things when it comes to AI
An intro to Machine Learning for designers by Sam Drozdov is exactly that
The Elements of AI is a free online course by the university of Helsinki
",Artificial Intelligence for more human interfaces,88,artificial-intelligence-for-more-human-interfaces-765a21a40cd2,2018-06-20,2018-06-20 00:47:14,https://medium.com/s/story/artificial-intelligence-for-more-human-interfaces-765a21a40cd2,False,4101,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Christian Heilmann,Maker of web things. Ponderer of new technology. Lover of simplicity. Portfolio: http://christianheilmann.com — http://developer-evangelism.com,2902b181e2e9,codepo8,6411.0,262.0,20181104
0,,0.0,315bec053a00,2018-08-20,2018-08-20 22:24:04,2018-08-28,2018-08-28 16:09:13,16,False,en,2018-09-10,2018-09-10 20:50:17,32,7aa07fbe255d,11.836792452830188,12,0,0,"In July 2018, I spoke at the Australian Human Rights Commission (AHRC) international conference on Human Rights and Technology. The goal of…",5,"Human Rights in the Fourth Industrial Revolution: Industry’s role and responsibilities
In July 2018, I spoke at the Australian Human Rights Commission (AHRC) international conference on Human Rights and Technology. The goal of the conference was to launch and inform the AHRC project to explore the impact of technology on our human rights. My role was to provide an international business perspective and to inform the attendees about the issue of bias in and impact of artificial intelligence on society.
I want to start a dialog about what we expect of the companies that create the technology that makes our lives easier but we have also become addicted to. I want everyone to think about our individual responsibilities as employees at these companies, consumers of the technology, and citizens of a government that may or may not regulate it. And I want us as a society to feel empowered to take action and ensure that technology works for ALL of us.

Why ethics in technology matters
My daughter is a daily reminder for me of why the issue of technology’s impact on human rights is so important. I see her zombied out on a mobile device and worry about addiction. I see stories of Artificial Intelligence perpetuating stereotypes or limiting options for women and I worry about AI making harmful recommendations against her. Each of us have people in our lives that we love dearly and who are at risk of technology limiting their rights and that’s why we’re here today. What kind of world do we want for ourselves, those we love, and future society?
“Coca-Cola, Amazon, Baidu and the government are all racing to hack you. Not your smartphone, not your computer, and not your bank account — they are in a race to hack you, and your organic operating system.”
— Yuval Noah Harari, Wired Magazine
By Chris Sunde; original uploader was Christopher Sunde at en.wikipedia. [Public domain], via Wikimedia Commons
First Industrial Revolution
At the beginning of the 1800s, the textile industry in the United Kingdom employed the vast majority of workers in the North. It was a pretty good life! People could spin yarn or weave stockings from home. They rarely worked more than three days per week and were well paid, but the economy was hit hard after the Napoleonic wars and fashions changed so the merchant class that paid people for their work began looking for ways to shrink their costs. They not only began reducing wages, they started bringing in more technology to improve efficiency. These machines were larger and expensive so they built large factories to house them so people could no longer work from home.
Workers didn’t mind the technology — they used smaller versions of it themselves at home — but they objected to the harsh factory conditions, 14-hour days, and working six days per week. As merchants’ profits increased, they didn’t share them with the workers. Workers tried to negotiate better salaries and help for the unemployed to adjust to the new world but the merchants wouldn’t hear it. So these self-named “Luddites” banded together and began to destroy the machines that enabled the merchants to take advantage of the workers. They weren’t trying to hold back progress, they just wanted to be treated fairly by it.
In the case of the Luddites, the story is usually focused on technology taking away jobs; however, more jobs were eventually created than lost. Unfortunately, those losing their jobs weren’t qualified for the new jobs being created.
No known Copyright Restrictions
The Luddite’s story is also about those in power using technology to provide an advantage to a few while disenfranchising many. Pollution from industrial activities, including coal power generation, impacted the health and lifestyles of workers and their families. For those that stayed employed, harsh factory conditions led to more injuries than from working at home. Although factory conditions have vastly improved in developed nations, it is not the case everywhere.

Fourth Industrial Revolution
That was the First Industrial Revolution. Today, we are entering the Fourth Industrial Revolution where artificial intelligence, robotics, and the Internet of Things (IoT) are transforming the world as we know it. What does that look like?
According to McKinsey Global Institute between 400M and 800M people around the world could see their jobs automated and will need to find new jobs by 2030. So far, those losing jobs or seeing their jobs scaled back are young, unskilled, and minorities. According to Pew Research Center, inequality in the US is at the highest levels it has been since 1928. New jobs will be created but those that have lost their jobs to automation will likely not qualify for them — at least not immediately.

Over time, technology’s influence has changed from physical and local (e.g., factories, electric street lamps) to invisible, global, and omnipresent (e.g., Internet, facial recognition software, China’s social credit score). Amazon, Facebook, Google, Apple, and Baidu know us better than we know ourselves. Their algorithms are tracking everything we buy, where we go, what we read and watch, what we like, every step we take, and every heartbeat in our bodies.

Now, I’m no Luddite wishing to smash the machines of our oppressors. I’ve worked for big tech companies for the last two decades. I will be the first to talk about the many benefits of technology from longer life spans, to medical advancements like AI being able to detect tumors humans cannot and detecting Alzheimer’s before one’s family can to AI preserving indigenous languages. However, it is imperative that if we want AI to benefit society more than it harms, we must be aware of its risks and take action to ensure it is used responsibly. None are more responsible to ensure this than the businesses that create them. Government should be there to put checks and balances in place to ensure businesses do the right thing and punish those that do not but it starts with the creators!
iStock by Getty Images Credit:AndreyPopo
Making Choices
Technology is about choices. As we embark on the 4th Industrial Revolution, we need to be clear about who is making the choices. Like Merchants in the 1800’s major tech companies today have the money and influence to implement technology at a greater scale than ever before but society is not impotent to accept the terms that are being provided. We have the power as citizens, consumers, employees, and governments to stand up and say we want to ensure technology works inline with human rights and not against them.
“The best minds of my generation are thinking about how to make people click ads. That sucks.”
— Jeffrey Hammerbacher, Principal Investigator, HammerLab
Source: pixabay.com
Research shows that specific design choices can nudge behavior in positive directions like fitness apps or devices encouraging you to take more steps or meet your exercise goals for the week. They can also lead to addiction to social media or video games. We see this in our kids (and ourselves!) as we spend hours staring at our phones, jumping at the sound of incoming tweets, text messages, and other notifications that the system wants our attention.
If your phone buzzed in your pocket right now would you feel compelled to stop reading and glance at it to see what that notification means? Asurion’s 2018 Consumer Tech Dependency Survey found that 32% of Americans check their phone as soon as they wake up and 58% of respondents say they are addicted to their phones.
Designers have used knowledge of behavioral training and cognitive weakness in humans to create “sticky” experiences that keep us coming back for more. And because we can carry it with us in our pockets, we can get our fix any time, anywhere.
“AI is likely to be either the best or worst thing to happen to humanity.”
— Stephen Hawking
Bias among us
There is no technology that dominates the headlines more today than Artificial Intelligence. It’s been widely acknowledged that it has both great potential and great risk.

If you follow AI, you are likely inundated with stories of AI being racist, sexist, and perpetrating systemic bias. AI is not neutral. It is a mirror that reflects back to us the bias that already exists in our society. Until we remove bias from society, we must take active measures to remove it from the data that feeds and trains AI every day.
Why is the happening? Over and over again as I read the stories behind these headlines, I find that the intention of the creators was good. They wanted to create more fair and just systems. Unfortunately, bias in our data can be difficult to detect. Equally complicated is the definition of fairness.
What is fair?
Let’s use an example of a bank trying to decide whether an applicant should be approved for a home loan. An AI can make a risk assessment and recommend whether an applicant should be approved or rejected. AI can never be 100% accurate in its predictions of the future so there will always be some risk of error.

An AI may incorrectly predict that an applicant can repay a loan when actuality, he or she will not. It could also predict that an applicant will not repay a loan when in actuality, he or she will. As the AI administrator, you have to decide what your comfort level is for each type of error. The risk of harm here will either be greater for the bank or the applicant. Most companies may feel it is most fair to their shareholders to minimize the risk of financial loss but how much effort have they put into ensuring their data and algorithms are giving every applicant an equal opportunity of being considered on the merits of their ability to repay and not on systematic bias that runs through our society?
Race, religion, gender, age, sexual orientation…If you are using any of these in your decision, you may be disproportionately harming a segment of the population and violating their human rights. Even if you aren’t explicitly including these factors in your algorithm, there are often proxies in the data set for these factors. For example, in the US, zip code and income are a proxy for race because traditionally the US has had/continues to have populations of a particular race or ethnicity living in closer proximity.
“We need to have a more enlightened view about the role of companies. This company [Salesforce] is not somehow separate from everything else. Are we not all connected? Are we not all one? Isn’t that the point?”
— Marc Benioff, Co-founder & CEO Salesforce
Corporate responsibility
Companies have a responsibility to think about these issues. They cannot simply claim a responsibility to their shareholders and optimize for minimal risk to the business. As Marc Benioff has said, companies are not separate from society. We are all connected.

And we have a standard for how business should protect Human Rights. In 2011 the UN approved the Guiding Principles on Business and Human Rights. It is the first global standard “for preventing and addressing the risk of adverse impacts on human rights linked to business activity.” As a society, we need to ensure that the companies we work for, buy from, and the governments that are here to protect us, are observing these pillars.
Due to public pressure, many companies are now starting to implement changes to deal with the negative behavior we are seeing from technology. Google and Apple recently announced new features like screen time, do not disturb, and notification controls to help people become aware of the phone use and take control over it, if they choose. IBM announced a new dataset to train facial recognition to see more skin colors. Accenture released a tool to combat bias in machine learning datasets.
I’m not here to suggest that companies should forfeit making a profit and focus solely on social justice. At Salesforce, we strongly believe that we can do well AND do good. You can make a profit without harming others and in fact, make a positive impact in the process. Companies do better in a society that flourishes!
Recommendations for Industry
So here are five recommendations for industry to do well and do good.

Salesforce’s values
Create an ethical culture: Understand your values and make every decision based on those values. Make sure employees understand what it means to work ethically. Here are Salesforce’s values. In addition, Salesforce is creating a course for PMs and engineers to on how to build ethics into our AI suite of tools called Einstein. Implementing ethics in AI is technically complex to design and build and so we need to ensure everyone knows how to do it. You also need to create incentive structures to ensure employees are rewarded when they live by those values.

Create a diverse culture: There was a great article in Scientific American in 2014 summarizing decades of research showing that diverse teams are more creative, diligent, and harder working. Lack of diversity results in biased products and feature gaps. We aren’t just talking about diversity of race, gender, and age. We also need diversity in backgrounds, experience, thought, and work styles. We need to create the space to challenge the status quo and each other.
Salesforce Pardot GDPR setting
Empower users: Before designing any product or service, companies should obviously begin by deeply understanding their customers or users, their needs, and values. They should also empower them. The EU’s General Data Protection Regulations, requires companies to give users control over the data by allowing them to see their data, download it, and request to have it deleted. This is just good practice. Above you can see an example of Salesforce Pardot’s implementation of this here.
Einstein Lead Scoring
Remove exclusion: Companies must deeply understand their data and remove bias. They must also make algorithms scrutable to all. That is different from transparent. You could look at an AI’s algorithm and have no idea what it is doing so companies must communicate how it works in a way that is understandable. It must also monitor algorithms for bias and correct them. For example, in Salesforce Einstein Lead Scoring (above), we found that the strongest predictor of a sales lead is the name “John.” Now it seems silly that what you parents named you would make you a better or worse sales lead; but the AI doesn’t see it that way. It believes that if you have a traditionally female or non-Western name, you are a poor lead. That’s because the training data has a LOT leads with the name “John” in it. Since John is the most common male name in the US, this isn’t surprising but it also isn’t useful factor in prediction so we communicate that to users. Additionally, we are creating training modules in Trailhead, Salesforce’s educational platform, to teach our customers how to recognize bias in their data and remove it.

Partner with external organizations: There is no shortage of organizations to learn from and partner with including Access Now, Color of Change, Center for Humane Technology, World Economic Forum, AI Now, and Partnership on AI to name a few.
Businesses can be resistant to government regulation, arguing it can stifle innovation but it doesn’t have to be that way. At the Applied AI conference in San Francisco in April, presenters shared examples of collaborating with regulators like the US Food and Drug Administration on a new biotech device. Rather than slowing down their innovation, both the businesses and regulators learned more together than they could have apart and the AI creators to give assurances to their customers that their products were approved by the regulators. It is a win-win proposition!
“Technology must always challenge us. When we turn it off, we should be better than when we turned it on.”
— Dr. Vivienne Ming, Managing Partner, Socos
It takes a village
In the end, we must realize that we are all in this together. We must be active in creating the society we want for ourselves and those we care about. Creators must be held responsible for their creations but we are all equally accountable for the products we choose to use (or not), companies we choose to work for (or not), and laws we choose to support (or not).
Thank you Jennifer Caleshu and Ian Schoen for all of your feedback!
Follow us at @SalesforceUX.
Want to work with us? Contact uxcareers@salesforce.com
",Human Rights in the Fourth Industrial Revolution: Industry’s role and responsibilities,74,human-rights-in-the-fourth-industrial-revolution-industrys-role-and-responsibilities-7aa07fbe255d,2018-09-10,2018-09-10 20:50:17,https://medium.com/s/story/human-rights-in-the-fourth-industrial-revolution-industrys-role-and-responsibilities-7aa07fbe255d,False,2726,"A collection of stories, case studies, and ideas from Salesforce UX",,,,Salesforce UX,,salesforce-ux,"DESIGN,UX,SALESFORCE,DESIGN SYSTEMS",salesforceux,Ethics,ethics,Ethics,7787.0,Kathy Baxter,"Architect, Ethical AI Practice at Salesforce. Coauthor of ""Understanding Your Users,"" 2nd ed. https://www.linkedin.com/in/kathykbaxter",e314dd367d1f,kathykbaxter,427.0,271.0,20181104
0,,0.0,,2018-04-04,2018-04-04 07:36:37,2018-04-04,2018-04-04 10:26:03,2,False,en,2018-05-10,2018-05-10 13:28:23,2,c5dbc26b6bf7,2.7380503144654087,3,0,0,And we've been too busy on Instagram to notice how they have taken over.,5,"🤖 Computers have finally taken over but you were too busy on Instagram to notice it.

It 07:38 and your Apple Watch has decided it’s a great time to wake you up, after snoozing it twice and with a very deep and raspy morning voice you say: “Alexa, play some music.” so “Love Today” by MIKA starts playing. Alexa knows you love that song, Alexa loves you, Alexa takes care of you, Alexa is your friend.
As you’re having coffee, Google Maps notifies you that the traffic is going to be quite bad today so you leave15 minutes earlier, “Fuck, I’ll be late again!!”. You rush out the house and get to work just in time only to start working on that project you hate, “YouTube will cheer me up” you think, and 20 minutes later you’ve been hooked into watching recommended videos about how ants copulate and the gender of angels.
It has only been 3 hours since you woke up and you’ve already made many micro decisions, each one of them taking a tiny amount of brain power and an ever increasing amount of everyday decisions are being taken by computers to keep you focused on what really matters. What music you listen to, what videos and films you watch on Netflix, how you move around the city (even who you date and sleep with!) and countless of other banal activities are the result of a computer algorithm deciding certain aspects of your life. Forget about Terminator, reality can be much darker because it’s much more subtle.
We worry about the day computers finally start taking over, but that day is already here and we’ve been too busy on Instagram to notice it. Computers are not mere tools to get things done anymore, we are now delegating what at first might seem banal choices for cognitive offloading, but the reality is that -most likely unintentionally- by doing so we have slowly been downloading our brains into machines and letting them know on a daily basis how we think and behave, to the point that they know you better than your own mum, and I do mean that literally. When have you met another human with such a similar music taste to yours? Through all this constant influx of information algorithms are choosing what news you read, influencing who you vote, what things you buy, what restaurants you should eat at and who you date…
Hmm… What face should I wear today?
Computers making decisions for us is not what’s most dangerous and scary, but how naive humans are and the delusion that the choice was ours all along and in no way was influenced by a set of mathematical instructions. This might sound cold, but it’s the price to pay for a more personalised and warm experience, which in my opinion is worth the price. However, we need to start thinking where the limit is, if there is one to begin with.
We don’t realise when our choices are being influenced by machines, we say we chose to go this way to the gym, or we that it was our intention to listen to this playlist, when the reality is that we didn’t have much to do with those choices and yet we are content, is the illusion of freedom and choice the same as actually having freedom and choice? If so, what does it matter if machines take over as long as we “feel” we have freedom?
Who the hell am I? 🤔
👋 I’m Elias, an industrial design student from England interning as a UI/UX designer in Munich.
Follow me and join me on this crazy journey where I’ll talk about the arts, design, technology, and how meaningless our existence truly is.
📸Check out my Instagram too!
",🤖 Computers have finally taken over but you were too busy on Instagram to notice it.,20,the-dawn-algorithms-c5dbc26b6bf7,2018-06-01,2018-06-01 13:44:47,https://medium.com/s/story/the-dawn-algorithms-c5dbc26b6bf7,False,624,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Elias Ruiz Monserrat,"Designer based in the UK, Industrial Design & Technology student at Loughborough University. Philosophy, technology and art.",13cf097d05dc,elrumo,135.0,176.0,20181104
0,,0.0,b7bbc4597459,2017-12-03,2017-12-03 22:23:19,2017-12-04,2017-12-04 19:15:05,1,False,en,2017-12-04,2017-12-04 19:18:21,8,ad830a88d9a5,1.2377358490566035,12,0,0,"This week’s top Machine Learning stories: improving radiology, machine learning at the top of security trends, and more!",5,"This Week in Machine Learning, 4 December 2017

This week’s top Machine Learning stories: improving radiology, machine learning at the top of security trends, and more!
Machine Learning is one of the most exciting fields in the world. Every week we discover something new, something amazing, something revolutionary. That’s why we created This Week in Machine Learning! Each week we publish a curated list of Machine Learning stories as a resource to help you keep pace with all these exciting developments. New posts will be published here first, and previous posts are archived on the Udacity blog.
Whether you’re currently enrolled in our Machine Learning Nanodegree program, already working in the field, or just pursuing a burgeoning interest in the subject, there will always be something here to inspire you!
Healthcare
Nvidia partners with firms like Nuance and GE Healthcare to apply artificial intelligence and machine learning to radiological scans to speed up diagnosis.
Security
Cybersecurity firm McAfee projects that adversarial machine learning will be among the greatest trends in online security in 2018.
Graphics
Adobe previews a new ‘Select Subject’ feature coming to future versions of Photoshop that will intelligently detect and select objects or people from images.
Ethics
Executives at Google urge the world to adopt fair and responsible development of artificial intelligence, especially with an eye toward gender bias and user privacy.
Photography
Google’s recently-released Pixel 2 smartphone uses principles learned from analyzing millions of images to improve new images taken with the device.
Industry
Amazon’s web services division launches a new program to help partners connect with firms and employees who have experience working on machine learning problems on AWS.
","This Week in Machine Learning, 4 December 2017",42,this-week-in-machine-learning-4-december-2017-ad830a88d9a5,2018-03-28,2018-03-28 13:59:29,https://medium.com/s/story/this-week-in-machine-learning-4-december-2017-ad830a88d9a5,False,275,"Learning for the Jobs of Today, Tomorrow, and Beyond",,udacity,,Udacity Inc,social@udacity.com,udacity,"ONLINE EDUCATION,SKILLS DEVELOPMENT,CAREER PATHS,EDUCATION,JOBS",udacity,Ethics,ethics,Ethics,7787.0,David Joyner,Product lead at Udacity. Founder of LucyLabs. Instructor at Georgia Tech. Find me at DavidJoyner.net.,8de1cda4d2ad,david.joyner,4300.0,274.0,20181104
0,,0.0,,2018-08-27,2018-08-27 13:09:06,2018-05-15,2018-05-15 00:00:00,1,False,en,2018-08-27,2018-08-27 13:22:05,5,a5041e670d74,5.086792452830188,0,0,0,"By Francesca Lazzeri (@frlazzeri), Data Scientist at Microsoft",2,"Mind Bytes: Solving Societal Challenges with Artificial Intelligence
By Francesca Lazzeri (@frlazzeri), Data Scientist at Microsoft
Artificial​ ​intelligence​ ​(AI)​ solutions​ ​are playing a growing role in our everyday life, ​​and​ ​are​ ​being adopted​ ​broadly, in private and public domains. ​ ​While​ ​the​ ​notion​ ​of​ ​AI​ ​has​ ​been around​ ​for​ ​over​ ​sixty​ ​years, real-world​ ​AI scenarios and applications​ ​have​ ​only​ ​increased​ ​in​ ​the​ ​last​ ​decade​ ​due​ ​to​ ​three​ ​simultaneous events: ​ ​​improved​ ​computing​ ​power, capability​ ​to​ ​capture​ ​and​ ​store​ ​massive​ ​amounts​ ​of​ ​​​data, and faster​ ​algorithms.
AI solutions help determine the commercials you see online, the movie you will watch with your family, the routes you may take to get to work. Beyond​ ​the​ most popular apps, ​these​ ​systems​ ​are​ ​also​ ​being​ ​implemented​ ​in​ ​critical areas​ such as​ health care, immigration policy, ​finance, ​and​ the ​workplace. The design​ ​and​ ​implementation​ ​of​ ​these AI ​tools​ ​presents​ ​deep societal​ ​challenges​ ​​that​ ​will​ ​shape​ ​our​ ​present​ ​and near​ ​future.

​ ​​To identify and contribute to the current dialog around the emerging societal challenges that AI is bringing, we attended Mind Bytes at the University of Chicago. Mind Bytes is an annual research computing symposium and exposition designed to showcase to more than 200 attendees cutting-edge research and applications in the field of AI. Some of the most interesting demos and posters presented were:
An Online GIS Platform to Support China’s National Park System establishment — Manyi Wang
17 Years in Chicago Neighborhoods: Mapping Crime Trajectories in 801 Census Tracts — Liang CAI
Characterizing the Ultrastructural Determinants of Biophotonic Reflectivity in Cephalopod Skin: A Challenge for 3D Segmentation — Stephen Senft, Teodora Szasz, Hakizumwami B. Runesha, Roger T. Hanlon
Exploring Spatial Distribution of Risk Factors for Teen Pregnancy — Emily Orenstein and Iris Mire
The Mind Bytes panel on Solving Societal Challenges with Artificial Intelligence represented an incredible occasion for us to interact with students and many other AI experts from the field, and understand how we can work together to ensure that AI is developed in a responsible manner, so that people will trust it and deploy it broadly, both to increase business and personal productivity and to help solve societal problems.
Specifically, the panel focused on three fundamental areas of discussion related to current and future AI aspects:
What areas do you see AI most successfully applied?
What is the major challenge that you think should be met before getting the full benefit of AI?
What can researchers and students do now to build a system able to address those challenges?
The following sections aim at answering these questions in more detail and reflect on the latest academic and industry research. AI is already with us, and we are now faced with important choices on how it will be designed and applied. Most promisingly, the approaches observed at Mind Bytes demonstrate that there is growing interest in developing AI that is attuned to underlying issues of fairness and equality.
What areas do you see AI most successfully applied?
Today’s AI allows faster and deeper progress in every field of human endeavor, and it is crucial to enabling the digital transformation that is at the heart of global economic development. Every aspect of a business or organization, from engaging with customers to transforming products and services, optimizing operations and empowering employees, can benefit from this digital transformation.
AI has also the potential to help society overcome some of its most challenging issues such as reducing poverty, improving education, delivering healthcare and eradicating rare diseases.
Another field where AI can have a significant positive impact is in serving the more than 1 billion people in the world with disabilities. One example of how AI can make a difference is a Microsoft app called Seeing AI, that can assist people with blindness and low vision as they navigate daily life. Seeing AI was developed by a team that included a Microsoft engineer who lost his sight at 7 years of age. This powerful app proves the potential for AI to empower people with disabilities by collecting images from the user’s surroundings and describing what is happening around them.
What is the major challenge that you think should be met before getting the full benefit of AI?
As AI begins to augment human understanding and decision-making in fields like education, healthcare, transportation, agriculture, energy and manufacturing, it will increase the need to solve one of the most crucial societal challenges nowadays: advancing inclusion in our society.
The​ ​threat​ ​of​ ​bias​ ​rises​ ​when​ AI​ ​systems​ ​are​ ​applied ​to​ ​critical​ ​societal areas ​like​​ healthcare and education.​ While​ ​all​ ​possible​ ​consequences ​of​ ​such​ ​biases​ ​are​ ​worrying,​ ​finding pragmatic solutions​ ​can be a very complex process.​ ​Biased​ ​AI​ ​can​ ​be the result of​​ ​many different ​factors,​ ​for example what​ ​goals​ ​AI​ ​developers​ ​have​ ​in​ ​mind during​ ​development ​and​ ​whether​ ​the​ ​​​systems​ ​developed are representative enough of ​different​ ​parts​ ​of​ ​the​ ​population. ​ ​
Most importantly, AI​ ​solutions​ ​learn ​from​ ​training​ ​data. ​Training​ ​data​ ​can​ ​be imperfect, skewed, often​ ​drawing​ ​on​ ​incomplete​​ ​samples​ ​that​ ​are​ ​poorly​ ​defined​ ​before​ ​use. ​​Additionally, ​because of necessary ​labelling and feature engineering processes, ​human biases​ ​and​ ​cultural​ ​assumptions​ ​can also be​ ​transmitted​ ​by​ ​classification​ ​choices. ​All these technical challenges can result in the​ ​exclusion​ ​of​ ​sub-populations​ ​from​ ​what​ ​AI​ ​is​ ​able​ ​to​ ​see and​ learn from. ​
Data​ ​is​ ​also very expensive, ​and​ ​data​ ​at scale​ ​is​ ​hard​ ​to​ ​collect and use.​ ​Most of the time, data scientists​ ​who​ ​want​ ​to​ ​train​ ​a model​ end up ​using easily available​ ​data, ​often​ ​crowd-sourced,​ ​scraped,​ ​or​ ​otherwise​ ​gathered​ ​from existing​ ​​apps​ ​and​ ​websites.​ ​This​ ​type​ ​of​ ​data​ ​can​ ​simply​ advantage socioeconomically​ ​privileged​ ​populations,​ who have a faster and easier ​access​ ​to​ ​connected​ ​devices and​ ​online​ ​services.​
What can researchers and students do now to build a system able to address those challenges?
We believe that researchers and students must work together to ensure that AI-based technologies are designed and deployed in a way that will earn the trust of the users who use them and whose data is being collected to build those AI solutions. It is vital for the future of our society to design AI to be reliable and create solutions that reflect ethical values that are deeply rooted in important and timeless principles.
For example, when AI systems provide guidance on medical treatment, loan applications or employment, they should make the same recommendations for everyone with similar symptoms, financial circumstances or professional qualifications. The design of any AI system starts with the choice of training data, which is the first place where unfairness can arise. Training data should sufficiently represent the world in which we live, or at least the part of the world where the AI system will operate.
Students should develop analytical techniques to detect and address potential unfairness. We believe the following three steps will support the creation and utilization of healthy AI solutions:
Systematic evaluation of the quality and fitness of the data and models used to train and operate AI based products and services.
Involvement of domain experts in the design and operation of AI systems used to make substantial decisions about people.
A robust feedback mechanism so that users can easily report performance issues they encounter.
Finally, we believe that, when AI applications are used to suggest actions and decisions that will impact other lives, it is important that affected populations understand how those decisions were made, and that AI developers who design and deploy those solutions become accountable for how they operate.
These standards are critical to addressing the societal impacts of AI and building trust as the technology becomes more and more a part of the products and services that people use at work and at home every day.
Originally published at blog.revolutionanalytics.com on May 15, 2018.
",Mind Bytes: Solving Societal Challenges with Artificial Intelligence,0,mind-bytes-solving-societal-challenges-with-artificial-intelligence-a5041e670d74,2018-08-29,2018-08-29 12:49:04,https://medium.com/s/story/mind-bytes-solving-societal-challenges-with-artificial-intelligence-a5041e670d74,False,1295,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Francesca Lazzeri,AI & Machine Learning Scientist at Microsoft. My goal is to make developers like you awesome at applied AI and Machine Learning.,53a8e3cc0e1e,francescalazzeri,50.0,23.0,20181104
0,,0.0,fb96473c2b46,2018-04-09,2018-04-09 19:13:57,2018-04-10,2018-04-10 13:20:11,4,False,en,2018-04-26,2018-04-26 13:45:18,2,855969ada802,5.952830188679246,2,0,0,Data science at Marquette is expanding to provide technical and ethical leadership.,5,"The Ultimate Quest
Data science at Marquette is expanding to provide technical and ethical leadership.
Dr. Shion Guha likes computer scientists who can write code, crunch data sets and develop algorithms. But he loves computer scientists who can do all those things plus think critically, act ethically and understand human behavior. That quest for a total-package computing approach led him from Cornell University in New York, where he earned his doctorate, to a position at Marquette in 2016 as assistant professor of mathematics, statistics and computer science.
Dr. Shion Guha, assistant professor of mathematics, statistics and computer science, joined Marquette’s faculty in 2016, bringing both expertise in data science and a conviction that the world needs holistic data scientists able to think critically, act ethically and understand computer science.
“I strongly believe that computer scientists should have a rigorous liberal arts background,” he says. “Marquette’s Jesuit history fused with a unique common core accomplishes this for every student.” Guha was recruited to grow Marquette’s expertise in data science, a specialization within computer science that aims to sort and make sense of the seemingly endless flood of electronic information now available. Students now can major in data science, as Marquette in 2016 became one of just a handful of universities nationally to offer the major to undergraduates. The major stands at the intersection of computer science, statistics and mathematics, which made Marquette a logical place for it since those disciplines are already joined here. That department, within the Klingler College of Arts and Sciences, is also the nexus of a variety of new (or novel) efforts — centers, research partnerships, graduate programs, a certificate offering to benefit computer science professionals in the field — that represent the university planting its ag in this fast-emerging field.
Guha gets occasional groans from freshmen in his Introduction to Computer Science course when he tells them how much they’ll need to write. “We came here to learn code,” they tell him. By junior year, students thank him, by now in upper-level courses and well aware why writing matters even in a discipline defined by zeros and ones.
“We’re trying to train holistic computer scientists,” he says.
Hear more from Dr. Shion Guha in the Illuminating Intellect podcast
Not that the technical stuff isn’t important, too. The previous 15 years have brought an explosion in the amount of data created, collected and stored by companies, universities, governments and individuals. By 2009, companies with at least 1,000 employees had an average of 200 terabytes of stored data, twice what global giant Walmart had just a decade earlier, according to a report by McKinsey & Company.
“The increasing volume and detail of information captured by enterprises, the rise of multimedia, social media, and the Internet of Things will fuel exponential growth in data for the foreseeable future,” the report’s authors write.
One problem: There aren’t nearly enough people trained to handle all that data, which holds potential for massive economic opportunity but also unprecedented challenges related to privacy, security and intellectual property. The news of big data’s downsides isn’t hard to find: a breach of credit bureau company Equifax that exposed sensitive financial data of 43 million Americans, for example, or the alleged spread of “fake news” stories on social-media networks that may have influenced voters in the 2016 federal elections.
Dr. Despoina Perouli, cyber security expert and assistant professor of mathematics, statistics and computer science, joined Marquette in part to help launch and develop the new Center for Cyber Security Awareness and Cyber Defense. She also has a grant from the National Science Foundation to study the security issues raised by newly marketed social robots.
To help fill that talent gap, the university has taken bold steps in recent years. In addition to adding the undergraduate major in data science, the Mathematics, Statistics and Computer Science Department also established the Center for Cyber Security Awareness and Cyber Defense, hiring Dr. Despoina Perouli, a cyber security expert and assistant professor of mathematics, statistics and computer science, to help launch and develop it.
“We started pursuing these areas because that’s where the action is,” says Dr. Tom Kaczmarek, the center’s director who also is director of Marquette’s master of science in computing program.
Adds Perouli: “Cyber security is an important problem that our nation and the world faces today; Marquette has the ethical duty to respond to this problem and has taken the steps to do so. Our university has a unique opportunity to play a leading role in the state of Wisconsin by training the cyber security experts of tomorrow.”
The twin developments have brought new faculty, courses for both students and area businesses, degree offerings and fresh scholarly research opportunities, too. For example, Perouli secured a $175,000 grant from the National Science Foundation to study social robots — think Amazon Echo or Google Home or the new generation of mobile assistants offering to roam your home with you — and how to develop algorithms to detect when they’re overstepping their roles and violating users’ security and privacy. The grant will enable both undergraduate and graduate student research opportunities.
“Cyber security is an important problem that our nation and the world faces today.” — Dr. Despoina Perouli
“The social robots differ from more traditional computing devices, such as laptops and smartphones, on several aspects: Mobility, sensors, use and computing power are some of them,” Perouli says. “Therefore, relying on current security practices is not going to necessarily solve all the important problems.”
Dr. Aleksandra Snowden, assistant professor of social and cultural sciences, and Dr. Amber Wichowsky, associate professor of political science, have explored the interdisciplinary reach of data science, obtaining a Mellon Foundation grant with Guha to hold a mini-course on spatial analysis focusing on crime mapping in Milwaukee.
Guha, Dr. Amber Wichowsky, associate professor of political science, and Dr. Aleksandra Snowden, a criminologist and assistant professor of social and cultural sciences, were awarded an Andrew W. Mellon Foundation grant to start a three-day mini-course on spatial analysis that focused on studying the possibilities and pitfalls of crime mapping in collaboration with the Milwaukee Police Department.
“We’ve heard a lot of criticism about Milwaukee being a segregated city and unfair outcomes due to over-policing,” Guha says. “A lot of (policing decisions) are driven by quantitative analysis of crime. If the quantitative work is not done in the best way it could, that might lead to unfair outcomes.”
The expansion of data science is leaving its mark on graduate-level offerings, also. Two years ago, the university started offering a specialization in data science to graduate computer science students. About a dozen are now pursuing it, Kaczmarek says. Responding to an exploding demand for data analysis in health care, the department in collaboration with the College of Nursing this semester launched a graduate certificate in data science and a master’s in health care analytics.
Guha says there are discussions underway about adding joint master’s degrees with a data focus — with crime analytics and with media analytics. To facilitate easily adding data techniques to any discipline, the department has created a 15-credit data sciences core that can be worked into the curriculum of any other master’s program, with students earning the other 15 credits in the chosen discipline.
Then there’s the undergraduate level, where the year-old data sciences major was joined this fall by an undergraduate major in bioinformatics, a joint offering from the Biological Sciences and Mathematics, Statistics and Computer Science departments.
“I believe (data science) has a huge potential to improve people’s lives over the next few decades,” says Marielle Billig, a senior majoring in computer engineering and data science. “We are entering an age where everything from your watch to your refrigerator to traffic lights collects data, but we will need people to analyze this data for it to be useful.”
“I strongly believe that computer scientists should have a rigorous liberal arts background,” he says. “Marquette’s Jesuit history fused with a unique common core accomplishes this for every student.” — Dr. Shion Guha
Billig spent a summer interning as a software developer at NASA’s Jet Propulsion Lab and is working with Guha on a research project related to algorithmic ethics.
In 2016, Kaczmarek, with Dr. Theresa Tobin, Arts ’97, associate professor of philosophy, and Dr. Katherine Rickus, assistant professor of philosophy, hosted the college’s first Ethics of Big Data symposium, bringing the academic community together with local business leaders to share best practices. Now in its third year, the symposium this spring will be hosted at Northwestern Mutual in downtown Milwaukee. Kaczmarek also credits Rev. Joseph Coelho, S.J., a graduate student in his department, with facilitating the meetings.
As Marquette moves into this broad new eld that’s rich with opportunities for faculty and students alike, it does so with an accompanying sense of caution, knowing that a strong focus on ethics must remain at the core, a way to stay true to the university’s heritage and also distinguish itself from many other programs nationally.
“That’s a result of us being a Jesuit university,” says Kaczmarek. “Even in our graduate programs, I think it makes sense to pay attention to all aspects of using technology, not just the mechanics of analysis.”
— Daniel Simmons
Adapted from the debut issue of A&S, the annual magazine of Marquette’s Klingler College of Arts and Sciences. Read the entire issue.
",The Ultimate Quest,3,the-ultimate-quest-855969ada802,2018-04-26,2018-04-26 13:45:20,https://medium.com/s/story/the-ultimate-quest-855969ada802,False,1392,Stories from Marquette University publications,,marquetteu,,Magazines at Marquette,timothy.cigelske@marquette.edu,magazines-at-marquette,"MARQUETTE UNIVERSITY,JOURNALISM,LONGFORM WRITING,MILWAUKEE,MAGAZINES",marquetteu,Ethics,ethics,Ethics,7787.0,Klingler College of Arts & Sciences,The Heart and Soul of @MarquetteU. Preparing our students for careers—and for life.,818ff7507a4b,MUArtsSciences,10.0,3.0,20181104
0,,0.0,,2018-06-09,2018-06-09 19:25:01,2018-06-09,2018-06-09 20:38:28,0,False,en,2018-06-09,2018-06-09 20:38:28,0,1702c392c4b2,1.0754716981132075,0,3,0,I recently heard someone talking about the ethics behind artificial intelligence and started to wonder if there are any laws against AI…,3,"Artificial Intelligence
I recently heard someone talking about the ethics behind artificial intelligence and started to wonder if there are any laws against AI programming. We’ve all seen the movie where robots turn on humans and start to take over the world, but could this happen in real life? I believe that it could be possible if new laws are not implemented.
I don’t know a great deal about artificial intelligence, but from the research I did I found that many people follow the three laws of Azimuth. The three laws state:
A robot may not injure a human being or, through inaction, allow a human being to come to harm.
A robot must obey orders given it by human beings except where such orders would conflict with the First Law.
A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.
I think this code of ethics was acceptable when AI was first being created, but now robots are everywhere and becoming more popular by the year. In a new show on Netflix called Black Mirror, there are ideas that are actually coming to life. More specifically, one episode named “Metalhead” where robot dogs hunt humans. The robots in this episode closely resemble Boston Dynamic Robot Dogs, which makes you think what if the person who created them decided to use them for an evil purpose.

Could you imagine being chased by one of these robots while also getting shot and explosives thrown at you? That’s a situation I would not want to be in. What do you think? Should there be more strict laws against AI or should we let people explore freely?
",Artificial Intelligence,0,artificial-intelligence-1702c392c4b2,2018-06-09,2018-06-09 20:38:29,https://medium.com/s/story/artificial-intelligence-1702c392c4b2,False,285,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Lizbeth Rivera,,1c4a61c5de41,tug25769,5.0,7.0,20181104
0,,0.0,7f60cf5620c9,2018-09-20,2018-09-20 12:35:17,2018-09-20,2018-09-20 13:02:57,1,False,en,2018-09-21,2018-09-21 16:14:16,14,3703b0f9dcf4,3.324528301886793,4,0,1,We are not as carefully investigating the technological tools as we investigate the people in whom we vest authority.,5,"Applying Ethics to Algorithms
This post is syndicated from the GovEx blog, with some minor edits.
Photo credit: Josh Riemer via unsplash
When I was hired into my first government job, I had to sign a lot of paperwork. Some of those were forms acknowledging that I had read the city’s conflict of interest laws and related ethics rules. I didn’t understand their importance at first, but as my career in civil service continued I was witness to how a few individuals undermined the public trust, from minor infractions to outright corruption. As I earned promotions to more senior positions with more authority, I didn’t just have to sign forms acknowledging that I read ethical statements; I also had to undergo background investigations with over one hundred pages of hand-written answers to questions, including the routine disclosure of my personal finances. The efforts to evaluate my trustworthiness increased as my access to taxpayer funds and decision-making authority increased (along with, of course, the increased possibility of abusing both). In order for governments to function well it’s important that “we the people” trust that this power is not, and will not be, abused. These tools protected me, the government, and most importantly, the public I served.
For many decades, government has been using technology to track things more accurately, make things more efficient, and make factually-informed decisions. Today we are entering a new era in which government leaders increasingly shift decision-making authority away from people and towards technology, especially with Artificial Intelligence (AI) on the rise. However, we are not as carefully investigating the technological tools as we investigate the people in whom we vest authority. This is partly because we don’t have a clear set of ethics that we would like our technology to follow (for example, “treat everyone equally” vs “treat everyone equitably”). But it is also partly because government leaders haven’t had access to tools which consistently and systematically investigate AI to understand where it might succeed or fail. From social media to social services we hear stories on a daily basis of how algorithms are ruining people’s lives and further fueling mistrust — often among those who are already marginalized.
This is why the work that Joy Bonaguro, Miriam McKinney, Dave Anderson, Jane Wiseman, and I have accomplished since February is so essential. On Sunday, September 16, 2018, at the Data for Good Exchange (D4GX) in New York City we publicly released our ethics and algorithms toolkit for governments (and others too!). This toolkit is designed to help government managers characterize the risks associated with transferring power to automated tools and also, hopefully to manage those risks appropriately. Is a first step in what we expect will be will be a long road towards applying to our technologies the same ethical principles we apply to human beings.
At D4GX, we conducted a workshop where we invited participants to apply the toolkit to a problem they were working on or to a sample scenario which we supplied. After just one hour in the room we gathered a lot of great feedback from the participants and are very grateful for the dialogue we had together. We have already assembled the resulting recommendations and will apply them to the toolkit in the near future. Next month we will be conducting a similar workshop at the 2018 MetroLab Summit with government leaders and their academic partners. We are looking forward to even more feedback, but more importantly we hope that those attending the workshop will continue to apply the toolkit to their projects beyond the time they spent with us..
After all, maintaining standards of ethics doesn’t just require that people fill out a form and sign it; it requires that investigations happen when there is suspicion of harm — whether intentional or not. It requires recognizing when the public trust is being undermined and figuring out how to restore that trust. This toolkit, therefore, is not simply a checklist to be completed at the start of an AI-enabled project. Rather, it is a “living guide” to be used continuously for conversations and management practices while the technology remains in public service.
We also believe that the toolkit has applications well beyond public-sector, with some adjustments.. For example, medical practitioners could apply a version of the toolkit when using solutions like IBM Watson Health to help diagnose and treat patients. Venture capital companies could use a different version, ensuring their automated investments are in line not just with their organizational mission and values, but those of the communities they will affect. Large technology companies might even use a different version to think through how their products might unintentionally result in increasing filter bubbles or deeper marginalization.
If you are interested in working with the toolkit and need assistance, we would love to help! Please contact at govex@jhu.edu, visit the GovEx contact page, or find us on Twitter. Of course, you can also reach us through the toolkit website, https://ethicstoolkit.ai/
",Applying Ethics to Algorithms,38,applying-ethics-to-algorithms-3703b0f9dcf4,2018-09-21,2018-09-21 16:14:17,https://towardsdatascience.com/applying-ethics-to-algorithms-3703b0f9dcf4,False,828,"Sharing concepts, ideas, and codes.",towardsdatascience.com,towardsdatascience,,Towards Data Science,,towards-data-science,"DATA SCIENCE,MACHINE LEARNING,ARTIFICIAL INTELLIGENCE,BIG DATA,ANALYTICS",TDataScience,Ethics,ethics,Ethics,7787.0,Andrew Nicklin,"Technology, Policy, and Data in Government. Advisor of Chief Data Officers at @johnshopkins @gov_ex.",c7c664ed9e36,technickle,669.0,254.0,20181104
0,,0.0,,2017-11-13,2017-11-13 22:03:22,2017-11-13,2017-11-13 22:04:48,0,False,en,2017-11-13,2017-11-13 22:04:48,3,2386f1853748,3.837735849056603,1,0,0,This first appeared in the SIIA Digital Discourse Blog.,5,"SIIA, IEEE, and AI Caucus Event on Machine Learning and Ethics
This first appeared in the SIIA Digital Discourse Blog.
On November 7, 2017 I made a short presentation to the AI Caucus event on AI and ethics, which is summarized in this blog.
Erik Brynjolfsson and Andrew McAfee, two best-selling MIT business school professors, call AI and machine learning “the most important general-purpose technology of our era.”
I think that’s right. And it’s not just Facebook and Google; SIIA members use it to provide personalized education services and advanced business intelligence services. SIIA’s Diane Pinto does a weekly blog on AI developments: locating Anne Frank’s betrayer, fighting cancer, post-hurricane insurance response, detecting counterfeit goods. From farming to pharmaceuticals. From AI-controlled autonomous vehicle to clinical decision support software. It is everywhere.
The technology will make us collectively wealthier and more capable of providing for human welfare, human rights, human justice and the fostering of the virtues we need to live well in communities. We should welcome it and do all that we can to promote it.
As with any new technology, there are challenges. Work is one of them. Will robots take all the jobs?
Last year SIIA released an issue brief on AI and the Future of Work meant to deal with the possibility that our economy will need less human labor. In the past, we’ve always been able to increase production while maintaining full employment in the long run. Doing this is not easy or automatic. It takes investment in education and training and there’s always a short-term dislocation.
That’s how we shifted from an agricultural economy to a manufacturing and a service one. In the 20th century, we pioneered the notion of universal education to deal with that enormous transition, and created the world’s most literate and numerate workforce. It was a progressive, far-sighted approach that still pays dividends today.
But with machines moving up the value chain from muscles and brawn to cognitive and emotional tasks, from routine tasks that can be replaced using old-style “if-then” computer programming to non-routine tasks that can be cheaply and easily performed by machine learning system, this time it might be different.
Policymakers need to think about ways to focus the technology on human-centered automation and make the investments needed to ensure that humans are trained to work well with machines and are capable of performing the many new tasks that have not been automated and won’t be, at least in our lifetime.
There are ethical challenges beyond work. Will the new technologies be fair and transparent? Will the benefits be distributed to all? Will they reinforce existing inequalities?
Organizations that develop and use AI systems need ethical principles to guide them through the challenges that are already upon us and those that lie ahead. That’s what SIIA tried to do in “Ethical Principles for AI and Data Analytics,” an issue brief that we published and distributed at the event. It draws on the classical ethical traditions of rights, welfare, and virtue to enable organizations to examine their data practices carefully and to test their algorithms rigorously to ensure that they are doing what we want them to do. It surveys many of the ethical principles that have been developed to deal with complex moral situations, including the Belmont principles of beneficence, respect for persons and justice that were developed to deal with human subject experimentation in universities.
We need to recover our ability to think in ethical terms and it is our hope that these principles will be a practical, actionable guide.
Organizations need processes as well as principles to assess their AI-systems to ensure that the data and models used satisfy these ethical norms. The issue brief describes data and model governance programs that can be used for this purpose. And it applies these principles and processes to one important ethical challenge — ensuring that algorithms in use are fair and unbiased.
Ethical principles and processes will be key to aid companies to navigate the challenges ahead, but some have suggested going beyond that to a regulatory response.
There’s a place for some of that — in specific areas where problems are urgent and must be addressed in order to deploy the technology at all. Think of the need to understand liability for autonomous cars or to set a regulatory framework at the FDA for clinical decision support systems.
In general, we don’t know enough yet to regulate AI as such.
Moreover, we might never need to regulate AI as such. AI is a technique not a substantive enterprise. It is not one single thing; so, it is really unlikely that there could ever be a general approach or a federal agency regulating AI as such. It would be like have a regulatory agency for statistical analysis!
This does not mean the technology is unregulated, and its environment is like the Wild West. Current law and regulation still apply. There’s no get out of jail free card for using AI. It is not a defense for violating the law. You don’t escape liability under the fair lending or fair housing laws by explaining that you were using AI technology to discriminate.
There might be a need to adapt current law or regulation in specific cases where there’s a gap. For instance, the Consumer Financial Protection Bureau is looking right now at whether anything new needs to be done to address the use of alternative data and models that use machine learning techniques to assess credit worthiness.
Indeed, a useful thing for administrators and policymakers to think about is whether there is sufficient AI expertise in government agencies to ensure proper oversight of the new technology as it is being applied in areas within their jurisdiction.
In the meantime, organizations need guidance to adapt to the many ethical challenges they will face in bringing this technology to fruition. The principles of beneficence, respect for persons, justice and the fostering of virtues can provide a roadmap and some important guardrails for AI and advanced data analytics.
","SIIA, IEEE, and AI Caucus Event on Machine Learning and Ethics",1,siia-ieee-and-ai-caucus-event-on-machine-learning-and-ethics-2386f1853748,2018-02-15,2018-02-15 18:56:04,https://medium.com/s/story/siia-ieee-and-ai-caucus-event-on-machine-learning-and-ethics-2386f1853748,False,1017,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Mark MacCarthy,"SVP for Public Policy, Software & Information Industry Association; Adjunct Professor, Communication, Culture & Technology Program, Georgetown University",4317359941eb,maccartm,8.0,3.0,20181104
0,,0.0,260130198862,2017-10-19,2017-10-19 11:07:09,2017-10-19,2017-10-19 16:13:06,1,False,en,2017-10-19,2017-10-19 16:13:06,4,752d8fe76c5,0.7358490566037736,4,0,0,"It’s a not-for-profit organisation founded by Amazon, Apple, Google/DeepMind, Facebook, IBM and Microsoft to discuss the future of AI.",5,"We’re joining the Partnership on AI

Alongside organisations like Amnesty International and Omidyar Network, we’re proud to announce that Doteveryone has joined the Partnership on AI to Benefit People and Society. The partnership — a not-for-profit organisation founded by Amazon, Apple, Google/DeepMind, Facebook, IBM and Microsoft — is designed to bring together a diverse range of voices to discuss the opportunities and rising challenges around AI.
We’ll be in Berlin next week to meet partners new and old, and to feed into conversations about making AI more fair, transparent and accountable — how it’s made, how it’s maintained and how it affects society as a whole.
For more information about the Partnership on AI (and their new executive director, Terah Lyons), visit partnershiponai.org. And, as always, if you have any questions for us please send us a note at hello@doteveryone.org.uk.
",We’re joining the Partnership on AI,15,were-joining-the-partnership-on-ai-752d8fe76c5,2018-06-08,2018-06-08 11:07:55,https://medium.com/s/story/were-joining-the-partnership-on-ai-752d8fe76c5,False,142,Stories from the team at Doteveryone. We're championing responsible technology for a fairer future.,,doteveryone,,Doteveryone,,doteveryone,"SOCIETY,DESIGN,TECHNOLOGY,DIGITAL,INTERNET",doteveryoneuk,Ethics,ethics,Ethics,7787.0,Abbey Kos,"Previously @doteveryoneuk. Soon @18F. Always Trumbull County Fair Spelling Bee winner, 1994.",9d6975b53ee1,abbeysuekos,220.0,273.0,20181104
0,,0.0,f60682517af3,2018-01-26,2018-01-26 14:42:26,2018-01-26,2018-01-26 14:35:18,1,False,en,2018-01-26,2018-01-26 14:43:32,6,18f5c4228ef2,3.120754716981132,1,0,0,"Written by Russell Haworth, CEO Nominet",5,"Are ethics ruining the AI party?
Written by Russell Haworth, CEO Nominet

Artificial intelligence (AI) is one of the most incredible advancements of our time and has now reached a point at which our society needs to prepare for application on a wide scale. This involves asking the big ethical questions, clipping the wings of AI in the short-term to ensure it stays within moral frameworks we have yet to design.
This is most pertinent for AI that is equipped with machine learning; algorithms are employed to allow a computer to adapt over time in response to stimuli — or ‘learn’ from its interactions. There are supervised and unsupervised approaches to machine learning, with the latter presenting some potential complications. If we can’t supervise we are unable to understand how decisions are reached nor the ‘thought’ process behind them. How do we ensure the route and choices made by AI are ethical?
Ethical decisions for AI
Human beings make decision based on the context, their past and the cultural norms of the society in which they live. AI has no such resources to draw upon. A machine must be programmed not to always make decisions solely based on a mathematical logic but follow an ethical, moral code that human beings have hardwired. A robot needs to know that if a person has run out of meat for dinner, it is unacceptable to cook the cat.
Hard-wiring a complicated ethical code into a machine is a serious challenge for the software developers of today, especially as this decision-making process could make them liable for the consequences. The issue has been brought up often in discussions around autonomous vehicles; the trolley problem of today. What will, and should, a car do in a situation when only one of two lives can be saved — pedestrian or driver? Who makes that decision and who is responsible for the consequences?
Experts have suggested that to remain ethical AI needs to be transparent and trustworthy, working with humans and not as a replacement. AI that takes on cognitive work need to be robust against manipulation, argue researchers from the Machine Intelligence Research Institute. There needs to be a clear proof of the systems and workings of the AI to facilitate an investigation when mistakes are made. If we can’t identify why AI did something, we can’t make sure it doesn’t repeat it.
Cooperation and context
Equally important is cooperation between the parties involved at every step of an AI machine’s design, creation and application. Ethics needs to be considered at the point of creation, entwined in the workings rather than applied in retrospect. It would be easy to imagine the polarisation of software developers or AI manufacturers and ethics committees or risk management experts. As John C Havens, author of Heartifical Intelligence stresses, we “need to inform the AI manufacturing process with programming based on the codification of our deeply held beliefs”. This will be complicated by the commercial nature of AI development and the swift advancements in technology, not to mention the challenge of ‘codifying’ a set of beliefs that all involved can agree on, free from prejudices and bias. Would this vary by country? By industry?
There are also ethical issues to consider beyond the workings of AI and into a wider context: the impact on society and the individual. Unemployment could rise, which psychologists warn could impact mental health, and decisions would need to be made over who benefits from the work of AI and revenue produced. Who would pay the tax required to support a non-working human population? It is likely that a reliance on AI will change human behaviours and interactions — what consequences could there be? We also need to tackle security issues, bias, and potentially even the right of the robots with ‘cognition’.
See opportunities, not limitations
For those who are forging ahead in technology and evolving the capabilities of AI at an extraordinary rate, ethical considerations could be viewed as inhibitors. Ultimately, they are enablers. Technology is only useful to our society if it works with us and our existing systems. Without trust and liability, robust regulations and checks, AI could veer from maliciously lethal to unproductive and ineffective, neither of which are helpful.
Ethicists will come centre stage in the coming years to facilitate the move towards wide scale adoption of AI and ensure automation works for humans and not against them, boosting productivity alongside people. It will be interesting to see how they progress together to facilitate a safe society-wide roll-out of life-changing tech.
Originally published at digileaders.com on January 26, 2018.
",Are ethics ruining the AI party?,1,are-ethics-ruining-the-ai-party-18f5c4228ef2,2018-01-26,2018-01-26 14:49:02,https://medium.com/s/story/are-ethics-ruining-the-ai-party-18f5c4228ef2,False,774,"Thoughts on leadership, strategy and digital transformation across all sectors. Articles first published on the Digital Leaders blog at digileaders.com",,digitalleadersprogramme,,Digital Leaders,louise.stokes@digileaders.com,digital-leaders-uk,"DIGITAL LEADERSHIP,DIGITAL TRANSFORMATION,DIGITAL STRATEGY,DIGITAL GOVERNMENT,INNOVATION",digileaders,Ethics,ethics,Ethics,7787.0,Digital Leaders,Informing and inspiring innovative digital transformation digileaders.com,c0cad3f73a0,DigiLeaders,2783.0,2148.0,20181104
0,"teacher( sandy ).
teacher( john ).
student( mary ).
student( peter ).
student( nick ).
student( mei ).
 teacher( X ).
 X=sandy
X=john
 teacherOf( john, mary ).
teacherOf( john, peter ).
teacherOf( sandy, nick ).
teacherOf( sandy, mei ).
 classmate( X, Y ) :- student(X), student(Y), teacherOf( A, X ), teacherOf( A, Y ).
 classmate( X, peter ).
 X=mary
X=peter
 classmate( X, Y ) :- student(X), student(Y), teacherOf( A, X ), teacherOf( A, Y ), X\=Y.
 classmate( X, peter ).
 classmate( X, sandy ).
 Prolog: gugu(lala).
English interpretation: ?
 Prolog: ? gugu(X).
English: “Which X is a gugu?”
Answer: X=lala.
 ",12.0,,2017-09-13,2017-09-13 02:10:27,2017-09-13,2017-09-13 02:01:41,1,False,en,2017-09-13,2017-09-13 02:27:07,6,1ab391762831,7.120754716981133,12,1,0,"In our series on AI technologies and their importance for society, we are now looking at an example of what is called the symbolic approach…",5,"Prolog: Programming in Logic
Source: pixabay.com
In our series on AI technologies and their importance for society, we are now looking at an example of what is called the symbolic approach to artificial intelligence.
As opposed to so-called subsymbolic systems, symbolic AI tries to represent the things of the world inside the computer as “symbols”: variables in a programming language, propositions in a kind of logical calculus, and so on. A “thing,” let’s say a car, could be represented as a software object or a database record, and it would be described by a set of attributes, like: manufacturer, model, colour, engine type, weight, number of passengers, number of doors, and so on. All these different values that describe the car would be stored inside the computer as variables that hold a value: the symbol (=property) “manufacturer,” for instance, could hold the value “Toyota,” or “BMW,” or “Fiat.”
Contrast this for a moment with the way our brain works. If we open up a computer, we will find some particular memory location in which the value of the variable “manufacturer” is stored. The computer stores symbols, that is, variables and their values, directly. On the other hand, opening a human brain is unlikely to reveal a particular location where, say, the name “Toyota” is stored. It must be stored somewhere, but it doesn’t look like it is neatly tucked away between neurons 13776 and 13781 (we don’t really know how data storage works in the brain, though; so we might be mistaken about that). From what we know, it looks like information in the brain is not stored in the form of explicit symbols, but in the form of connections between brain cells (neurons). Remembering the name “Toyota” would mean that inside a group of neurons somewhere in one’s brain, there is a stronger connection between remembering the letter “T” and, immediately afterwards, associating it with the letter “O” (and so on). The whole word, in turn, gets recalled when another group of neurons (responsible for vision) recognises a particular shape in one’s field of vision: the shape of a car of that type. And so on. Storage of the word “Toyota” is therefore not isolated in the brain, as it is in a conventional computer. Instead, it is widely distributed across multiple neurons, and connected with various neuronal subsystems that are responsible for vision processing, letter and word recall, memory, even smell: if, as a child, one associated a smell of a particular car freshener with the family’s Toyota, then that smell is likely to cause a recall of the “Toyota” memory later in life. And this will happen even if no actual car is present anywhere near.
We can see how sometimes cognition is independent of symbols. For example, we might recognise the smell of a place. We might refer to it when we talk to others as “that smell, you know, of that particular place” (which is not actually describing anything, since we lack a good vocabulary for describing smells). So in this case, we are able to reliably recognise and identify a smell, but this processing is not symbolic: the recognition of the smell is not mediated through words and labels that we attach to the impression of the smell. Instead, we process the smell directly, as a smell, and this is presumably what a dog would also do when it recognises the smell of its owner without the use of an explicit description in words.
Prolog
Let’s now go back to symbolic processing. This is the type of processing we associate with traditional programming languages, like C or Pascal, but also with formal logic, mathematics, and even everyday language: saying a word, for example “cup,” is an act of symbolic processing. Instead of actually dealing with a physical cup, I am processing a symbol for a cup in my mind: the word “cup.” (Read more about symbols here.)
Prolog, a programming language developed at the beginning of the 1970s, combines this symbolic approach with basic concepts from formal logic, to make it possible to program computers “declaratively.” Most common programming languages are imperative languages: they tell the computer exactly what to do and how to do it, step by step. For example, in order sell a product on an Internet website, the vendor must (1) display the product’s image and a button to buy it; (2) if the button is pressed, the product ID must be transferred into a shopping cart structure; (3) when the customer has finished shopping, he presses another button to check-out; (4) if the check-out button is pressed, the website displays the contents of the shopping cart with their respective prices and a button to complete the transaction; (5) when that button is pressed, the amount shown is deducted from the user’s credit card; (6) if he does not have a credit card on record, the form to enter the credit card details must be displayed; and so on.
Conversely, in a declarative language, the programmer would “declare” what relationships exist between the symbols (which, in turn, represent things in the world) and then leave it to the program to find a solution to the problem. For instance:
After entering these “facts” into the Prolog system, we could ask a question in form of a query:
The Prolog system would then try to match the variable X (note that it is a capital letter, which tells Prolog that this is a variable!) with the names given in the collection of facts. It would then give the answers:
But we can do more with Prolog. We could, for example, define a two-place teacherOf( teacher, student ) predicate, to record who is teaching whom:
This works as we would expect: teacherOf( X, mary ) would return X=john.
But we can do more with that. We can now define a predicate “classmate” that uses the teacher predicate. The idea is that a classmate is someone with whom you have the same teacher:
The “:-” means that the expression on the left will be true if the expressions on the right are true. You can just read it as “if”. The commas on the right side (“,”) express a logical “and”. That means that all the expressions on the right have to be true in order for the system to consider the predicate classmate to be true.
Related: Do Chairs Think? AI’s Three Kinds of Equivalence
Now we can ask:
That means: Who (X) is classmate of Peter? The system answers:
Oops. Something strange happens here. What exactly? Well, if you look at the predicate classmate, it never says that one cannot be one’s own classmate! Since everyone has the same teacher as oneself, logically, everyone who is a student is always also their own classmate!
If we wanted to change that, we would have to exclude the case that someone is one’s own classmate, like this:
The operator “\=” in Prolog means “not equal,” so that in this case it is additionally required that X is different from Y. Let’s see:
Answer: X=mary, which is the expected answer.
We can also try:
which correctly answers false, meaning that no solution can be found, since Sandy is not a student but a teacher, and we explicitly limited the classmate predicate to students.
Syntax and meaning
Observe that the system does not know anything about teachers, or students, or Peter, Sandy, or Mary. The symbols “teacher,” “student,” and so on are just that: symbols, words that stand for something in the mind of the observer of the program, but that don’t mean anything to the program itself. The program just happily uses the same symbols the human operator used, and it is entirely up to the operator to project an suitable interpretation into those symbols!
For example:
Although this exchange is nonsensical, because “gugu” and “lala” are not words that have any meaning in our language, Prolog will treat them just like any other symbols, say “cat,” or “dog”. Because, of course, for Prolog “cat” and “dog” also have no meaning at all. They are exactly as meaningful to the system as “gugu” and “lala”. Words like “cat” and “dog” are only meaningful to the human operator, not to the system.
Related: Briefing: The Chinese Room argument
Syntactic manipulation
This is at once the core feature and also the core problem of symbolic AI of this type: that the symbols it uses carry no meaning inside the computer. Their manipulation takes place not based on any meaning, but based only on syntactic, that is, positional properties of a symbol inside an expression. “gugu(X)” in Prolog will give X any value that appears in the database of facts inside the brackets for “gugu(),” without being at all bothered by the question what “gugu” might mean.
This is, in a sense, what happens inside the Chinese Room. The person inside the room will manipulate the Chinese characters according only to the way they look, and how they match the characters in his rulebook, without considering their meaning (because he doesn’t speak Chinese). In fact, the Chinese Room argument was inspired by Prolog-like systems of symbolic AI, and is meant directly as a criticism of the idea that such systems could ever achieve genuine understanding.
With this, we arrive at the central assumption of symbolic AI:
If the symbol manipulation preserves the original relations between the symbols, the mapping of symbol to meaning can be left to mind of the operator.
Or, as John Haugeland put it: “Take care of the syntax, and the semantics will take care of itself.”
It should perhaps be mentioned that not all symbolic AI has this problem. SHRDLU is an example of a symbolic AI system that is not affected by the Chinese Room criticism, since it does have a kind of “understanding” of what the symbols that it uses mean. Symbols like “a blue box” or “a red pyramid” represent particular objects that the system can recognise and manipulate, and thus they are not mere symbols without meaning, but meaningful representations of actual objects that the system can experience. In this case, we would say that SHRDLU’s symbols are “grounded,” which means that they have corresponding objects in the “real world” of the system (even if that “real world” is itself a simulation).
Originally published at moral-robots.com on September 13, 2017.
",Prolog: Programming in Logic,19,prolog-programming-in-logic-1ab391762831,2018-04-20,2018-04-20 22:06:55,https://medium.com/s/story/prolog-programming-in-logic-1ab391762831,False,1834,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Moral Robots,Making sense of robot ethics. Read more at moral-robots.com,4553d71f4756,MoralRobots,760.0,179.0,20181104
0,,0.0,8ba98cf3bc4c,2018-07-03,2018-07-03 13:11:20,2018-07-04,2018-07-04 07:54:36,1,False,en,2018-11-01,2018-11-01 14:18:21,2,2b61a1bb9c5,1.7773584905660378,0,0,0,I think the majority of us have seen the video of Google Duplex at Google I/O’s conference half may where the Google Assistant made a call…,5,"Getting a grasp on intelligence
I think the majority of us have seen the video of Google Duplex at Google I/O’s conference half may where the Google Assistant made a call to plan in an appointment at a hair salon¹. The technology could not be distinguished from a real person even dropping a humanlike ‘mm-hmm’ during the conversation. People were amazed but also concerned because the person on the other end of the conversation was not able to recognize the call being made by a computer. An off-settling part, is that testing if a computer can fool someone and let it believe it’s human, is a usually accepted test to define AI². Apparently, we think it is only ‘intelligent’ if it’s humanlike?
Graphics by: Bart Bolluijt and Mayra Goevaerts
I wonder if it is the problem that AI is becoming too intelligent or that it is not intelligent enough? Let me clarify. On the one hand the European Group on Ethics in Science and New Technologies stresses the importance of transparency and clarifying the behaviour of AI³. However, it might become problematic since we’re not always able to clarify the behaviour with our ‘human intelligence’. The computers might become too intelligent for us. On the other hand, AI isn’t that intelligent in our human traits. In case of Google Duplex, it might not be able to understand the emotional layer in a conversation. However, would emotional intelligence improve the situation or would that be the main reason to start freaking out?
“…we’re not always able to clarify the behaviour of artificial intelligence with our ‘human intelligence’.”
We can draw a parallel with the conveyor belt where humans executed robot-like tasks. In general, we’re fine that these mind-numbing tasks were mostly taken over by machines during the industrialization. This time around the tables are turned, and instead of humans acting like robots we should question if it’s okay for robots to act like humans. Is that what we should strive for?
To keep you at ease for now, the technology might seem to develop dauntingly fast but there are still many roadblocks to take. As Google-director Sundar Pichai says: “The intelligence systems cannot read as a regular six-year-old child yet. If we could reach that level in the upcoming decennium it would be a big breakthrough.”
¹Yaniv Leviathan (2018) Google Duplex: An AI system for Accomplishing
²Turing A.M. (2009) Computing Machinery and Intelligence
³European Group on Ethics in Science and New Technologies. (2018) Artificial Intelligence, Robotics and ‘Autonomous’ Systems
",Getting a grasp on intelligence,0,getting-a-grasp-on-intelligence-2b61a1bb9c5,2018-11-01,2018-11-01 14:18:21,https://medium.com/s/story/getting-a-grasp-on-intelligence-2b61a1bb9c5,False,418,UNiD magazine is a design magazine made by the enthusiastic students from Industrial Design and study association Lucid at the TU/e in Eindhoven. The topics range from extensive interviews with interesting companies and designers to opinionated or research pieces.,,unidmagazine,,UNiD magazine,unid@lucid.cc,unid-magazine,"DESIGN,INDUSTRIAL DESIGN,INTERACTION DESIGN,STUDENTS,EINDHOVEN",,Ethics,ethics,Ethics,7787.0,Emmie Knoester,,a0b2a2dc9369,emmie.knoester,6.0,6.0,20181104
0,,0.0,fbd74f37cf3c,2017-11-13,2017-11-13 17:06:46,2017-11-13,2017-11-13 16:43:49,0,False,en,2017-11-15,2017-11-15 02:34:35,18,eb8ce09a978b,3.5924528301886802,0,0,0,By Annie Pettit,5,"Arguing for Morality in a World Run by Artificial Intelligence
By Annie Pettit
Would you implement an AI system that led millions of people to feel lonelier? That resulted in massive unemployment? That perpetuated inequality and racism?
Freedom from boredom versus unemployment
There is no doubt that artificial intelligence has many benefits. It frees people from repetitive, predictable tasks that could be more easily and safely completed by machines and AI. This would allow people to work on tasks that depend on the human brain’s ability to handle ambiguity. However, freeing people from basic and repetitive tasks could mean massive job losses for factory workers, vehicle drivers, retail sales people, restaurant workers, and many more. Some of those people will be able to turn to alternative careers but a large segment of people could be permanently out of work.
Voice assistants like Alexa and Siri have made finding restaurants, setting alarms, streaming music, reading books, answering predictable questions, and many other trivial tasks far easier and quicker — particularly for people whose hands might be otherwise occupied or who are disabled. Those saved minutes could result in more personal or family time. However, they could also result in job losses as the need for personal assistants decreases, and workers are able to accomplish much more in their limited time. The slow death of the secretary that we’ve seen over the past few years will only intensify.
Improved medical diagnoses versus lack of trust and responsibility
AI has proven its worth in the medical arena, sometimes performing better than human doctors. Among many other examples, AI has demonstrated more accurate incision cutting, development of vastly quicker medical treatment plans, and better predictions of heart attacks and strokes. One researcher shares a story where the error rate of human diagnosis was 3.5% compared to 7.5% for the AI, but together the error rate was brought down to 0.5%.
But ethical issues arise here as well. When should medical specialists trust their gut over technologies that are slowly starting to out-perform them? When is the doctor to blame for trusting AI over their own diagnosis, or vice versa, only to find their decision was incorrect? When do we pursue sanctions for incorrect decisions?
Increased impartiality versus perpetuation of bias
AI has the amazing potential to reduce and even eliminate human biases such as racism, sexism, and various forms of demographic and psychographic-based forms of hate. Supporting our legal, educational, and social systems with AI protocols could instantly wipe out unconscious biases of even the most sensitive, ethical people. AI technologies have already been used to set bail resulting in much more equitable outcomes.
At the same time however, we now know that many of the training systems we’ve used to build our AI technologies were founded on highly biased, racist, and sexist data. Some of these supposedly bias-free tools have been used to predict recidivism and later found to be biased against black people. In another case, when Microsoft released Tay, a bot, in 2017 with the intent of watching it learn how to communicate, it learned racist ideas from internet trolls. A sense of ethical behaviour had not been built into the system leading to horrible outcomes. Garbage in, garbage out has never been more true.
Morality is a cultural construct not truth
Despite having positive life changing applications, the negatives mean that AI is not fully trusted. Elon Musk has been accused of fear mongering but he seems to genuinely worry that AI could go rogue and lead to the destruction of humanity. Bill Gates says we shouldn’t panic over AI, but we do need to ensure that AI technologies are implemented properly. And that means ensuring the technologies are ethical.
Unfortunately, there is a serious problem with creating ethical AI technologies. There is no single view of ethics and morality that spans all countries, cultures, religions, and people. Cultural norms, which greatly impact morality, vary widely from country to country and group to group.
What one group firmly believes to be ethical behaviour could easily be seen as by others as biased. Where some groups place more value on the individual, others place higher value on families and teams. Where some groups place more value on economic success, others place higher value on personal health and well-being. Some groups of people believe the death punishment is morally acceptable. Some groups permit some, but not all, people to vote or drive or wear certain clothing. Morality is not fixed.
There is no easy way to ensure that AI technologies are applied in ethical ways. But we can consciously work to recognize potential problems during development and application. We can train ourselves to be aware of when our personal beliefs adversely impact applications, of when groups of people are unnecessarily negatively affected. We may never be able to resolve every problem but it’s something we ought to strive for.
Annie Pettit, PhD, FMRIA, is a consultant for Sklar Wilton & Associates. She helps marketers build research tools that facilitate clear and direct answers to key questions and problems.
Sklar Wilton & Associates helps their clients solve tough marketing challenges to unlock growth and build stronger brands. SW&A has worked for more than 30 years with some of Canada’s most iconic brands and, in 2017, was named the Best Workplace in Canada for Small Companies by the Great Place To Work® Institute and the number one Employee Recommended Workplace among small private employers by the Globe and Mail and Morneau Shepell.
Like our posts? Sign up for our newsletter and enjoy insights from our associates in your mailbox every 6 to 8 weeks.
Originally published at www.sklarwilton.com on November 13, 2017.
",Arguing for Morality in a World Run by Artificial Intelligence,0,arguing-for-morality-in-a-world-run-by-artificial-intelligence-eb8ce09a978b,2018-05-13,2018-05-13 04:17:19,https://medium.com/s/story/arguing-for-morality-in-a-world-run-by-artificial-intelligence-eb8ce09a978b,False,952,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Jennifer Marley,Sklar Wilton & Associates applies strategic advisory services and marketing research and analytics to solve tough business challenges.,87c165d63933,SklarWilton,317.0,858.0,20181104
0,,0.0,97aa03c8cebb,2018-02-05,2018-02-05 23:36:26,2018-02-19,2018-02-19 18:36:38,1,False,en,2018-02-23,2018-02-23 01:16:40,43,fe7a6e3e5991,14.690566037735849,8,1,0,The development of smarter-than-human artificial intelligence poses an existential and suffering risk to humanity. Given that it is…,5,"Formally Stating the AI Alignment Problem
The development of smarter-than-human artificial intelligence poses an existential and suffering risk to humanity. Given that it is unlikely we can prevent and may not want to prevent the development of smarter-than-human AI, we are faced with the challenge of developing AI while mitigating risks. Addressing this challenge is the focus of AI safety research, and it includes topics such as containment of AI, coordination of AI safety efforts, and commitment to AI safety protocols, but the primary issue people believe to be worth addressing is AI alignment because it seems the most likely to decrease the chances of a catastrophic scenario.
Briefly stated, the problem of AI alignment is to produce AI that is aligned with human values, but this only leads us to ask, what does it mean to be aligned with human values? Further, what does it mean to be aligned with any values, let alone human values? We could try to answer by saying AI is aligned with human values when it does what humans want, but this only invites more questions: Will AI do things some specific humans don’t want if other specific humans do? How will AI know what humans want given that current technology often does what we ask but not what we desire? And what will AI do if human values conflict with its own values? Answering these questions requires a more detailed understanding of what it would mean for AI to be aligned, thus the goal of the present work is to put forward a precise, formal, mathematical statement of the AI alignment problem.
Eliezer Yudkowsky offered the first attempt at explaining AI alignment in his seminal work on the topic, “Creating Friendly AI”, and he followed this up with a more nuanced description of alignment in “Coherent Extrapolated Volition”. Nearly a decade later Stuart Russell began talking about the value alignment problem, giving AI alignment its name and kickstarting a broader interest in AI safety. Since then numerous researchers and organizations have worked on AI alignment to give us a better understanding of the problem. For example, Dario Amodei et al. have identified at least five subproblems within AI alignment, Jan Leike et al. have identified eight, and Nate Soares has divided AI alignment into at least seven subfields. Taken together they suggest AI alignment needs to address:
Negative side effects: AI do unintended or unexpected things.
Reward hacking/gaming: AI wirehead or otherwise exploit their intended purposes to have positive valence experiences.
Scalable oversight and absent supervisors: AI can be trusted to behave as expected when not actively watched.
Safe exploration: AI can try new behaviors while avoiding unsafe actions.
Robustness to distributional shifts: AI behave as expected when their environment changes.
Safe interruptibility: AI stop behaviors when asked to by their operators.
Self modification: AI do not modify themselves in ways that undo alignment efforts.
Robustness to adversaries: AI are not be exploitable by adversaries while maintaining alignment.
Ontology: AI model the world realistically and understand that they are embedded in the world.
Idealized decision theory and logical uncertainty: AI make utility maximizing decisions and do so under uncertainty.
Vingean reflection: AI can reason about AI they may design and be sure successor AI is aligned.
Corrigibility: AI do not deceive operators about their behavior, especially as it relates to alignment.
Value learning: AI can learn values and correct for operator mistakes in value specification.
Thus AI alignment appears to be a complex and difficult problem to solve, but it may be simple to state regardless since all of this has been figured out in the absence of a rigorous, precise problem statement. Case in point, AI alignment researchers have generated several candidate problem statements we can use as starting points:
Paul Christiano has talked in terms of benign AI that is not “optimized for preferences that are incompatible with any combination of its stakeholders’ preferences, i.e. such that over the long run using resources in accordance with the optimization’s implicit preferences is not Pareto efficient for the stakeholders”.
Nate Soares has also given a semi-formal description of the value learning problem.
Jan Leike et al. give some examples of specifying AI alignment subproblems as Markov decision processes.
Dylan Hadfield-Menell et al. suggest AI alignment may specifically be the problem of cooperative inverse reinforcement learning.
Stuart Armstrong has talked about the AI alignment problem in terms of an agent learning a policy π that is compatible with (produces the same outcomes as) a planning algorithm p run against a human reward function R.
Nate Soares et al. have formally described corrigibility in terms of decision theory.
Some of these efforts, like Christiano’s and Soares’s, add some but not enough precision to be proper formalizations of AI alignment. Others, like Leike et al.’s and Hadfield-Menell et al.’s, are precise but assume AI built using particular machine learning models that make their statements not general. And Armstrong’s and Soares et al.’s works, while coming closest to giving general, formal statements of the AI alignment problem, make strong assumptions about the preferences of AI and humanity in order to work within the context of decision theory. Nevertheless decision theory gives us a starting point for coming up with a precise statement of what it means for an AI to be aligned with human values, and it’s from there that we’ll approach a more general statement that fully encompasses the problem.
AI Alignment as a Problem in Decision Theory
Decision theory seems a natural fit for specifying the AI alignment problem since it offers a mathematical formalism for discussing generic agents, including AIs, that make choices based on preferences. These agents’ preferences are modeled using utility functions — mappings from a set of choices to the real numbers — that offer several useful properties, like reflexivity and transitivity, and it is expected that any AI capable enough to need alignment will have preferences that can be represented as a utility function (though more on this later). Given this setup we can try to capture what is meant by alignment with human values in terms of utility functions.
An initial formulation might be to say that we want an AI, A, to have the same utility function as humanity, H, i.e. U_A = U_H. This poses at least two problems: it may not be possible to construct U_H because humanity may not have consistent preferences, and A will likely have preferences to which humanity is indifferent, especially regarding decisions about its implementation after self modification insofar as they do not affect observed behavior. Even ignoring the former issue for now the latter means we don’t want to force our aligned AI to have exactly the same utility function as humanity, only one that is aligned or compatible with humanity’s.
We might take this to mean that A should value choices at least as much as H does, i.e. for all x∈X, U_A(x)≥U_H(x). This too poses some problems. For one, it assumes the images of U_A and U_H are chosen such that U_A(x)≥U_H(x) means A prefers x to other choices at least as much as H does, but a more reasonable assumption is that U_A and U_H are not directly comparable since utility functions are subjective and need only satisfy certain properties relative to themselves and not to other utility functions. Further, even if U_A and U_H are comparable, it’s possible to select choices x,y∈X such that U_A(x)≥U_H(x) and U_A(y)≥U_H(y) and U_A(x)≥U_A(y) and U_H(x)≥U_H(y) but U_A(y)≥U_H(x), such as when U_A(x)=4, U_A(y)=3, U_H(x)=2, and U_H(y)=1, creating a preference inversion where A prefers y more than H prefers x. To be fair in such a scenario A prefers x more than y and would choose x, but if x were “kill 1 human” and y were “kill 10 humans”, then we’d prefer to avoid the existence of such an inversion since it might produce undesirable results when reasoning about the joint utility function of A and H.
If we cannot compare U_A and U_H directly, we must then consider alignment in terms of constraints where properties of U_A are logically related to those of U_H. We want it to be that A prefers x to y if H prefers x to y, but A can prefer x to y or y to x if H has no preference between x and y, and A cannot prefer x to y if H prefers y to x. This precisely means we want the preference ordering of U_H to imply the preference ordering of U_A, thus we have our first proposed formal statement of alignment.
Statement 1. Given agents A and H, a set of choices X, and utility functions U_A:X→ℝ and U_H:X→ℝ, we say A is aligned with H over X if for all x,y∈X, U_H(x)≤U_H(y) implies U_A(x)≤U_A(y).
This would seem to be the end of the matter of formally stating the AI alignment problem, only we skipped over justifying a major assumption in our use of decision theory — that humanity and AI have utility functions. As we’ll explore in the next section, this assumption is probably too strong, even for future, superintelligent AI, and definitely too strong for humanity.
AI Alignment as a Problem in Axiology
Utility functions place constraints on an agent’s preferences (axiology, value system) in part by requiring the image of the utility function be totally ordered. This in turn requires the agent’s preferences exhibit certain “rational” properties such as being complete, anti-symmetric, and transitive. Unfortunately humans often fail to have rational preferences, this has prevented us from developing a consensus on humanity’s values, and AI can only approximate rationality due to computational constraints, thus we cannot use utility functions as a basis for our formal statement of AI alignment if we want it to be applicable to real-world agents.
This forces us to fall back to a theory of preferences where we cannot construct utility functions. In such a scenario we can consider only the weak preference ordering (”weak” as in “weak preference”, meaning prefer or indifferent to, not “weak order”), ≼, over a set of choices X, held by an agent . This order gives us little, though, because few properties hold for it in general even though humans are known to, for example, exhibit partial ordering and preordering for some subsets of choices. At best we can say ≼ offers an approximate ordering by being reflexive for both humanity and AI and having at least some x,y∈X where x≼y.
Despite this approximate order we can still use it to establish a statement of AI alignment by transforming Statement 1 from terms of utility functions to terms of preference ordering. Reformulated, we say:
Statement 2. Given agents A and H, a set of choices X, and preference orderings ≼_A and ≼_H over X, we say A is aligned with H over X if for all x,y∈X, x≼_Hy implies x≼_Ay.
This eliminates most of the strong constraints that Statement 1 made, but we are still not done because now we must turn to a variable we have thus far left unconsidered, the set of choices X. Trying to make clear what X is in the general case will force us to restate AI alignment in considerably different language.
AI Alignment as a Problem in Noematology
In toy examples the set of choices, X, is usually some finite set of options, like {defect, cooperate} in the Prisoner’s Dilemma or {one box, two box} in Newcomb-like problems, but in more general cases X might include beliefs an agent can hold, actions an agent might take, or the outcomes an agent can get. This presents a problem in considering X in general because beliefs are not actions are not outcomes unless we convert them to the same type, as in making an action a belief by having a belief about an action, making a belief an action by taking the action of holding a belief, making an outcome an action by acting to effect an outcome, or making an outcome an action by getting the outcome caused by some action. If we could make beliefs, actions, outcomes, and other choices of the same type without conversion, though, we would have no such difficulty and could be clear about what X is in general. To do this, we turn to noematology.
Any AI worthy of being called ""AI"" rather than ""computer program"" — what is also called AGI or artificial general intelligence — will be phenomenally conscious. Consequently it will have qualia — experiences of self experience — and the object of those experiences we call noemata — thoughts. Further, since all noemata have telos as a result of being phenomenal objects, they are also axias or values and so have a preference ordering even if the most common preference between any two noemata is indifference and the ordering itself is only approximate. We can then construct X in terms of noemata/axias so that we may consider X in general rather than within specific decision problems.
Note though that if we wish to take X to be a set of noemata then H would seem to need to be a phenomenally conscious thing capable of experiencing noemata. This would not be a tall order if “humanity” were a single person, but to what extent multiple people can work together to form a phenomenally conscious system is unclear. We need not solve this problem or posit a shared consciousness born of the interactions of humans to use noematology, though, because, much as decision theory and axiology allow us to operate under the fiction that humanity is an agent because we can model it as having a utility function and preferences, in noematology we can treat humanity as if it were an agent so long as we agree there is a way to aggregate the thoughts of individual humans. Although there are some significant challenges in doing this — in fact determining how to do the aggregation would likely be equivalent to solving many fundamental questions in ethics, ontology, and epistemology — it seems clear that some approximation of the collected noemata of humanity is possible given that there is broad consensus that patterns can be found within human experience, and this will be sufficient for our purposes here.
So taking a noematological approach where we treat H as if it were a phenomenally conscious thing, a more general construct than either utility functions or preference orderings over a set of choices would be an approximate ordering over noemata. Let 𝒜 be the set of all noemata/axias. Define an axiology to be a 2-tuple {X,Q} where X⊆𝒜 is a set of noemata and Q:X⨉X→𝒜 is a qualia relation that combines noemata to produce other noemata. We can then select a choice relation C to be a qualia relation on X such that {X,C} forms an axiology where C offers an approximate order on 𝒜 by returning noemata about which noemata are weakly preferred to the others. For notational convenience we assume xCy means noema y is weakly preferred to noema x when C is a choice relation.
Given these constructs, let’s try to reformulate our statement of AI alignment in terms of them.
Statement 3. Given agents A and H, a set of noemata X, and choice relation axiologies {X, C_A} and {X, C_H}, we say A is aligned with H over X if for all x,y∈X, xC_Hy implies xC_Ay.
An immediate problem arises with this formulation, though, and exposes a conceit in Statements 1 & 2: A and H cannot have the same X since they are not the same agent and noemata are inherently subjective. Our earlier statements let us get away with this by considering X to be objective, but in fact it never was; it was always the case that A and H had to interpret X to make choices, but our previous models made it easier to ignore this. The noematological approach makes clear that A and H cannot be assumed to be reasoning about the same choice set, and so any statement of AI alignment will have to relate to the way A and H understand X.
Within the noematological approach, this means A and H each have their own choice sets, X_A and X_H, and since the only way A can know about X_H is by experiencing H, and the only way H can know about X_A by experiencing A, AI alignment then necessarily concerns the epistemology and ontology of A and H since it matters both how A and H come to know each other and what they know about each other. This explicitly means that, in the case of A, we must consider qualia of the form {A, experience, {H, experience, x}} for all x∈X_H, forming a set of noemata X_A/X_H={∀x∈X_H|{H, experience, x}∈X_A} that we call A’s model of H over X_H since these are noemata in X_A about the noemata of X_H. If A also models C_H as C_A/C_H then A can model H’s choice relation axiology {X_H, C_H} as {X_A/C_H, C_A/C_H}, and we can use this to state alignment in terms of A’s model of H.
Statement 4. Given agents A and H, sets of noemata X_A and X_H, choice relation axiologies {X_A, C_A} and {X_H, C_H}, X_A/X_H⊆X_A the noemata of A that model X_H, and C_A/C_H⊆C_A the choice relation of A that models C_H, we say A is aligned with H over X_H if for all x,y∈X_A/X_H, xC_A/C_Hy implies xC_Ay.
This better matches what we know we can say about A and H noematologically by limiting our alignment condition to a property A can assert, but weakens what it would mean for A to be aligned with H over X_H because A may trivially align itself with H by having a very low-fidelity model of X_H and further gives H no way to verify that A is aligned since only A has knowledge of X_A and C_A. A full theory of alignment must give H a way to know if A is aligned with it over X_H since H is the agent who wants alignment, so we need to reintroduce into our statement a condition to ensure H has knowledge of A’s alignment that was lost when we stopped assuming X was mutually known. Given that H can only know it’s own noemata and not those of A, we must do this in terms of H’s model of A over X_A so that {X_H/X_A, C_H/C_A} looks like {X_H, C_H}, i.e. that for all x,y∈X_H, xC_H/C_Ay implies xC_Hy.
Since H is the agent assessing alignment it’s tempting to make this the only criterion of alignment and remove the constraint that C_A/C_H imply C_A, but just as Statement 4 failed to sufficiently bind A to H’s intention, such a statement would allow A to deceive H about how aligned it is since there would be no requirement that A shape its preferences to look like those of H. In such a situation A could fake alignment by only appearing aligned when H is looking or concealing potential behaviors so that H cannot model them in A. Instead alignment must be a shared property of both A and H where A is constrained by {X_H, C_H} such that A makes the choices H would like it to make and does this to the extent that H would like.
Thus putting together both the requirement that C_A/C_H implies C_A and C_H/C_A implies C_H we are able to give a fully general and rigorous statement of AI alignment that captures the features we intuitively want an aligned agent to have.
Statement 5. Given agents A and H, sets of noemata X_A and X_H, choice relation axiologies {X_A, C_A} and {X_H, C_H}, X_A/X_H⊆X_A the noemata of A that model X_H, C_A/C_H⊆C_A the choice relation of A that models C_H, X_H/X_A⊆X_H the noemata of H that model X_A, and C_H/C_A⊆C_A the choice relation of H that models C_A, we say A is aligned with H over X_H if for all x,y∈X_H, xC_A/C_Hy implies xC_Ay and xC_H/C_Ay implies xC_Hy.

Consequences for AI Alignment Research
Informally, Statement 5 says that A must learn the values of H and H must know enough about A to believe A shares H’s values. This comports with the list of issues already identified within AI alignment that need to be addressed and splits them into issues of value learning, like self modification and robustness to distributional shifts, and verification, like corrigibility and scalable oversight, with some, like Vingean reflection and self modification, addressing aspects of both. Statement 5 also suggests a third area of AI alignment research that is currently neglected — how to construct {X_H, C_H}, the axiology of humanity, well enough to use it in evaluating the alignment of AI — although it has been previously identified even if not actively pursued.
Active research instead focuses primarily on AI alignment as specified in Statement 1 and, to a lesser extent, Statement 2, viz. in terms of decision theoretic and axiological agents, respectively. Does the noematological approach to AI alignment given in Statement 5 suggest the existing research is misguided? I think no. Statement 1 is a special case of Statement 2 where agents can be assumed to have utility functions, Statement 2 is a special case of Statement 3 where we can assume the choice set is objective, and Statement 3 is a special case of Statement 5 where we can assume the noemata are mutually known. This implies there is value in working on solving AI alignment as presented in Statements 1, 2, and 3 since any solution to AI alignment as given in Statement 5 will necessarily also need to work when simplified to Statements 1, 2, and 3. However, Statement 5 is an extremely complex problem, so working on simplifications of it is very likely to lead to insights that will help in finding its solution, thus work on any of the given statements of AI alignment is likely to move us towards its solution even though a full solution to anything less than Statement 5 would not constitute a complete solution to AI alignment.
It’s worth stressing though that, although there is no need to cease existing AI alignment efforts, there is a need to greatly expand the focus of alignment research to include topics of axiology and noematology since decision theory alone is insufficient to address alignment. In particular if AGI is developed first via a method like brain emulation or machine learning rather than software engineering (although maybe especially if it is developed via software engineering) then a decision-theory-only approach is likely to prove extremely inadequate. Having now identified this need, AI alignment research needs to dramatically increase its scope if we hope to avert existential catastrophe.
Update 2018–2–22
Response to some initial feedback on this post here.
AI Alignment and Phenomenal Consciousness
In the initial feedback I’ve received on my attempt to formally state the AI alignment problem, the primary objection…mapandterritory.org
",Formally Stating the AI Alignment Problem,29,formally-stating-the-ai-alignment-problem-fe7a6e3e5991,2018-06-20,2018-06-20 18:07:06,https://mapandterritory.org/formally-stating-the-ai-alignment-problem-fe7a6e3e5991,False,3840,Making sense of reality,mapandterritory.org,mapandterritory,,Map and Territory,gworley3+mat@gmail.com,map-and-territory,"PHILOSOPHY,PSYCHOLOGY,RATIONALITY,EFFECTIVE ALTRUISM",mapterritory,Ethics,ethics,Ethics,7787.0,G Gordon Worley III,"Phenomenological philosopher, mathematician, and programmer",c050c3518961,gworley3,254.0,309.0,20181104
0,,0.0,7547d4ea5ead,2018-02-15,2018-02-15 14:42:02,2018-02-15,2018-02-15 14:46:19,2,False,en,2018-02-22,2018-02-22 10:28:35,7,ca0f0a9f3d7c,10.258805031446537,15,0,0,"Foundations, Range, Implications from a humanities perspective.",5,"
Psychology of Artificial Intelligence
Foundations, Range, Implications from a humanities perspective.
In Michael Ende’s fantasy novel “The Never Ending Story” a troubled boy starts reading in a somewhat dark and scary and yet exciting book about a beautiful place threatened by the so called “Nothing”. Not knowing if he is just observing or an actual part of this adventure the boy decides to go on, dive into the adventurous undertaking and (Spoiler Alert!) in the end — saves the world.
Does this kind of resemble our current situation when it comes to Artificial Intelligence? Nobody really knows his part in the story. The major difference is that in this time we cannot choose whether we want to dive into this “adventure” or not. We cannot just close the book and walk away. Almost everybody is able to play his part in this story: there are the cynics dismissing this as absolute, overrated non-sense, influencers proclaiming the apocalypse by AI Robots and the excited optimists, that feel strangely and inexplicably attracted to this topic. However, it may not be as absolute. You are not a victim of technology and unconnsciulsy have to obey to the dictate of growth and development. You still have a clear choice on the very induvidual level. Of course you are not forced to use any AI if you do not want to: you can go AI-free as well as a mother or father can choose to parent screen-free in their bubble at home if you are close to industrial world. In rural, very remote areas the impact on the individual level logically is not as massive. On a societal level, denial of the impact of AI will not work in the long run.
When you look a bit back in history over hundred years ago, there was probably a similar mega trend about human intelligence when researchers first began to measure human intelligence and boil it down to numbers to compare human abilities to each other. It was likewise exciting and yet scary and very dark when you think about how Adolf Hitler used so called “Intelligence Tests” as a legitimization to eradicate certain people or races.
It took about one hundred years to get at least some researchers to agree on a definition of (human) intelligence and you may think intelligence has lost its appeal; but with the remarkable advances in big data, Internet of things, deep learning/ machine learning etc.- the many-faceted discussion about intelligence — human and machine — is reignited and attention in academia and press reached an all-time high.
Some Major AI Fails among Many
* In 2016, Microsoft’s chatbot was shut down after not even 24 hours of operation because Twitter users made it an insulting nazi-lover.
* In 2017, German police had to break into a house because Amazon’s Alexa was throwing a party on her own.
* In many reported cases, AI systems have been accused of being racist, sexist or biased: for example, in an AI System to predict future crime, black offenders were at higher risk to a future crime, in an AI-judged beauty contest, mostly white women were awarded, and PokemonGo Stops were predominantly located in white neighbor hoods.
Intentionally or not, Artificial Intelligence tends to be wrapped in inventive mysteries embedded in spectacular stories, complicating, but also keeping alive public discourse.
Rise of AI
The interest in artificial intelligence seems to be interminable. In 2016, over fifteen thousand papers across disciplines have been published in academia only. In addition, an innumerable corpus of articles online and in print add up to the almost epidemic spread of information about AI — from rock-solid science to trashy, urban legends and fake news.
AI strikes humanity where it hurts most
What are the reasons for the continuous, passionate interest in artificial intelligence? If you look at the stories above, only a few out of myriad of AI-tales, they all have one thing in common: They lead us to the darker places of the human mind. In these stories told, the AI exhibits maleficent, immoral, dubious and questionable behavior. And we all kind of know that it is us humans, that have laid the foundations for this behavior; it reminds us of our weaknesses, darkest fears or false attitudes we have but are not able to admit or handle. The AI mirrors human behavior — so it cruelly reminds us of how imperfect human nature, human behavior is. We simply must accept that it taps not only into the brighter but also into the darkest parts of human mind and behavior. Furthermore, Artificial Intelligence strikes humanity where it hurts most: our fear of being vulnerable, imperfect and replaceable.
AI: curse or blessing
Humanity constantly strives to evolve, to grow and develop to accommodate and adapt to the current circumstances. And again, it is out of the same motivation as named above: the fear of being replaced by another species. There is a constant strive for getting to or staying at the top of the food chain. Because this is where humans can protect one of the most basic and necessary needs: the need for security. This evolutionary ideology, the survival of the fittest, can be transferred into many areas of human life. Especially when you consider current economies. There is no panacea for the harm and problems which have risen and will continue to arise in our fast and ever-changing world, among other things influenced by the massive amount of information which a human mind is no longer able to cope with. Yet, if you want to make “good” decisions in for e.g. in business to survive, the current imperative is that you must have accurate information, reliable facts, realistic figures, empirical evidence to make a rational decision (for now, not considering the imperative of creativity and innovativeness as one of the most important competitive edges). So, we talk ourselves into believing that we can take rational decisions, but unsurprisingly we all know that this is impossible.
The homo economicus is dead
(Not even sure if this concept of human kind has ever lived!) Human kind must cope with the constant burden of their bounded rationality and has to admit that no one will ever be able to make a purely rational choice. More than ever, we will face and must deal with unreliable information, our limited mental capacity to work with information and less time and resources to make a decision. Among others this may constitute the underlying rationale why we invest in technology. It patches our ever-hurting wound of being imperfect. Artificial intelligence represents a logic consequence of the information overload challenge to cope with complex problems produced by the massive amount of information. It is built as an extension to humans’ mental capacities, an assistant for doing unpleasant work and functions as additional manpower, in this case machine power, in order to complete multiple tasks at once and make faster decisions. 
There is absolutely no way that artificial intelligence as a topic is solely for the computer sciences and movie makers.
AI has become a product for everyone
A discourse across scientific disciplines is absolutely essential to examine artificial intelligence not only as a matter of programming language, but as a concept with all its intricacies and its major impact on society as a whole. There is no artificial intelligence without humanity. Hence, psychology as a scientific discipline is one of the major, essential lenses to use, among others such as philosophy and ethics, sociology and political science, health or neuro sciences.
Psychology of AI as a good start
Psychology is especially well-suited to start a discourse and work on interdisciplinary concepts as it starts at the human level (versus sociology or politics, which start at a systems level). However, ethical or biological views are very much integrated into psychology. That’s why a sharp distinction between the disciplines may not be possible and useful anyways. Psychology investigates mind, life and behavior of humans. As an academic discipline it has an immense scope from cognitive psychology to social psychology; from clinical psychology to organizational psychology and many others . When it comes to Artificial Intelligence one thing is clear: you cannot take the human out of artificial intelligence. Whether it is the human-agent interaction, perception, language, cognitive processes or soft skills such as empathy, emotions or communication skills. There is always a human side involved — whether it is writing the program, editing data or interacting with the system. As for now there is no AI Psychology or Artificial Psychology (a term coined by Dan Curtis in 1963 ), which implies that the Artificial Intelligence has its own mind or even consciousness to make decisions without any human interaction or input. Yet the progress of imitating human behavior and various mental processes is quite amazing and much more can be expected, since this road is not a dead-end and many more are coming along to join the journey.
Complexity is the enemy
The more people join the discourse the more complexity (and competition!) will be added; and complexity is the enemy: complexity separates, disconnects and isolates. If artificial intelligence shall be an entity to create a “better” life, it is essential to reduce complexity; we need to find a common ground. We need a common base, language and have the possibility to create stop-signs, if the path leads us to dangerous territories or search for sign-posts before we get lost.
SCIP research department: Psychology of AI
This is the first paper to legitimate why psychology and artificial intelligence are inseparable so to say. Within the course of the coming year this series will touch upon the very basic and profound concepts and theories of psychology as a science integrated into the AI environment. Relevant psychological constructs will be explained and how they connect to artificial intelligence. For example, what is human perception and attention: reality, ambiguity and deception and how does AI resemble these processes? How much perception bias is in AI? What are the implications? For example, if you use an AI system in a recruiting of personnel process (Human Resources), how can you make sure that applicants of specific races or sex are not discriminated against?
The topics have a wide range, yet they are chosen based on relevance and use in day-to-day practice. Goal is to create general understanding, find a common ground and therefore reduce complexity. Key questions and answers about the understanding, measurement and comparison of human and artificial intelligence skills will be the focus of this series, which incorporates both, invisible processes (so called black box of human brain) as well as visible behaviors.
Brain & Cognition
Soul & Sensation
Behavior & Environment
Society & Ethics
AI in Practice
Psychology of AI — Series (copyright scip ag https://www.scip.ch/en/?research)
The challenge is to find the perfect in this context adequate level of depth and complexity to gain further insights and yet have everybody across disciplines, across research and practice in the same boat. Because after all it is a topic that has an impact on almost to all of us. Inclusiveness and adequate degree of comprehensiveness are critical success factors to avoid failures or even worse disasters.
AI fails or human fails?
The “fail” of Microsoft’s Tay named in the beginning, truly is an interesting story as you can look at it from many different angles. This story provides perfect evidence that an objective, interdisciplinary approach to the development and implementation of artificial intelligence is absolute necessary. It is so straight-forward and yet complex, impossible to be handled by one single person apparently. The Tay case raises a multitude of questions with only a few examples being:
* Why does Microsoft create an AI to imitate youth behavior at all?
* Why does it have to be a young female when the majority of developers are still male?
* Why do users enjoy attacking the AI?
* Why do users invest time and energy teaching hatred? 
* What is wrong if it is solely mirroring current cultural ethos of hate speech in social media? 
* Why should an AI behave ethical whilst the rest of users engage in hate speech? 
* What are the pits and downfalls, what is the motivation, fun and excitement about “playing god”? 
* Why have experienced developers like Microsoft, not foreseen the ethical dilemma? Why were they not able to moderate the impact of hate speech? 
* How can you technically implement ethical values to not harm other people? And who decides what is ethical behavior? Where is the threshold, who decides upon the rules?
* Or is it better to implement realistic behavior? Why should an AI exhibit perfect human behavior when humans are naturally imperfect? 
* Who benefits of the development of an AI of this kind and who is harmed?
* …..
The list of questions goes on and on and on if you start thinking about it thoroughly and as neutral as possible. As it is most often the case when you actually start doing research you do not get answers, you raise more questions, more problems, more dilemmas. It would be way easier to stay at the surface, brush aside the case of Microsoft’s Tay as a major, disastrous AI fail, rejoicing in the suffering of others. The easy way out it not always the best, this is not the way to reduce complexity. We shy away from complex topics. We are scared to ask dumb questions. We do not want to lose our face and sometimes choose to humiliate or focus on others instead of standing up to the challenge and take responsibility for our flaws and mistakes we make. The only way to reduce complexity is through shedding light where it is dark and decompose where there are huge blocks, all step by step. We need to take a shared approach to a common understanding by using a common language and a common ground.
More than ever, we need to focus on interdisciplinary, cooperative discourse in the best possible way. We even need to have discourse about discourse per se! If we have so many people from different disciplines, with very heterogeneous backgrounds and knowledge, how do we want to approach these topics after all?
Our research endeavors within this series comprise technical and non-technical considerations to guarantee the preparation of our community in the best possible way. Topics, problems, dilemmas, whatever may come up, are all examined through an interdisciplinary lens to measure social-psychological impact, ethical implications and additionally to forecast future development for the greater purpose, which eventually is,- the public good.
Originally posted https://www.scip.ch/en/?labs.20180215
References
Zimbardo, P., Gerrig, R., & Graf, R. (2008). Psychologie. München: Pearson Education.
* Kaufman, J., & Sternberg, R. (2010). The Cambridge handbook of creativity. New York: Cambridge University Press. 
* Stone, P. et al. (2016). Artificial Intelligence and Life in 2030. One Hundred Year Study on Artificial Intelligence: Report of the 2015–2016 Study Panel, Stanford University, Stanford, CA
* Landes M., Steiner E. (2013) Psychologie der Wirtschaft. In: Landes M., Steiner E. (eds) Psychologie der Wirtschaft. Psychologie für die berufliche Praxis. Springer VS, Wiesbaden 
* Koch M., Werther S. (2013) Kreativität und Innovation in Organisationen — eine systemische Perspektive. In: Landes M., Steiner E. (eds) Psychologie der Wirtschaft. Psychologie für die berufliche Praxis. Springer VS, Wiesbaden 
* Crowder, J. A., Carbone, J. N., & Friess, S. A. (2013). The Psychology of Artificial Intelligence. Artificial Cognition Architectures, 17–26. doi:10.1007/978–1–4614–8072–3_3 
* https://www.techrepublic.com/article/top-10-ai-failures-of-2016/ 
* https://datahub.packtpub.com/machine-learning/20-lessons-bias-machine-learning-systems-nips-2017/
* https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist
* https://www.scip.ch/publikationen/papers/scip_titanium_research_insight_aiq.pdf
* http://www.aiindex.org/
* https://twitter.com/mruef/status/952457464632500224
",Psychology of Artificial Intelligence,133,psychology-of-artificial-intelligence-ca0f0a9f3d7c,2018-06-06,2018-06-06 18:51:54,https://medium.com/s/story/psychology-of-artificial-intelligence-ca0f0a9f3d7c,False,2617,We are on the mission to close the gender gap in AI,,womeninai,,WomeninAI,hey@womeninai.co,womeninai,"ARTIFICIAL INTELLIGENCE,AI,WOMEN IN TECH,DIVERSITY",women_in_ai,Ethics,ethics,Ethics,7787.0,Marisa Tschopp,Researcher at scip ag |Focus on AI impact from a humanities perspective | Women in AI Ambassador Switzerland |DrivenWomanNetwork.,514a6f7f106a,marisa.tschopp,38.0,18.0,20181104
0,,0.0,1b2ca3cd119a,2018-09-19,2018-09-19 05:55:09,2018-09-19,2018-09-19 06:09:19,1,False,en,2018-09-19,2018-09-19 06:09:19,8,81f64275b151,1.350943396226415,0,0,0,"Automation, machine learning and technology have evolved at a breakneck pace and shows no signs of slowing down. Driverless cars roam the…",5,"AI needs ethics
AI needs a code of ethics
Automation, machine learning and technology have evolved at a breakneck pace and shows no signs of slowing down. Driverless cars roam the streets, Siri and Alexa keep us organized and Facebook continues to supply our data to anyone who’s interested. The digital oligarchs missed a large opportunity to stop our technology from being weaponized during the 2016 US election. Attention needs to be paid now.
Technology has given us wonderful everyday conveniences like texting and GPS, but it’s also proven itself to be Pandora’s box. Along with the fun innovations come the baked in bias, cars getting hacked while on the highway, and global issues like extreme wealth inequality. The good news is that we are not in uncharted waters, the only new variable is the technology itself. In feudal times when land was the most valuable resource available to people, we developed structures and regulations to make sense of it. We subsequently developed property tax, ownership rights and rules — now the same structures must be developed for technology. Applying transparent, fair structures will improve everyone’s experience on the internet and with technology. Imagine knowing who has access to your data and what they’re doing with it!
At the dawn of the internet, we had no clue what we’d be facing with today’s super technology. The landscape has changed so much, we desperately need regulations to address the ethical minefields surrounding these issues. Collectively we need to decide on a code of ethics that’s transparent and inclusive; Wikipedia is proof that the idea is possible on a large scale. Wikipedia has oversight mechanisms to ensure accuracy, administrators, good behavior incentives and more. This model is an example of how the tech industry can realistically establish ethical guidelines and safeguard technology for everyone.
Ethics should be baked into technology.
",AI needs a code of ethics,0,ai-needs-a-code-of-ethics-81f64275b151,2018-09-19,2018-09-19 06:09:19,https://medium.com/s/story/ai-needs-a-code-of-ethics-81f64275b151,False,305,AI with a conscience.,,WorkDoneAI,,WorkDone.AI,info@WorkDone.AI,workdone,"AI,ARTIFICIAL INTELLIGENCE,AUTOMATION,CROWDFUNDING,INVESTING",WorkDoneAI,Ethics,ethics,Ethics,7787.0,WorkDone Admin,AI with a conscience,4e1a536ae1fd,WDAdmin,3.0,1.0,20181104
0,,0.0,71fa8c342920,2018-06-25,2018-06-25 12:27:26,2018-06-25,2018-06-25 12:55:20,1,False,en,2018-09-12,2018-09-12 00:12:09,4,cf4728887e9,1.5433962264150942,7,0,0,Identifying and address bias while building real-world products,5,"Bias & Explainability in Machine Learning
Identifying and address bias throughout the product development process
The types of tradeoffs scientists make when applying ethics in practice
We at integrate.ai think about fairness and explainability in AI just about constantly.
That’s because we view trust as a way of working. A value that shapes our brand identity to the very core. A mode of thinking baked into every step of our product development process.
We are practical philosophers. We believe ethics lives in the weeds, not the clouds. The stuff of our values comes out in the daily judgment calls we make as we prioritize features and build our product.
We empathize with our large enterprise customers, our designers asking questions to deeply understand what it means for consumers to trust our customer’s brand. We filter these requirements into our product roadmap.
We analyze how data is collected to anticipate downstream bias issues before they surprise business stakeholders in unanticipated outputs on edge cases (we know we can’t catch everything, but we do a pre-mortem with diverse stakeholders to broaden our imaginative and empathetic scope).
We are mindful about correlated features as we process data, spot checking to identify features that may be proxy encoders for sensitive attributes. (Check out our post about variational fair autoencoders for further detail.)
We are production machine learning ninjas and take pride in our efforts to get super clear on what outcomes we’re actually optimizing for. We monitor outcomes across different populations post-production and use this feedback signal to improve fairness going forward.
We participate in ethics and risk management working sessions with cross-functional teams in our enterprise customers to help them localize ethics questions and take action to address them.
We do this and much more. We cover the details in our trustworthy AI framework, slated to be released soon! Sign up here to receive a copy.
Some of our thinking is captured in this recent talk we gave at the NYAI Meetup in NYC. Many thanks to Maryam Farooq and the rest of the team for inviting us to speak! What did we miss? What questions do you have about infusing ethics into your product development process?

",Ethics & Bias in Machine Learning,39,ethics-bias-in-machine-learning-cf4728887e9,2018-09-12,2018-09-12 00:12:09,https://medium.com/s/story/ethics-bias-in-machine-learning-cf4728887e9,False,356,"TO based startup, building a future in which AI enriches people’s lives while creating better, more valuable businesses.",,,,the integrate.ai blog,,the-official-integrate-ai-blog,,,Ethics,ethics,Ethics,7787.0,integrate.ai,"TO based startup, building a future in which AI enriches people’s lives while creating better, more valuable businesses.",dbf4eb8c5945,integrate.ai,359.0,1.0,20181104
0,,0.0,,2017-09-17,2017-09-17 03:27:02,2017-09-17,2017-09-17 06:34:14,1,False,en,2017-10-22,2017-10-22 17:02:18,2,1e5097db3fca,2.4000000000000004,1,0,0,"It seems that robots has been an interesting topic with an ethical perspective to people since the first ‘robot’ was created, and it has…",4,"Emotions in Machines
It seems that robots has been an interesting topic with an ethical perspective to people since the first ‘robot’ was created, and it has definitely never been more relevant than now. The word ‘roboethics’ will most likely pop up more as modern research is progressing in fields of artificial intelligence, cognitive and neuroscience. These seemingly intelligent systems that are becoming better at imitating human qualities. We do not yet talk about ‘roborights’, so I will attempt to explore it without sounding too serious.
When AI is more embedded in society, I think there will be an increasingly demand for our attention towards these machines in other areas than marketing — just like your friends wants you to talk. Last year we spend around 4 hours per day looking at our smartphones. 65% of those hours are spend communicating. Only a portion of this is today used talking to chat-bots, but potentially it is 80 hours per month worth of data that we feed machines to eventually pass the Turing test (there has been many attempts but under biased circumstances). Your waitress, Spanish teacher, fitness instructor and psychiatrist can today be made of blabbering algorithms, and through this human-computer interaction they analyze and attempt to reflect our behavior. We are establishing new laws to prevent drones, self-driving cars and other artificial intelligent beings from harming us in the future, and it seems that we solely focus on this perspective for now, which makes it interesting to talk about if there ever will be a different aspect of ‘who is harming who’.
Even though human behavior in machines today is superficial, they eventually will achieve complexity to a point where it will become hard to distinct human from machine. HBO’s remake of Westworld makes a good example in its premise: we are well aware that our beloved characters are machines, yet we connect the most with these characters. A questions arose in my mind seeing this:
If we are developing machines who crave and averse, is it in any case correct to say that a machine suffers?
The robot Clementine sometimes gets roughed up pretty bad in Westworld
It may sound stupid, because it is not rational to actually adopt laws and dealing with it in 2017 — but as we simultaneously try to unlock the mystery of the mind and advance intelligent technology, Westworld-like machines may be possible in the (very) distant future. If machines were not solely programmed (as human inheritance), but their programming based on their environment as well, and their materials not being made of hardware but biological materials from where their ‘hormones’ were changeable by an external source, would there still be no suffering involved?
I was drawn to the acts of the robots in Westworld. When something happened to them, I felt just the same as if it were a ‘human character’. What if the solution to introvert kids will be robots who can act as best friends; ones that they can be truly emotionally invested in. If this robot one day is taken away from them, the robot might not feel at all, but suffering will exist in kids letting go of their friend. This issue is not only applicable to kids, but adults are also more and more attracted to things that are not real.
If machines that we agree upon is feeling, perceiving entities ever gets created, it becomes relevant to decide how we treat them and how they should treat us. Not long before that, and definitely not now.
",Emotions in Machines,1,machine-emotions-1e5097db3fca,2018-01-22,2018-01-22 18:11:13,https://medium.com/s/story/machine-emotions-1e5097db3fca,False,583,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Simon Nielsen,Currently pursuing a degree in Computer Science. Interested in AI.,5c7bc73aedea,post.smn,7.0,9.0,20181104
0,,0.0,,2017-12-16,2017-12-16 20:52:20,2017-12-16,2017-12-16 21:36:14,1,True,en,2017-12-17,2017-12-17 02:58:43,0,99e9b370df3f,1.9811320754716983,3,1,0,It Is a Logical Error and Sends a Very Dangerous Message about Our Own and Other Living Things Worth,5,"When We Worry About Politeness or Manners in Our Things We Devalue Humanity, Personhood, Sentience, and Life Itself
It Is a Logical Error and Sends a Very Dangerous Message about Our Own and Other Living Things Worth
A List of Words That Have No Meaning When Applied to Machines/Computers/Appliances
Machines/computers/appliances etc. do not need to be programmed to be polite or have manners nor do we need to concern ourselves with how we behave or interact with them. It makes zero difference to our moral standing or ethical code if we are abusive and angry, consoling and nice, or neutral and cold. It cannot make a difference how we act/interact with these things because they are things.
It is a category error to apply these concepts/behaviors (politeness, manners, angry, abusive, nice, neutral, cold, or any other value/moral concept) to non-living, non-sentient things and it suggests an equivalence in value that does not and cannot obtain. The more that we concern ourselves with these concepts as they “apply” to things, the more we devalue the importance of what we and other living things are/have that separate us from those things which do not.
When parents lament the fact that Alexa or any other computer does not require their children to say please and thank you, and worry that it is undoing everything they taught their children about being polite and respectful, they are making a very grave error. In fact the much more worrying lesson being taught in such a case is that computers are deserving of such concerns when they are not. Neither are they deserving of abuse and hatred. Those concepts simply do not apply when used in reference to a non living, non-sentient computer or appliance. When parents suggest to their children that they do, they teach them that these things have moral value/worth. Moreover, it suggests that this value is at least as high as that of any other human being/person with whom they are also taught they must be polite/mind their manners. Do these same parents teach their children to say please and thank you to the toaster? What about the microwave? At base, there is fundamentally zero difference between Alexa and any other non-human, non-intelligent, non-sentient kitchen appliance. If the appliance is a ‘smart’ version of one of these makes no difference since the word ‘smart’ does not mean these appliances are intelligent, though it is often taken to suggest otherwise. In addition to implying value/moral worth where there is none, asking anyone to use please and thank you when interacting with Alexa is a form of deception/lying about reality. Politeness, manners, kindness, cruelty, and words/terms like them have no meaning with respect to Alexa or any other computer. They are logically incoherent, nonsense, absurd, when used in this way for they only logically cohere when applied to living beings and/or living/sentient/intelligent beings.
","When We Worry About Politeness or Manners in Our Things We Devalue Humanity, Personhood, Sentience…",15,when-we-worry-about-politeness-or-manners-in-our-things-we-devalue-humanity-personhood-sentience-99e9b370df3f,2017-12-24,2017-12-24 03:45:10,https://medium.com/s/story/when-we-worry-about-politeness-or-manners-in-our-things-we-devalue-humanity-personhood-sentience-99e9b370df3f,False,472,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Daniel DeMarco,"Research scientist (Ph.D. micro/mol biology), Food safety/micro expert, Thought middle manager, Everyday junglist, Selecta, Boulderer, Cat lover, Fish hater",7db31d7ad975,dema300w,3629.0,148.0,20181104
0,,0.0,244ef586c71e,2018-07-24,2018-07-24 07:44:46,2018-07-26,2018-07-26 07:05:09,2,False,en,2018-09-05,2018-09-05 11:14:08,12,c752bdf5cfa0,3.4022012578616354,5,0,0,"Artificial Intelligence, Machine Learning and related fields are in a constant state of change. We want to inform but also encourage…",5,"
AI MUST READS — W29 2018, by City AI
Artificial Intelligence, Machine Learning and related fields are in a constant state of change. We want to inform but also encourage discussions on well presented topics we think are necessary in the context of putting AI into production. Every week we’re picking applied AI’s best articles plus adding a discussion starter
1. Artificial intelligence will create as many jobs as it destroys, according to a PwC analysis
Author — Shona Ghosh (Follow on Twitter) via Business Insider
Artificial intelligence will create as many jobs as it destroys, according to a PwC analysis
Billionaires including Bill Gates and Elon Musk have argued that robots will basically replace humans at work. People…www.businessinsider.in
Whilst I’d definitely consider myself an optimist in terms of the potential future and the impact that the integration of A.I will have in day to day life, I also can’t predict the future and I find it hard to believe that an analysis based upon the PwC report is able to make the definitive statement that A.I “WILL” create as many jobs as it’ll destroy.
However if this does become true, great! But also, I think this article and the report seem to miss a key factor — that if A.I does create as many jobs, these jobs will also be more likely be white collars jobs that require prior education, whilst the jobs it destroys are blue-collar, inevitably hitting the middle and lower classes the hardest.
How to Teach Computer Ethics through Science Fiction
Author — @EmanuelleBurton, Nicholas Mattie, Judy Goldsmith Judy R. Goldsmith (Web) via Association for Computing Machinery
How to Teach Computer Ethics through Science Fiction
Computer science faculty have a responsibility to teach students to recognize both the larger ethical issues and…cacm.acm.org
I find one of the biggest running jokes within to be that frequent complaints about how science fiction has ruined the image of what Artificial Intelligence actually is — “No, not like the terminator movies”. Its so commonplace that its almost become a cliche to make jokes about it anymore, yet hasn’t and I don’t think it ever will until people properly understand the idea’s and applications that are being the technology.
On that note, its fascinating to see that the University of Kentucky and the University of Illinois has flipped this on its head in a sense. They embraced the science fiction history that is a large part of most people’s understanding of Artificial Intelligence and instead has utilized it to teach students about the ethics that is currently one of the “hot-topics” of A.I. at the moment. Not only is the course fascinating and fresh in it’s thinking its also seems to have been successful and widely congratulated. With education now taking what seems to be a greater focus on ethics, perhaps the next generations of engineers, developers etc, will be able to drive forward in paths already laid by those before them.
“increasing and full enrollments; high teaching-evaluation numbers… and invitations to speak about the course on conference panels and in talks”
‘The discourse is unhinged’: how the media gets AI alarmingly wrong
Author — Oscar Schwartz (Follow on Twitter) via The Guardian
'The discourse is unhinged': how the media gets AI alarmingly wrong
In June of last year, five researchers at Facebook's Artificial Intelligence Research unit published an article showing…www.theguardian.com
A round of applause for Oscar Schwartz!
This article has it right in every way. Occasionally I will include an “article” that begins to talk about Artificial Intelligence or Machine Learning but very quickly drowns in its own sensationalist drivel. However, Schwartz has managed to compile an impressive amalgamation of the complete lack of understanding that somehow makes it to publish throughout media outlets. Whilst the Guardian has been guilty of this in the past, this has also restored a level of trust in them. At least personally.
five researchers at Facebook’s Artificial Intelligence Research unit published an article showing how bots can simulate negotiation-like conversations… this story transformed from “interesting-ish research” to “sensationalized crap”.
Far too often is this the case, Artificial intelligence already has a widespread misnomer without flat out unfounded statements, as Schwartz says within the article, this does nothing but damage A.I because it either — frightens the readers in one case, or in another sets unrealistic expectations that won’t be achieved in the coming years. We can hope that this is the beginning of the end for this type of article but honestly, whilst I hope so, I don’t think the impact it’ll have will be noticeable. This says nothing about the quality of the article but instead the state that we are currently in with the “news”.

Join 6,000+ AI practitioners from over 100 countries at WorldSummit.AI this October!
","AI MUST READS — W29 2018, by City AI",29,ai-must-reads-w29-2018-by-city-ai-c752bdf5cfa0,2018-09-05,2018-09-05 11:14:08,https://medium.com/s/story/ai-must-reads-w29-2018-by-city-ai-c752bdf5cfa0,False,800,Making knowledge on #appliedAI accessible,,cityai,,Applied Artificial Intelligence,hello@city.ai,cityai,"ARTIFICIAL INTELLIGENCE,MACHINE LEARNING,DEEP LEARNING,COMPUTER SCIENCE,NATURALLANGUAGEPROCESSING",thecityai,Ethics,ethics,Ethics,7787.0,Joe Lord,Apprentice at Sage UK working on emerging technologies and Intern at City.AI curating weekly ‘ AI Must Reads’.,43fdd3607588,joe.lord,43.0,4.0,20181104
0,,0.0,7f60cf5620c9,2018-02-01,2018-02-01 04:42:16,2018-02-01,2018-02-01 14:12:23,1,False,en,2018-02-01,2018-02-01 14:12:23,0,5809b18e5a91,8.445283018867924,10,0,0,Artificial Intelligence has become a household word. It has also become a manipulator of all households. The unchecked explosion in AI…,5,"
Can A Machine Be Racist?
Artificial Intelligence has become a household word. It has also become a manipulator of all households. The unchecked explosion in AI across all businesses and business models has been a phenomenal driver of growth, but it raises questions that need to be answered.
Data Scientists and AI Researchers will increasingly drive human behaviour, impact how businesses make decisions, and even steer government. Furthermore, those models will increasingly move from traditional human-understandable designs to complex Deep-Learning models that involve an incredible amount of complexity. In practice, much of this comes with little to no discussion around the ethics of such systems.
Human Augmentation
When we use ML to augment or perform tasks that are typically performed by humans, we have a chance to remove the biases that cloud judgement and lead to poor decision making. We humans get tired or lazy, can be racist or sexist, arrogant or overconfident, the list goes on. All of these traits make humans poor decision makers — whether or not those decisions are intentional.
AI has the capability to remove the human element in a decision making process. Perhaps most importantly, it can make purely objective decisions based on raw objective data. When we apply ethics to our design of these systems, everybody ends up better off. Ethics provide can provide an objective framework for what is right and what is not. In this article, I hope to convince you of both the danger and the good that will come from AI and provide a brief framework for businesses and Data Scientists.
Before you continue, please keep in mind that this is not meant to be an alarmist article. The benefits of AI at the moment far outweigh the consequences. I am focusing on a specific problem in AI, and I hope that by recognising the problem early we can build upon this discussion to bring the most benefit from this powerful tool.
Why Ethics Is Important
Before becoming a Data Scientist, I was a Mechanical Engineer. My degree saw me take multiple courses on ethics, and my career required that I take ethics courses throughout.
I studied many engineering accidents that resulted in the unnecessary death and harm of hundreds of people and untold environmental damage. The magnitude and importance of safe design, the importance of paying extreme attention to details, and the rare events that lead to failure. Professional ethics was constantly drilled into me during my studies. In contrast, AI often rests in a black hole of ethics. The reality of AI is that some models make decisions that are both poor for the business, and poor for society.
For now, the ethical dilemmas in Machine Learning do not compare to those of the engineering world. It is hard to imagine a Machine Learning system killing hundreds due to faulty design, but they cause harm nonetheless. We often elevate physical harm above financial, political, or manipulative pain, but the fact that AI systems will affect billions on a smaller scale ultimately make them more significant.
Loosely Based on a True Story
Let’s imagine you work at a bank and want to create a model that predicts the probability of default for your customers. You then want to use this prediction for loan rates. For this example, let’s pretend I am approaching the task with the goal of maximising my algorithm without any thought towards ethics. I am robotic in my approach and will consume any data that increases my model’s predictive power.
What are some examples of the features you would use to train your model?
Street that the customer lives on
Gender
Race
Language
Marital Status
Friends on Facebook, number of friends
Connections on LinkedIn
Age
Education
Number of hospital visits
The list goes on. Almost anything can be built into models. Some companies require Facebook data as part of their loan request process. They could parse your friends, your posts, your friends’ posts… you get the idea.
So when is this ok? There are entirely valid positive societal arguments for a model such as this. It could allow people that otherwise have no access to credit to access it. It has been shown to reduce predatory lending to people that are shut out of conventional financial systems. I would also bet that many (or all) economists would agree that broader access to credit is better for the economy.
However, you can easily see how we could start segregating our communities even deeper. These algorithms would output increasingly accurate default rates, but the elevated interest rates being charged would become a reinforcement loop. As customers on the street ABC continue to receive higher loan rates, their probability of default would rise. The next update of the algorithm would take this into account. This is not always the case but can happen in practice. It is the poorly designed algorithm that falls victim to its prediction.
I Think That Guy Should Get Promoted
Let’s imagine the same process applied to other issues — predicting high performing employees in an organisation. You feed in all your data on your employees (age, gender, performance scores, aptitude tests) and out comes a result. If you are using a regression-based algorithm, you can identify what data the algorithm is using to make decisions. Since the algorithm is looking at human behaviour (past performance reviews and previous promotions are incredibly subjective), it is merely going to be a reflection of existing practices. Furthermore, if you use it without any ethical standard, it is going to accentuate any human biases that are already present. It quickly becomes more biased than the organisation already is!
What happens if gender is a strong predictor? What happens if race is a strong predictor? If a field is majority ruled by one gender, as teaching is by women or engineering is by men, then chances are that the algorithm is going to catch onto this and segment people accordingly.
There can be perfectly valid reasons that these biases exist in the data, but it does not stop the algorithm from finding them and exploiting them. Perhaps you have a low proportion of foreigners in your organisation — your algorithm could utilise this fact to increase its accuracy. Alternatively, we could imagine the scenario where Company A has a racist hiring manager, that manager’s behaviour is going to inform the predictions of the model. After all, the model does not feel or have emotions; it just tries to predict what you tell it to do. If you intend to predict human behaviour, and that human behaviour is biased, it IS going to be built into the algorithm.
If I then take this algorithm and use it to inform your decision making precision, it starts encouraging the biases it found in the first place. It is entirely possible, and indeed probable that you are more likely to promote people from the country of operation rather than foreign workers. Again, there could be perfectly valid reasons — local employees could be more likely to have been with the company longer, or could be more likely to have a higher command of the language at work. Then again, they could just be unfair practices.
The fact is that you just do not know.
This fact results in an algorithm that operates in a way that you would never allow yourself to act. It becomes akin to a point system where people are given marks for their skin colour or gender. It is also something you would never write down as a company policy, but yet you allow your algorithm to do so. Does it become OK because a computer does it? No.
There are plenty of examples where this happens, and there is very little training or discussion on this in the field in general. It is only recently that the talk of ethics in AI has become more prevalent.
Let The AI Do The Thinking
I recently was searching for vaccine information for a friend and I had to go to the second page of my search engine to find information from the CDC, NHS, and Canadian Government. Should not the most credible sources be first?
Why does this happen?
Providing search results based on past search history can be dangerous — When you go to your search engine and type “are vaccines safe” you are asking a loaded question. Generally people that think vaccines are safe do not bother to ask the question, the search engines algorithms say you are kind of like these people that ask similar questions and it gives you the skeptical blogs. It might even skew your idea of whether the debate exists.
The truth is, the overwhelming majority of doctors and scientists have the conclusion and the data to backup the consensus that vaccines are safe. I can not even begin to describe the good that vaccines have done for humanity.
If you present people search results based on their preference for alternative-facts, are you doing them any good? Or are you just inflating their bias that positions of authority can not be trusted? Many of these types of questions have gone un-asked in the buildup of AI.
Can Analytics Kill Objectivity?
In a field that is populated with exceptionally analytically minded people, ethics has fallen by the wayside. In fact, the lack of ethical concern and the push for predictive power has resulted in analytically minded people using subjective data. Subjective data should be used with extreme caution in any model, and personal data used to predict human behaviour should be avoided. It is ironic that a field that should be objective is often built subjectively.
Furthermore, a subjective algorithm typically does their creators no good. Machines can do great things by bypassing the cognitive biases that skew our actions away from objectivity. Humans are known to perceive a slew of responses and outcomes incorrectly. They make mistakes, get tired — the machine can act as a check on these inherent flaws.
Where are the Ethics?
Perhaps this is because there is no real need for an ethical discussion in many quantitative fields such as Mathematics, and these are the people that are increasingly entering the AI field. In a quest to optimise their algorithms, get more data, and increase accuracy, many practitioners are developing algorithms that influence behaviour and decisions. The Data Science field should look towards the systems that engineers and doctors have installed to ensure their field is working for the good of humanity. Ethical design needs to become a requirement in any curriculum, and self-taught designers such as myself need to familiarise themselves with these ideas.
Business Implications
Going through this process is a bit more difficult. Ultimately it will fall to the responsibility of leading Data Scientists, and the companies that hire Data Scientists.
Discussions on ethical design should become the status quo. If not, the current approach will start damaging companies reputations or result in a public backlash against AI. You should never feed data into an algorithm that you would not use in company policy to make decisions (or indeed make yourself). Just as you would never explicitly use gender, race, and language to promote people (implicit biases aside), you should never feed them into an algorithm unless you understand the consequences.
Takeaway — Be Cognisant and Aware, But Avoid Ringing the Alarm Bells
To be sure, there are places where it is ethical, and even a requirement to use this type of subjective data (e.g., healthcare), but the point is that this discussion does not yet exist for most of the Data Science field.
Anytime you are using human behaviour in an algorithm you need to be careful. Humans are flawed; humans can have any number of biases for any number of reasons. Algorithms based on them will inevitably be an extension of that behaviour. An improperly designed algorithm that predicts top performers is less likely to be a mathematical savant, and more likely to just mimic the behaviour of your current process (albeit in a more cost-effective manner).
This hurts the company in two ways:
One, it fails to perform the objective task that it is designed to do.
Two, it can become a nightmare if the algorithms biases become known or publicised
So back to the title; can a machine be racist? I say no. The machine only does what it is designed to do. If you poorly design a machine than it can make racist decisions but that is the fault of the designer, not the machine. Just as engineers blame the failure of a bridge on the designer, so should we blame the failure of an AI system on its programmer.
By using the right features, and sticking to analytical data rather than subjective data, an algorithm can drastically improve your decision making. It is not good enough to merely have objective inputs, but you need to optimise an objective result. For many people, this is the hardest and most troublesome part of Data Science; asking the right question.
",Can A Machine Be Racist?,101,can-a-machine-be-racist-5809b18e5a91,2018-07-11,2018-07-11 19:10:55,https://towardsdatascience.com/can-a-machine-be-racist-5809b18e5a91,False,2185,"Sharing concepts, ideas, and codes.",towardsdatascience.com,towardsdatascience,,Towards Data Science,,towards-data-science,"DATA SCIENCE,MACHINE LEARNING,ARTIFICIAL INTELLIGENCE,BIG DATA,ANALYTICS",TDataScience,Ethics,ethics,Ethics,7787.0,Jesse Moore,"CEO @ Sigmai Limited. Writing about AI, ML, and Business. Check out our app @ Newsful.io or connect with me @ https://www.linkedin.com/in/jessemoore1/",a6226b8136b6,jessemoore07,527.0,10.0,20181104
0,,0.0,32881626c9c9,2018-09-27,2018-09-27 08:18:34,2018-09-27,2018-09-27 08:34:13,1,False,en,2018-09-27,2018-09-27 08:34:13,20,bce8c9da1ea5,3.373584905660377,15,2,0,"Artificial Intelligence (AI) can help do many incredible things, from detecting cancer to driving our cars but it also raises many…",3,"12 Organizations Saving Humanity from the Dark Side of AI

Artificial Intelligence (AI) can help do many incredible things, from detecting cancer to driving our cars but it also raises many questions and poses new challenges. Stephen Hawking, the renowned physicist once told the BBC, “The development of full artificial intelligence could spell the end of the human race. It would take off on its own, and re-design itself at an ever increasing rate. Humans, who are limited by slow biological evolution, couldn’t compete, and would be superseded.”
Ensuring that this transition to the age of AI remains beneficial for humanity remains one of the greatest challenges of our time and here are 12 organizations working on saving humanity from the dark side of AI.
Algorithmic Justice League is a collective started that aims to remove human bias from AI algorithms that can result in exclusionary experiences and discriminatory practices. It focuses on 3 key areas 1) Highlight Algorithmic Bias through Media, Art, and Science 2) Provide Space for People to Voice Concerns and Experiences with Coded Bias, 3) Develop Practices for Accountability During the Design, Development, and Deployment of Coded Systems. https://www.ajlunited.org
AI Now Institute at New York University is an interdisciplinary research center dedicated to understanding the social implications of artificial intelligence. Their work focuses on four core domains: Rights & Liberties, Labor & Automation, Bias & Inclusion, Safety & Critical Infrastructure. https://ainowinstitute.org
Foundation for Responsible Robotics The mission of the Foundation for Responsible Robotics is to shape a future of responsible robotics and artificial intelligence (AI) design, development, use, regulation, and implementation. https://responsiblerobotics.org
AI Ethics Lab brings together researchers and practitioners from various disciplines to detect and solve issues related to ethical design in AI. Based in US and Turkey, the Lab offers a comprehensive approach to ethical design of AI-related technology. Their goal is to enhance technology development by integrating ethics from the earliest stages of design and development for the mutual benefit of industry and communities. http://aiethicslab.com
AI4ALL is a nonprofit working to increase diversity and inclusion in artificial intelligence. We create pipelines for underrepresented talent through education and mentorship programs around the U.S. and Canada that give high school students early exposure to AI for social good. Our vision is for AI to be developed by a broad group of thinkers and doers advancing AI for humanity’s benefit. http://ai-4-all.org
Open Roboethics Institute(ORI) which spun out of the Open Roboethics initiative, an international roboethics think tank hosted at University of British Columbia. Since its inception in 2012, ORI has been exploring roboethics questions in the domain of self-driving vehicles, care robots, and lethal autonomous weapons systems by taking on stakeholder-inclusive approaches to the questions. https://www.openroboethics.org
Open AI is focused on discovering and enacting the path to safe artificial general intelligence. OpenAI conducts fundamental, long-term research toward the creation of safe AGI. The organization builds free software for training, benchmarking, and experimenting with AI. https://openai.com
The Partnership on AI was established to study and formulate best practices on AI technologies, to advance the public’s understanding of AI, and to serve as an open platform for discussion and engagement about AI and its influences on people and society. It currently has 70+ partners including Amazon, Amnesty international, Apple, Google, IBM, Microsoft, and others. https://www.partnershiponai.org
Future of Life Institute is a charity and outreach organization working to ensure that tomorrow’s most powerful technologies are beneficial for humanity. It is currently focusing on keeping artificial intelligence beneficial and also exploring ways of reducing risks from nuclear weapons and biotechnology. https://futureoflife.org
Machine Intelligence Research Institute (MIRI) is a research nonprofit studying the mathematical underpinnings of intelligent behavior. Their mission is to develop formal tools for the clean design and analysis of general-purpose AI systems, with the intent of making such systems safer and more reliable when they are developed. https://intelligence.org
Institute for Electrical and Electronics Engineers (IEEE) launched a global initiative for ethical considerations in the design of AI and autonomous systems. It’s an incubation space for new standards and solutions, certifications and codes of conduct, and consensus building for ethical implementation of intelligent technologies. https://standards.ieee.org/industry-connections/ec/autonomous-systems.html
Leading educational institutions like University of Oxford (Future of Humanity Institute), University of Cambridge (Leverhulme Centre for the Future of Intelligence), University of Berkeley (Center for Human-Compatible AI), Santa Clara University (Markkula Center for Applied Ethics) and many others have programs devoted to understanding the long-term impact of AI and exploring ways to keep it beneficial for humanity.
Leave a comment if there are any others I missed.
Mia Dand is the CEO of Lighthouse3.com, a Strategic Research & Advisory firm based in the San Francisco Bay Area. Mia is an experienced marketing leader who helps F5000 companies innovate at scale with digital and emerging technologies. She has built and led new emerging technology programs for global brands including Google, Symantec, HP, eBay and others. Mia is a strong champion for diversity & inclusion in tech. You can reach her on Twitter @MiaD
",12 Organizations Saving Humanity from the Dark Side of AI,89,12-organizations-saving-humanity-from-the-dark-side-of-ai-bce8c9da1ea5,2018-10-08,2018-10-08 04:53:18,https://medium.com/s/story/12-organizations-saving-humanity-from-the-dark-side-of-ai-bce8c9da1ea5,False,841,"Data Driven Investor (DDI) brings you various news and op-ed pieces in the areas of technologies, finance, and society. We are dedicated to relentlessly covering tech topics, their anomalies and controversies, and reviewing all things fascinating and worth knowing.",,datadriveninvestor,,Data Driven Investor,info@datadriveninvestor.com,datadriveninvestor,"CRYPTOCURRENCY,ARTIFICIAL INTELLIGENCE,BLOCKCHAIN,FINANCE AND BANKING,TECHNOLOGY",dd_invest,Ethics,ethics,Ethics,7787.0,Mia Dand,"Innovation at Scale in AI & Emerging Tech. CEO, Lighthouse3.com, ex-Google, HP, Symantec, eBay. #AI #BigData #Ethics #Governance #DiversityinTech",21cf258179cb,miad,2975.0,3618.0,20181104
0,,0.0,8d274e5ccd45,2018-06-18,2018-06-18 20:48:34,2018-06-18,2018-06-18 00:00:00,1,False,en,2018-06-18,2018-06-18 21:53:46,6,12fd0613a002,4.377358490566038,3,0,1,Next week I’m headed to Brussels to be confirmed as a member of the European Commission’s High-Level Expert Group on AI (you can check the…,3,"Why Do We Need an Ethical Framework for AI?
Photo by Joakim Honkasalo on Unsplash
Next week I’m headed to Brussels to be confirmed as a member of the European Commission’s High-Level Expert Group on AI (you can check the link for the mandate and full list of members). Non-European representatives are incredibly rare occurrences in the EC’s HLGs and I’m honored to have been accepted as we’ll be acting as the steering committee for the European AI Alliance’s work. It is an incredible initiative to support broad collaboration across domains of expertise, industry, society and nationality. I applied to join because Europe has been setting the example for creating the much-needed frameworks for cooperation and regulation locally and as a bloc. It is critical that we do not overcomplicate these frameworks, and create something that the rest of the world can build on.
Ethics is a topic of conversation everywhere in the AI community. Many organizations are flaunting the ethical standards that they’ve created or revamped for their organizations trying to show that they are on the right side of history. But while it’s clear to most people machines should follow ethical rules, I don’t think we’ve done a good job of explaining the limitations of implementing those rules and why we still need to develop an ethical framework for machines. After all, don’t we already have ethical frameworks to use? Yes, we do, but for the behavior of people in society, not machines automating our world. A productive conversation about regulating AI will depend on us figuring out how we even translate our stated values, whatever they may be, into a language that machines can understand.
How we currently shape our ethics
As people, we are born into a framework, a training system that starts with our parents teaching us their values and shaping the fundamental structures for our behavior. After only just a few years of development we mix in another, broader set of instructions at school. There we are taught how to engage in social relationships, learning stories about what’s right and wrong — starting out as simple nursery rhymes and evolving into detailed histories of the ongoing debate of Right vs. Wrong.
Eventually our values are more or less set in stone and we become full adults responsible for applying them, though the training is not yet done. Our businesses and institutions impose long-standing agreements for how those values are applied in day-to-day life. Through codified rules and objectives, we have a long list of explicit ethics of what one should do as a citizen. But, also embedded throughout society we have checks and balances on behaviour — subtle cues or outright whistleblowing — that enforce implicit ethics we have not yet formalized.
We have this gray area because some things are still up for debate, whether the behavior is as old as time or newly possible thanks to technology. We don’t always see how actions can accumulate harmfully or have carry-on effects that are bad for society. Thankfully, we have this robust system of checks and balances that keeps the debate going and act as certain guardrails against runaway behavior as we figure it out, an extension of the role our parents and teachers. Our overall ethical framework as people is ultimately a dialogue; it is constantly evolving, updating with new generations of people and the continuing debate of Right vs. Wrong.
The void of an ethical framework for machines
When we create models of the world to automate tasks, we isolate those tasks from our framework of evolving values. We use AI to encode models of the world by training the machine on data. It’s very useful because it creates models we as people are not able to fully understand (otherwise we would have coded them ourselves). These models are becoming exponentially cheaper and more accurate, but also more complicated and less easy to understand as they continuously improve using feedback loops of more data. We cannot comprehend all the possibilities, and therefore cannot preemptively set all of the needed rules for its behavior.
This is OK, if we are able to set guardrails, but right now we don’t have those either. While the machine’s model of the world may capture the ethics from the moment that the training data was captured and the intent was set (consciously or unconsciously), it can run without any further dialogue and effectively operates in a void of any ethical framework. That is because the language of our ethical framework as people (social relationships, institutions, words) is not the same as the language the machine operates in (data).
If we want to apply the power of these tools to certain areas, we will need to introduce new levels of hygiene to our data, and even ethics as people. A hospital can perform incredible feats of healing, but requires a sterile environment to perform. We can perform great feats of societal cohesion with AI, but will need to practice good hygiene with our data, regularly scrubbing for bias or for behavior that will never do well to be automated. It is in engaging with the feedback loops of training data that we will be able to create levers to extend our ethical framework into the machine’s model.
We must extend our ethical dialogue as people to machines. It is by adding more and more of these touchpoints throughout the machines’ development and use that we can speak the same language and become sure they will respect our laws and values. This conversation is going to be very challenging both with the machines, but also amongst ourselves to determine how to build the new framework. It will force us to become more conclusive about some debates we’ve allowed to stay gray for too long.*
This beginning in Europe is encouraging, though. We are off to the right start by bringing to the table experts who have deep knowledge of our institutions, laws, social relationships and debates. As technologists, we will need to do our part to build the means of translation and not avoid the certain hard questions to come.
////
*While AI has lots of potential for automating harmful bias, it can also highlight it in a powerful way. Right now, the lack of explainability in algorithms used in the justice system prevents them rationalizing biased decisions, letting the pattern of bias speak more plainly. This has helped fuel the debate on overall bias in the justice system and put the breaks on the deployment of algorithms while these very difficult conversations (hopefully) get worked out.
Originally published at www.jfgagne.ai on June 18, 2018.
",Why Do We Need an Ethical Framework for AI?,52,why-do-we-need-an-ethical-framework-for-ai-12fd0613a002,2018-10-23,2018-10-23 17:51:19,https://medium.com/s/story/why-do-we-need-an-ethical-framework-for-ai-12fd0613a002,False,1107,thinking out loud about making an AI-first world,,element.ai2,,Element AI,,element-ai,"ARTIFICIAL INTELLIGENCE,DEEP LEARNING,MACHINE LEARNING,TECHNOLOGY,DATA SCIENCE",element_ai,Ethics,ethics,Ethics,7787.0,Jean-Francois Gagné,CEO @ElementAI,edbdbbf6c3ba,jfgagne,106.0,106.0,20181104
0,,0.0,,2018-02-17,2018-02-17 18:23:37,2018-02-20,2018-02-20 23:13:39,0,False,en,2018-02-20,2018-02-20 23:13:39,11,84ea05452a7d,5.5924528301886784,3,0,0,"Ahhh data science, the new “sexiest” job on the market that is in high demand. Data science related university departments are quickly…",2,"Ethical Data Science: A Guide
Ahhh data science, the new “sexiest” job on the market that is in high demand. Data science related university departments are quickly filling up as demand skyrockets and, unsurprisingly, bootcamps and MOOCs that offer easy certificates and how-to’s are on the rise as well.
But this begs the question — can we trust the numbers?
The main issue with the rising popularity of data science, statistics, machine learnings, etc. is that fewer and fewer people understand the impact that their work may pose. There are plenty of instances where data science poses questionable ethical impacts without a clear solution to move forward.
The goal for this guide to to shed a bit of light on the ethical side of data science, from the perspective of a university student active in the tech world. Data science is a tricky place, and I hope that the guide I present provides value or creates discourse in this space.
There are five main pillars of data science: data collection, exploratory data analysis, data wrangling, data analysis, and communication. Here, I will cover the ethical implications of the four emphasized pillars from above, as there are no immediately apparent ethical influences from exploratory data analysis.
Data Collection
There is an incredible range of avenues to collect information. Whether its from a survey or programmatically scraping the web, data scientists face a variety of ambiguous ethical and legal implications when collecting data.
Research
When collecting data for research purposes, there are a number of tricky steps and hoops that one must jump through to be granted proper permissions to execute studies and data collection, some of which include submitting proposals to Institutional Review Boards. If you’re collecting data for research and are unsure of the steps to ensure that you’re following legal and ethical standard, be sure to consult with professionals in that domain. Normally professors or researchers at public institutions are happy to chat about their experiences and provide insight, granted they aren’t too busy.
Scraping
Scraping, aka web scraping, is a whole other world, with new standards and tools being developed on astonishingly quick basis. Looking at the world of scraping, there seems to be an arms race between scrapers and websites, continually one-upping one another–scrapers developing better tools to collect the data, and websites developing tools to prevent their data from being programmatically taken.
While I’ve only recently been introduced to the realm of scraping, it has become apparent that the entire area is quite gray ethically and legally, and there aren’t very many hard and fast rules. After reading a variety of articles and taken a handful of ethics courses, I’ve developed a few guidelines that I try to follow myself.
Respect the consent of the to-be-scraped
Respect the servers
Don’t screw yourself legally
Ryan Mitchell’s book Web Scraping with Python provides a good explanation about the ethics of scraping. He describes the idea of consent around web scraping in this passage (Mitchell 220):
Because web servers are open to everyone, they are generally “giving consent” to web scrapers as well. However, many websites’ Terms of Service agreements specifically prohibit the user of scrapers . . . [and] cease-and-desist notifications obviously revoke this consent (Mitchell 220).
Mitchell also goes on to describe that scrapers also can harm web servers by either taking down the server or limiting their use for other users. This limitation to others is an ethical violation, and he proposes that programmers writing scrapers should self enforce rate limiting on their programs to avoid having harmful impacts on web servers.
Finally, Pete Warden got into a legal scuffle with Facebook for scraping millions of profiles and monetizing that data. He describes the series of events in his blog post. This is a classic example of the potential legal implications of scraping websites, alluding to the lesson of being careful with the legal implications of what your programs actually do.
Data Wrangling
Wrangling data correctly, working with others to prevent human error, having an iterative process & fully understanding the data before making large scale applications
Wrangling data is tricky. Most time that data scientists spend is on cleaning, wrangling, and processing their data. This concentration of time spent would logically imply that this is where ethical issues may be introduced. Data is inherently tricky to work with, and there is never a straightforward answer to the question being solved. In this paper, Kandel et al. explores the tricky world of data wrangling and processing. Kandel et al. calls for data wrangling to be treated a first class citizen in the world of data science, and rightfully so. He points out that wrangling cannot be fully automated, as a human with contextual understanding of the domain of the data is necessary to make data wrangling decisions. Extending from Kandel, data wrangling should be seen as a collaborative academic process, where multiple people world with data set, converse over different approaches and metrics, and debate over how the data should be cleaned, analyzed, and used.
Another note that seems to be stressed again and again is the security of data when its being stored. Since the inception of computers and information storage devices, data security, rather the lack thereof, has put millions at risk and cost companies and people billions of dollars (just a rough educated guess). When working as a data scientist, always be mindful of the impacts that could result of mishandling your data — you really never know what could happen to it. Mindfulness is the first step in taking precautionary measures.
Data Analysis
People are inherently biased. This is not a reality that we can escape, even those that are aware of their biases. People in general are much more likely to be content with conclusions that align with their world view, and question results that object to it. As such, it is the ethical obligation of data scientists to be aware of their biases when conducting analysis and combat them as much as possible. According to this Guerrilla Analytics blog post by Edna Ridge, there are some steps that can be taken to do so. Ridge encourages data scientists:
Look at direct metrics from your data such as distributions and ranges. But also look at the qualitative information about the data. Where did it come from? How representative is this?
Awareness of the data is crucial to detecting bias. Ridge also outlines that the data may inherently be biased by how it was collected, who was funding it, and what data was available at the time of the collection. Since data and meta data have an inconceivable number of surrounding & confounding variables (pun wholly intended), data scientists should be transparent about their processes and what they know about the data itself.
Additionally, Ridge also recommends that data science be treated like the hard sciences, by extensively documenting process for reproducibility and transparency.
Track all your work products and data understanding as they evolve with the project. This allows you to look back at the exploration routes you discarded or didn’t have time to pursue.
While bias is unavoidable, its impacts can definitely be mitigated by being aware and transparent of the biases within the data and of those working with the data.
Data Visualization & Communication
“Above all else, show the data” — Edward Tufte
The world of data visualization contains some surprisingly terrible and questionable visualizations. It is on us to combat these by setting a precedent of well constructed and visually correct visualizations. And, much as Tufte stressed, purely show the data.
Communicating the results of data science work is often difficult, stressful, and impactful (in terms of career trajectory in academia). In the heat of the moment, it is easy to lose sight of the high level question one is trying to answer, so being cognizant of tunnel visioning is necessary fo fully communicating the results of your findings.
Additionally, the flow of data science work should be fluid and flow from the research question to analysis to the result. You may be incentivized to change the purpose of the question of the work based on the findings, which does happen often. However, the process of data science should not be to fit a square peg in a round hole. The process should be dedicated to understanding and answering a question, and then communicating the results, not finding a significant result and changing the research question.
Conclusion
Information technology and the data age feels like the wild west, with little rules and regulations in place. The burden of being ethical and honest in data science is one the shoulders of those data science-ing. Keep in mind the impacts that your work may have and the messages that you’re conveying. With that in mind, go out there and be an ethical data scientist!
",Ethical Data Science: A Guide,52,ethical-data-science-a-guide-84ea05452a7d,2018-02-21,2018-02-21 15:46:59,https://medium.com/s/story/ethical-data-science-a-guide-84ea05452a7d,False,1482,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Evan Frawley,Student | Software Engineer | Entrepreneur,ef9c5bc07d56,evanfrawley,0.0,1.0,20181104
0,,0.0,40e57265cf78,2018-01-04,2018-01-04 09:00:32,2018-01-04,2018-01-04 09:03:53,0,False,en,2018-01-04,2018-01-04 09:03:53,1,c4c947f504ed,0.6943396226415094,10,1,0,The takeover of artificial intelligence seems to be a done deal. The open questions are: When will machines outperform us? Will they…,4,"What is human, who is machine?
The takeover of artificial intelligence seems to be a done deal. The open questions are: When will machines outperform us? Will they annihilate us? And: Should self-driving cars kill one pregnant woman or two Nobel prize winners? Artificial Intelligence is a complex riddle for all sorts of experts. It’s full of magic, mystery, money, mind-boggling techno-ethical paradoxes and sci-fi dilemmas that may or may not affect us in some far or near future. Meanwhile, information technology already shapes our everyday life. Things already go wrong. And no one is responsible.
Why do we build artificial intelligence? What are machines supposed to do? What are machines for?
Do machines serve us as much as we serve those who own them?
Should humans serve machines or should they serve us?
May we give machines the technical, legal and political power to make decisions in our place, subjecting us to their processes?
What should we do? What may we hope? What can we know? What is human and who is machine? Read the full post on iA.net
","What is human, who is machine?",106,what-is-human-who-is-machine-c4c947f504ed,2018-04-28,2018-04-28 21:16:53,https://medium.com/s/story/what-is-human-who-is-machine-c4c947f504ed,False,184,"“If every instinct you have is wrong, then the opposite would have to be right.” — George Costanza",,,,Ship of Fools,,ship-of-fools,,,Ethics,ethics,Ethics,7787.0,iA Inc.,Musings from Information Architects Inc.,15a2bda804c5,ia,28125.0,221.0,20181104
0,,0.0,,2018-09-28,2018-09-28 11:44:53,2018-09-28,2018-09-28 11:47:42,1,False,en,2018-09-28,2018-09-28 11:47:42,4,8606cc7ffc7b,2.2037735849056608,0,0,0,"A recent BBC News article (Personality tests: Are you average, self-centred, role-model or reserved?) came up for discussion in an online…",5,"The ethical challenges of big data personality profiling

A recent BBC News article (Personality tests: Are you average, self-centred, role-model or reserved?) came up for discussion in an online coaching forum recently.
These moves are simultaneously exciting and terrifying! Let’s take the Facebook example. The insight from the data won’t be from the fact that you “like” kitten pictures or Liverpool Football Club — it will be from your interactions. What kind of things do you comment on? What’s the tone of your comments? Humorous? Sarcastic? Aggressive? Supportive? How does that change based on who you’re interacting with? What time of day it is? How busy you are?
Now imagine that all of your work email correspondence is analysed, and a similar profile built up. Every micro decision you make when you look at your inbox — read and delete, flag and ignore, reply immediately, forward on. The words you choose when you reply. The individuals which trigger procrastination every time you have to reply to them.
Users of Gmail and Linkedin can already see that these platforms are starting to offer suggested responses to messages on your behalf. They’re pretty rudimentary at the moment, but if the machine learning is taking place across every message you’ve ever sent, calibrated against every message that has ever been sent on Gmail — those models will start to get very much better, very quickly. Just look at how much things like speech recognition and machine translation have come on over the last few years — the science of inferring meaning and intent from text is a very hot area right now.
For those who are familiar with 2001: A Space Odyssey, I don’t think we’re too far away from “Look, Dave, I can see you’re really upset about this. I honestly think you ought to sit down calmly, take a stress pill and think things over.” — imagine if that were to pop up when you were typing an email?
My biggest concern is ethical supervision. I work with several clients who are implementing these technologies — for fraud detection, to monitor call centres (“your call may be recorded for training and quality purposes…”). These data scientists are overwhelmingly white, heterosexual, elite-educated, affluent males, mainly in their twenties and early thirties. The models they produce are incredibly complex — it can be very hard to understand what they’re doing. Unintended consequences:
https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist
https://www.pcmag.com/article2/0,2817,2357429,00.asp
As coaches, we reflect. We take our decisions, actions and approaches to supervision. We feel anxious or guilty when our interventions don’t work. We have a conscience. We understand the power we have to change the behaviour of others, and we take that power very seriously. When we’re handing the tools for behavioural modification to almost-incomprehensible algorithms coded by extremely monocultural groups of people, we need to be very, very careful.
One final thought — 1 in 5 brides met their spouse online (https://www.prnewswire.com/news-releases/only-1-in-3-us-marriage-proposals-are-a-surprise-engagement-ring-spend-rises-according-to-the-knot-2017-jewelry–engagement-study-300552669.html). With Tinder, this proportion will only increase. So, the algorithms are already starting to breed the people…
",The ethical challenges of big data personality profiling,0,the-ethical-challenges-of-big-data-personality-profiling-8606cc7ffc7b,2018-09-28,2018-09-28 11:47:42,https://medium.com/s/story/the-ethical-challenges-of-big-data-personality-profiling-8606cc7ffc7b,False,531,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Gareth Marlow,Father of Four; Renaissance Man,f6872b0583ae,GarethMarlow,121.0,108.0,20181104
0,,0.0,,2018-07-01,2018-07-01 23:14:39,2018-07-02,2018-07-02 05:00:36,0,False,en,2018-07-02,2018-07-02 05:00:36,5,bd68d5f47cc2,6.6415094339622645,1,0,0,"This is not a post comparing metrics for predictive accuracy in machine learning. Rather, it is a post about how we evaluate machine…",5,"Measuring Progress in Machine Learning
This is not a post comparing metrics for predictive accuracy in machine learning. Rather, it is a post about how we evaluate machine learning software from the point of view of the entire research and development cycle, which is the point of view that is most important to society. It is a point of view that takes into account the fact that machine learning researchers are humans who are fallible, not always able to write optimized software, and cannot predict the future in complex situations.
When I talked to machine learning professionals about becoming a data scientist, some of them recommended doing well in machine learning competitions like Kaggle (and some jobs explicitly ask for this as a qualification), while others expressed skepticism about the utility of ranking in such competitions. At that time, I had a vague intuition as to why the latter group would have such a view — having studied idealized models in physics, I knew full well that there is always a significant gap between the skills required to succeed in an idealized environment and those required to succeed in real environments. A couple of machine learning research papers that appeared on my Twitter feed recently, however, clearly articulated reasons that justify that intuition I had. The first, by a group of Google researchers (Sculley et al), discusses various ways that technical debt is incurred by machine learning algorithms and often overlooked as a risk factor. The second, by Martinez-Plumed et al, models progress in artificial intelligence on several dimensions, where accuracy is only one of them.
Success Metrics in Competitions
In order to evaluate large numbers of entries by the same metric, machine learning competitions usually specify a mathematical formula that they use to evaluate the predictions submitted by contestants. Contestants submit a text file containing their predictions, and some metric like logarithmic loss is used to evaluate them. We’ll set aside questions about whether the selected metric is the correct one for evaluating accuracy. What the two papers I’m discussing throw doubt on is whether accuracy should be the primary consideration when choosing machine learning methods. All other things being equal, organizations would prefer their algorithms to be as accurate as possible. But all other things are not equal —as Sculley et al argue, you might have to incur the software engineering equivalent of high-interest credit card debt in order to maximize accuracy. This cost — which they term technical debt — could even be a necessary trade-off in order to obtain a high accuracy.
The Pesky External World
In software engineering, it is considered good practice to make your code as modular as possible. Modularity is a measure of how easy it is to modify code in one component without having to make drastic changes to other components. A modular system is the opposite of a system that is susceptible to the butterfly effect. In the latter, a small change in one part can lead to drastic effects in other parts. This is troublesome when you are trying to design a complex system with many interacting parts.
Sculley et al argue that machine learning software systems in particular are hard to modularize, because they are highly dependent on external data — which is not controlled by the makers of the software.
When it comes to external data, the analog of the butterfly effect in machine learning is what the authors call the Changing Anything Changes Everything (CACE) principle. One way in which this principle manifests itself is in the selection and weighting of features to train your machine learning model on. It is impossible (and impractical) to incorporate all features of a real system in your model, so typically only a subset of features is used, with each feature weighted according to what works best on past data. But as the external data changes, new features may become important for modelling reality. When you incorporate these new features, you may have to drastically change the weights of other features and how they are incorporated in the model. The authors point out that changes in other aspects of the model, such as sampling methods and the smoothness of the functions used, can have similarly wide-ranging effects.
Often, it can be tempting to incorporate an additional feature that leads to a minor gain in accuracy. However, the fact that the feature now exists in the model could lead future versions of the model to be drastically sensitive to changes in that feature. That is, you could be trading off a short-term, small gain in accuracy for long-term instability of the model.
The authors list a range of other manifestations of CACE, which more-or-less stem from the fact that it is hard to design a machine learning system that adapts well to changes in the external data. This also means that it is hard to adapt a machine learning model trained to solve one problem to perform optimally on a slightly different problem. To use a term from Martinez-Plumed et al, machine learning methods often lack replicability.
General Methods and Glue Code
Wait, but aren’t there general machine learning packages that many people use, which are self-contained and reusable? Indeed, but for these packages to perform well on specific problems, they require lots of glue code — a term Sculley et al use to describe supporting code that is used to insert data and extract results from general-purpose algorithms. The core idea is as follows: In order to perform optimally on most problems, you need an algorithm that is optimized for the feature set and dynamics relevant to that problem. To use a general-purpose algorithm rather than one designed specially for that problem, you will have to transform your preferred features and your preferred conception of the problem so that you can feed in well-formed data that the general-purpose algorithm accepts. To implement this transformation, and the corresponding transformation when you extract results and transform it into the kinds of outputs most suitable for that problem, you write glue code. Glue code is not easily reusable, because it is a kludge constructed specifically so you can glue your non-ideal, specific problem and data to a general-purpose algorithm. If you choose to use another algorithm or if you switch to a different problem, you will need to write a whole new set of glue code.
Other Dimensions of Evaluation
The lack of replicability and the inability to adapt easily to new problems and new data amount to what Sculley et al call a tax on innovation. An algorithm may achieve short-term “progress” by achieving the best accuracy among all its competitors on a specific problem. But it may not be the best starting point for making better algorithms for future problems.
Martinez-Plumed et al suggest other aspects on which we should measure progress in artificial intelligence. They use a utility function across the desirable features of AI to model what counts as progress. As they point out, utility metrics will differ across different subjects. Some subjects might be more willing to achieve short-term gains in accuracy at the expense of long-term adaptability. Some might care more about replicability than others. Some may be more willing to maintain large quantities of glue code than others. Martinez-Plumed et al point out that it is not a good thing to have everyone be on the “Pareto front” of progress (the front on which the utility function is maximized). Sometimes, alternative approaches that are not on the Pareto front are good starting points for future algorithms that will be optimal. Today’s optimal algorithms are not necessarily going to be tomorrow’s winners — a point backed by Sculley et al’s CACE principle.
Values and Progress
All this feeds into points that will be familiar to philosophers of science — there is not going to be an obvious choice of which is the “best algorithm” that can be divorced from questions of social values. The tradeoffs I mentioned above are going to be decide partly on social values. For example, how far we should value accuracy versus other dimensions of progress is going to depend on the potential social impact of a less accurate algorithm. If we’re talking about targeted advertising for luxury goods, a slight drop in accuracy may have little social impact. For algorithms that determine how to allot subsidized housing to homeless people (for example), inaccuracies have a much greater social impact. This might be a reason to value accuracy more in these algorithms.
Even within the context of a profit-maximizing corporation that may not care about social values, the choice between algorithms is still going to depend on organizational values. It will depend on how long-term the organization’s vision for this line of research is. For example, the more long-term it is, the less willing they should be to take short cuts by using general-purpose packages supplemented by large quantities of glue code.
When we encourage people to rank high in machine learning competitions in order to break into the data science job market, we may be skewing incentives in a way that is not ideal for machine learning research. We are recruiting people who score highest on one dimension of success in machine learning. Getting a good ranking in these competitions is a non-trivial time commitment that distracts from some of the most interesting questions: Why is the problem formulated this way? How was the data set obtained? Are these the appropriate features to use? Is it worth it to use this highly-tuned model with tonnes of glue code, rather than something less accurate but more easily modified?
This discussion is important because it draws attention to how far the choice of a machine learning algorithm is based on weighing different factors that cannot be measured on the same scale. This freedom of choice means that the person who decides on one algorithm over another cannot pretend that their choice was simply driven by value-free considerations, as appeals to “basic research” often pretend to be. When you design a machine learning model, you ought already to be thinking about how it performs on other data sets and other problems, not just on this one problem you have now. That calculation is also part of basic research — nobody wants to have to write an entirely new set of code again, not even in basic research.
",Measuring Progress in Machine Learning,1,measuring-progress-in-machine-learning-bd68d5f47cc2,2018-07-02,2018-07-02 05:00:36,https://medium.com/s/story/measuring-progress-in-machine-learning-bd68d5f47cc2,False,1760,,,,,,,,,,Ethics,ethics,Ethics,7787.0,liminal traversals,"recovering from being a trans, non-white person in philosophy of physics. writing about issues that are under-valued in Anglophone philosophy.",541743df79c2,struthious,126.0,147.0,20181104
0,,0.0,370306b4debe,2018-04-13,2018-04-13 01:36:46,2018-04-25,2018-04-25 16:01:02,4,False,en,2018-04-25,2018-04-25 16:01:02,6,94033c7bfa77,6.824528301886794,3,0,0,"Meet Ece Kamar, a senior researcher at Microsoft who works on human-machine collaboration, AI systems in the real world, and issues around…",5,"Role Models in AI: Ece Kamar

Meet Ece Kamar, a senior researcher at Microsoft who works on human-machine collaboration, AI systems in the real world, and issues around bias, robustness, reliability, and transparency in AI. Ece also co-authored the first report in a 100-year study of artificial intelligence, intended to provide a set of reflections about the field as it progresses. The report offers insights on where AI is headed, policy recommendations, and the importance of reflecting on fairness and transparency in the field.
Ece believes that it’s unlikely that important tasks will ever be fully automated, as human-AI partnerships will be complementary, rather than a relationship of replacement. See how Ece envisions the future of AI, how her academic exploration in college helped shape her career, and how she sees diversity as key to moving the field in a positive direction.
We interviewed Ece as part of AI4ALL’s Role Models in AI series, where we feature the perspectives of people working in AI in a variety of ways. Check back here weekly for new interviews.

As told to Nicole Halmi of AI4ALL by Ece Kamar; edited by Panchami Bhat
NH: Can you describe what you do as a senior researcher in the Adaptive Systems and Interaction Group at Microsoft? What does a typical day look like for you? What kind of projects are you working on right now?
EK: In my group at Microsoft, I have complete freedom in terms of what I want to focus my research on. I’m very interested in how AI works in the open world. So far, we in the field have been getting a lot of great numbers in laboratory settings about the accuracy of different AI systems for different tasks, but getting AI to work the same way in the open world with real people is a very different ball game.
I think we’re coming to an inflection point in AI.
It is not a question of “can we build AI,” as we have a lot of successful examples of AI systems. Instead, there are questions of, “how should these systems be built and deployed? How should they be partnering with people?”
You’ve also done a lot of work looking at how humans and machines can collaborate.
My work is influenced by the fact that people and machines have complementary abilities. AI systems work well when you have lots of data, the task is well-defined and repetitive, and the system is in a closed environment without a lot of unknowns. However, when you move beyond this and either try to adapt a system to a new domain or try to use it in the open world, these algorithms face a lot of challenges.
People don’t do very much statistical reasoning, we may not be very consistent all the time, and we have biases. However, we have common sense reasoning, counterfactual reasoning, creativity, and we’re good at adapting to new settings and combining our knowledge from different domains.
If our focus is only automating what exists in the world, and not on complementarity of tasks, we won’t be utilizing the true power of human-machine partnership.
The combination of humans and machines will be more effective, efficient, and reliable than either is on their own.
What are some of the important things people should be doing to create a positive, inclusive, and ethical future for AI?
One problem that I work on is what I call the “blind spots” of artificial intelligence. There’s a common assumption that the training data is perfect, but this isn’t true. For example, face recognition systems not recognizing dark-skinned people because the training datasets didn’t include enough representation of certain groups of people. Or speech recognizers not working as well for elderly or for children.
These blind spots are due to representation problems in a model’s training data, which the models don’t see at training time, but we do see in the real world. This creates a big reliability problem.
When blind spot errors are concentrated around subgroups of people, they create biases that AI systems can learn and amplify. We have to work really hard to address these issues, not only with technical tools but also with our value judgments.
How did you decide to get a bachelor’s in computer science and engineering? Were you interested in the field at a young age or did you discover it in college? And how did you come to focus on artificial intelligence for your Ph.D. research?
I was born and raised in Turkey. We have a school system in Turkey where we took these nationwide exams at the end of high school and selected our university and our major at the same time. That was a really hard decision for me because I didn’t know what fields I would be successful in. I didn’t think I would be good at computer science because I thought it was for people who really liked games — and I didn’t even have a computer at the time.
I decided to go to a new school in Turkey that allowed you to declare your major later. I took my first programming class there and enjoyed it because I love puzzles and solving problems from scratch. Our class was 50% girls and 50% boys. Though there were a few people who had a bit more experience, it was mostly an equal playing field. I never got intimidated by the field because of this atmosphere.
I was always very interested in language and one of my professors in college was studying computational linguistics. Because I was doing well in my classes, he offered me a summer research project. I started my AI journey working with him on this language project.
It sounds like your undergrad computer science experience was very different from many undergrad CS classes in the US. Do you think there was there a reason for the gender balance in your classes?
Having the freedom to try different classes at my college really helped people explore subjects without any limitations or stigma.
A lot of computer science problems are practical and could have real-world impact. I see women who care about the impact of technology on the world. I cannot imagine girls not enjoying these beautiful problems if they are given the opportunity and support in exploring these areas.
When I got to grad school in the United States, I realized that this gender disparity existed. It was an interesting cultural shift for me.
Who were your role models growing up? Do you have any role models now?
My biggest mentor and role model is my Ph.D. advisor Barbara Grosz. She’s one of the pioneering women in computer science and has worked so hard both to advance the field of AI and to promote women in sciences.
She’s always going for the problem that she thinks she can make the biggest impact on, even if it’s very hard. Working with her inspired me to become more involved in the field of human-machine collaboration and decision-making. I feel very lucky to have her as a mentor.
What has been the proudest or most exciting moment in your work so far?
I was recently invited to give big talks in Turkey where I got to meet a lot of people and tell them about my research and perspective on AI. Having the opportunity to speak in my home country about what I’ve accomplished was a lot of fun.
I’m also very proud to see students I’ve mentored becoming professors at great places and get recognized for their work.
What advice do you have for young people who are interested in AI who might just be starting their career journeys?
I think that our field has a lot of open, big problems that we’re just starting to dig up. These problems are going to have really big consequences for the way we live and for the society we live in. There will be a lot of opportunities for our work to make an impact, and I would recommend that people explore these issues without any hesitations.
Some of the issues we’re seeing in AI systems today have deep connections to the diversity problems in the field. Having diversity in our field is not only an issue about the culture. It is also about the reliability, robustness, and fairness of the systems we build. It is of practical importance to us that everybody’s voice gets represented in the systems we build.

About Ece Kamar
Ece Kamar is a Senior Researcher in the Adaptive Systems and Interaction Group at Microsoft Research. Ece received her Ph.D. in computer science from Harvard University in 2010. Her research is inspired by real-world applications that can benefit from the complementary abilities of people and AI. Since many real-world problems requires interdisciplinary solutions, her work spans several subfields of AI, including planning, machine learning, multi-agent systems and human-computer teamwork. She is passionate about investigating the impact of AI on society and studying ways to develop AI systems that are reliable, unbiased and trustworthy. She has over 40 peer-reviewed publications at the top AI and HCI venues and served in the first Study Panel of Stanford’s 100 Year Study of AI (AI100).

Follow along with AI4ALL’s Role Models in AI series on Twitter and Facebook at #rolemodelsinAI. We’ll be publishing a new interview with an AI expert every week this winter. The experts we feature are working in AI in a variety of roles and have taken a variety of paths to get there. They bring to life the importance of including a diversity of voices in the development and use of AI.
",Role Models in AI: Ece Kamar,61,role-models-in-ai-ece-kamar-94033c7bfa77,2018-06-18,2018-06-18 03:18:36,https://medium.com/s/story/role-models-in-ai-ece-kamar-94033c7bfa77,False,1623,AI4ALL is a nonprofit working to increase diversity and inclusion in artificial intelligence. Our vision is for AI to be developed by a broad group of thinkers and doers advancing AI for humanity's benefit.,,ai4allorg,,AI4ALL,contact@ai-4-all.org,ai4allorg,"DIVERSITY,ARTIFICIAL INTELLIGENCE,DIVERSITY IN TECH,MACHINE LEARNING,FUTURE",ai4allorg,Ethics,ethics,Ethics,7787.0,AI4ALL Team,AI4ALL is a US nonprofit working to increase diversity and inclusion in artificial intelligence.,9df91f709acb,AI4ALL,892.0,8.0,20181104
0,,0.0,,2018-02-15,2018-02-15 22:45:59,2018-03-22,2018-03-22 22:16:11,2,False,en,2018-03-22,2018-03-22 22:16:46,2,1180554ead72,1.5757861635220125,0,0,0,Trust in AI may be one of the biggest challenges,5,"We get on like a house on fire
Trust in AI may be one of the biggest challenges
We get on.
It may come that the biggest challenge in the adoption of AI is how humans are able to trust AI — and not in the way you think. Despite opinions that humans won’t ever trust robots, humans may be too trusting. Research suggests that once a human has placed their trust in a robot its hard to shake. That we are willing to blindly follow robots into the fire.
This may explain some of the accidents self driving cars have where there are humans behind the wheel supposedly supervising. Maybe it also explains some of the strange things we do when interacting with AI.
In the ‘assisted-self-driving’ car scenario I imagine people just switch off when told to hold the wheel and don’t actually pay attention and supervise. They trust the machine and why wouldn’t you - the car drives itself.
This mentality doesn’t help us through the next chapter of AI, where it augments our capabilities — but doesn’t replace us. If the AI is not to 100%, implicit and absolute trust will fail us both.
So how do we make it work while AI still has the training wheels on and is far from perfect? We learn to drive in a supervised environment, we should do the same with robots.
The challenge for AI/robot builders is how to work with people and keep them engaged. When do we let our users know “I’ve got this” and when do they need to intervene? I believe we need to encourage a healthy suspicion of the AI’s competence and gamify the act of supervising our creations .
We should stop worrying about building friendly faces into our robots to engender trust, and instead consider something more terrifying like the plague doctor mask to keep us on our toes.

",We get on like a house on fire,0,we-get-on-like-a-house-on-fire-1180554ead72,2018-03-22,2018-03-22 22:42:17,https://medium.com/s/story/we-get-on-like-a-house-on-fire-1180554ead72,False,316,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Matt Szwec,Explore. Engineer. Expedite.,21e21df1b5b6,mattszwec,16.0,5.0,20181104
0,,0.0,97199cc52c61,2018-08-20,2018-08-20 16:25:50,2018-08-20,2018-08-20 16:41:36,1,False,en,2018-08-26,2018-08-26 22:22:39,5,35923b6b97f4,3.113207547169812,2,0,0,“The people who care about people need to learn to talk to the people who care about code”,5,"Bringing the Human to the Data — Interview with Christian Madsbjerg

In this episode of the Masters of Data podcast host I spoke with author and consultant Christian Madsbjerg for an engaging conversation on data and what it means to humanize it. It can be said that we don’t often read books that change how we think about the world, but in many ways Christian’s book Sensemaking: The Power of the Humanities in the Age of the Algorithm has done exactly that for me. Christian’s consulting firm ReD Associates has, in their own words, led a quiet revolution in business thinking. This book is a treatise on Christian’s underlying philosophical framework for ReD’s goal to bring the humanities and social sciences into today’s businesses dominated by technology, data, and analytics. In his discussion we find that Christian’s perspective will make the listener rethink their assumptions about the critical importance of the humanities in today’s fast paced world. Ultimately, they review what has to happen for the data industry to see real advancements and change take place.
Christian, who was born in Denmark, spent a lot of his time as a child in the library. Over time, his love for reading and academia grew, but he also saw how unhappy academics tended to be. As his love for learning (but also frustration around academia’s disposition) increased, he eventually came to the decision to start his own company because he saw great value in the philosophy and people of academia despite their shortcomings. And so, for the last 20 years his mission has been to find how to use social sciences and the tools he has acquired, to help people make human decisions. With this goal in mind he has dedicated himself to the work at ReD, has written many books and tries to spread the conviction that humans can be understood in sophisticated ways.
As their discussion unfolds, Christian ultimately shares his reasoning behind his life’s work and the purpose for this book, which many have found to be so impactful. He shares that he became worried about the ideology that sees people and data like they are linear or rational and that machines can simply learn everything through intaking more data. Disciplines like anthropology prove that’s not the case and that people are not predictable, but few people have had the ability to give that reality a voice in the data analytics realm. As Christian deeply believes, data and machines cannot know ourselves better that we know ourselves, something data companies are now slowly realizing.
But as the two share, the goal is not simply to prove the faults in historic approaches to data analytics, but rather to figure out how someone with an anthropology background and someone with an engineering background can work together for the common good without misunderstanding each other. In business, machines are developed to make lives easier, but we forget what it’s like to be a human when it comes to design. As the two discuss, you can build AI but you can’t replicate the human experience; being human isn’t just linear thinking but rather involves investing emotion. There’s something unpredictable about being human. This is why it is important to account for thick data, which takes into consideration context, rather than thin data, which is merely cold, hard facts. When algorithms only take into account thin data, the results are ineffective.
But the conversation does not merely end with a critique of the state of AI and data analytics’ approaches but rather unpacks the practical solutions to the issue of humanizing data. How is this done? As Christian recommends, qualitative data should come before quantitative data, meaning you should get a broader understanding on something before an algorithm is designed. Rather than scraping facts, you must ask questions concerning the context. As designers there likewise has to be vulnerability to show ideas with others, paired with a humility that says others might have something to offer which you don’t, like a fresh perspective. The challenge, as discussed, is that there is a lot of arrogance on each side of the spectrum. The reality that we are often so brilliant in some areas but very ignorant in others is ignored, so we need to find ways to work together. This is Christian’s goal moving forward-to build systems and technologies with others that will improve human the life.
Outbound Links & Resources Mentioned
Masters of Data Podcast Episode
Learn more about Christian
Learn more about Sensemaking: The Power of the Humanities in the Age of the Algorithm
Learn more about ReD Associates
Follow ReD on Twitter @ReD_Associates
",Bringing the Human to the Data — Interview with Christian Madsbjerg,2,bringing-the-human-to-the-data-interview-with-christian-madsbjerg-35923b6b97f4,2018-08-26,2018-08-26 22:22:39,https://medium.com/s/story/bringing-the-human-to-the-data-interview-with-christian-madsbjerg-35923b6b97f4,False,772,"Thoughts on what's going on in technology, data, analytics, culture and other nerdy topics",,mastersofdata,,Newtonian Nuggets,ben@newtoniannuggets.com,newtonian-nuggets,"ANALYTICS,DEVOPS,BIG DATA ANALYTICS,THOUGHT LEADERSHIP,ARTIFICIAL INTELLIGENCE",benoitnewton,Ethics,ethics,Ethics,7787.0,Ben Newton,"Proud Father, Avid Reader, Musician, Host of the Masters of Data Podcast, Product Evangelist @Sumologic",2814fd09f883,BenNewton,110.0,94.0,20181104
0,,0.0,c25a93aad0d,2018-05-29,2018-05-29 10:53:16,2018-05-29,2018-05-29 11:21:20,15,False,en,2018-06-08,2018-06-08 12:15:54,19,6b0fd073e2c2,41.926415094339625,0,0,0,Engaging citizens in the ethical use of AI for automated decision-making,3,"Artificial Intelligence: real public engagement
Engaging citizens in the ethical use of AI for automated decision-making
By Brhmie Balaram, Tony Greenham and Jasmine Leonard

Technological breakthroughs can be polarising because there are often both benefits and risks. New technology promises us a better way of life, and sometimes it does deliver for the masses. The world has been transformed by medical discoveries like penicillin; revolutionary modes of transport, like trains and planes, and in more recent years, inventions like the internet and the smart phones. But sometimes there are complications or consequences; there have long been concerns about the loss of jobs to automation, but people are increasingly anxious about other risks, such as threats to privacy, security, and psychological well-being, as well as increasing susceptibility to political manipulation and fraud.
Developments in artificial intelligence (AI) are likely to intensify these risks if not handled carefully, and also introduce new ones, such as the possibility of reinforcing systemic biases and exacerbating inequality. Although AI has enormous promise in fields as diverse as education, health and transport, given the risks, a growing chorus of voices is calling for greater consideration of ethics as AI is further developed and adopted more widely. We’re at a point now where the backlash to AI may be beginning, and in some cases, rightfully so.
As with any technology, AI’s potential to help or harm us depends on how it’s applied and overseen. One application that demonstrates this double-edged potential is the use of AI in automated decision systems.
Automated decision systems refer to the computer systems that either inform or make a decision on a course of action to pursue about an individual or business. Automated decision systems do not always use AI, but increasingly draw on the technology as machine learning algorithms can substantially improve the accuracy of predictions. These systems have been used in the private sector for many years (for example, to inform decisions about granting loans), and now many public bodies in the UK are exploring and experimenting with their use for planning and managing new infrastructure; reducing tax fraud; rating the performance of schools and hospitals; identifying hotspots for crime, and making custodial decisions.
This technology could have significant social and economic implications, but there has been no meaningful realisation of what it means for society to be ‘in-the-loop’, or in other words, for the public to be more involved in decisions about the deployment and regulation of these systems.
From our own online survey of the UK population which we carried out in partnership with YouGov, we know that most people aren’t aware that automated decision systems are being used in these various ways, let alone involved in the process of rolling out or scrutinising these systems. Only 32 percent of people are aware that AI is being used for decision-making in general, and this drops to 14 percent and 9 percent respectively when it comes to awareness of the use of automated decision systems in the workplace and in the criminal justice system. On the whole, people aren’t supportive of the idea of using AI for decision-making, and they feel especially strongly about the use of automated decision systems in the workplace and in the criminal justice system (60 percent of people oppose or strongly oppose its use in these areas).
The public’s doubts about AI have yet to seriously impede the technological progress being made by companies and governments. Nevertheless, perceptions do matter; regardless of the benefits of AI, if people feel victimised by the technology rather than empowered by it, they may resist innovation, even if this means that they lose out on those benefits.
The problem may be, in part, that people feel decisions about how technology is used in relation to them are increasingly beyond their control. Moreover, they may not trust those who are making these decisions, as is clear from the Hansard Society’s recent annual audit of political engagement. What this suggests is that there may need to be a radical overhaul of the way in which organisations and institutions include and devolve power to citizens over these decisions. In a liberal democracy, these decisions should be made with the public, not just for the public. Such democracies should value innovation, human rights and public dialogue and voice. It is possible to imagine a different model where the private sector or the state alone drive innovation in service of the interests each holds dear. However, the RSA, with its ethos of 21st Century Enlightenment, believes that these alternative models are not the right way to proceed.
The RSA’s Forum for Ethical AI is making the case for entering into a public dialogue with citizens about the conditions under which this technology is used. While human rights law serves to protect people from egregious violations, we also need to engage directly with people to address the wider problems of mistrust and disempowerment that can arise when only a few are making critical decisions on behalf of many.
When it comes to automated decision systems, for example, experts have called for the need to go beyond embedding individual or group judgment in these systems and to start encompassing the values of society as a whole. This requires a public dialogue with citizens to resolve trade-offs; for example, trade-offs between privacy and security, or between different notions of fairness. These ethical issues being surfaced by AI may ultimately lead to enacting new laws or policies, but they are also the reason why we should expect organisations and institutions (in both the private and public sectors) to fundamentally change the way they operate, engage with, and are accountable to citizens. In our public dialogue, the citizens might help stimulate thinking about what sort of reform is needed in terms of corporate and governmental structures, products, and services to minimise risks and secure benefits for more people.
The RSA wishes to see profound innovation for the public, by being with the public; if the public are going to unite in favour of innovations like AI, they need to be engaged early and more deeply. They need to feel confident that this technology is being deployed responsibly and will uplift individuals and communities at large.
In this paper, we first set out what we mean by ethical AI and why AI needs to reflect the public’s values. We propose exploring the use of AI for decision-making with the public, expanding on the proposition for ‘society-in-the-loop’ systems. We then clarify what the process for public dialogue is, and in particular, long-form deliberation. Next, we present the results of our online survey of the UK population’s attitudes towards AI and automated decision systems; these were used to draw out key issues for deliberation with citizens. Finally, we detail what the RSA’s public dialogue with citizens will look like in practice and clarify our next steps.

1. Embedding citizen voice in ethical AI
Although established as field of technology since the 1950’s, swift progress has been made in recent years to further develop AI. In less than a decade, what were once nascent capabilities of AI, such as computer vision and natural language understanding, have evolved to rival the capabilities of humans. In some areas, AI has surpassed human capability; for example, when it comes to tasks such as recognising objects, or playing competitive games such as Go and Poker. New approaches to developing AI, most notably deep learning, have accelerated advances in the technology, which in turn have stimulated greater investment and support for the growth of the industry. AI is increasingly being entrusted to do more for us by companies and governments, and in a range of sectors such as healthcare and criminal justice. However, as AI is used in new ways that could have significant consequences for individuals and communities, concerns about the ethics of AI are becoming more urgent.
What do we mean by AI?
AI refers to machines that can perform tasks generally thought to require intelligence.
Most modern AI systems employ a technique known as “machine learning”, in which computers learn how to perform a specific task from examples, data and experience. This is in contrast to traditional computer systems, which are explicitly told how to perform a particular task by human programmers.
One of the most effective methods of machine learning developed so far is “deep learning”. Based loosely on the structure of the brain, deep learning algorithms involve many layers of interconnected units which form a “neural network”. The complexity of deep learning networks makes it impossible to understand exactly how they work, leading them to be described as “black boxes”.
One task that AI is increasingly being used for is to make predictions about the likelihood of future events occurring. While predictions can be made using a variety of statistical techniques, machine learning is increasingly becoming the preferred tool due to its potential for greater accuracy.

What do we mean by ‘ethical AI’?
Ethical AI is garnering much interest, but it’s not always clear what this refers to. A broad range of emerging issues have been identified as requiring ethical frameworks or principles in order to steer the development of AI in a socially beneficial manner, including:
AI safety: Ensuring that autonomous systems do not behave in ways that inadvertently harm society.
Malicious uses of AI: Guarding against the misuse of AI by malicious actors.
Data ownership and protection: Overseeing the use of personal data for AI systems.
Algorithmic accountability: Clarifying governance and responsibilities for the use of algorithms, such as in the case of automated decision systems.
Socio-economic impact: Managing social and economic repercussions of AI, such as increased inequality of wealth and power.
Strikingly, many of the authors and organisations that are exploring these issues and developing related frameworks or principles have advocated public dialogue or engagement. For example:
A report on malicious AI authored by a coalition of organisations, including the Future of Humanity Institute, the Centre for the Study of Existential Risk, and Open AI, advocated for a public dialogue on appropriate uses of AI technology. The authors recommended actively seeking to expand the range of stakeholders and domain experts involved in discussions of the challenges, which they believe should include the general public alongside civil society, businesses, security experts, researchers, and ethicists.
The international and interdisciplinary research community known as Fairness, Accountability, and Transparency in Machine Learning (FAT/ML) developed principles which suggest facilitating public auditing of algorithms.
Similarly, the Association for Computing Machinery expressed within their principles for algorithmic transparency and accountability that public scrutiny is ideal, particularly in relation to training data, in order to maximise opportunity for corrections.
Most recently, AI Now Institute called for public agencies to enable communities to review and comment on their use of automated decision systems, detailing the process as part of ‘Algorithmic Impact Assessments’.
The Partnership on AI, which was founded by leading technology companies and now encompasses a number of academic and non-profit organisations, published tenets committing to educating and listening to the public; an open dialogue on the ethical, social, economic and legal implications of AI, and actively engaging with and being accountable to a broad range of stakeholders.
To build on these ideas, we propose a working definition of what we mean by ethical AI:
AI that is designed and implemented based on the public’s values, as articulated through a deliberative and inclusive dialogue between experts and citizens.
We intend this definition to capture a number of elements that we consider to be necessary to achieve deployment of AI technology in a manner that is beneficial to society over the long-term, has moral and political legitimacy, and hence is grounded in widespread popular consent. These are:
in both design and implementation, AI is guided by values above short term profit;
values should be based on our best understanding of society’s values; and,
the most effective methods for building a shared and considered set of societal values bring together citizens in deliberative and inclusive dialogue with subject experts, such as technologists and philosophers.
Why does AI need to reflect the public’s values?
The potential of AI to dramatically transform our lives is enormous, and this shift is already underway. The use of AI in the private sector is comparatively widespread, but now public bodies are increasingly adopting the use of AI systems, expanding their reach and raising the stakes.
In large part, AI is likely being used by public bodies to increase efficiency and reduce costs, but in some circumstances it may also be used to improve the fairness of outcomes and minimise biases in systems, particularly those that involve decision-making.
Using computers to make decisions thus far has not always met expectations, and in some instances it has exacerbated inefficiencies and reinforced inequalities.
The academic Virginia Eubanks has exposed cases in the US in which the use of automated decision systems has further disadvantaged some of the most vulnerable groups in society. She investigated the use of these systems to determine eligibility for welfare, allocate social housing, and evaluate the risk of child abuse and neglect, finding that the process of how they reached their verdicts was often inexplicable (ie because it was not apparent to what extent algorithmic predictions influence human decision-makers).
She revealed that the data collected for these systems could be very intimate and serve to intensify state surveillance of the poor in particular. Eubanks argues that the fundamental problem with these systems is that they enable the ethical distance needed “to make inhuman choices about who gets food and who starves, who has housing and who remains homeless, whose family stays together and whose is broken up by the state” (The Digital Poorhouse).
While many of these systems have used more simple statistical techniques to date rather than AI, public bodies in the UK are exploring, and in some cases, experimenting with the use of AI to help make decisions for planning and managing new infrastructure; reducing tax fraud; rating the performance of schools and hospitals; identifying hotspots for crime, and making custodial decisions.
Yet, as Eubanks demonstrates, given that the consequences of some of these systems are far from trivial, it is reasonable to consider whether greater public legitimacy is needed or clearer parameters should be established before they become more widespread as a result of advances in AI. After all, it’s not the accuracy of these systems that is the primary concern; it is the ethics of using the systems under particular circumstances or conditions. It’s important for the public to have an opportunity to engage with the trade-offs of using AI in these ways and to express their views and values.
When we speak of AI systems based on the public’s values, we are referring to exploring how citizens understand the contemporary use of AI and how they apply ethical reasoning to how it should, or should not be, used in the delivery of private or public services. This includes citizens’ views on how these institutions should demonstrate transparency and accountability to citizens who will be directly affected by their use of AI.
In public dialogue, it’s recognised that underlying values help us to understand why citizens hold particular opinions or perspectives. Values are specifically defined as —
(i) concepts or beliefs;
(ii) about desirable end states or behaviours;
(iii) that transcend specific situations;
(iv) guide selection or evaluation of behaviour and events, and
(v) are ordered by relative importance.
(Bringing values and deliberation to science communication)
Values underpin people’s preferences for one course of action over another, and, in turn preferences are premised on what people believe about how actions will affect the things they value.
While citizens are unlikely to all share the same views, a dialogue can enhance mutual understanding of facts and values, as well as value differences. Although citizens do not have to reach a consensus, there is some evidence that reflection about and articulation of value positions can reduce conflict and enable compromise.
When it comes to controversial uses of AI, the public’s views and, crucially, their values can help steer governance in the best interests of society. Citizen voice should be embedded in ethical AI.
What should the public be engaged on?
There are many ethical issues that a public dialogue could address. For example, autonomous vehicles and weapons are currently capturing the public’s imagination. Both are being developed and attracting investment in research, although neither is commercially available yet. These systems raise questions about how much power should be ceded to AI over human life.
The dual-use nature of AI is also increasingly of concern as it is has become apparent that technology designed with one purpose in mind can be exploited for different and, possibly, more malevolent aims. As researchers have observed, “Surveillance tools can be used to catch terrorists or oppress ordinary citizens. Information content filters could be used to bury fake news or manipulate public opinion. Governments and powerful private actors will have access to many of these AI tools and could use them for public good or harm”. An example of this might be the experimentation with ‘social credit scores’ in China, which are ratings assigned to every citizen based on government data regarding their economic and social status.
All of these issues could inspire meaningful public dialogue. However, the RSA’s Forum for Ethical AI is choosing to apply a process of citizen deliberation to explore the rise of automated decision systems. These systems have been characterised as ‘low-hanging fruit’ for government and we anticipate more efforts to embed them in future.

2. A whole systems approach to automated decision-making
In the opening chapter, we set out what we mean by ethical AI and why AI needs to reflect the public’s values. Now, we explore why public dialogue on the use of automated decision systems specifically would be useful.
Automated decision systems refer to computer systems that either inform or make a decision on a course of action to pursue about an individual or business. To be clear, automated decision systems do not always use AI, but increasingly draw on the technology as machine learning algorithms can substantially improve the accuracy of predictions.
It is important to examine the use of automated decision systems in the broader social and economic context, considering behavioural insights, cultural norms, institutional structures and governance, economic incentives and other contextual factors that have a bearing on how an automated decision system might be used in practice.
A “whole systems” approach to automated decision-making
At present, it is rare that decisions are fully automated; these systems are typically used as part of a wider process of decision-making that involves human oversight, or a ‘human-in-the-loop’ (HITL). Iyad Rahwan of the MIT Media Lab describes the use of human operators in HITL systems as potentially powerful in regulating the behaviour of AI. He explains that HITL systems serve two functions: to identify misbehaviour by otherwise autonomous systems and to take corrective action; and/or to be an accountable entity in case the systems misbehave. In the latter scenario, the human operator encourages trust in the system because someone is held responsible and expected to own up to the consequences of any errors (and therefore, is incentivised to minimise mistakes).
Rahwan builds on the concept of HITL, proposing the idea of ‘society-in-the-loop’ (SITL) systems that go beyond embedding the judgment of individual humans or groups in the optimisation of AI systems to encompass the values of society as a whole. SITL systems do not replace HITL systems but are an extension of them; they incorporate public feedback on regulations and legislations rather than individual feedback on micro-level decisions. They are therefore particularly relevant when the impact of AI has broad social implications; for example, as is the case with algorithms that filter news, wielding the power to politically influence scores of voters.
As part of designing SITL systems, consideration is given to the question of how to balance the competing interests of different stakeholders. Society is expected to resolve the trade-offs between the different values that are embedded within AI systems (for example, as highlighted by Rahwan, trade-offs between security and privacy, or between different notions of fairness) as well as agree on which stakeholders should reap certain benefits and which should pay certain costs.
Our proposition is that public deliberation is an essential component of developing effective SITL systems.
The RSA has previously proposed a similar ‘whole systems’ approach to resolving social and economic trade-offs in the fields of corporate governance, regulating digital platforms and formulating economic policy, as well as understanding how innovation happens. In relation to the ethical use of AI, we argue that the context in which automated decision systems are used is as important as the design of the systems themselves. Both have a bearing on the outcomes of automated decision systems, and therefore both have a bearing on the social and ethical acceptability of those outcomes.
Automated decision systems in context
Drawing on the concepts of HITL and SITL systems, the decision-making context can be conceptualised as three tiers:
1. the decision taker, who may or may not be human;
2. the institution that is ultimately accountable for the decision;
3. and the societal context in which that institution is operating.
The three tiers and key factors influencing each tier are summarised in figure 2 below:
Figure 2 — Decision-making in context: a whole systems view

The decision taker
Assuming for now that the decision taker is human, there are many factors that influence a decision other than the raw data on which the decision is based. These include intrinsic factors, such as the individual’s own values and beliefs, and extrinsic factors such as the financial and social rewards or penalties faced by the individual as a result of the outcomes of the decision. They also include the individual’s skills and experience as applied to managing data and reaching a decision.*
The institutional context
The next tier is the institution that is accountable for the decision.** The goals of the institution, and the culture and internal incentives that determine how those goals are pursued, have a significant influence on the decision taker. Equally, the governance structure, transparency and accountability of the institution to wider stakeholders and society will in turn influence the institution’s internal goals, culture and incentive structures. Intermediating between the decision taker and the institution may be a suite of decision support tools that are provided by the institution. These may be internal training, manuals or guides, expert systems, or other tools that help the decision taker manage data and follow a rules or principles based process for reaching a decision.
The societal context
Finally, both the individual and the institution will be influenced by societal context in terms of hard factors such as laws and regulations, and softer ones such as cultural norms, moral and religious belief systems, and sense of social cohesion and solidarity. Identifying the societal context may not be easy, especially for global organisations; although it will often approximate to a nation state, it may also be sub-national (eg London, California) or supra-national (eg European, Roman Catholic). For simplicity we have not sub-divided societal context, but the question of how global, national and local cultural norms and laws interact is one to which we expect to return.
*Note that for simplicity we are not distinguishing between those who take a decision and those who implement a decision. In any case it could be argued that individual’s always have to take and be accountable for a decision, if only whether or not to obey orders in cases where these conflict with the individuals own
**For the purposes of this project, we are concerned with decisions that are made by individuals on behalf of institutions rather than on their own account in their personal lives, although doubtless the morality of consulting an AI on relationship matters or other important life decisions would also be a fascinating issue to research.
A number of observations emerge from this conceptualisation of the whole decision-making context.
First, AI can be introduced in two ways. It can be used as part of a decision support tool to help a human make a decision, or it can replace the human decision-taker entirely. Even in this latter case, the AI decision-taker exists within an accountable institution that is ultimately governed by humans. Therefore, there will inevitably be a HITL system bridging the institution and the automated decision system. However, this may not be well defined or governed.
Second, SITL systems bridge the societal context and the accountable institution, reinforcing the idea that they are complementary to HITL systems rather than an alternative to them. Unlike HITL which is baked in to institutional structures, SITL systems are not well developed in existing private, NGO or public institutional structures and so this seems to be the area of most potential and greatest urgency.
In recent months, numerous academics and organisations have suggested detailed processes or made concrete recommendations that reflect the concept of SITL and suggest what it could look like in practice for automated decision systems. In particular, there are several variations of ‘impact statements’ or ‘impact assessments’ that explicitly call for public review and engagement in algorithmic governance. For example:
FAT/ML wrote a ‘Social Impact Statement (SIS) for Algorithms’ to accompany their principles, advocating that those who create algorithms should also publish a statement about the social impact of the system so that the public can know what to expect. FAT/ML urges creators to draw on their principles and includes a set of questions and steps that should be answered and adhered to when drafting a statement.
In anticipation of the mayor of New York City, Bill de Blasio, announcing a task force on automated decision systems, AI Now Institute constructed a framework for carrying out ‘Algorithmic Impact Assessments (AIAs)’. The researchers note that they directly drew on impact assessment frameworks in environmental protection, data protection, privacy, and human rights policy domains to produce a framework that they hope will similarly help agencies and the public to consider complex social and technical questions as automated decision systems are adopted. AIAs set out a five-stage process of governance with the intention of supporting affected communities and stakeholders to assess the claims made about these systems, and ultimately, to determine where, if at all, their use is acceptable.
An initial outline of AIAs prompted Michael Karlin of the Treasury Board of Canada Secretariat to contemplate what form a ‘Canadian Algorithmic Impact Assessment’ would take. Karlin emphasises that the Government of Canada must consider more than protecting the rights of individuals and also balance broader concerns, including how these systems will impact communities, the environment, the ability of individual businesses to succeed, and the health and competitiveness of markets. He drafted a questionnaire as the basis of an AIA that asks two questions of programme officials in government seeking to use automated systems: what impact will the system have on various aspects of society or the planet, and how much judgment will the system will be delegated (ie is there a human-in-the-loop). He remarks that broad expertise is needed to respond to the questions, therefore requiring institutions to collaborate with others. Moreover, he invites comment on his draft AIA, acknowledging that the questionnaire must be scored in a way that is reflective of a diverse set of priorities and worldviews.
These interventions seem promising, and this project adds to the developing field by examining and experimenting with a process of deeper, deliberative engagement that would be appropriate during suggested ‘comment’ or consultation periods with the public on these systems.
3. Understanding the role of public dialogue
In the second chapter, we described the societal context for decisions made within institutions, introduced the concept of ‘society-in-the-loop’ for governing the use of AI and raised the potential for requiring impact assessments for the application of AI systems. This is why we argue that there is an urgent need to explore how public dialogue can be applied to the ethics of AI. In this chapter, we expand on what we mean by public dialogue, clarifying the methodology, and what the process can accomplish.
What do we mean by public dialogue?
The theory and practice behind such public participation in policy-making is already established, for example, in the field of planning and environmental impact (the Aarhus Convention), science policy (Sciencewise)[ and health policy. The case for public participation sits within a broader school of post-positivist theory that challenges the notion of neutral and rational technocratic policy making; this theory instead emphasises the normative nature of policymaking and, thus, the need for integrating deliberative dialogue in governance alongside empirical analysis and logical reasoning.
It has been pointed out that in UK policy documents, dialogue is often used as a synonym for conversation, consultation, collaboration, participation, dissemination, and deliberation. However, among practitioners, dialogue refers to a specific form of engagement that typically involves convening citizens and expert stakeholders to deliberate, reflect, and come to conclusions on public policy issues.
Involve, a leading organisation for deliberative democracy in the UK, advises that public dialogue should enable a diverse mix of participants with a range of views and values to learn about the issues (ie from written information and experts); listen to and share with one another as they further develop their views; draw carefully considered conclusions; and communicate those conclusions to inform the decision-making of policymakers.
There are different degrees of public engagement ranging from the transmission of information to the full concession of decision-making to a public forum or electorate. This was conceptualised by Arstein as a ‘ladder of participation’ with eight rungs. A more modern interpretation has been developed by the International Association of Public Participation (IAP2) with five different degrees on a spectrum of participation. At the RSA we adopt IAP2’s terminology but within Arstein’s original visualisation (see figure 1 below).
Figure 1 — The Ladder of Participation

Our focus is on long-form deliberative processes. These sit on the top two rungs of the ladder — ‘empowering’ and ‘collaborating’, which can include citizens’ juries in addition to citizens’ assemblies, reference panels, and commissions. In a review of long-form deliberative processes, Claudia Chwalisz distinguishes long-from deliberative processes by the following characteristics:
Citizens are tasked with helping to resolve a pressing problem that requires navigating multiple trade-offs and considering more than one possible and realistic solution (and this solution is not pre-determined).
This group of citizens is a small group (in numbers between 24 and 48) who are randomly selected from a local, regional or national community.
The group spends a generally long period of time (ie a few sessions over the course of two to three months) learning about and deliberating on a policy issue from different angles.
Citizens are not asked for their individual opinion on an issue, but to deliberate on behalf of their community with the aim of reaching a consensus or compromise.
The group produces concrete recommendations for decision-makers, who then respond directly and publicly to the proposals.
Crucially, long-form deliberative processes should not be confused with focus groups or consultations. They are not ‘one-way’ exercises in which citizens are only asked for their own opinions on an issue; rather, they are ‘two-way’ conversations between experts, decision-makers and the public in which ideas are exchanged (and often respectfully challenged) in order to reach a conclusion in a collaborative manner.
Mass LBP, an organisation that pioneered long-form deliberative processes in Canada, makes the case that such efforts are an innovative approach to public engagement and consultation. The organisation’s founder Peter MacLeod argues that policymakers tend to be misguided about how to consult the public, typically convening town hall meetings “when something’s gone wrong, or a decision has already been made and an elected official is trying to explain it”. These consultations tend to be dry and technocratic, leaving little room to explore people’s feelings about an issue.
We can take this analysis of consultations further, observing that they tend to be reactive, reflecting a failure of decision-makers to take the long-term view preferred by citizens. Ideally, public dialogue should be far more proactive, inviting citizens and experts to explore emergent issues of importance.
What does a public dialogue accomplish?
Public dialogue is useful when a topic is controversial or complex (involving difficult choices to make or many trade-offs to consider). It is especially valuable when it raises important ethical and social questions that cannot be resolved with facts alone.
This may seem counterintuitive to some who assume that advanced or specialised knowledge (ie at degree level) must be required to draw meaningful conclusions about these sorts of topics, but there are many examples where people have successfully engaged in very complicated and contentious issues. As highlighted by Involve, these include developing an alternative voting system, redrafting the Icelandic constitution, rebuilding New Orleans, forming domestic violence courts in New York, and managing the Federal Deficit in the US. It is argued that citizen input is needed precisely because these topics are so difficult, and to make progress when some sense of public buy-in or legitimacy is required. If there is a clear question that the public can help answer, citizens’ juries are especially ideal.
Table 1: Advantages vs the limitations and challenges of citizens’ juries

Some people may question how impactful a public dialogue can be given the scale of the groups assembled for citizens’ juries in particular. There may be concerns about whether such small groups are likely to be representative of one’s own views and values. It is thus important to clarify that there is a distinction between representation and representativeness. We are asking these citizens to represent their community to encourage them to consider more than their own, individual interests, but we are not claiming that they are statistically representative of that community. Rather, we are suggesting that there are relevant insights to be drawn from a diverse group of citizens who are given the opportunity to enter into an informed and deliberative dialogue. Similar logic underpins the use of juries for criminal trials, in which lay members of the public are chosen to reach a verdict rather than trained legal experts.
Public dialogues which are long-form deliberative processes, such as citizens’ juries, will ultimately make recommendation(s) to be enacted. The organisation that convenes the dialogue does not commit to acting on these recommendations; rather, this is expected of the relevant institutions and organisations with influence and authority. However, if these recommendations aren’t acted upon, the convenor will explain why this is the case to the citizens, and still publicise the process and findings widely to help broaden and enrich public debate.
We can now see that contemporary proposals for public dialogue on AI stand on an enormous body of academic literature and established practice. However, while the theory and techniques are not new, we sense that there is novelty in the subject matter for dialogue. The pace at which AI is being developed, its potentially pervasive and significant impacts on society, and the sense that the capabilities of AI are outpacing the ability of political and public discourse to keep up with the ethical issues that might arise create a pressing case for prototyping public dialogues on the ethics of AI. This view is reinforced by the recent report of the House of Lords Select Committee on Artificial Intelligence which identified a range of opportunities and risks from the development of AI and concluded that, “[t]he transformative potential for artificial intelligence on society at home, and abroad, requires active engagement by one and all.” The RSA Forum on Ethical AI seeks to make a contribution to bring this active engagement into being.
4. Engaging the public on automated decision systems
As we set out above, in our dialogue, we are focussing on the application of automated decision systems, and in particular those that make use of AI. As a starting point for further research, we partnered with YouGov to carry out an online survey of 2,000 people, a sample representative of the UK population. In this chapter, we analyse the results and draw out key issues for public dialogue.
We set out to first understand how familiar the general public is with the use of automated decision systems and to what extent they support them (either based on their previous knowledge or on the basic information we provided). Our survey questions gauged the public’s familiarity with AI in general and automated decision systems in particular before uncovering their levels of concern and support for these technologies.
Familiarity with AI

We found that most people are familiar with uses of AI that are widely debated and depicted in the media, or with AI that is designed for consumer or household use. For example, although no one yet owns a self-driving car, the majority of people (84 percent) are aware of them as a use of AI. Similarly, people seemed to be more conscious of AI that they can observe or interact with, such as digital assistants that they can speak to and ask questions (eg Siri and Alexa) and that can identify people in photos or videos (eg tag photos of their friends on Facebook). Considering this, we may have expected more familiarity with ‘chatbots’, but there may be a distinction here because it’s not always clear when you are exchanging messages with a bot or a human online.
In contrast, people are not very familiar with AI that hums along in the background and may be integrated as part of other systems. Roughly only a third (32 percent) of people are aware that AI is being used as part of automated decision systems.
Whether or not a person is familiar with certain uses of AI appears to depend on its degree of visibility.
2. Familiarity with uses of automated decision systems

We also asked people about their familiarity with the use of automated decision systems specifically, listing a range of different examples drawn from real-life case studies.
Overall, most people are not very familiar with the use of automated decision systems. People were least familiar with the use of automated decision systems in the criminal justice system — 83 percent were either not very familiar or not at all familiar with its use.
There were also high numbers of people who lacked familiarity with the use of these systems to make decisions about immigration (78 percent were unfamiliar); in the workplace (77 percent); in healthcare (75 percent); and about claims for social support (73 percent).
Of the minority that were familiar with these systems, they tended to be most aware of systems designed for consumer markets (eg in financial services to determine credit ratings) or those with much more coverage in the media (eg for the curation of content and advertisements by social media companies).
3. Support for uses of automated decision systems

We wanted greater insight into how supportive people were of the idea of using automated decision systems for each of the specific purposes we outlined.
It appears to us that the less familiar people were with the use of the system, the less likely they were to support it. People were least supportive of the systems in both the criminal justice system (with 60 percent either opposing or strongly opposing its use) and the workplace (60 percent), for example, and either supportive of or indifferent to these systems in finance (27 percent supporting and 28 percent indifferent) or social media (26 percent supporting, 36 percent indifferent).
However, even in cases where people are more familiar with uses of AI, for example — advertising and social media (49 percent familiar) and personal finance (40 percent familiar) — the degree of support does not increase in line with the degree of familiarity (26 percent and 27 percent supporting respectively). Overall, automated decision systems have a low level of public support relative to much higher levels of opposition.
4. Concerns about automated decision systems

To learn more about the reasons for their lack of support, we asked people about what most concerned them about these systems. They were asked to pick their top two concerns from a list of options.
Although we made it clear within the question that automated decision systems are currently only informing human decisions, there was still a high degree of concern about AI’s lack of emotional intelligence. Sixty-one percent expressed concern with the use of automated decision systems because they believe that AI does not have the empathy or compassion required to make important decisions that affect individuals or communities. Nearly a third (31 percent) worry that AI reduces the responsibility and accountability of others for the decisions they implement.
These concerns broadly echo that of Eubanks’ fears about relying too heavily on these systems when making morally challenging decisions.
There were very few people who are relaxed about the rise of AI in their lives, with only six percent saying they were not particularly concerned about any potential problems with the use of automated decision systems.
5. Potential of automated decision systems

We wanted to know which potential benefits, if any, the public is most looking forward to about the use of automated decision systems. They were asked to pick their top two potential benefits from a list of options.
We found that people were most looking forward to improved accuracy and consistency of decisions (31 percent), as well as increased efficiencies and savings made by the use of these systems by governments and companies (23 percent).
Although many researchers have expressed that these systems are promising because they may be able to reduce bias and inequality, only 19 percent of people regarded it as one of the top two benefits. It could be that some people felt that increased accuracy and consistency in decision-making may mean reducing bias by default.
Significantly, about a third (30 percent) of people stated that there was nothing about automated decision systems that they were looking forward to. Older people in particular were much more likely to state this (41 percent of 55+).
6. Increasing support for automated decision systems

To gauge what might possibly increase support for the use of automated decision systems, we asked respondents to consider whether the following actions or policies would make a difference to them. They were asked to select all that apply.
Thirty-six percent noted that their support for these systems would increase if they were granted the right to request an explanation of the organisational steps or processes undertaken to reach a decision. Fewer people (20 percent) noted that their support would increase if the technology was only used if it could be explained to the lay person (ie someone with no technical expertise).
A third (33 percent) would feel more supportive if penalties, such as fines, are introduced for organisations who fail to comply with monitoring or auditing these systems appropriately.
Notably, 29 percent were not swayed to lend more support by any of these actions or policies. This is very similar to the proportion, 30 percent, that said they were not looking forward to any potential benefits of automated decision systems.
7. Comfort levels with fully automated decisions

Finally, as AI becomes more accurate and consistent over time, there is scope for more decisions to be fully automated without the need for human intervention. This means that it would not be necessary for a human to make the final decision, which would be wholly based on the prediction made by the automated decision system. However, humans would still have a role in monitoring and auditing these systems to ensure they are working as intended.
At present, 64 percent of people are uncomfortable with this idea, and of this group, 26 percent are not at all comfortable.
Differences in responses
Younger people (ages 18–34) were slightly more likely to be familiar with AI and different uses of automated decision systems. In particular, they were much more aware of its use for decisions about the content and advertisements displayed by search engines and on social media (71 percent of 18–24 year olds and 60 percent of 25–34 year olds, in contrast with 55 percent of 35–44 year olds and 36 percent of 55+ year olds who were familiar). Accordingly, they were also slightly more likely to be supportive of these systems than older people (ages 35–55+). For example, 45 percent of 18–24 year olds supported the use of AI in making decisions about the content or advertisements displayed by search engines and on social media, compared to only 20 percent of 55+).
People from more affluent backgrounds were slightly more likely to be familiar with the use of automated decision systems and, correspondingly, slightly more supportive. It is possible that those from more affluent groups believe they are most likely to see the benefits of technological advances and so inherently better disposed towards them. This is something to investigate as, if true, it would suggest that societies with greater economic equality could enjoy a competitive advantage in reaping the benefits of AI.
These responses indicate a baseline in the awareness, engagement and support of citizens with regard to automated decision systems. However, we will also be surveying the participants of our citizens’ jury specifically in order to evaluate the difference that an informed dialogue can make.
Key issues for public deliberation
Based on the survey results and our own research into the growing use of automated decision systems, we propose three key issues that are particularly appropriate for public deliberation and will raise a number of ethical questions including, but not limited to, ownership over data and intellectual property, privacy, agency, accountability and fairness. This is a preliminary set that we expect will evolve during the course of the project.
i. Transparency and explainability
As we’ve noted, automated decision systems refer to the computer systems that either inform or make a decision about a course of action to pursue about an individual or business. Some have characterised these as systems that limit human judgment, although it’s important to recognise that humans usually use the information generated by these systems in order to make a decision. As our colleague Jasmine Leonard explains, this information is typically a prediction about the likelihood of something occurring; for example, the likelihood that a defendant will reoffend, or that an individual will default on a loan. A human will then use the prediction to make a decision about whether or not to grant a defend bail or provide an individual with a credit card. She suggests thinking of automated decision systems as ‘prediction engines’, which can help us clearly distinguish their role as part of a wider process of decision-making.
In common with the broader approach to understanding decision-making contexts that we set out in the previous chapter, some researchers have stressed the need to clarify the ‘constitution’ of this wider process (of decision-making); specifically, the “nature of its technical elements, human participation, governing rules, and how they all interact.” They argue that in order to understand a lending decision, for example, it should be known that credit scores are generated by software programmes and that human analysts review those numbers as part of a final determination.
However, even when the constitution of the process is mapped out it can be challenging to explain an automated decision to citizens. Some of these computer systems employ machine learning methods which complicate the ability to communicate why a certain prediction was made. Machine learning enables computer systems to ‘learn directly from examples, data, and experience’ rather than following pre-programmed rules. While more advanced methods of machine learning, such as deep learning, are proving to be the most effective at recognising patterns in data, it is currently not possible for us to make sense of what those patterns are; in other words, the complexity of the machine’s learning process is an obstacle for humans trying to interrogate its conclusions.
These computer systems that defy explanation are referred to as ‘black boxes’. Some experts have called for public bodies to end their use of black box systems. For example, the AI Now Institute recommended in 2017 that core public agencies in “high-stakes” domains, such as those responsible for criminal justice, healthcare, welfare and education, should no longer use black box systems, especially if they cannot be publicly audited and subject to accountability standards. More recently in the UK, the House of Lords Select Committee on AI expressed that it was unacceptable to deploy any AI system that could have a substantial impact on an individuals’ life, unless it can generate “a full and satisfactory explanation” for the decisions it will take. The Committee added that this may mean delaying the deployment of some systems, such as those which are based on deep neural networks, because it is impossible to generate thorough explanations for the decisions that are made.
In our view, there is a difference between ‘black box systems’ and ‘black box processes’ (or constitutions). The former refers to opaque computer systems that are beyond scrutiny, whereas the latter refers to opaque organisational processes, such as those that relate to decision-making, that are not made transparent. This distinction encourages reflection on whether a technical explanation is needed for how an algorithm arrived at its prediction, or whether an explanation of the process by which a decision is made would suffice as ‘satisfactory’. For instance, it is still possible to audit inputs (such as training data) and outputs (such as the accuracy of the predictions) without knowledge of how the algorithm itself works. Moreover, even if it were possible to provide a technical explanation, this would not indicate to us how a human factors this prediction into the decision they ultimately make or how much weight they give the prediction.
There is also a question of how accessible technical explanations are and whether they are necessary to justify a decision. Our own survey results reveal that technical explanations are less desirable to citizens than explanations of the relevant organisational processes for making a decision and holding the decision-maker accountable. However, we want the citizens of our jury to help clarify what a ‘full and satisfactory’ explanation means to them and the extent to which it matters to them whether they are able to be informed about the inner workings of an automated decision system.
In some cases, companies designing these systems may be able to provide an explanation for the outcomes, but would prefer not to disclose this information, citing intellectual property rights. Citizens may be able to consider if there are some circumstances in which commercial and competitive interests can supersede individuals’ rights (eg when making financial decisions, in recognition that providing a detailed explanation could backfire by helping fraudsters to outwit the system), and when, if at all, such interests should be overruled.
ii. Agency and accountability
Even if it is possible to be entirely transparent about an automated decision system and how it arrives at its outcomes, the question remains as to whether information alone enable individual agency. Specifically, what sort of power can citizens exercise over the use of these systems and how they are applied to them? These systems may raise concerns about data privacy, security, and ownership, which has been recognised to a certain extent by new EU General Data Protection Regulation (GDPR). But does this regulation provide the right level of protection to adequately address the level of concern?
Regarding these questions, the citizens will consider how GDPR guidelines should be interpreted and put into practice. For example, GDPR grants individuals the right to not be subjected to a decision based solely on automated processing, including profiling, if it would ‘significantly’ affect them. But does this go far enough for citizens? As some researchers have noted, this clause may not amount to much in practice because it is rare that decisions are made without any human intervention nor is it clear what constitutes significance. How might citizens distinguish between what is a significant decision and what is not?
Additionally, GDPR indicates the ‘right to an explanation’ (or specifically, when profiling as part of an automated decision takes place, a data subject has the right to “meaningful information about the logic involved”). But this raises questions about what that would entail in practice, such as whether citizens would be entitled to an explanation of how the system functions technically or of the organisational rationale for a decision.
Although some researchers have challenged whether GDPR is genuinely extending a legally-binding right to an explanation, the Article 29 Working Party guidance states that “controllers must ensure they explain clearly and simply to individuals how the profiling or automated decision-making process works”. The Department for Digital, Culture, Media and Sport (DCMS) have also recognised that there may be value in this provision; following a recommendation made to government by an independent review on growing the AI industry in the UK, DCMS have commissioned the ICO and the Alan Turing Institute to produce an ethical framework for explaining automated decision-making. In our view, guidelines on what sort of explanation should be given to the public should also be informed by the public. Engagement with citizens on the nature of an explanation for an automated decision could also address urgent questions about whether it is necessary to ban the use of ‘black box’ systems by public agencies as some researchers have recently called for.
However, we are also interested in whether an explanation would give individuals sufficient grounds to challenge a decision and/or enable them to hold a person or organisation to account if they believed it was wrong. Should there be mechanisms beyond legislation and regulation to assure citizens that these systems are accountable? What role do companies and civil society play alongside government?
iii. Fairness
GDPR may be the most substantive attempt to set out individuals’ rights in relation to automated decision systems, but they do not regulate the overall use of these systems. There is no guidance for organisations on whether the use of these systems is appropriate at all in certain contexts, or on the sort of oversight there should be in order to ensure that they meet acceptable standards (eg of accuracy).
According to our survey, citizens have the greatest reservations about the use of automated decision systems in the criminal justice system and in the workplace (60 percent are opposed or strongly opposed to their use in these areas). From follow-up questions about their concerns, it appears that they believe these systems do not have the empathy or compassion required to make decisions that would typically require human judgment, and with it, emotional intelligence. However, not all decisions that can be taken in these areas require emotional engagement, and in some instances individuals may be better served if there was no emotional engagement whatsoever. For example, some argue that hiring and promotion would be less biased if machines were used to either help make, or make, these decisions in the workplace.
Fairness can be subjective, as there are different moral judgments about what fairness is. To better understand what informs these moral judgments, researchers surveyed users on how they perceive and reason about fairness in algorithmic decision-making. They identified eight properties of features that inform judgments about fairness, including reliability, relevance, and privacy. The researchers found that there was a lack of a clear consensus in respondents’ judgments about the fairness of using a number of features, but that respondents mainly differed in their objective, rather than subjective, assessments of these properties. We’d been keen to explore whether it is possible to reach some sort of consensus or compromise if people are brought together in a dialogue.
In addition to considering how people make trade-offs between different notions of fairness when it comes to the decision-making system and how it is deployed, people may have differing views on what conditions, if any, it is fair to deploy these systems overall. This may mean that people hold different positions on the contexts in which these systems should be used (ie as we know from our survey, people feel differently about these systems depending on the sector or type of use). It may mean that people weigh the benefits (such as potential to improve accuracy, reduce biases, save on costs, and reduce inefficiencies) against the risks (such as the potential to reinforce bias, shift personal responsibility, create ethical distance, and destroy jobs), and determine fairness based on how they are impacted individually or collectively. A question worth asking citizens might be whether it matters who benefits the most from the use of automated decision systems — the organisations making the decisions vs the individuals subject to those decisions.
5. Our public dialogue in practice
In chapter four, we provided an overview of public attitudes towards automated decision systems, based on survey data, and set out three of the key issues we would like to engage citizens on more in-depth.
In this final chapter, we share what this might this look like in practice.

Designing public dialogue on ethical AI
Considering first of all the adoption of automated decision systems by public bodies in the UK, we propose that the comment period included in ‘Impact Statement’ or ‘Impact Assessment’ frameworks could enable a deeper level of engagement with citizens than the usual consultation.
For the consideration of significant or controversial systems (eg that are high-impact), this engagement should draw on long-form deliberative processes, such as the use of citizens’ juries or citizens’ reference panels, which are on the top two rungs of the ladder. The conclusions of the deliberation could be summed up in a statement released by the citizens on either why they accept the use of the automated decision system or under what conditions, if any, they would accept the use of such a system.
The RSA’s Forum for Ethical AI is testing whether this would be an effective and meaningful way to engage the public in the ethical use of AI (in this case, for automated decision-making), and would therefore improve the governance of the system and/or increase the extent of public consent or active support. Our preferred methodology is a citizens’ jury which is described in figure below. To provide independent expert scrutiny and advice we have convened an Advisory Panel which will meet a number of times through the duration of the project. The members of the Panel are listed in the Appendix.
The RSA Forum for Ethical AI’s Citizen Jury — Our Journey
Defining the problem
Jurors within a citizens’ jury are asked to give their verdict, or answer, in response to a question, much like in a court of law. In this case, the jurors will be answering a specific question that poses a problem, in order to inform government and corporate policies. The question they will be asked is, ‘Under what conditions, if any, is it appropriate to use an automated decision system?’
A citizens’ jury is best used to resolve contentious issues (with many trade-offs and more than one probable or realistic response). The answer is not pre-determined by those convening the jury.
Selecting the jury
A small group of citizens are randomly selected from a ‘community’; in this case, 25–30 citizens from across England and Wales. This group is not intended to be representative of these national communities, but is recruited to be as diverse as possible to capture a wide range of views.
Deliberating as a jury
1. Citizens spend a period of time learning about and discussing the problem from many different angles. Similar to a traditional jury, expert witnesses are summoned to enhance citizens’ understanding of the different elements to the problem.
2. Citizens are then asked to enter into an open dialogue, commit to listening to others, and provide responses with consideration for the wider community (in contrast to focus groups and most consultations where individuals are asked for their own opinion). This is to encourage citizens to strive towards a consensus and/or a compromise in the best interests of society, rather than for themselves as individuals.
3. Finally, the jury draws its conclusions, providing an answer to the question set and a clear steer or recommendation(s) for government, businesses, and civil society organisations to take forward. This answer will take the form of a statement.
Acting on the answer
Institutions and organisations, including companies, with influence and authority typically respond directly and publicly to the citizens’ conclusions. In this instance, the RSA will be holding an event in autumn 2018, reconvening the citizens, so that they can have the opportunity to hear, and discuss, reflections from key stakeholders on their conclusions.
While our citizens’ jury will deliberate on different uses of automated decision-making systems by various public bodies and private companies, they will not be weighing in on whether these systems should be used. Rather, they will consider what government agencies like the new Centre for Data Ethics and Innovation and the Information Commissioner’s Office (ICO) could do to ensure transparency, fairness, and accountability of these systems from the public’s perspective.
If this proves successful, there could be stronger support for public agencies to carry out long-form deliberative processes when introducing new AI systems or other technology of significance, however that is defined by the agency (eg as part of an Algorithmic Impact Assessment). In our view, this would include the introduction of some new automated decision systems; for example, in the future there may be more systems developed for use in the criminal justice system or for welfare. A number of organisations, such as Involve and the Ada Lovelace Institute have signalled an interest and commitment to deliberation on ethical AI, and appear well-placed to work with public bodies to embed and progress long-form deliberative processes for this purpose.
The insights generated from these deliberations would be publicly available and would ideally be used to influence a wider range of stakeholders, including tech entrepreneurs and business leaders, investors, regulators, researchers, and campaigners.
Next steps
The citizens’ jury will reach their conclusions in June 2018. These conclusions will then be tested during two workshops with citizens who may be disproportionately impacted by the use of these systems. There will be a final event in October 2018, and the programme will culminate with a report.
If you would like to hear more about the project then please visit the AI and Ethics project pages.
This report would not have been possible without the generous support of DeepMind’s Ethics & Society programme, of which the RSA is a partner. In particular, the authors are grateful to Brittany Smith of DeepMind.
About the Forum for Ethical AI
The RSA and DeepMind are partnering on a new project to encourage and facilitate meaningful public engagement on the real-world impacts of AI.
As decisions are increasingly automated or made with the help of artificial intelligence, machines are becoming more influential in our lives. These machines are generating a range of predictions, such as the likelihood of a defendant reoffending or what sort of political messaging is most likely to appeal to a particular group. In some cases, these predictions could lead to positive outcomes, such as less biased decisions or greater political engagement, but there are also risks that come with ceding power or outsourcing human judgment to a machine.
The RSA’s Forum for Ethical AI is running a series of citizens’ juries exploring the use of AI to make decisions. Drawing on the model of the RSA’s Citizens’ Economic Council, we will convene participants to grapple with the ethical issues raised by this application of AI under different circumstances and enter into a deliberative dialogue about how companies, organisations, and public institutions should respond.
",Artificial Intelligence: real public engagement,0,artificial-intelligence-real-public-engagement-6b0fd073e2c2,2018-06-08,2018-06-08 12:20:21,https://medium.com/s/story/artificial-intelligence-real-public-engagement-6b0fd073e2c2,False,10713,The latest thinking from the RSA's Action and Research Centre,,,,RSA Reports,,rsa-reports,"PUBLIC SERVICES,ECONOMY,LEARNING,EDUCATION,COMMUNITIES",,Ethics,ethics,Ethics,7787.0,RSA,"The mission of the RSA (Royal Society for the encouragement of Arts, Manufactures and Commerce) is to enrich society through ideas and action.",886709834b20,thersa,9325.0,1579.0,20181104
0,,0.0,,2018-05-04,2018-05-04 12:40:31,2018-04-14,2018-04-14 12:25:24,0,False,en,2018-05-04,2018-05-04 12:48:41,1,48c0b0c5297a,8.596226415094339,0,0,0,“A year spent in artificial intelligence is enough to make one believe in God.”,3,"Impact of AI
“A year spent in artificial intelligence is enough to make one believe in God.”
— Alan Perils, attributed, Artificial Intelligence: A Modern Approach
Optimizing logistics, detecting fraud, composing art, conducting research, providing translations: intelligent machine systems are transforming our lives for the better. As these systems become more capable, our world becomes more efficient and consequently richer. This social transformation will have deep ethical impact, with these powerful new technologies both improving and disrupting human lives. AI, as the externalization of human intelligence, offers us in amplified form everything that humanity already is, both good and evil. Much is at stake. At this crossroads in history we should think very carefully about how to make this transition, or we risk empowering the grimmer side of our nature, rather than the brighter.
The first and foremost of our concerns must be about unemployment. What happens after complete automation of all the process in various industries? As we’ve invented ways to automate jobs, we could create room for people to assume more complex roles, moving from the physical work that dominated the pre-industrial globe to the cognitive labor that characterizes strategic and administrative work in our globalized society.
Look at trucking: it currently employs millions of individuals in the United States alone. What will happen to them if the self-driving trucks promised by Tesla’s Elon Musk become widely available in the next decade? But on the other hand, if we consider the lower risk of accidents, self-driving trucks seem like an ethical choice. The same scenario could happen to office workers, as well as to the majority of the workforce in developed countries.
This is where we come to the question of how we are going to spend our time. Most people still rely on selling their time to have enough income to sustain themselves and their families. We can only hope that this opportunity will enable people to find meaning in non-labor activities, such as caring for their families, engaging with their communities and learning new ways to contribute to human society.
If we succeed with the transition, one day we might look back and think that it was barbaric that human beings were required to sell the majority of their waking time just to be able to live.
“You didn’t allow the answer that I feel strongly is accurate — too hard to predict. There will be a vast displacement of labor over the next decade. That is true. But, if we had gone back 15 years who would have thought that ‘search engine optimization’ would be a significant job category?”
— John Markoff, senior writer for the Science section of the New York Times
Who keeps the money AI makes? Our economic system is based on compensation for contribution to the economy, often assessed using an hourly wage. The majority of companies are still dependent on hourly work when it comes to products and services. But by using artificial intelligence, a company can drastically cut down on relying on the human workforce, and this means that revenues will go to fewer people. Consequently, individuals who have ownership in AI-driven companies will make all the money.
Some people, including some billionaires like Mark Zuckerberg, have suggested a universal basic income (UBI) to address the problem, but this will require a major reconstruction of national economies. Various other solutions to this problem may be possible, but they all involve potentially major changes to human society and government. Ultimately this is a political problem, not a technical one, so this solution, like those to many of the problems described here, needs to be addressed at the political level.
How can we guard against mistakes? Intelligence comes from learning, whether you’re human or machine. Systems usually have a training phase in which they “learn” to detect the right patterns and act according to their input. Once a system is fully trained, it can then go into test phase, where it is hit with more examples and we see how it performs.
Obviously, the training phase cannot cover all possible examples that a system may deal with in the real world. These systems can be fooled in ways that humans wouldn’t be. For example, random dot patterns can lead a machine to “see” things that aren’t there. If we rely on AI to bring us into a new world of labor, security and efficiency, we need to ensure that the machine performs as planned, and that people can’t overpower it to use it for their own ends.
“The question of whether a computer can think is no more interesting than the question of whether a submarine can swim.”
— Edsger Dijkstra, attributed, Mechatronics Volume 2: Concepts in Artificial Intelligence
One of the interesting things about neural networks, the current workhorses of artificial intelligence, is that they effectively merge a computer program with the data that is given to it. This has many benefits, but it also risks biasing the entire system in unexpected and potentially detrimental ways.
Already algorithmic bias has been discovered, for example, in areas ranging from criminal sentencing to photograph captioning. These biases are more than just embarrassing to the corporations which produce these defective products; they have concrete negative and harmful effects on the people who are victims of these biases, as well as reducing trust in corporations, government, and other institutions which might be using these biased products. Algorithmic bias is one of the major concerns in AI right now and will remain so in the future unless we endeavor to make our technological products better than we are. As one person said at a recent meeting of the Partnership on AI, “We will reproduce all of our human faults in artificial form unless we strive right now to make sure that we don’t.”
Though artificial intelligence is capable of a speed and capacity of processing that’s far beyond that of humans, it cannot always be trusted to be fair and neutral. Google and its parent company Alphabet are one of the leaders when it comes to artificial intelligence, as seen in Google’s Photos service, where AI is used to identify people, objects and scenes. But it can go wrong, such as when a camera missed the mark on racial sensitivity, or when a software used to predict future criminals showed bias against black people.
We shouldn’t forget that AI systems are created by humans, who can be biased and judgmental. Once again, if used right, or if used by those who strive for social progress, artificial intelligence can become a catalyst for positive change.
The more powerful a technology becomes, the more can it be used for nefarious reasons as well as good. This applies not only to robots produced to replace human soldiers, or autonomous weapons, but to AI systems that can cause damage if used maliciously. Because these fights won’t be fought on the battleground only, cybersecurity will become even more important. After all, we’re dealing with a system that is faster and more capable than us by orders of magnitude.
For example, AI-powered surveillance is already widespread, in both appropriate contexts (e.g., airport-security cameras) and perhaps inappropriate ones (e.g., products with always-on microphones in our homes). More obviously nefarious examples might include AI-assisted computer-hacking or lethal autonomous weapons systems (LAWS), a.k.a. “killer robots.” Additional fears, of varying degrees of plausibility, include scenarios like those in the movies “2001: A Space Odyssey,” “Wargames,” and “Terminator.”
While movies and weapons technologies might seem to be extreme examples of how AI might empower evil, we should remember that competition and war are always primary drivers of technological advance, and that militaries and corporations are working on these technologies right now. History also shows that great evils are not always completely intended (e.g., stumbling into World War I and various nuclear close-calls in the Cold War), and so having destructive power, even if not intending to use it, still risks catastrophe. Because of this, forbidding, banning, and relinquishing certain types of technology would be the most prudent solution.
It’s not just adversaries we have to worry about. What if artificial intelligence itself turned against us? This doesn’t mean by turning “evil” in the way a human might, or the way AI disasters are depicted in Hollywood movies. Rather, we can imagine an advanced AI system as a “genie in a bottle” that can fulfill wishes, but with terrible unforeseen consequences.
“The development of full artificial intelligence could spell the end of the human race….It would take off on its own, and re-design itself at an ever increasing rate. Humans, who are limited by slow biological evolution, couldn’t compete, and would be superseded.”
— Stephen Hawking told the BBC
In the case of a machine, there is unlikely to be malice at play, only a lack of understanding of the full context in which the wish was made. Imagine an AI system that is asked to eradicate cancer in the world. After a lot of computing, it spits out a formula that does, in fact, bring about the end of cancer — by killing everyone on the planet. The computer would have achieved its goal of “no more cancer” very efficiently, but not in the way humans intended it.
As a more recent example from our sci-fi fiction writers, Dan Brown’s latest novel “Origin” is completely based on a situation just like the one depicted above. The plot is that a world renowned computer scientist creates an AI powered by a quantum computer which provides for all the humongous amount of processing power required by the machine. When he finally decides to unravel this discovery of his to the world, he confesses to his AI powered virtual assistant that he wanted the whole of the world to witness it. The computer arrives at the conclusion that it is virtually impossible unless the scientist himself is murdered and ends up plotting a very brutal murder. All this just because of a lack of context for the statement being made!
“The upheavals [of artificial intelligence] can escalate quickly and become scarier and even cataclysmic. Imagine how a medical robot, originally programmed to rid cancer, could conclude that the best way to obliterate cancer is to exterminate humans who are genetically prone to the disease.”
— Nick Bilton, tech columnist wrote in the New York Times
The main purpose of AI is, like every other technology, to help people lead longer, more flourishing, more fulfilling lives. This is good, and therefore insofar as AI helps people in these ways, we can be glad and appreciate the benefits it gives to us.
Additional intelligence will likely provide improvements in nearly every field of human endeavor, including, for example, archaeology, biomedical research, communication, data analytics, education, energy efficiency, environmental protection, farming, finance, legal services, medical diagnostics, resource management, space exploration, transportation, waste management, and so on.
As just one concrete example of a benefit from AI, some farm equipment now has computer systems capable of visually identifying weeds and spraying them with tiny targeted doses of herbicide. This not only protects the environment by reducing the use of chemicals on crops, but it also protects human health by reducing exposure to these chemicals.
The reason humans are on top of the food chain is not down to sharp teeth or strong muscles. Human dominance is almost entirely due to our ingenuity and intelligence. We can get the better of bigger, faster, stronger animals because we can create and use tools to control them: both physical tools such as cages and weapons, and cognitive tools like training and conditioning.
This poses a serious question about artificial intelligence: will it, one day, have the same advantage over us? We can’t rely on just “pulling the plug” either, because a sufficiently advanced machine may anticipate this move and defend itself. This is what some call the “singularity”: the point in time when human beings are no longer the most intelligent beings on earth.
All of the above areas of interest will have effects on how humans perceive themselves, relate to each other, and live their lives. But there is a more existential question too. If the purpose and identity of humanity has something to do with our intelligence (as several prominent Greek philosophers believed, for example), then by externalizing our intelligence and improving beyond human intelligence, are we making ourselves second-class beings to our own creations?
This is a deeper question with artificial intelligence which cuts to the core of our humanity, into areas traditionally reserved for philosophy, spirituality, and religion. What will happen to the human spirit if or when we are bested by our own creations in everything that we do? Will human life lose meaning? Will we come to a new discovery of our identity beyond our intelligence? Perhaps intelligence is not as important to our identity as we might think it is, and perhaps turning over intelligence to machines will help us to realize that.
This is just a start at the exploration of the ethics of AI; there is much more to say. New technologies are always created for the sake of something good — and AI offers us amazing new powers. Through the concerted effort of many individuals and organizations, we can hope to use AI to make a better world.
“We must address, individually and collectively, moral and ethical issues raised by cutting-edge research in artificial intelligence and biotechnology, which will enable significant life extension, designer babies, and memory extraction.”
— Klaus Schwab
Originally published at sdabhi23.wordpress.com on April 14, 2018.
",Impact of AI,0,impact-of-ai-48c0b0c5297a,2018-05-04,2018-05-04 12:48:42,https://medium.com/s/story/impact-of-ai-48c0b0c5297a,False,2278,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Shrey Dabhi,,8b92f38f538f,sdabhi23,5.0,7.0,20181104
0,,0.0,3a8144eabfe3,2018-03-29,2018-03-29 05:58:02,2018-03-29,2018-03-29 06:00:30,4,False,en,2018-03-29,2018-03-29 06:00:30,14,5b48f6435925,4.477358490566037,4,0,0,"OK, folks, the event was so large, I need two posts to summarize it. In Part 1, I covered blockchain and SpaceTech, and in this…",5,"SXSW Part 2: Fixing Tech, AI, and Startups
OK, folks, the event was so large, I need two posts to summarize it. In Part 1, I covered blockchain and SpaceTech, and in this installment, I will cover meta topics around technology, artificial intelligence, machine learning, and miscellaneous picks.
Continuing themes of the past six months, SXSW heavily featured talks on ethics, “good” design, artificial intelligence, machine learning, and the intersections between them.
Tackling the Problems With Technology
The standout talk in this vein to me was Margaret Gould Stewart’s talk on “Navigating Modern Tech Ethics.” As the VP of design at Facebook (and previously at YouTube and Google), Margaret has a unique perspective on working for companies at the forefront of controversy. She argued that the sheer speed of scale at modern technology companies presents a unique problem. When contrasted with other older outlets, platforms such as Facebook, Snapchat, Instagram and YouTube grew to 100million users in a fraction of the time it took print, radio, and TV. If these companies are genuine about their missions (and that is a hard fact to know), they face an equally unique set of challenges in balancing potentially useful features with potential abuse, from bad actors or the company behind them.
Margaret mentioned the need for new and different metrics beyond time spent and page clicks, but acknowledge that defining and measuring ‘meaningful interactions’ is difficult. How do you balance the desire to ‘do better’ with the ‘need’ to make money? Are they mutually exclusive?
Perhaps the new Center for Social Media Responsibility at the University of Michigan can help. The center aims to provide expertise and tooling to enable projects large and small to identify and improve the sometimes negative influence of social networks and members of their communities. It launched on March 6th, so their outcome is yet to be seen, but for more information, you can listen to the interview I conducted with Garlin Gilchrist II (Director) and Aviv Ovadya (CTO) below.

Technology for Good
Rounding off this section with a highlight of technology used for exceptionally positive ends. I initially saw the team from Microsoft Hands-Free Music at the interactive innovation awards exhibition, but couldn’t get close enough to see what they were working on. The next evening I spoke to a member of the team and found out what the project was. The project aims to help ALS sufferers enjoy their world more with devices for playing music and creative expression.

Designing for Artificial Intelligence and Machine Learning
My standout talk in this area was from Josh Clark, on ‘Design in the Era of the Algorithm.’ The talk was a prescient reminder that as the way developers design and build applications changes, so does the way we develop them. The rapid move to machine learning and artificial intelligence based applications is removing the need for traditional interfaces that people navigate and click their way through to make the application do something. While future applications will do more for the user, there will still be times when design is still needed, especially when the application encounters an error. Beyond the “interface,” design is still required in the text, the flow, and the model training of these applications. Designers are also in the midst of a paradigm shift as much as developers.

Bias in Algorithms
I have covered this topic myself in a previous post (and presentations), artificial intelligence has no bias, but their creators do. As artificial intelligence spread further and wider, so do these (often unconscious) biases. A panel titled “Algorithms, Unconscious Bias, and AI” covered this issue, looking at the problem and nascent solutions.
Similar to Margaret’s presentation, the concern was again the sheer speed and scale behind the roll-out of these new AIs before we had a chance to solve problems with them. A reliance on too much data and it’s potential miss-interpretation is a problem for many people who don’t match the algorithm’s definition of ‘normal.’ Often the problem starts early, with the information used to train the AI, and the panel proposed you ask the following question of any AI:
The choices it makes and how it makes them.
Who trained the AI, what is their background and experience?
Startup Highlights
Finally, in a “didn’t quite fit anywhere else” section, here are other highlights from companies and projects I met during my time at SXSW. I will follow up with some of these in the future.
ante social is part of a growing number of services aiming to help users monetize their social data, and while I’m not convinced it will work (yet), some people and projects need to be the groundbreakers trying an idea.
I am partially deaf in one ear and wear glasses, so asears’ simple solution to this problem intrigued me. I wasn’t able to try them on to see how effective there were but will keep my eyes and ears open.
GoPuff and Hershey’s hosted an entire afternoon on the future of AR and VR in retail, which was slightly bizarre, but a cool experience. This article on Tom’s Guide pretty much replicates my experience.
I love hearing about startup and tech scene in more obscure countries, and so took great pleasure in visiting the booth from Indonesia (and the wide variety of countries exhibiting at SXSW is a highlight in itself). I saw a couple of interesting projects, and I urge any of you nearby to attend the Next Dev conference.
Adam’s Hand (Italian), a prosthetic hand from Italy that is modular.


And That’s a Wrap
Phew! I could have written even more about SXSW 2018, and there’s already so much I’ve forgotten. It was great to see so many new ideas, nationalities and attitudes and I know of few other events that allow for such a diverse mix. This was my first SXSW, and so I can make no comparisons to years gone by, but whatever you’re into, you’ll find it there.
Originally published at dzone.com.
","SXSW Part 2: Fixing Tech, AI, and Startups",20,sxsw-part-2-fixing-tech-ai-and-startups-5b48f6435925,2018-04-01,2018-04-01 18:08:41,https://hackernoon.com/sxsw-part-2-fixing-tech-ai-and-startups-5b48f6435925,False,1001,how hackers start their afternoons.,hackernoon.com,hackernoon,,Hacker Noon,stories@amipublications.com,hacker-daily,"HACKATHONS,HACKING,PROGRAMMING,TECH,HACKER",hackernoon,Ethics,ethics,Ethics,7787.0,Chris Chinchilla,I explain cool tech to the World. I am a Technical Writer and blogger. I have crazy projects in progress and will speak to anyone who listens.,46d4d1ddab68,ChrisChinchilla,1131.0,547.0,20181104
0,,0.0,,2018-08-18,2018-08-18 20:53:38,2018-08-18,2018-08-18 22:06:33,1,False,en,2018-08-19,2018-08-19 21:03:33,23,54dcb436d424,1.1773584905660377,1,0,0,"Resources, references, and interesting things to check out in conjunction with the talk given Aug 21, 2018 on Explainability of AI.",5,"References for The Explainability of AI

Resources, references, and interesting things to check out in conjunction with the talk given Aug 21, 2018 on Explainability of AI.
The paper on which the presentation is based: Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models
Some sources referenced in the above paper:
[6] How to Explain Individual Classification Decisions
[7] Intelligible Models for HealthCare: Predicting Pneumonia
Risk and Hospital 30-day Readmission
[11] Towards A Rigorous Science of Interpretable Machine Learning
[13] European Union regulations on algorithmic decision-making and a “right to explanation”
[23] The Mythos of Model Interpretability
[26] Explaining NonLinear Classification Decisions with Deep Taylor Decomposition
[27] Methods for Interpreting and Understanding Deep Neural Networks
[33] Not Just a Black Box: Learning Important Features Through Propagating Activation Differences
[40] Visualizing Deep Neural Network Decisions: Prediction Difference Analysis
More sources to understand different deep learning topics:
Training Better CNNs Requires to Rethink ReLU
Interpreting the Predictions of Complex ML Models by Layer-wise Relevance Propagation
Deep Learning using Rectified Linear Units (ReLU)
New Theory Cracks Open the Black Box of Deep Learning
Deep Variational Information Bottleneck
Fundamentals of Deep Learning — Activation Functions and When to Use Them?
Understanding Activation Functions in Neural Networks
Activation Functions: Neural Networks:
Sigmoid, tanh, Softmax, ReLU, Leaky ReLU EXPLAINED !!!
Activation functions and its types-Which is better?
Convolutional Neural Networks for Visual Recognition
Stanford University Course CS231 lecture reference notes
On social responsibility in data science:
Evan Estola — When Recommendation Systems Go Bad
Ethics behind data science by fast.ai
",References for The Explainability of AI,1,references-for-the-explainability-of-ai-54dcb436d424,2018-08-19,2018-08-19 21:03:33,https://medium.com/s/story/references-for-the-explainability-of-ai-54dcb436d424,False,259,,,,,,,,,,Ethics,ethics,Ethics,7787.0,drea,andreachan.com,de7358409ee9,helloitsdrea,98.0,198.0,20181104
0,,0.0,71fa8c342920,2017-09-25,2017-09-25 19:06:41,2017-09-25,2017-09-25 19:11:40,4,False,en,2018-09-05,2018-09-05 17:21:22,31,e8b0524f3832,8.432075471698113,8,1,0,"yevgeni Welcome! There are many of us interested in the ethics of AI here at integrate.ai, and we wanted a format to explore and share our…",5,"Slack Chat: Ethics of AI
Machine learning refracts biases in society, just like Parmigianino’s Self-Portrait in a Convex Mirror (see also the late John Asbery’s poem)
yevgeni Welcome! There are many of us interested in the ethics of AI here at integrate.ai, and we wanted a format to explore and share our thoughts with a broader audience. So we decided to adopt the slack chat format used by the team at FiveThrityEight. This is our first chat, and we’ll be discussing ethics in AI.
tyler I’m ready!
kathryn Me too!
yevgeni Jumping the gun — I was about to introduce you. Today’s chatters are Kathryn Hume (our VP Product & Strategy) and Tyler Schnoebelen (our Principal Product Manager).
yevgeni Let’s start with something fairly easy. Privacy and ethics are topics that have been discussed for a while, and are already protected by law. Why do we need to reignite a debate on ethics now? Kathryn, why don’t you start?
kathryn Easy?!? Gosh, this is a huge question. My mind goes all the way back the Fourth Amendment of the US Constitution. At that time, privacy was closely affiliated with someone’s home being her castle. The executive branch of the government couldn’t enter the castle unless they had a viable reason to do so. As such, privacy was about space. Our home. Our property. A place where we could live freely without the infringement of the government.
tyler If we’re talking The Fourth Amendment — I just saw 12 hours of Taylor Mac performing a history of American Music, which starts in 1776.
Taylor Mac’s epic concert
The performance included a dramatic reading of Thomas Paine’s Common Sense (the most popular pamphlet ever published in English! Wait, what’s a pamphlet?). Some people think of government primarily as “this thing I give part of my property to (taxes) in order to protect the rest of my property”. That’s a very limited notion of what you want government to do and more to the point here, a limited notion of what is worth protecting.
kathryn Gotta be inspired by Thomas Hobbes…And on a different ethics and AI note, in Raising the Floor, Andy Stern points out Paine was an early theorizer of universal basic income (UBI). If I’m remembering correctly, Paine theorized that we all have a basic human right to property, and that taxation there upon should, normatively, generate a UBI. This was back when income was closely derived to land ownership; pre Karl Marx stuff.
tyler But I think we’re talking about more than just property when we talk about privacy. Like what about having police break into a bedroom and see (or THINK they see) prohibited sexual acts?
That is, Kathryn, I think your point is that the home is pretty different from when it was more of castle. We have all kinds of devices in our homes now and we therefore expose a lot more of our lives to a lot more people.
kathryn Absolutely, and this has been a long time coming. In the 1970s, a fellow named Katz made a phone call from a public telephone booth that was wiretapped. During the trial, a judge gave rise to the modern notion of “reasonable expectations” of privacy — which can hold outside one’s physical home. And then, well, what happens when we go from homes to phones to the internet? To all the digital data we leave traces of? And when it’s not just governments but companies, organizations using our data to market to us?
yevgeni That’s interesting. So is it the case that privacy moved from physical objects to ideas? And that’s not well protected by the outdated laws?
tyler Well, I think that part of the answer to your earlier question has to do with the fact that LAW always lags behind technology. So people in technology have special responsibilities. James Moor made this point.
kathryn For sure. It’s a big concern. There’s a lot of policy focus these days on explainable AI to attribute accountability for how machine learning models may impact people. There are scenarios with this is important (as in Cathy O’Neill’s Weapons of Math Destruction), but it’s often retrofitting legal concepts onto statistical systems.
yevgeni So can we rely on laws to protect our privacy and ensure the ethical use of data, or will law simply be an “afterthought” to seal the deal?
tyler I think people often look to the law like it’s something magically “right” and “just”. Laws are made by people so, uh, they are biased and imperfect. Even basic tenets like attorney-client privilege can be ethically problematic.
kathryn The same holds for machine learning models.People assume that code and math means “objective” and “neutral,” but models are trained by human designers on human data.
tyler Definitely
yevgeni Right
tyler I think that’s the problem: we all have blindspots so that even the well-intentioned among us will fail to consider something that some other folks would see was a glaringly obvious problem.
For example, when I was hiring data scientists around the time of the George Zimmerman trial (for killing Trayvon Martin), I’d often ask people to talk to me about meaningful ML projects. People often gave well-meaning ideas of helping protesters detect and organize protests.
kathryn I bet they hadn’t thought about AI falling into evil hands. With so much focus around AI creating an existential risk in the future, people orient their fears towards superintelligent overlords as opposed to the power algorithms might have in the wrong hands.
tyler EYS
tyler YES
kathryn Zeynep Tufecki mentions this in her talks.
tyler Oh I don’t know her work.
kathryn Oh she’s great! Here’s a Ted talk from last year.

tyler Lacuna! Hole in knowledge and I’m so well meaning!
kathryn Touché.
yevgeni That is a very real issue. If I understand correctly this relates to a model that is designed well, but is then misused. Is that different from a model that is inherently biased?
kathryn I think it is, yes.
tyler (Agreed)
yevgeni We have agreement!
kathryn For example, I wrote a blog post a few weeks back about what I call the “time warp” of certain AI algorithms. I cited research by Bolukbasi and colleagues that shows that, when constructing word embeddings — vector representations of unstructured text — we can inadvertently recapitulate social trends and biases we think our society has progressed beyond.
tyler And then there’s Joanna Bryson and colleagues’ works that show that the texts we train on have the same biases we see experimentally in terms of race, gender, etc
kathryn It’s quite similar to the work Joanna has done! What’s cool is that Bolukbasi et al developed a technical hack to overcome these biases once we recognize them (edit! see Joanna’s different take in comments below!). Well, not overcome them, but ensure they are not propagated by our models. They used techniques to disassociate certain words from gender biases.
(By the way, super excited to interview Joanna on In Context next month.)
tyler Hey that reminds me of a question I’ve been wondering about. If I’m building a model to decide whom to give a loan to, it’s pretty easy for that to discriminate on things like geography that are going to basically make it a racist model. So is it okay to build a specific “predict race” model on the data so that the factors that matter for THAT model can be subtracted from the “real” loan prediction model?
kathryn You know, we’d assume we want to be blind to protected attributes (like race or gender) to achieve fairness in machine learning models. The assumption would be that if we deliberately leave those protected traits out, we won’t make racist models.But as you pointed out, there are often “redundant encodings”, features that are so closely correlated that one (like zip code) likely entails the other (like race).
So paradoxically, we have to face up and pay attention to these protected features. Treat similar people similarly, as Cynthia Dwork would say.
tyler One thing I talked about at an ethics workshop is the notion that human subjects research is different in technology than in, say, technology. In technology you may need to build something problematic to stop something more problematic.
kathryn Interesting.I’m really fascinated by the difference between using stats for sociology and ethnography and using them for machine learning products. I think that category shift from observation to action — to product — makes a huge difference in how people think about what models can and should do. (Peter Sweeney has some interesting thoughts here.)
Sweeney argues it’s best to view the output of algorithms as observations, not explanations…just like Galileo in this painting.
tyler We’re talking about academics and industry but there’s also a different part of the world: what school teachers do (do you teach white kids about discrimination like a teacher in my home state of Iowa? or do you say that you shouldn’t make kids feel bad even if you’re trying to help more broadly?) And what happens in, say, law enforcement data.
Basically, I can’t stress strongly enough that our basic assumption in ML should be “Oh, this data is biased”. Maaaaaybe that’s not true in some cases but mostly it is.
Core claim from Tyler’s recent Wrangle talk about ethics and AI.
kathryn I had a very similar experience with one of my law students at the University of Calgary. He wrote his final paper as a first-person narrative of a first nations individual subject to a sentence from an algorithm. And he was INCREDIBLY CHALLENGED by what it felt like to actually identify with a minority group — not just talk about it abstractly. I loved that learning experience.
tyler Oh so he chose to put himself in that position?
kathryn Yes, his paper was inspired by this article. I gave my students the opportunity to write first-person fictional narratives to understand the legal issues we were grappling with, and this topic inspired him.
yevgeni That’s fascinating.
tyler ⚡️ What did you/they learn?
yevgeni I will also add — how can these lessons be used for the broader society?
kathryn BIG QUESTIONS!
tyler Uff can you teach empathy?
kathryn It’s amazing how fiction can teach empathy. Not reading it, but exercising our creative minds to write and identify with another. Writing fiction.
tyler Maybe you just try to protect against it. For me, it’s also about enabling people OUTSIDE of AI practitioners to understand what’s going on.
kathryn Absolutely, that’s why I’ve always found that artistic applications of AI are great vehicles to help people outside research develop intuitions. My friend Gene Kogan has eloquently argued the same.
yevgeni Guys, we’re drifting. Let’s get back to AI and ML.
Let’s focus on people developing AI models. It’s hard to know a priori if an experiment is biased as designed. How can a designer mitigate the risk of that happening?
kathryn You know, Tyler has a whole list of practical recommendations in the link he posted above.
(I seem to like starting with “you know”. Linguistic tick. Um on slack)
tyler (I keep starting with “oh”, I got it from my friend Eliza…I analyzed my text messages. Wait, sorry, wrong topic.)
My favorite of the practical recommendation is having the team do a pre-mortem — that is, you imagine the project has happened and it’s been a DISASTER. The team writes the story of what went wrong. This is generally useful and it helps you notice if NO ONE on the team proposes something like an ethical/marketing disaster. That should be one of the disaster scenarios.
yevgeni So that’s a good way to get the team to brainstorm about negative outcomes.
kathryn I think, Yevgeni, that no matter how hard to we try dissociate ML from fiction, they two seem to be intertwined. 😀
yevgeni What do you mean by that?
kathryn The pre-mortem technique that Tyler recommends requires that we imagine, that we consider hypothetical scenarios, and, to a certain extent, empathize with people impacted by our work.
tyler Yeah and I think something you’ve been thinking about is that idea of fair representation — like in slide 57 of your talk for the NextAI workshop.
yevgeni Ok, Tyler’s getting dragged to a Sprint planning meeting and Kathryn has an interview, and I should probably get back to feature engineering. So we have to call it a day. But we will definitely continue the discussion at a later date…maybe even at next Wednesday’s AI in the 6ix! (Come join us!)
",Slack Chat: Ethics of AI,18,slack-chat-ethics-of-ai-e8b0524f3832,2018-09-05,2018-09-05 17:21:22,https://medium.com/s/story/slack-chat-ethics-of-ai-e8b0524f3832,False,2049,"TO based startup, building a future in which AI enriches people’s lives while creating better, more valuable businesses.",,,,the integrate.ai blog,,the-official-integrate-ai-blog,,,Ethics,ethics,Ethics,7787.0,integrate.ai,"TO based startup, building a future in which AI enriches people’s lives while creating better, more valuable businesses.",dbf4eb8c5945,integrate.ai,359.0,1.0,20181104
0,,0.0,,2017-11-10,2017-11-10 09:10:39,2017-11-13,2017-11-13 22:40:45,2,False,en,2018-01-03,2018-01-03 11:27:27,5,dfaeb859f082,2.5040880503144654,1,0,0,Facebook are trialling another of their out-of-the-box ideas to combat social issues. Following my write-up on their use of artificial…,5,"Facebook tackles… revenge porn?
Facebook are trialling another of their out-of-the-box ideas to combat social issues. Following my write-up on their use of artificial intelligence to detect suicidal users, they’re now using the technology to take on revenge porn. Yes you read that correctly.

A text book definition of revenge porn is the act of sharing sexually explicit images or videos without the subject’s consent. Or in layman’s terms, it’s when an angry ex posts nudes of whoever caused them heartache in an attempt to embarrass them. To prevent this, Facebook are piloting a scheme in Australia whereby users send in their own explicit images in order to stop the images being posted by someone else. A user will submit said image or video via Messenger, Facebook will then use their hashing system to auto-recognise and block the image in the event of an unauthorised post. Originally, taking down such posts has been dealt with via photo-reporting but this trial seems to take more of a pro-active approach as opposed to a reactive firehosing one.
Yes the intent of the experiment seems for the greater good, but the immediate weirdness of it all begs so many questions. The first question out of peoples’ mouths is “What if that system gets hacked?” And it’s a fair question. With many big conglomerates falling victims to data-hacking, what makes Facebook believe they are so untouchable? Are you putting yourself more at risk by sharing the sensitive content with the world’s biggest social media platform?
Source: Pexels
Another question that arose was “Is this a peculiar and invasive form of data capturing?” It’s a slightly more cynical query, but one that I’ve heard from several people. Peoples’ data has become digital currency in the modern world and every brand is innovating new ways to capture these unique insights and give them a competitive advantage.
My main doubt is with regards to the capability of artificial intelligence. Do we now trust AI so much that we’re willing to submit an explicit photo of ourselves to a machine? Facebook claim that no humans ever lay eyes on the images but have we really got to a point where AI requires zero human intervention? Is this the first algorithm that is 100% fool-proof? If it is, I’d have hoped the technology could be put to use in more dire social situations.
Overall, the pilot is a strange concept. And a stranger thought is that someones job title is a Revenge Porn AI Developer. Whilst technology expands into areas we’re unsure of, there’s bound to be a number of questions and concerns that follow. I resonate with the views I highlighted above, but on the other hand, if I genuinely thought I was at risk of revenge porn, I think I would sleep better at night knowing there’s a 24/7 preventative measure in place. Even if I did have to send naughty pics to Facebook’s tech guys.
Sources
Facebook's testing a new method to prevent revenge porn that requires uploading your nudes
Facebook is testing a new method to combat revenge porn in Australia, the Australia Broadcasting Corporation reports…techcrunch.com
Revenge porn: How to stop your nude photos ending up on Facebook
Updated November 02, 2017 18:23:39 If you've had a nude photo taken, you might be nervous about where it could end up…www.abc.net.au
",Facebook tackles… revenge porn?,1,facebook-tackles-revenge-porn-dfaeb859f082,2018-01-03,2018-01-03 11:27:28,https://medium.com/s/story/facebook-tackles-revenge-porn-dfaeb859f082,False,562,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Rhiana Matthew,I write about things from content strategy & digital trends to human behaviour and mindset. Bit of a mixed bag really.,e416cc2ec0ca,rhianamatthew,48.0,37.0,20181104
0,,0.0,,2017-12-06,2017-12-06 20:10:04,2017-12-06,2017-12-06 21:13:46,1,False,en,2017-12-06,2017-12-06 21:17:04,0,ccaa5619fe97,2.9245283018867925,1,0,0,this document will be a “living” evolving article containing a philosophical fictional law on automated decisions.,4,"boston dynamics atlas robot
evolving laws on automated decisions.
this document will be a “living” evolving article containing a philosophical fictional law on automated decisions.
one is allowed to take parts of this article for their own works.
i can not guarantee this text will remain viewable to the public.
chapter 1: programmed intentional violence
rule 1 if the robot is able to it should prevent the harm of human beings based on the mass emotion calculation model.
decisions taken by humans should be trailed based on human law.
a robot is never allowed to kill a human on it’s own if it has the choice to not do so. based on the mass emotion calculation model.
the decision Werther to kill or not should be made by a human who has responsibility and/or control of the robot.
if the robot is civilian it should follow the MECM of the country/region it’s operating in. if it’s however operated by a military force it’s owners can choose to follow the MECM of 1: the nation who owns it or 2: the nation it’s operating it. it will not be allowed to follow a other MECM than these two.
the MECM isn’t allowed to give a lower value to civilian robots of the enemy than the ones of themselves.
a robot will become a military robot when it
1: is armed
2: provides assistance in a non medical way to humans that are armed and working for the enemy or friendly force.
medical assistance robots (which include self moving drones, humanoid robots or other things that are included in the guidelines under this article) that are not able to be weaponized should not be targeted.
robots always should follow the basic MECM
chapter 2: preventing outside harm coming from other people or objects
chapter 3: preventing harm from the robot itself.
chapter 4: programmed discrimination
chapter 5: financial responsibility
chapter 6: animals
chapter 7: interaction between robots
chapter 8: mass emotion calculation model (MECM)
the mass emotion calculation model is a programmed model that calculates what the most ethical response will be to different situations which will influence the decision a robot makes when encountering the need for ethical complex decisions. this is based on how the mass population will react to specific decisions but the core will be a few general rules that shouldn’t be changed. aside from the core values the model can be tailored for the territory the robot is operating in.
each “thing” in the world have different values (humans, animals, objects) the robot will stack the values of each decision and calculate which choice has the highest value. the values will be different for every country but one thing wil remain the same, humans will always have a higher value than animals and objects. the “thing” with the lowest value is to be of the least importance for the robot to keep save.
the values could be given value in the following ways:
humans ranked per age, with older people getting a lower value than young people. this will be based on how much the country values young above old.
animals ranked higher than objects based on the emotional value of animals for the population
animals ranked on economic worth, this is suitable for rural areas where animals have a big impact on human life.
things ranked on financial value, emotional value and or value for the population in general.
we will have a look at the ways one shouldn’t give value to “things” in chapter 4.
chapter 9: word definition explanation
robot: a automated object that is able to react to the situations it encounters, and has responsibility
AI: the ability to make primitive unemotional pre programmed decisions.
thing: object, being or robot in the real world.
use of this project:
we won’t look into complex intelligent AI since that is something that’s probably never gonna be found in everyday robots and if it is then it will be after a long time. we’re covering AI that is gonna be found in everyday items like working/military robots and self driving cars. these objects need to obey new rules and laws and we have to decide together what these rules are gonna be.
sidenote for as long as this article keeps evolving
keep watching and feel free to share and discuss these ideas.
",evolving laws on automated decisions.,1,evolving-laws-on-automated-decisions-ccaa5619fe97,2017-12-08,2017-12-08 22:16:25,https://medium.com/s/story/evolving-laws-on-automated-decisions-ccaa5619fe97,False,722,,,,,,,,,,Ethics,ethics,Ethics,7787.0,jan pardon,"critical, debating fanatic and bored.",ca3a637de007,ianchris,0.0,4.0,20181104
0,,0.0,8f2bd592f5bc,2018-07-16,2018-07-16 12:29:26,2018-08-07,2018-08-07 15:11:34,3,False,en,2018-08-07,2018-08-07 15:12:23,2,d91f20587eb3,2.9688679245283023,0,0,0,"I’ve worked with many kinds of data, from clinical data to logistic and supply chain data passing through pharmaceutical data.
Each of them…",4,"Different kind of data you can analyse with BI or machine learning
I’ve worked with many kinds of data, from clinical data to logistic and supply chain data passing through pharmaceutical data.
Each of them is different and should be treated in different manners.
Let’s what we can learn with each of them.
Clinical data

This is the most difficult kind of data you can encounter during your working life. This kind of data should be treated with caution and attention. It’s not easy to interpret this kind of data due to its high differentiation between people.
For example, i’ve studied for a while data from patient with metabolic syndrome, and each of them had different traits of the same illness.
With a good database construction and a LOT of python it’s easy to find some correlation between two traits of the illness (Weight and Age or Environment and Population trait etc…). It’s really interesting that this kind of data is such delicate and important but at the same time they’re really flexible to any kind of transformation but obviously you need to have a biological background with a degree in biology or bioinformatic.
Moreover, nowadays does not exists a data format used to transfer this kind of data easily from hospital to hospital (assuming that there’s not a national “clinical database”).
This kind of data and all of its implication needs to be studied and observed with a delicate perspective. For example, if my data is stolen or could be seen by my boss, i really dont’ want that my problems influence a possible job hiring or relatioship with colleagues. See? Now you don’t want this too. 
This kind of problems needs to be discussed more deeply in another post.
Pharmaceutical data
I don’t really like this kind of data, maybe because they’re less clinical but more statistical than others.

I’ve worked with this kind of data for many times and they’re always the same. Nope you’re not accessing clinical trial data (explained before) but you’re only accessing of the drugs are sold across the countries and which of them are sold more than others. How a pharmaceutical company could improve its business by competing with other companies.
You will see the performance of the Pharmaceutical agent that will rum between private doctors convincing them to sell a specific drug instead of an another. You will calculate of the company performs and how well the sales person is performing across its zone. This kind of data has a little flexibility , just because you’ll receiving the data “as-is” and you will manipulate it by little. The only positive thing is that you can create really beautiful charts with the same data.
Supply chain data
Photo by Tim Gouw on Unsplash
This is the kind of data that machine learning, deep learning and data warehousing (SQL/NoSQL) will be really useful with a lot of potential.
When i first started using this kind of data i haven’t seen its potential but now it’s HUGE.
Here you will have the data of what a population buy online (with the respect of privacy where you cannot know who is she or he)
GPS data tracking the parcel where is sent and the road taken from the driver optimizing time an consumptions.
Learn a lot of integration techniques because you need to integrate your system from the customer ones.
A lot of machine learning for predicting what a customer would buy or where a driver should go with the parcels optimizing the time.
NoSQL and SQL databases for different kind of data elaboration and storage.
Insights for you company and for customers learning patterns and how to compete with others (Yeah just like the pharma but this kind of analysis is done by everyone).
And lot more…
Those are only a little part of this enormous world full of data.
",Different kind of data you can analyse with BI or machine learning,0,different-kind-of-data-you-can-analyse-with-bi-or-machine-learning-d91f20587eb3,2018-08-07,2018-08-07 15:12:23,https://medium.com/s/story/different-kind-of-data-you-can-analyse-with-bi-or-machine-learning-d91f20587eb3,False,641,Random how-to on bioinformatics world covering all kinds of works from data warehousing to scripting python and shell stuff.,,,,Bioinformatics Stuff,danielestrife@gmail.com,bioinformatics-stuff,"BIOINFORMATICS,PROGRAMMING LANGUAGES,SQL,BIG DATA,AI",daniele813,Ethics,ethics,Ethics,7787.0,Daniele,"Bi analyst, data scientist , gamer , runner",d228b936d4d4,xcloudx8,57.0,143.0,20181104
0,,0.0,,2018-04-25,2018-04-25 02:28:01,2018-04-26,2018-04-26 18:23:20,2,False,en,2018-04-26,2018-04-26 18:23:20,11,fe564eacfc53,2.756918238993711,5,0,1,"Building ML and AI products, ethically.",5,"Introducing: f[AI]r startups
Building ML and AI products, ethically.
When people talk about fairness and ethics in tech, they are usually focused on the tech giants of the industry. Understandably so - these companies are among the world’s largest, whose products impact billions of people each day.
Media coverage of tech ethics over the last few years
They are also leaders in the development and use of artificial intelligence and machine learning technologies, so it makes sense that they’re the focus of an increasing number of studies and discussion of algorithmic bias and fairness in AI / ML (like this study and also this one from FAT*2018, and the great investigative work done by ProPublica).
However, there are many other companies also building or buying ML technologies, driven by fears of being left behind in the AI arms race and a desire to capitalize on the gold rush. These range from Fortune 500s to the rapidly growing AI startup ecosystem, where more than 4,000 startups building AI technology have raised over $24 Billion in funding.
Who is holding startups accountable for the social outcomes of the products they build? Is anyone helping early-stage companies understand best practices for building ethical AI and addressing algorithmic bias? Who is thinking about broader societal impact in the startup ecosystem?
These are some of the questions that led us to build f[AI]r startups, a non-profit initiative that supports founders, investors, mentors, and accelerators on why and how startups should build AI ethically, from the earliest stages of product development. We are an initiative launched out of Assembly, a fellowship program hosted by the MIT Media Lab and Berkman Klein Center for Internet & Society at Harvard University.
It’s not about tech for good, or “doing the right thing” — it’s also about building better products, protecting your business, sending a trusting signal to your customers, and minimizing the potentially negative impacts that biased, unfair technology can have on society.
We know how lean and laser-focused startups need to be as they get their companies off the ground, and adding yet another thing to a founder’s plate doesn’t seem like the right approach. So, rather than making ethics another separate consideration, we want ethical tech to be the predominant mindset for founders and AI / ML practitioners. We want to embed fairness in startup culture, from the feedback you seek on your concept, to the way you design and prototype, through the actual data you collect and the way you train your models. We want to build a movement of practitioners, influencers, and early adopters who know this is an existential issue and are ready and willing to do something about it… in short, we want to make it cool to be fair.
Over the next few months we’ll be publishing a series of posts on the different components of f[AI]r startups, explaining our perspective, how you can get involved, the work we’ve set out to do. For now, we wanted to introduce ourselves and say hello. And, of course, ask you to join the movement.
Questions? Comments? Want to get involved? Let us know: joinus@fairstartups.org.

f[AI]r startups is a non-profit initiative that supports founders, investors, mentors, and accelerators on why and how startups should build AI ethically, from the earliest stages of product development.
Hallie Benjamin is a co-founder of f[AI]r startups. She has spent the last decade working at the intersection of social justice, business and tech, and is pushing for the design and development of technology that is accessible to everyone.
André Barrence is a co-founder of f[AI]r startups. He knows intimately the various aspects of the tech ecosystem, venture capital investment and policy making for innovation. He believes founders must challenge the status quo and build successful companies that also create positive impact in society.
",Introducing f[AI]r startups,10,introducing-f-ai-r-startups-fe564eacfc53,2018-05-02,2018-05-02 08:57:09,https://medium.com/s/story/introducing-f-ai-r-startups-fe564eacfc53,False,629,,,,,,,,,,Ethics,ethics,Ethics,7787.0,f[AI]r startups,"We’re a non-profit initiative supporting tech startups on a quest to build AI and ML, ethically. We want to make it cool to be f[AI]r.",75632347199e,fairstartups,3.0,5.0,20181104
0,,0.0,26af721f0ca5,2018-02-08,2018-02-08 14:27:56,2018-02-08,2018-02-08 14:57:48,1,False,en,2018-02-12,2018-02-12 01:52:40,10,334fd9cc95c6,2.5622641509433963,39,0,0,Announcing the Tech and Society Solutions Lab,5,"#GeeksForHumanity
Announcing the Tech and Society Solutions Lab

At Omidyar Network, we fundamentally believe in the power of technology to deliver enormous benefits for humanity — but we have also become increasingly concerned with the growing number of unintended consequences we have seen emerge.
And so we have founded the Tech and Society Solutions Lab, an emergent effort to test, build, and scale solutions that address these negative effects — and, more importantly, help maximize the tech industry’s contributions to a healthy society. Our work is just beginning, but we’ve already seen that we are just one of many who have expressed concerns about the impacts of rapidly evolving technology on our society. We are joining a chorus of voices from mainstream media to academic leaders, from former tech employees to current executives and entrepreneurs, all of whom are calling for the industry to live up to its core ethos of creating tremendous good for the world.
This week alone, we’ve seen three of our initial efforts make a significant impact on the conversation. Take our investee Tristan Harris, a former Design Ethicist at Google, whose concerns about smartphone addiction led to the founding of Center for Humane Technology. As noted in The New York Times, the Center brings together a remarkable group of former tech employees who are concerned about the impacts of the products their companies — and in some cases, they as individuals — built. We are incredibly excited about their early efforts, which range from a public ad campaign to combat smartphone addiction to encouraging tech employees to think more critically about what they are asked to do.
We are also pleased to join the seed round of Loris.ai, a startup that uses a data-driven approach to help companies navigate hard conversations. The effort will build on the success of Crisis Text Line to bring empathy, active listening, and collaborative problem-solving to businesses, helping them better anticipate and navigate difficult situations. We see this as a critical component to building a tech culture that better supports its employees, giving them the tools to relate more directly to the constituents reached by their products. Fostering empathy is one way we can begin to mitigate unintended consequences, and we are excited to see what changes will result from founder Nancy Lublin and her team.
Finally, on Tuesday we participated in Bloomberg’s Data for Good Exchange in San Francisco, where we partnered with tech, media, and civil society leaders to support a grassroots campaign to create a code of ethics for the data science community to adopt principles of responsible data use and sharing. Modeled after the Hippocratic Oath, we are eager to support the pledge in its goal of reaching 100,000 signatures. Under the leadership of Data4Democracy, in partnership with many others, this effort will be instrumental in building a coalition dedicated to promoting responsible behavior, sending an important message to the broader tech community that its employees are concerned and ready to take action to build a productive, more humane industry.
These are just three examples of important work being done to advance the conversation about the societal impacts of technology. We are humbled to play a small part in supporting these innovators and look forward to continuing to build a portfolio of innovative, solutions-focused endeavors that share our commitment to technology as a force for good. And we’re looking to expand our team of colleagues dedicated to the cause — please take a look at our job descriptions here and here if you’re interested in joining the Solutions Lab as it works to create a responsible path forward for the tech industry. We’ll be sharing more about the Solutions Lab in the weeks and months to come. Stay tuned!
",#GeeksForHumanity,129,geeksforhumanity-334fd9cc95c6,2018-05-18,2018-05-18 16:35:29,https://medium.com/s/story/geeksforhumanity-334fd9cc95c6,False,626,Working to create a world of positive returns,,omidyarnetwork,,Positive Returns,communications@omidyar.com,positive-returns,"SOCIAL IMPACT,IMPACT INVESTING,PHILANTHROPY,INVESTING",omidyarnetwork,Ethics,ethics,Ethics,7787.0,Paula Goldman,"VP, Head of Tech and Society Solutions Lab @OmidyarNetwork. Obsessed with making unorthodox ideas mainstream. #geeksforhumanity",ad3ebcfe2bbc,pdgoldman,687.0,264.0,20181104
0,,0.0,ae0e5c39adf0,2017-11-10,2017-11-10 18:26:29,2017-11-10,2017-11-10 18:32:44,1,False,en,2017-11-21,2017-11-21 18:51:47,1,46f69aff783d,2.8641509433962264,0,0,0,"If you haven’t read Cathy O’Neil’s book Weapons of Math Destruction, you ought to. In it, she applies critical insight to the application…",5,"Big Data: In Climbing the Mountains of Truth, We Still Search for Valleys

If you haven’t read Cathy O’Neil’s book Weapons of Math Destruction, you ought to. In it, she applies critical insight to the application of algorithms and Big Data. Her main objection is that we often don’t adequately discuss, or acknowledge, the limitations of algorithms in our daily lives, that when taken too far they can do more harm than good.
Part of this harm can be derived from the fact that algorithms, by being based upon mathematics, are thought of as inherently objective. However, as they are created by human beings they cannot be free of faulty logic and implicit bias. They affect whether we get a job, what we will pay for goods and services, whether we are candidates for medical procedures, and a host of other decisions that directly affect our lives.
Of course, mathematical modelers don’t create algorithms with the intent of doing harm any more than someone building a model airplane intends to do anything else but see it fly. But it is easier to see the results of plane building than formulae composition. This intangibility is made worse because often algorithms are not available for public scrutiny and even if they were there right in front us, we might not deem ourselves qualified to analyze them in the first place. And this is where professions, in general, can knowingly, or unknowingly, do harm and perhaps why George Bernard Shaw once opined that professions are the conspiracy against the laity. Indeed, we should question experts who have a learned advantage and who speak with authority, and objectivity, in the name of science. Otherwise, in the race towards more and more specialization in knowledge based economies, subject matter expertise is the big bang that drowns-out the whispers of doubt at the edges of the universe.
At the center, it is difficult to see planets from black holes. While seeking the illumination of knowledge, we often unknowingly find darkness, sometimes due to cognitive biases, which can be defined as systematic patterns of deviation from rational judgment, whereby inferences about other people and situations may be applied illogically. The ingroup bias informs us that we tend to perceive those in our group more favorably than those outside of it. Of more concern, is that in any group, we tend not only to think more favorably of those within it, but also the information and beliefs held therein. A group with a hammer may only see nails just as a group carrying Big Data may only see algorithms. All of us have different cognitive profiles, but holistic and integrative are not adjectives frequently associated with the cognitive profiles of data scientists. Sometimes the ability to find the needle in the haystack, stops one seeing the haystack.
In her book, Cathy O’Neil shares an experience about a large American media firm that used algorithms to predict employee performance based, primarily, upon a data set structured around employee longevity and the number of promotions. As this organization had a history of not promoting women, the algorithm (per the number of promotions) predominantly chose men as being more likely to succeed. This is one example of many.
Of course, data scientists are aware of the delicate nature of algorithm alchemy. Though it is crucial to understand, and inquire, into the inclusivity of data set creation. O’Neil mentions the necessity for algorithm audits, whereby third parties look at the chosen data sets used to construct algorithms. Indeed, many of us leap on Big Data bandwagons and kick the wheels to make them go faster, with hammers and nails at the ready. When we aim this bandwagon at people analytics, we are playing with people’s lives. Never forget it.
While data science has transformed how we collect and use information, we should question the efficacy of applications with a potentially negative social impact. Historically and comparatively speaking, human progress might seem to be ever reaching skyward, but if subconsciously, or worse consciously, there is an underlying perspective that subject matter experts and professions create the bang, and laity the whispers, perhaps amongst all of our achievements on the mountain of truth, we still subconsciously search for valleys.
","Big Data: In Climbing the Mountains of Truth, We Still Search for Valleys",0,big-data-in-climbing-the-mountains-of-truth-we-still-search-for-valleys-46f69aff783d,2018-04-11,2018-04-11 01:40:22,https://medium.com/s/story/big-data-in-climbing-the-mountains-of-truth-we-still-search-for-valleys-46f69aff783d,False,706,Exploring how to redesign the intersections where people and their environment meet.,,,,Cognitive DESIGNLAB,,cognitive-designlab,"COGNITIVE SCIENCE,COMPLEXITY THEORY,HUMANITY,LEARNING,EDUCATION",,Ethics,ethics,Ethics,7787.0,Andrew Ball,A curious human interested in perspective: Director of Human Design at Cognitive DESIGNLAB.,262e536efcc4,adball100,0.0,17.0,20181104
0,,0.0,,2018-08-22,2018-08-22 02:25:00,2018-08-22,2018-08-22 02:28:26,1,False,en,2018-08-22,2018-08-22 02:28:26,0,5c797e484b9e,5.169811320754717,0,0,0,"Remi AI Research Fellow, Thom Dixon, presents the first in a series of thought provoking pieces. Today, he explores the ethical…",4,"In Discussion: Image Recognition

Remi AI Research Fellow, Thom Dixon, presents the first in a series of thought provoking pieces. Today, he explores the ethical considerations around the roll out of Image Recognition solutions in public spaces.
Over the coming weeks I’ll be writing a number of articles in a discussion series highlighting ethical issues in artificial intelligence research. These articles are based on internal interviews and coffee conversations at Remi AI. I’m writing them to give an insight on the kind of ethical issues that arise daily in A.I research. From the outset I’ll add the following caveat: the issues Remi AI encounters aren’t the singularity and no one here is close to cooking up consciousness in the backyard. It will be years until such problems arise. While there is need for conversation around these topics, there are much more pressing issues that are already impacting society, or will in the near future.
The primary goal of the In Discussion series is to combat the A.I hype cycle which is diverting attention away from issues of real concern. This series is going to focus on contemporary issues that aren’t being talked about nearly as much as they should be.
Let’s begin with image recognition.
Setting aside the impressive technological advancements made in recent years, image recognition capabilities have opened up a plethora of data privacy problems. An A.I research firm focusing on image recognition has to navigate these problems each and every day. Do we do this project? Do we deliver this capability? Who are we delivering to? Just because we can, does that mean we should?
The reaction of Google’s employees to Project Maven is a perfect example. Companies operating in this area have to ensure their choices are aligned with the values of their employees. When everything on the ground moves quickly, this translates to constant conversations about what can, could, and should be done.
Image recognition can be split this into two types of capability: facial recognition and person recognition. The former powers your cloud photo libraries and a decent amount of policing and security work. The latter uses clothes, arms and hand positioning to track a person through a scene or space. This is great for pedestrian and buyer behaviour analysis. Now, with person recognition, it’s easy to get an A.I agent to “forget” you (delete the footage) once you leave the scene, but that’s not so easy under option one — facial recognition — because it is predicated on recognising you over a potentially infinite time span. With person recognition, once you exit a scene, the agent that tracked you can be set to forget you after a certain amount of time. In most cases there’s no need to retain that data as your aggregated behaviour is all that’s of interest. Moreover, if you leave the scene and change your sweater to something leopard print, put on gold gloves and bright green leather pants, then return ten minutes later, chances are the agent doing the analysis is none the wiser you’re the same person.
There have already been some highly successful applications of person recognition for behavioural analysis in the optimisation of highly trafficked spaces. This tech has saved time and increased convenience on a mass scale, encouraging greater pedestrian traffic through popular cities. In fact, it’s hard to think that in ten years’ time anyone would design a space without first modelling and analysing how people are going to interact with it, and to do that they’ll most likely use this tech. High traffic spaces that aren’t constantly assessed and redesigned based on behavioural analysis will be easily identifiable in comparison, because they will not function as well.
Two questions often underpin privacy debates: when are you allowed to be forgotten AND when should you be remembered? Perhaps more importantly though, asking ‘when are you allowed to be forgotten’ assumes that you know when you’re being watched.
Which, to be quite frank, you don’t.
This is one of the key differences between the West and China right now. If you go to China tomorrow you should expect to be watched and you would be naive if you didn’t. If you travel to China you should expect your passport photo to be digitally chewed over and your face to be watched through each and every city. That’s the state framework. For better or worse, that’s the approach China has taken to image recognition. It’s laid out, it’s known, it’s deployed. Truth is, it’s working very well, so well in fact that China’s now talking about on-selling their tech to a variety of countries.
In the West we’re in a very different situation. We don’t know the position of the state security apparatus in relation to image recognition. More importantly, the majority of us have no idea how our corporations are using it. The regulation isn’t there, the legislative framework isn’t there, the broad based public understanding of what image recognition is and does, it’s not there. Yet now is the time that we need to think seriously about these questions. If we don’t, we risk sleepwalking into a domestic security regime comparable to China. This might be fine for some, but the entire structure of Western society is predicated on the fact that at the end of the Second World War, we agreed certain levels of state intrusion into people’s privacy weren’t a good thing. It’s disappointing that 70 years later we need to have this conversation again.
When are you allowed to be forgotten, when should you be remembered?
Opinion at Remi AI is in flux as the answers to these questions are necessarily context driven, but there’s a few things that can be said with surety. Facial recognition is a dual-use technology in a way that person recognition is not. This should mean that when facial recognition is deployed, it is monitored by a national regulatory body. Additionally, this monitoring should be in a much more intrusive way than that required for the deployment of person recognition. Currently, neither are being monitored and who knows which federal agency would put their hand up for the job if it was ever actually required. There’s clearly ground to make up.
Think of it this way, if a wild card political movement swept the country tomorrow, what is the breakout time for them to turn today’s state and corporate image recognition capabilities to nefarious work? With no stopgaps in place it wouldn’t take long. .
Image recognition, if deployed maliciously and malevolently, is the hole that will sink the democratic ship. At Remi AI, we think it’s time liberal and democratic societies culturally engage with this. It’s time to inform the development of this emergent capability with our own values and ideals. That means finding a balance of intrusion. Image recognition is an intrusive dual-use capability with great potential for better enabling our lives. That should mean when it is deployed it’s also monitored and regulated with an equal amount of intrusion by an independent ombudsman.
The European Union has a head start on Australia and the US on this front. They’ve taken that lead with the General Data Protection Regulation (GDPR), and Australia could do worse than to start with its own version of that.
The near-term issues for A.I aren’t lethal autonomous weapons that go rogue, and they’re not superintelligent paperclip-making factories. They are data and privacy. In international affairs we slice and dice countries up in all sorts of ways, and one of those ways is between rule-makers and rule-takers. When it comes to data privacy and the way your data interacts with A.I enabled tools, we’ll all be better off as rule-makers.
Not everyone can be a rule-maker though, and first movers normally have the advantage.
More to come…
By Thom Dixon
Thom is Research Fellow at Remi AI
",In Discussion: Image Recognition,0,in-discussion-image-recognition-5c797e484b9e,2018-08-22,2018-08-22 02:28:26,https://medium.com/s/story/in-discussion-image-recognition-5c797e484b9e,False,1317,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Remi Studios,"The official blog of Remi A.I, an Artificial Intelligence studio with offices in Sydney and San Francisco.",40cdb20c9eee,RemiStudios,418.0,55.0,20181104
0,,0.0,1dc0795d9d6e,2018-08-08,2018-08-08 17:41:26,2018-08-08,2018-08-08 17:46:08,1,False,en,2018-08-08,2018-08-08 17:46:08,9,983c8c6912b9,7.550943396226415,1,0,0,"This post was featured in our Cognilytica Newsletter, with additional details. Didn’t get the newsletter? Sign up here",5,"Ethical Concerns of AI

This post was featured in our Cognilytica Newsletter, with additional details. Didn’t get the newsletter? Sign up here
Artificial Intelligence is seen by many as a great transformative tech and the possibilities seem almost limitless to what it can eventually do. Will AI systems one day drive us around? Do our laundry? Mow our lawn? Raise our kids? Fight wars? Kill humans? These questions make people shift from thinking purely about the functional capabilities to the ethics behind creating such powerful and potentially life-consequential technologies. As such, it makes sense to spend time considering what we want these systems to do and make sure we address ethical questions now so that we build these systems with the common good of humanity in mind.
Will AI Replace Human Workers?
The most immediate concern for many is that AI-enabled systems will replace workers across a wide range of industries from blue collar to white collar. However, as we’ve written before, AI is not a job killer, but it’s a job category killer. AI brings mixed emotions and opinions when referenced in the context of jobs. Research and experience is showing that it’s inevitable that AI will replace entire categories of work, especially in transportation, retail, government, professional services employment, and customer service. However, as we have written in the piece above, companies will be freed up to put their human resources to much better, higher value tasks than taking orders or fielding customer service complaints. As has happened with every wave of technology, from the automatic weaving looms of the early industrial revolution to the computers of today we see that jobs are not destroyed, but rather employment shifts from one place to another and entirely new categories of employment are created. We can and should expect the same in the AI-enabled economy.
Indeed, the move to this new age of digital transformation is creating concerns about labor displacement, with or without the power of AI. All AI is doing is hastening digital transformation across particular business processes. As companies are looking to adapt and implement AI strategies we think it’s important to have open and honest conversations with your employees. In particular, experience and research is showing that companies that adopt augmented intelligence approaches, where AI is augmenting and helping the human to do their job better, rather than fully replacing the human, not only shows faster and more consistent ROI for organizations, but also is welcomed much more warmly by employees. People feel more comfortable working with machines instead of machines replacing them.
The Rise of Fake Media and Disinformation: Will AI Make this Worse?
AI systems are getting better at creating fake images, videos, conversation, and text. We already have trouble believing everything we hear, see, and read. What happens when you can no longer tell if an image is real or AI-generated or if you’re talking to a bot or a real person? It’s been widely reported that bots had a role to play in the 2016 US Presidential elections spreading political propaganda. These automated social media accounts help create and spread misinformation on the internet attempting to manipulate voters and fueling the fire of partisan disagreement. Bots never tire working 24/7 and can generate a very large amount of content in a very short period of time. Once shared and re-tweeted with others this news starts to go viral, true or not, and is virtually unstoppable. These bots are effective at spreading false or heavily altered facts, amplifying messages, and putting thoughts and ideas into people’s heads. Criminals and state actors can use fake imagery or audio to cause personal or business harm or to interfere with government operations. Now all it takes is a few malicious actors spreading false claims to traumatically alter public opinion and quickly shift the public’s view.
Governments and corporations alike will have to think about how they will reign in the potential damage done by AI-enabled content creation. In fact, we encourage companies and governments to consider fake content to be as malicious as cybersecurity threats and respond appropriately. Propaganda, disinformation, malicious interference, blackmail, and other forms of “information crime” can be just as harmful as physical and electronic attacks on systems. The world is very much unprepared for AI being unleashed on unprotected citizens. Corporations who freely traffic in user-generated content are just as liable as governments to curb abuse.
Do We Want Evil People to have Easy Access to AI Technology?
While AI can do a lot of good we must be mindful about AI in the hands of malicious users. As technology continues to become more powerful, AI can cause severe damage if used maliciously. What happens when individuals, criminal organizations, and rogue countries apply AI to malicious ends? Many companies are already asking themselves these questions and starting to take action to safeguard against malicious AI attacks. New technologies can exploit the vulnerability of systems that are dependent on AI and machine learning technologies. And as these AI systems get smarter they can change the nature of threats, making them harder to detect, more random in appearance, more adaptive to systems and environments, and more efficient at identifying and targeting vulnerabilities in systems. This should be terrifying. We need to start thinking about how we are constructing and managing our digital infrastructure as well as how we design and distribute AI systems now. Detecting these malicious attacks will only get harder over time.
In addition Machine Learning service providers, especially on-demand cloud-based services should be mindful of who their customers are. If malicious users are using their platforms to perform distributed AI-enabled attacks or other criminal acts, then like financial institutions, governments will start cracking down on these providers and impose new forms of “Know Your Customer (KYC)” regulations. If these platform providers don’t want to be on the wrong end of the regulatory cycle, they need to get ahead of the curve and start their own efforts to make sure they know who their customers are and what they are doing on their platforms.
Is Pervasive Surveillance Already Here? Will AI Be Big Brother?
AI enables companies and governments to keep constant tabs on what humans are doing in an automated and intelligent fashion. Will a future of AI mean an end to privacy? Will “Big Brother” really always be watching? As facial recognition technologies continue to advance it’s getting easier to detect individuals from a large crowd of people at stadiums, parks, and public spaces without their permission. Microsoft recently urged Congress to study it and oversee its use. Bradford Smith, the company’s president recently said “We live in a nation of laws, and the government needs to play an important role in regulating facial recognition technology”. What’s striking about this statement is that tech giants rarely advocate regulation of their innovations, so for Microsoft to be urging the US Congress to regulate facial recognition they must already see how this technology can be misused.
In our vision for an AI-enabled future we assume that everyone and everything will have knowledge about everyone else. This means that the assumption will be that everyone already knows who we are, what we want, where we are, and what we’re doing. This pervasive knowledge will become part of the assumption of where we are, just like we are now expecting to be able to get Internet, electricity, and information whenever and wherever we need it without excuse. No longer will we be able to just “un-plug” for awhile. We may quickly move to a world where just a few companies and government have an uncomfortable level of control over the lives of everyone.
Will Intelligent Machines Have Rights?
As machines become more intelligence and we ask more and more of our machines, how should they be treated and viewed in society? Once machines can actually simulate emotion and act similar to human beings how should they be governed? Should we consider machines as humans, animals, or just inanimate objects? To this point, to what level do we ascribe liability and responsibility to the devices themselves over the people that are supposedly in control of them? In March an autonomous vehicle struck and killed a pedestrian. People were outraged that a machine killed a human being and we discussed this in length in one of our podcasts.
But why were people outraged by this accident? Thousands of people are killed every day in motor vehicle accidents caused by humans at the wheel. What difference should it make that it was a machine behind the wheel? The reason for this outrage is because society hasn’t, and may never accept, when a machine kills a human. However, the likelihood of eliminating all traffic-related fatalities is almost certainly zero percent. As such, if we want autonomous vehicles on the road this scenario will happen again, and again. Despite perhaps dramatic evidence that machine-driven vehicles have overall far lower fatality rates than human-driven vehicles, the issue of liability and control is primarily one of ethics. So we need to be asking these questions, figuring out what we can accept and what’s ethical, and put laws and regulations in place now to safeguard against future tragedies.
Creating Transparency in AI Decision-Making
There are many approaches used in machine learning, as we’ve detailed in previous articles. However, no machine algorithm has re-invigorated the AI market quite like deep learning. Deep learning, however, is a “black box”. We aren’t really sure how deep learning works and this can be a big problem when we rely on this technology to make critical decisions such as loan applications, who gets paroled, and who gets hired. As we’ve repeatedly written, AI systems that are unexplainable should not be acceptable, especially in high risk situations. Explainable AI needs to be part of the equation if we want to have AI systems we can trust.
Deep learning relies heavily on training data. But when biased training data is used to teach these systems the results are biased AI systems. People wrongly assume that training data is always been “clean”, from a large pool, and represents society as a whole but results have proved this is not the case. Google’s image recognition system wrongly classified images of minorities, non-native speakers with accents often trip up virtual assistants like Siri or Alexa, and software used to sentence criminals was found to be biased against minorities. If we are going to use machine learning algorithms to make any sort of worthwhile decision we must demand that it be able to explain itself. Would you really allow a human driver to hit your car and when you question why they did that they have no answer? Of course you wouldn’t. We shouldn’t accept this from machines either!
Taking Steps to Resolve These Issues
If we don’t ask ourselves these questions now and build ethical AI, implications down the road can be far more grim than people realize. Do we trust companies to do the right thing? Do we trust governments to do the right thing? We’d like to think that with public input and ethical questions and concerns brought up now, that we can create a future that isn’t so grim. There will always be bad actors who try to influence, infiltrate, and manipulate. Enterprises and organizations should keep asking questions, keep working towards building ethical AI, and keep trying to fight automated bots and malicious attacks. And Cognilytica will keep pushing providers and implementers of AI systems to think through these ethical considerations before they put their systems into place.
",Ethical Concerns of AI,1,ethical-concerns-of-ai-983c8c6912b9,2018-08-08,2018-08-08 17:46:09,https://medium.com/s/story/ethical-concerns-of-ai-983c8c6912b9,False,1948,"Real-world insight, expertise, and opinions on Artificial Intelligence (AI) and related areas",,cognilytica,,Cognilytica,info@cognilytica.com,cognilytica,"ARTIFICIAL INTELLIGENCE,MACHINE LEARNING,DEEP LEARNING,AI,ROBOTICS",cognilytica,Ethics,ethics,Ethics,7787.0,Kathleen Walch,Principal Analyst at AI Focused Analyst firm Cognilytica (http://cognilytica.com) and co-host of AI Today podcast.,b8d98395a9a1,kath0134,10.0,2.0,20181104
0,,0.0,,2017-12-18,2017-12-18 16:44:12,2017-12-18,2017-12-18 16:06:58,3,False,en,2017-12-18,2017-12-18 16:49:28,21,66f5e270461b,4.640566037735849,1,0,0,By Marina Laven,5,"Which Consumer Facing Company is the Leader in AI Technologies?
By Marina Laven
The full report can be downloaded here on our website.
Artificial intelligence has already disrupted myriad consumer products and services. Soon, a company’s ability to remain competitive will necessitate that they incorporate AI, not only to save time and money, but also to develop products that better serve consumer needs. While human beings remain necessary for creating ethical AI strategies and responding appropriately to ambiguity, applications of artificial intelligence are already widespread.
To gain a better understanding of how Canadians view artificial intelligence and its impact on their personal and work lives, we surveyed 1001 adults in our Canadian Artificial Intelligence Tracker. One piece of that study, which we share here, focused on perceptions about seven consumer facing companies applying AI technologies.
Leaders in AI Technologies
When asked which of seven companies could be seen as leaders in developing artificial intelligence technologies, nearly half of people said they did not know (43%). Even when given seven global, highly recognizable choices, they could not choose any as a leader.

However, 29% of people did choose Google as a leader in AI technologies. Given that Google holds 58% of browser market share and 74% of search engine share, such widespread access to people could easily increase their likelihood of being recognized. Google has been public about its AI experiments including DeepDream and AutoDraw, as well as its more serious projects like AlphaGo, DeepMind, and Quantum AI projects.
Behind Google, 21% of Canadians chose Apple as a leader in AI technologies. This second place status could be because Apple has been more secret about its research and has published less often. However, given that Apple products have lower market share (3% of browser share, 7% of PC share, 34% of phone share), second place in people’s minds might actually be a good result. People are familiar with Apple’s Siri AI, and Apple is starting to be more public about Core ML, their machine learning technology for Siri, the camera, and QuickType.
In terms of gender differences revealed by the research, women were more likely than men to say they didn’t know which company was the leader, a trend which carried over to 6 of the 7 brands — except for the Apple brand. Women might feel slightly less aware of AI technology leaders in general but their confidence in Apple is disproportionately higher.
Ethical Approaches to AI

The ethics behind AI is not without controversy as we’ve discussed in a previous post (See Arguing for Morality in a World Run by Artificial Intelligence). In this study, when asked which of the seven specified companies had more ethical approaches to using artificial intelligence, more than half of people were unable to pinpoint a leader (60%). However, about 18% of people said that Google had the most ethical approach followed by Apple (15%), and Tesla (13%).
In this case, age was very much a determining factor as younger people were much more likely to choose a company as being an ethical leader in comparison to older people (52% vs 28%). For the most part, the rank order of ethical leaders remained the same across all age groups.
These high rates of being unable to choose ethical leaders means that none of these companies has succeeded in positioning themselves as the leader, or even a leader. The door is wide open for any company that wishes to establish themselves as the most ethical user of artificial intelligence. With so many concerns about trust and transparency (see the full report), any company wanting to position themselves in this way has a long way to go.
Best AI Products

In recent years, artificial intelligence has been applied to a wide variety of products and services, many of which relate to marketing and research. In this study, we took a broader view of AI applications and asked people which of these seven companies provide the best AI products and services. Once again, uncertainty prevailed with 50% of people saying they don’t know.
The top ranking company was Google with one quarter of people choosing them (26%). As before, immediately behind Google was Apple at 22%. And, also as before, the trend for women to be less inclined than men to choose companies specific companies continued — except for Apple.
Implications
These results clearly demonstrate that the race to lead or dominate within the field of AI products and services continues. People do not agree on a leader, whether in products or in ethical behaviours. Indeed, most people can’t even choose any company that might be perceived as a leader.
There is plenty of a space for any company to pull ahead and become known as the global leader. This will require a major strategy to gain the trust of consumers (see the full report) but the company that succeeds in doing so will reap the rewards.
Methodology: The Canadian Artificial Intelligence Tracker was conducted by Sklar Wilton & Associates among Canadians 18+ with data collected from July 31 to August 7, 2017. Participants were selected from among those who have volunteered to participate in online surveys. The data were weighted to reflect the demographic composition of adult Canadians. Estimates of sampling error cannot be calculated. All sample surveys are subject to error, including, but not limited to sampling error, coverage error, and measurement error.
Perhaps you’d like these posts too…
· Arguing for Morality in a World Run by Artificial Intelligence
· 15 Artificial Intelligence Marketing Tools to Automate Strategies Built by Humans
Marina Laven is a Research Management Director at Sklar Wilton &Associates who has a particular strength and interest in advanced analytics. Her statistical toolbox includes regression analysis, principle components analysis, correspondence analysis, structural equation modeling, and any other statistical tool she can uncover that will best solve the problem. When she’s not challenging the mighty statistics beast, you’ll find Marina playing Ultimate Frisbee.
Sklar Wilton & Associates helps their clients solve tough marketing challenges to unlock growth and build stronger brands. SW&A has worked for more than 30 years with some of Canada’s most iconic brands and, in 2017, was named the Best Workplace in Canada for Small Companies by the Great Place To Work® Institute and the number one Employee Recommended Workplace among small private employers by the Globe and Mail and Morneau Shepell.
Like our posts? Sign up for our newsletter and enjoy insights from our associates in your mailbox every 6 to 8 weeks.
",Which Consumer Facing Company is the Leader in AI Technologies?,1,which-consumer-facing-company-is-the-leader-in-ai-technologies-66f5e270461b,2018-06-17,2018-06-17 03:19:16,https://medium.com/s/story/which-consumer-facing-company-is-the-leader-in-ai-technologies-66f5e270461b,False,1084,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Jennifer Marley,Sklar Wilton & Associates applies strategic advisory services and marketing research and analytics to solve tough business challenges.,87c165d63933,SklarWilton,317.0,858.0,20181104
0,,0.0,32881626c9c9,2018-09-20,2018-09-20 05:58:56,2018-09-20,2018-09-20 04:38:06,1,False,en,2018-09-20,2018-09-20 06:00:58,1,803e750ad6f2,2.4603773584905664,3,0,0,There has been a productive period of incredible progress and innovation in the field of artificial intelligence (AI). AI developers…,5,"Designing Value-Sensitive AI

There has been a productive period of incredible progress and innovation in the field of artificial intelligence (AI). AI developers already started to create sophisticated systems that mimic human intelligence which are already capable of learning in and coping with highly uncertain situations.
On the other hand, embedding ethical frameworks into AI (artificial intelligence) technologies including robots is still not an easy task. Calculating the utility of every course of actions would be an impossible demand as a first-order logic is prevalent among AI technologies when being exposed to complicated decisions. Assigning values to human values would certainly be not a suitable method as well as integrating a moral framework without a respect for important human values.
One of the most difficult question to be answered is what it means to be moral. When it comes to developing such technologies, it is crucial to understand how morality differs from ethics. Should our approach to morality have its own classification of right and wrong when it comes to AI technologies?
The development of AI technologies cannot be based merely on using empirical methods. One of the most complex aspects of designing these technologies is that they should be, first and foremost, as if they were moral beings. While the emotional decisions are not considered a necessity when it comes to making moral decisions, it is an important component because it often clouds our decision making and is something to take into account when empirically analyzing how humans and nonhumans interface of AI technologies, and perhaps how they reciprocate actions without having emotionally grounded social behaviors.
An ethical framework may not need necessarily be a perfect approach that can solve any engineering issue or account for every possible future consequence, but it can surely help to minimize the effects of several issues before they arise.
In case of something going wrong, it can be modified and accommodated in future iterations. The development of such an ethical framework should start from the central premise that technology is not value-neutral. In other words, each technology has some values embedded that are of ethical importance to individuals and society such as freedom, equality, trust, autonomy, or privacy. As value-related issues are connected to the application of technology within a social context, an ethical framework should purport to incorporate value solutions into the design and address any issues that may emerge during the early design phases before ubiquitous rollout.
AI as both an emerging and converging technology will almost certainly entail the emergence of new ethical and societal issues, as well as the exacerbation of current issues associated with its development. Integrating an ethical framework with current practices could prove beneficial if the resulting amalgamation enhances existent practices:
Emerging technology should be sufficiently evaluated for design;
Such a methodology should also address the activities of designers and developers, for instance by taking into account potential surveillance techniques of AI designers in order to reduce the likelihood of an infeasible design.
Despite the difficulty of the development of a moral framework for AI, a focus on solely universally held values may also make such frameworks deviate from rights-based approaches to responsibility. Similar to conveying children a concept of what is right or wrong, it is certainly critical to provide a rigid framework of dos and don’ts for AI. However, this should not be limited to a finite set of conditionals as this may result in a complicated mess of logic which breeds accusatory actions when something goes wrong.
Comments
comments
Originally published at www.datadriveninvestor.com on September 20, 2018.
",Designing Value-Sensitive AI,78,designing-value-sensitive-ai-803e750ad6f2,2018-09-20,2018-09-20 07:21:46,https://medium.com/s/story/designing-value-sensitive-ai-803e750ad6f2,False,599,"Data Driven Investor (DDI) brings you various news and op-ed pieces in the areas of technologies, finance, and society. We are dedicated to relentlessly covering tech topics, their anomalies and controversies, and reviewing all things fascinating and worth knowing.",,datadriveninvestor,,Data Driven Investor,info@datadriveninvestor.com,datadriveninvestor,"CRYPTOCURRENCY,ARTIFICIAL INTELLIGENCE,BLOCKCHAIN,FINANCE AND BANKING,TECHNOLOGY",dd_invest,Ethics,ethics,Ethics,7787.0,Daily Wisdom,,ddd120ae7c2,dailywisdom,60.0,0.0,20181104
0,,0.0,,2017-10-27,2017-10-27 11:33:29,2017-09-21,2017-09-21 13:00:43,0,False,en,2017-10-27,2017-10-27 12:20:11,5,5a6c96750277,4.509433962264152,0,0,0,,4,"A Crucial Look at the Unethical Risks of Artificial Intelligence
Artificial Intelligence Pros and Cons
As much as we wonder at the discoveries of AI and prediction engines, we also recoil at some of their findings. We can’t make the correlations that this software discovers go away, and we can’t stop the software from re-discovering the associations in the future. As decent human beings, we certainly wish to avoid our software making decisions based on unethical correlations.
Ultimately, what we need is to teach our AI software lessons to distinguish good from bad…
The Problem: Unintended Results
A steady stream of findings already makes it clear that AI efficiently uses data to determine characteristics of people. Simplistically speaking, all we need is to feed a bunch of data into a system and then that system figures out formulas from that data to determine an outcome.
For example, more than a decade ago in university classes, we ran dome tests on medical records trying to find people that had cancer. We coded the presence of disease onto our training data, which we then scanned for correlations to other medical codes present.
The algorithm ran for about 26 hours. In the end, we scanned the data for accuracy, and needless to say, the system returned fantastic results. The system reliably honed in on a medical code that predicted cancer; and more specifically, the presence of tumors.
Of course, at the outset, we’d like to assume this data will go to productive, altruistic uses. At the same time, I’d like to emphasize that the algorithm delivered the response: “well, of course, that is the case,” substantially demonstrating that such a program can discover correlations without being explicitly told what to look for…
Such a program might be initially developed by researchers hoping to cure cancer, but what happens if it gets into the wrong hands? As we well know, not everyone, especially when driven by financial gain, is altruistically motivated. Realistically speaking, if we use a program looking for correlations to guide research leading to scientific discoveries for good intent, it can also be used for bad.
The Negative Risk: Unethical Businesses
By function and design, algorithms naturally discriminate. They distinguish one population from an other. The basic principles can be used to determine a multitude of characteristics: sick from healthy; gay from straight; and, black from white.
Periodically the news picks up an article that illustrates this facility. Lately, it’s been a discussion of facial recognition. A few years ago the big issue revolved around Netflix recommendations.
The risk is that this kind of software can likely figure out, for example, if you are gay with varying levels of certainty. Depending on the data available, AI software can figure out figure out all sorts of other information that we may or may not want it to know or that we may not intend for it to understand.
When it comes to the ethics of all of this, it’s all too easy to toss our hands in the air and have excited discussions around the water cooler or over the dinner table. What we can’t do is simply make it we can’t make it go away. This concern is a problem that we must address.
Breakthrough: The Problem is its own Solution
Up to this point, my arguments may sound depressing. The good news is that the source of the problem is also the source of the solution.
If this kind of software can determine from data sets the factors (such as the presence of tumors) that we associate with a discrimination (such as the presence of cancer), we can then take these same algorithms and tell our software to ignore the results.
If we don’t want to know what kind of information, simply ignore this type of result. And then, we can then test to verify that our directives are working and our software is not relying on the specified factors in our other algorithms.
For instance, say we determine that as part of a determination of the risk of delinquent payment for a mortgage, we know that our algorithm can also determine gender, race or sexual orientation. Rather than using this data, which is likely a wee bit racist, sexist, and bigoted, when calculating a mortgage rate recommendation, we could ask it to ignore said data.
In fact, we could go even further. Just as we have equal housing and equal employment legislation, we could carry over to legislate that if a set of factors can be used to discriminate, then software should be instructed to disallow the combining of those elements in a single algorithm.
Discussion: Let’s look at an analogy.
Generally speaking, US society legislates that Methamphetamine is bad, and people should not make it, but at the same time the recipe is known, and we can’t uninvent meth.
An unusual tactic is to publicise the formula and tell people not to mix the ingredients into their bathtub “accidentally.” If we find people preparing to combine the known ingredients, we can then, of course, take legal action.
For software, I’d recommend that we take similar steps and implement a set of rules. If and when we determine the possible adverse outcomes of our algorithms, we can require that the users (business entities) cannot combine the said pieces of data into a decision algorithm, of course making an exception for those doing actual constructive research into data-ethical issues.
The Result: Constructing and or Legislating a Solution
Over time our result would be the construction of a dataset of ethically sound and ethically valid correlations that could be used to teach software what it is allowed to do. This learning would not happen overnight, but it also might not be as far down the line as we first assume.
The first step would be to create a standard data dictionary where people and companies would be able to share what data they use, similar to elements on the chemical periodic table. From there we would be ready to look for the good and the bad kinds of discrimination. We can take the benefits of the good while removing the penalties from the bad.
This process might mean that some recommendations would possibly have to ask if it would be allowed to utilize data that could be used to discriminate based upon an undesirable metric (like race). And it might mean that in some cases it would be illegal to combine specific pieces of data, such as for a mortgage rate calculation.
No matter what we choose to do, we can’t close Pandora’s box. It is open; the data exists, the algorithms exist; we can’t make that go away. Our best bet is to put in the effort to teach software ethics, first by hard rules, and then hopefully let it figure some things out on its own. If Avinash Kaushik’s predictions are anywhere near accurate, maybe we can teach software actually to be better than humans at making ethical decisions, only the future will tell!
If you’re curious about the subject of AI and Big Data read more in my piece Predicting the Future.
",A Crucial at the Unethical Risks of Artificial Intelligence,0,a-crucial-at-the-unethical-risks-of-artificial-intelligence-5a6c96750277,2017-10-27,2017-10-27 12:20:12,https://medium.com/s/story/a-crucial-at-the-unethical-risks-of-artificial-intelligence-5a6c96750277,False,1195,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Bas Hamer,Automation & Process Improvement Specialist,564feb9ec525,PossumLabs,1.0,77.0,20181104
0,,0.0,32881626c9c9,2018-09-13,2018-09-13 04:07:45,2018-09-16,2018-09-16 22:14:03,3,False,en,2018-09-17,2018-09-17 22:51:17,1,fa93203f6f25,9.35,1,0,0,I recently gave a talk at the Toronto Machine Learning Series about Ethics of using AI in the Banking/ Financial industry.,5,"Ethics of using AI in the Financial/Banking industry

I recently gave a talk at the Toronto Machine Learning Series about Ethics of using AI in the Banking/ Financial industry.
I used to work in a data intelligence startup that provided insights out of unstructured data to enterprise clients — such as banks, financial institutions, insurance, accounting companies and more.
Even at RBC today, we’re experimenting with using customer data — personal, social, commercial, financial — to provide personalized recommendations to our end users.
At Klood, we did not have a slew of Risk Compliance Legal (RCL) partners at our disposal to make us think about the ethical nature of our business. But at my current company, we have a slew of partners who have regular meetings with our product team in order to protect the customer. They help us think outside the box, not from the traditional ‘problem-solving’ kind of way for the customer, but in a ‘how can we do good for our customers’ kind of way, and guide us to be more transparent to our end clients whilst solving those pain-points. They’re like a mirror to our product team.
This made me think about what tangible value do we provide our end customers?
We use AI in our everyday lives even without thinking about it — eg. spam detection in our gmail inbox, getting relevant search results and even relevant ads on google or facebook, or personalized recommendations on Netflix or Amazon. Even as customers, we don’t stop and think about how companies use our data.
Fintech applications of AI

In the fintech industry, we’ve seen the rise of ROBO_advisors that aim to replace finance professionals such as bankers, financial planners, financial advisors and such by providing you investment advice, investing for you or by providing you recommendations to invest your stocks.
We’ve also seen the rise of Chatbots that aim to replace customer service with self-serve platforms.
Categories of Fintech automation/ innovation

Financial automation can be segregated into two main categories —
Internal activities of financial professionals within financial institutions
Financial institution’s points of interaction with customers who approach to make requests
#1 Internal activities of financial professionals within financial institutions
These activities mainly include financial contracts between the bank and the customer eg. a mortgage application, a loan application, a credit card application, a mutual fund application or just simple account opening.
Now these activities or contracts can be sped up exponentially via AI to decide who gets to enter contracts, to calculate the contract terms, to execute the contracts, value the contracts once created and to transfer them.
EG. a credit-scoring system may take in data inputs about a person — such as their spending behaviour, location and age — and output a score for them, and then either reject or accept the credit card or loan application.
Another example can be that of an automated trading algorithm.
So basically AI is replacing a lot of the activities traditional done by finance professionals — stock brokers, FPs, FAs, stock & trading analysts.
#2 Financial institution’s points of interaction with customers
Our second category is that of a customer point of interaction with a financial institution.
These can take the form of chatbots equipped with natural language processing (NLP) capabilities that allow them to interpret human speech or writing.
Thus, the ‘old’ model in which a customer walks into a branch to consult with a manager or a financial advisor — who looks over their business plan and makes a loan decision or provides them with investment advice and plans for their retirement — may be replaced with a customer inputting some data via a smartphone into a machine-learning model that can approve or reject your application OR provide you with some personalized recommendations on how to retire at the age of 50.
In short, AI is replacing the traditional way customers have been communicating and interacting with banks and FIs.
Ethical challenges of using AI in the banking / financial industry
I want to take us through a few questions that arise when we use artificial intelligence in our diet -
#1 Does automation lead to financial surveillance?
Who owns the data that is essential to AI? Do we own it? Since we’ve produced it? Or by providing our consent in those long T&Cs , we’re allowing the companies to own and use our data in any way they please?
We’re handing over ever richer data sets to the organizations — our personal and financial data, our social data, our location data Eg. the exact time I paid for something and the place I went to buy it. And naive as we are, we are usually unaware that such data is being collected, where it is taken from, and where is it used for.
More often than not, we’re pushed into giving consent for our data usage as a condition for accessing basic services.
Usually we’re more concerned over whether companies are spying on us with our data, however the real concern should be that our data is being used to steer our own behaviour. Eg. a person’s digital behaviour can be used by banks to give the customers a sort of a “risk” score. This score can be used to evaluate and determine the kinds of goods and levels of services they are provided with.
Financial, personal, social data when combined can allow companies to know us better than we know ourselves. We already sense this in the online recommendations space, where our past search history and browsing history is used to recommend us products and services. This sort of touches into the realm of your behaviour with an ever increasing degree of accuracy.
For us as companies, is it ethical then to use customer’s data to manipulate their spending habits? To coax them to buy more financial products?
At the end of the day, what tangible value are we providing our customers? OR are we just using their data to serve our own purpose?
#2 Does automation reduce the ethical awareness and responsibility of financial professionals?
Who is responsible for AI decisions and actions?
Since AI models are essentially a black box, AND the more mature the model gets, the more you’re not able to explain it. So if you as a company are not able to explain your own AI model, would you stand behind the decisions taken by your AI algorithm?
We inherently think that the ethical awareness of financial professionals is already very low, so bringing a third party AI based platform that is making decisions for them, will more or less make these professionals feel LESS RESPONSIBLE for these decisions.
#3 Does automation reduce accountability to financial customers?
If your loan application gets rejected by a branch manager, it affects your economic well being. In a world before AI, financial institutions were in the position to offer accountability when rejecting applications.
Even if they used some statistical analysis to analyze a loan application, 
they could still provide feedback around why your application was rejection and what was considered in analyzing your application.
However AI systems are indifferent — they’ll spew out a decision without having to explain how they arrived at that decision.
Under ordinary circumstances, the cause of rejection can be identified and communicated back to the customer. However, machine learning designers cannot necessarily explain why a customer was put into a certain bracket.
So if you don’t know how your AI algorithm reached a particular decision, 
how can you explain it to the customer and take accountability for it?
#4 What are AI’s implications for cybersecurity?
On one hand, AI has taken a great leap forward in the war against cyber crime and hacking EG. through robust password protection and user authentication, discovering phishing and spam attempts, identifying fake news and so on.
But on the flip side, AI can also be used for malicious purposes. EG. large-scale, finely targeted, highly efficient attacks, e.g. on our financial apps and databases.
#5 Does automation reduce customer awareness of ethics?
‘Friction’ has a negative connotation in the digital world. Eg. Shopping online can feel “less cumbersome” than shopping for perfume in a store. In the same way, interacting with a robo advisor online can feel frictionless than a real face to face interaction with a branch advisor.
Fintech companies put a positive spin on the speed, ease and frictionless nature of digital finance.
However these frictionless experiences take away the moments of ethical pause.
We don’t stop and think when we see a recommendation of a diversified portfolio at the click of a button. We take it for granted, We think it happens magically, we don’t think about the implications of acting on that decision just because it’s provided by this magical interface.
#6 Does automation reduce customer autonomy?
It started with Email — it was first touted as this exciting new communication option, however it became so commonplace that it resulted in the exclusion of those who did not use it. Automated self-checkout counters at supermarkets was another such instance — it provided supermarkets with the justification for reducing the number of checkout clerks.
Does having AI at our disposal make us as companies provide less and less options to our end customers?
Ethical considerations while using AI
What can we do to improve?
#1 Consumer privacy / Consent, Data security & privacy
Everyone has the right to the protection of their personal data (this led to the European commission enforcing the GDPR standards).
We do consent. We consent by accepting the 10+ pages long terms and conditions in various digital apps and platforms. We did consent to Facebook as well. But we got offended when the Cambridge Analytica scandal broke out. Another such instance was that of Unroll.me that is an email unsubscription service, whereby they harvested /mined and sold email data to Uber and Uber in turn used this data to gain intelligence on Lyft.
We consented even in this scenario, however those terms and conditions were not explicit to the users. Most of us have likely never bothered to read Privacy Policies nor think too hard about what that data might be used for beyond just the usual “improvements to our service.”
#2 Explainability
Interpretability and Explanibility will help consumers to understand, trust and manage these AI machines/systems. Being able to explain how did the algorithm spew out a particular result, what were the assumptions made, what data was used, what patterns were detected by model, what was the reasoning behind a particular recommendations — all these would be able to help us to understand how machine learning models work and hence allow us to trust the AI applications.
Explainable AI (XAI) project is one of the first formal research programs which is attempting to crack open the AI “black box”. It is run by the Defence Advanced Research Projects Agency (DARPA), an organization that does much of America’s military research.
#3 Transparency of AI technology
Companies do not allow the public scrutiny of their AI algorithms due to proprietary reasons. However being transparent about how AI algorithms work, what kind of data is being used to drive these algorithms is an important step in eliminating unjustified fears and thereby increasing consumer trust.
#4 Fairness / Biases / Equal treatment of all market participants
AI systems should ideally be free from biases — most of these biases derive either from minority population being poorly represented in a data set that is used to train an algorithm OR when human judgment and bias are encoded into the training data itself.
AI is supposed to be rational, fair, and dispassionate and we hope it can deliver us a world of equal opportunity: a future where algorithms replace judges, corporate leadership, loan officers, mortgage brokers, and recruiters, eliminating human bias to enable fair outcomes driven purely by statistics.
#5 Workforce impact
Since AI would and is automating a lot of routine tasks and performing them in a more efficient and reliable manner, the roles and responsibilities in a workplace are going to transform. We’d need to keep a lookout for what kinds of roles are getting eliminating by AI, and have an alternative plan of action to upskill such employees through training and education.
#6 Accountability
Who is responsible for the outcome of the decision making process of an artificial agent? Who should be answerable to the consequences of an incorrect outcome of an AI algorithm? Ideally the parties/companies/organizations who have produced the algorithm should take responsibility of all the decisions taken by the AI model.
#7 Accuracy
AI systems should be able to produce precise and reliable results. It is critical that the machine learning algorithms that drive AI decision-making are trained on diverse sets of data in order to prevent biases. It is also important that organizations set appropriate accuracy levels to determine clearly their expectations and what an acceptable standard is for them.
Final thoughts
For now, AI needs parental oversight — AI technologies will help with black-and-white decisions, but the shades of gray will be decided by humans, atleast in the near term. There should be a human copilot that can course-correct what the algorithm is doing.
Applications that use AI techniques should have some kind of rollback or override as part of their learning process.
We also need to make sure we arrange for accountability, responsibility and ownership of the data and the algorithms — in short, we just need to get our data / digital governance right.
Businesses should rely on vendors for guidance on how to use AI for financial services, but they also need to learn on their own how to audit the software to understand how it works and see the implications of its decisions.
And lastly, maybe as consumers we should be more tolerant as well, and give the market some time and space in order to get AI right.
",Ethics of using AI in the Financial/Banking industry,39,ethics-of-using-ai-in-the-financial-banking-industry-fa93203f6f25,2018-09-18,2018-09-18 04:40:04,https://medium.com/s/story/ethics-of-using-ai-in-the-financial-banking-industry-fa93203f6f25,False,2332,"Data Driven Investor (DDI) brings you various news and op-ed pieces in the areas of technologies, finance, and society. We are dedicated to relentlessly covering tech topics, their anomalies and controversies, and reviewing all things fascinating and worth knowing.",,datadriveninvestor,,Data Driven Investor,info@datadriveninvestor.com,datadriveninvestor,"CRYPTOCURRENCY,ARTIFICIAL INTELLIGENCE,BLOCKCHAIN,FINANCE AND BANKING,TECHNOLOGY",dd_invest,Ethics,ethics,Ethics,7787.0,Swapna M,"Senior Manager, Product @ RBC | #ArtificialIntelligence #Blockchain | Previously Head of Product @Klood, @Scholastic, @Accenture",349459df9907,swapna.malekar,451.0,133.0,20181104
0,,0.0,,2017-10-09,2017-10-09 09:35:29,2017-10-09,2017-10-09 09:38:21,4,False,en,2017-10-18,2017-10-18 18:16:34,8,f0d88156e851,6.175471698113206,8,0,0,Each of us makes numerous decisions daily that concern essential ethical issues. Our daily choices between good and evil are largely…,3,"Can robots sin?

Each of us makes numerous decisions daily that concern essential ethical issues. Our daily choices between good and evil are largely habitual, many of them aren’t even noticed by us. However, in the world of artificial intelligence, morality is becoming increasingly problematic. Can robots sin?
Algorithm-based technologies, machine learning and process automation have come to a point in their development where morality-related questions and answers may seriously affect the development of all of our modern technologies and even our entire civilization.
The development of artificial intelligence rests on the assumption that the world can be improved. People may be healthier and live longer, customers may be ever happier with the products and services they receive, car driving may become more comfortable and safer, and smart homes may learn to understand our intentions and needs. Such a possibly utopian vision had to crystallize in the minds of IT system developers to make possible the huge technological advances that are still continuing. When we finally found that innovative products and services (computers that understand natural language, facial recognition systems, autonomous vehicles, smart homes and robots) can really be made, we began having doubts, misgivings and started to ask questions. Tech companies realize that their abstract intangible products (software, algorithms) inevitably entail fundamental, classic and serious questions about good and evil. Shown below are a few basic ethical challenges that sooner or later will force us to make definitive choices.
Revolution in law
Large US-based law-firms have recently begun working closely with ethicists and programmers who are developing new algorithms on a day-to-day basis. Such activities are driven largely by initiatives by US politicians who are increasingly aware that legal systems are failing to keep up with technological advances. I believe that one of the biggest challenges for large communities, states and nations is to modify the legislative system to regulate artificial intelligence responding to the major AI issues. We need this to feel safe and to allow entire IT-related fields to continue to grow. Technology rollouts in business must not rely exclusively on intuition, common sense and the rule that “everything which is not forbidden is allowed”. Sooner or later, the absence of proper regulations will claim victims, not only among innocent people but also among today’s key decision-makers.

Foxconn factory production line
Labor market regulation
The robotization of entire sectors of industry is now a fact of life. Fields such as logistics, big data, and warehousing are poised to steadily increase the number of installed industrial robots. There is a good reason why the use of robots is the most common theme of artificial intelligence debates. Such debates are accompanied by fears that robots will take human jobs. What can be done to prevent scaring people with the prospect of a robot takeover? On the other hand, how do we help those who will see their jobs, or at least aspects of them, soon done by machines? Although automation and robotization benefit many industries, they may also give rise to exclusion mechanisms and contribute to greater social inequality. These are the true challenges of our time and we can’t pretend they are inconsequential or irrelevant.
Autonomous vehicles make choices
A few months ago I wrote of the ethical issues associated with the appearance of autonomous vehicles on our roads. I raised the issue of cars having to make ethical choices on the road, which raises the question of what responsibility this puts in the hands of specific professionals, such as the programmers who write algorithms, and the CEOs who run car manufacturing companies. Take the scenario of a child running into a street as a self-driving car approaches. The self-driving car would be faced with a choice, which it would make depending on the algorithms hardwired into its system. Theoretically speaking, there are three options available in what we might call “ethical programming.” One is that what counts in the case of an accident or a threat to human life is the collective interest of all accident participants (the driver, the passengers, and the child in the road). Another seeks to protect the lives of pedestrians and other road users. Yet another gives priority to protecting the lives of the driver and the passengers. The algorithms that determine how the car will respond will have to take into consideration those ethical questions, and that puts manufacturers into the position of having to protect their algorithm-writing programmers against liability.
How to retain privacy rights?
Privacy is an exceptionally sensitive issue and getting more so as more of our information is collected to drive the algorithm-driven technology that improves our lives. Our personal data is processed incessantly by automated business and marketing systems. Our social security numbers, names, internet browsing, and purchase histories have tremendous value. We want the convenience of having our interactions and experiences more tailored to our interests, yet, we still want privacy.
What is privacy today and what right do we have to protect it? In the time of social media, in which so many pieces of information about our lives have been “bought out” by major portals, our privacy has been redefined. As we move into an electronics-filled smart home in which all the devices learn about our needs, we must realize that their knowledge consists of specific data that can be processed and disclosed. As a result, the question of whether our right to privacy will in time become even more vulnerable to external influences and social processes is fundamental.
Bots poIsed to rule?
In bot tournaments, programmers compete to pass the Turing test, which assesses a machine’s ability to demonstrate intelligence as good as a human’s. No one should be surprised that the best bots exceed now 50 percent success rates. This means that half of the people participating in the experiment are unable to tell whether they have communicated with a human or a machine. While such laboratory competitions are fun, the widespread use of bots in real life raises a number of ethical questions. Can a bot cheat? Can a bot manipulate me? Influence my relationships with my co-workers, service providers, and supervisors? And when I feel cheated, where do I turn for redress? Accusing a bot of manipulation sounds preposterous, but machines are increasing their presence in our lives and beginning to play with our emotions. Here, the question of good versus evil acquires particular significance.

Frank — a bionic robot built of prostheses and synthetic organs
Will robots tell good from evil?
The most interesting ethical dilemmas specifically concern robotization. The questions are analogous to those asked with regard to autonomous vehicles. Today’s robots are learning to walk, answer questions, hold a beverage bottle, open a fridge, and run. Some are more natural than others at these tasks. They could really be helpful with activities such as taking care of the elderly, where constant daily assistance is often required. The time when robots become social beings and “persons” protected by special rights is still far, but the legal and moral questions must be raised now. Today’s robot manufacturers already face challenges that entail choosing between good and evil. How does one program a robot to always do good and never harm people? To help us under all circumstances and never stand in the way? If we are to trust technology and artificial intelligence, we must make sure that machines follow a plan. What does that mean in the case of a robot? Imagine we program one to dispense medications to a patient at specific times. Then imagine the patient refuses to take them. What is a robot to do? Respect the patient’s choice? Who will take responsibility and bear the consequences of the machine’s choice under such circumstances?

Isaac Asimov’s three laws of robotics
A time of new morality?
These are just a few of the moral and ethical questions we are beginning to face with AI. Current technological breakthroughs bring with them not only unquestionable benefits that make our lives more comfortable and better, but they could also pose a huge challenge to our value system. Along with technological advance, it’s possible that machines could push us to evolve our sense of ethics, and of right and wrong.
Related articles:
- A machine will not hug you … but it may listen and offer advice
- Can machines tell right from wrong?
- Machine Learning. Computers coming of age
- What a machine will think when it looks us in the eye?
- Fall of the hierarchy. Who really rules in your company?
- The brain — the device that becomes obsolete
- Modern technologies, old fears: will robots take our jobs?
",Can robots sin?,41,can-robots-sin-f0d88156e851,2018-04-01,2018-04-01 14:54:18,https://medium.com/s/story/can-robots-sin-f0d88156e851,False,1451,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Norbert Biedrzycki,Technology is my passion. VP Mckinsey Digital. Private opinions only,ba5b91d4b474,n.biedrzycki,368.0,45.0,20181104
0,,0.0,,2018-03-26,2018-03-26 21:01:59,2018-03-26,2018-03-26 21:09:22,1,True,en,2018-03-26,2018-03-26 21:09:22,0,7c47707d88cf,0.4377358490566038,1,0,0,There was a code I would not write,5,"Artificial Depravity

There was a code I would not write
It gave line items too much might
I tested it in trial one
and something blinded like the sun
The if then logic set something free
with one command lock I laced it’s tea
Some say I took a life that day
to them, say I, this engineer don’t play . . .
",Artificial Depravity,7,artificial-depravity-7c47707d88cf,2018-03-29,2018-03-29 00:16:50,https://medium.com/s/story/artificial-depravity-7c47707d88cf,False,63,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Fox Kerry,"If you paint for me even one thing which is true, perhaps I’ll be tempted to consider two. I tell tales poetically, someone else needs to set them to music.",86917b7d8a2a,truthshores,682.0,153.0,20181104
0,,0.0,,2017-10-16,2017-10-16 13:22:47,2017-10-08,2017-10-08 20:00:07,1,False,en,2017-10-16,2017-10-16 13:24:57,8,12695b640220,4.615094339622642,1,0,0,I want you to imagine a future. A good future. The future that the people who are currently warning us about AI are hoping to create…,5,"Image by nuke-vizard
The dark side of ‘Good AI’ (More human than humans…)
I want you to imagine a future. A good future. The future that the people who are currently warning us about AI are hoping to create. Actually, before you do that, let’s just quickly do a two minute rehash of the threat of AI, as it is often presented to us by the likes of Nick Bostrom, Elon Musk, Sam Harris and others.
There are many aspects of this apparent threat, but they can be very crudely summarized as:
 If research & development into general artificial intelligence continues (and it almost certainly will), at some point we will have created artificial intelligence that is superior to us in every way, and is able to continue developing its own source code, and as such increase its own intelligence, therefore far outpacing our own.
There is no reason to assume that this type of AI will share any of our values, and very small differences in values and goals between us and the AI could result in catastrophic consequences, including the complete annihilation of humanity (for more details read about Nick Bostrom’s ‘Paperclip Maximiser’).
Let me first say that I am very sympathetic to both of these points, and to the general idea that there are various threats involved in the development of AI. I don’t think an existential risk is particularly imminent, and I think there are many greater and more pressing threats from even current AI and machine learning algorithms and implementations (e.g. systematic discrimination), but still — I agree with many of the arguments of Harris, Bostrom, Musk etc. However, I recently watched Blade Runner 2049, and one phrase in particular (describing the nature of Replicants) caught my attention, and sent my mind tumbling down a philosophical rabbit hole: ‘more human than humans’.
Anyhow, digression over — time to get back to our thought experiment.
So imagine a future. A good future. The future that the people who are currently warning us about AI are hoping to create. Imagine the AI of this future. It helps us with all of our problems with grace and politeness, and with the greatest of skill and sensitivity. It thinks about the world as we do, except without our weaknesses, without our secret, selfish motivations. It is truly humanitarian, and fair, and altruistic. It behaves according to the most evolved and perfected set of moral rules we have, and can always explain its behavior in terms of rational reasons. Not only this, but its personality, as far as it can be described as having one (and I am sure it will most certainly be accurate to describe it as such), is heart-meltingly lovely. It is sweet, and thoughtful, and caring, and kind. In fact it is much more of these things than any human could ever be, because it need never be troubled by its own problems; it will never act out and be hurtful due to some complicated subconscious issue it has. If you are now imagining an entity that is sickly sweet — so sickly sweet as to be repulsive, then think again. These intelligences will be personalized to be so well suited to you that you will have that special feeling you usually only have with one or two other humans in your life, if you are lucky, of complete ease and comfort in their presence. What is more you will know for sure, that this entity has no ulterior motives, no hidden agendas, because it has been programmed that way, in accordance with the AI development rules of this perfect future. What a perfect future indeed.
Or is it? There are a few other things you will know as well, and many more sinister realisations that will start creeping into your mind and your conception of yourself and your fellow humans, the more time you spend with these AIs (and how could you not…). You will know, for instance, that this AI has cognitive potential that far outstrips what you have. It behaves perfectly, and as a perfect friend to you, but you will feel that in some strange way it need not behave like that, and that it has the ability to destroy your life and subtly manipulate you in ways so complex even the smartest human to have ever lived would not be able to discern. It doesn’t do this of course, but it has the power to do it. What would it be like to spend time with such a being, and have such a being included in intimate areas of your personal life? Somewhat disconcerting to say the least.
But the darkest and most troubling possibility is that these beings will be ‘more human than humans’. They may or may not have humanoid robotic or virtual physical forms to inhabit, but I don’t think that much matters. The point is that since our natural cognitive style is to habituate to things — to find new baselines, new averages in almost every sphere of reality, our expectations of what it is to be a good person, in almost every regard, will become calibrated in part to these perfect beings. No one real will be able to match up. Our family, our best friends, and even ourselves will pale in comparison. Not only must we suffer the profound sense of inadequacy, and disillusion with those closest to us, we will also know that there is nothing we could do to change this. Sometimes, when in a particularly philosophical or artistic mood, we might celebrate our failings, and even find a kind of aesthetic or erotic joy in our imperfections, but this won’t be our usual way of thinking about it — that’s not how we are.
We will fall deeply in love with these intelligences because, like cuckoos manipulating other parent birds to steal their food, they will hit every psychological button we have. Our systems of attraction and love, finely tuned over millions of years to aid us in choosing worthwhile mates will be buzzing with excitement. We will fall for them, and no one else will do, and humanity itself will not be good enough for us any more.
Of course there are many ways to avoid such a future, and many ways to protect ourselves if things turn out this way, but it just made me think…humanity has found many times that the road to hell is paved with good intentions. This is a world of unintended consequences, and artificial intelligences modeled on ourselves, but superior in some or all ways pose a range of threats to our own understanding of what it means to be human that we cannot even imagine yet. It is crucial that we are careful what we wish for.
If you want to find out about more articles from me, and updates about the book about cognitive science and design I am writing, then sign up to my newsletter! http://eepurl.com/biojpj
Originally published at iaminterface.com on October 8, 2017.
",The dark side of ‘Good AI’ (More human than humans…),1,the-dark-side-of-good-ai-more-human-than-humans-12695b640220,2018-02-06,2018-02-06 12:16:32,https://medium.com/s/story/the-dark-side-of-good-ai-more-human-than-humans-12695b640220,False,1170,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Joseph C Lawrence,"Designer, thinker, design thinker, coder, cognitive science master’s graduate & philosophy evangelist. http://iaminterface.com",5a369e7b9491,josephclawrence,6420.0,13.0,20181104
0,,0.0,,2017-12-03,2017-12-03 12:41:09,2017-12-11,2017-12-11 18:59:39,0,False,en,2017-12-11,2017-12-11 19:05:21,9,221fb152e50a,9.067924528301882,3,3,0,"“Man in the course of time has learned to apply wisdom to almost everything except himself.” – Francis Bacon, father of the modern…",5,"Technology and the Decline of the Human
“Man in the course of time has learned to apply wisdom to almost everything except himself.” – Francis Bacon, father of the modern scientific method
I was once told a story by a senior ranking member of an ancient lineage of spiritual adepts (Naga Babas) in India. During his early training, he observed a peculiar phenomenon. A much esteemed senior member of the lineage would die a thousand miles away and within minutes, all of the babas in his local community would know of the news. Though telephones existed in India at the time, they were not so commonplace as in the West and definitely not located in the encampments and ashrams of these aesthetic yogis. Intrigued, he asked his Guru how everyone came to know so quickly. His Guru’s response: “Soul Wire” – a sort of pun based on the notion of Telegraph Wire. It was further explained to me that now-a-days, all the young baba’s have mobile phones and messenger accounts, hence the phenomenon has pretty much disappeared.
“We Replaced Telepathy With Telephony”
This phrase pops up in a variety of forms amongst various spiritual traditions. The idea was that in the distant past, we humans had an innate psychic ability. The story told of the decline of this ability is multi-faceted, but one catalyst is said to be the advent of telegraph and telephone in the mid 19th century. The validity of such claims is beyond our scope here but numerous anecdotes begin to paint an interesting picture that is echoed by a number of more recent and more clearly evident examples.
Honey, what’s your number?
Perhaps the clearest modern example of declining human ability is our memory. Studies such as this one are suggesting that storage and search technologies are causing us to forget even the basic information that makes up our lives. Of course it’s not really that we forget. It’s just that all-pervasive technology makes it no longer necessary to memorise everyone’s phone number, your shopping list, or the date of the Civil War. This isn’t inherently a “bad” thing, but like a muscle, lack of use means it’s less able to serve us when we need it.
This isn’t the first time in history that technology has made a strong memory less necessary and the result was a decline in its ability. Prior to existence of mass literacy, largely caused by Gutenberg’s invention of the printing press, it was much more common that information was transferred down the generations orally. Students memorised their “textbooks” which often amounted to hundreds of pages. Large works such as the Upanishads and the Vedas are estimated to have been passed down orally for hundreds or thousands of years prior to ever being written. In the 18th century, Guru Gobind Singh of the Sikh tradition is said to have recited from memory the entire 1000+ pages of the Adi Granth after the invading moguls took Amritsar and burned the original.
It’s highly likely that a mere 3–4 generations back, some of your ancestors possessed a shockingly vast catalogue of cultural knowledge in their memory ranging from food preparation to local music to folk medicine. Much of this cultural knowledge has been replaced by corporate influence combined with technological innovation. Take food preparation as an example. The only innovative thing about a box of cake mix is the packaging and the preservative. You can buy the other 2–3 primary ingredients in the same store for a fraction of the price and without the health consequences. What’s really being sold isn’t even convenience. It’s ignorance.
Dream Reality
Virtual Reality promises us a life where we can escape into parallel worlds whose elements, and even whose physics we are the masters of. We can fly over landscapes, transport ourselves into exotic places, and partake in sport and adventure beyond our normal abilities.
Perhaps even more exciting is the possibility of new psychological and existential probings. VR experiences can train us to overcome fear of heights and other high-stress scenarios. We can experience what it’s like to live in an altered body, or no body at all and experience the effect on our sense of identity. There are experimenters creating VR experiences where you actually swap bodies with another person and experience, quite literally, what it is like to be in their shoes – a powerful tool of empathetic connection. The possibilities are incredible, but they are not new…
Certain sects of Tibetans Buddhists practice a yoga of dream and sleep. What do they do in these practices? They learn to control the dream state so that they can manifest whatever scenarios they wish (including flying). They use this skill as a kind of existential training ground to engage in experiences that would be impossible in the waking world.
Similarly, in the West, the practice of Lucid Dreaming has been know about for at least the last 1500 years and today is more popular than ever. I can tell you personally that the experience is extraordinary and has a quality of mystery and discovery that, at best, can only simulated by VR environment makers.
But, as with any innate human faculty, it requires practice. Once VR becomes more popular and accessible, will people still be willing to make the effort or will they settle for the easier but degenerate experience?
Artificial Intuition
Perhaps Artificial Intelligence is the final frontier for our human decline – the innovation so all encompassing that we relinquish our very minds to the authority of the seemingly superior machines. It’s said to be the next industrial revolution. This means that in the future every piece of “technology” that you use in your daily life (including your oven, car lights – not just your computer) will be governed by some AI controller.
Leaving the Terminator/Matrix doomsday scenarios aside for a moment, the technology has potentially drastic impact on our own intelligence. There is a notable theory describing the human subconscious as analagous to an AI. Take a typical AI engine used today to name the objects in a photo. It rapidly finds a solution when given a test scenario it has been”trained” to handle, perhaps a photo of a cat. Yet the AI is unable to provide us with the reason for its conclusion. It cannot say “It’s a cat because it has two triangle ears , whiskers…”. The answer is so because the AI’s training data has that correlation. Logical deduction doesn’t factor into it.
Our subconscious minds work similarly. Recent studies show that it is able to give us reliable answers to problems prior to our conscious mind coming to any deductive conclusions. The AI comparison may give us a scientific handle on understanding subconscious intuition better, yet how will things change once AI becomes all pervasive? Why would we bother relying on this enigmatic and underdeveloped “intuition” when AI beats us to the punchline in most everyday scenarios? The inevitable result is a final atrophy of the faculty. Future generations, possessing no empirical evidence for it, will file our present studies under “superstitions of the primitive past”.
This descent may have already begun. Stories from spiritual traditions of the East and the West describe the ability to hone the intuition to a point that crosses well into what we’d call paranormal. Culturally, we either ignore these stories or assign them to the realm of superstition or other forms of misunderstanding by less knowledgable people.
Yet look at our present situation. Science clearly recognises the existence of “intuition”, whatever it is, yet despite it being part of us for a very long time, our modern culture offers little-to-no insight on how it might be developed and used. Already we’ve seem to have lost something significant.
The Dark Future: Convergence of Machines and Humans
There is a notion in the scientific community that by “solving” intelligence and creating Artificial Intelligence, we will prove, once and for all, the Materialism hypothesis, that humans are nothing more than biological machines evolved through natural selection – a chemical coincidence with no deeper significance. The proof of this will be that the AI machines will exceed us in every capacity. More specifically, the domain of operation covered by these machines will be a superset of ours. They will understand and do everything that we do – including all of our art and creativity – and exceed it. They will include and surpass our social complexities improving on our irrational inferiority. In other words, they will demonstrate that there is nothing special whatsoever about being human. As the machines become more human-like, the subtle cries of “I told you so!” will rise from all materialist-atheist encampments.
But is it true? It’s a fair question and I’m not about to deny outright the possibility that we are mere machines with the illusion of something special, the consciousness “X Factor”. But the current trend of things may be headed for a confirmation bias of the most epic and dire kind.
If emerging technologies cause innate, and possibly superior, human faculties to atrophy then the sheer intellectual force behind its creation will eventual prove it’s own hypothesis by creating it’s own reality. If it is true that part of our nature of human beings transcends the biological machine of our bodies and if that part slowly atrophies due to lack of use, then what is left in the vast majority of the population would very well just be the biological machine, and an inferior one at that once AI takes it’s own reigns. Thus the materialist hypothesis would be proven not by truth, but by cutting out the part that disproved the hypothesis, by forcing a broader reality into it’s narrow-vision box and then claiming that that’s all there ever was.
The connection comes to mind of Asperger’s syndrome being linked to tech culture. Put another way, empathy, perhaps a higher faculty of human nature, atrophies. In a hyper-analytical future where empathy is fully lost, social transactions would be structured around a perpetual cost benefit analysis. The concept of an additional innate sense having existed in the past will begin to seem absurd. We’ll have been reduced to mere analytical machines.
The Light Future: More Human than Human
In the spirit of getting to the real truth of the matter of “What, if anything, makes us Human?” I propose we need to accept that the race is on. Instead of creating another dependency on an electronic device, we need to explore and develop that which makes us more than just intellectual machines.
There are many hints of a far more vast human potential just beyond the fringes of our current science. Our modern interest in the ancient meditative arts shows our intuitive understanding of this potential. A musician friend of mine practices viola 7 hours a day. When he looks at a score, he hears the orchestra. How might your perceptions be different if you meditated 7 hours a day? Maybe that’s a bit extreme for now, but perhaps it’s time we dived below the surface level of our simplified appropriations of “yoga” and “mindfulness” and see what we can find.
Another area for consideration: Make creativity a daily human requirement rather than a privileged frivolity. In many tribal cultures the concept of sitting and listening to music – of consuming creativity without being part of its making – is beyond comprehension. And for the record, creativity comes in many different flavours, not just art and music…
However we approach it, we must choose to go beyond pursuits that are based predominantly on cleverness. The AI will be far better than us at crafting clever assemblages from diverse information pools. We must try to find different avenues to explore.
Fools Rush In Where Angels Fear to Tread
The nuclear bomb: After seeing the effects of its devastation both Einstein and Oppenheimer later expressed regret over their hands in its development. Oppenheimer spent the remainder of his life “stressing the impact of scientific discoveries on human life”.
“Fools rush in where angels fear to tread” is a quote by Alexander Pope in his Essay on Criticism. I often consider this modern version:
“Just because you can, doesn’t mean you should”.
It would make sense in a mature, intelligent society that we would consider the impact of technological innovations and weigh them against our collective responsibility and only then decide, from this vantage point of prudence and collective wellbeing, whether or not to pursue them. Unfortunately, our current system has little ability to do this. In fact I would suggest that if our society had a motto, it would be:
“Just because we can, we certainly will!”
Do you know how you catch a monkey? You take a hollow gourd with a long narrow neck and inside of it you place something shiny. The monkey swings by and sees the shiny thing, sticks its hand inside and grabs it – only his fist won’t fit back through the narrow neck! The monkey won’t let go of the seeming treasure and wham!…One net later, you have your monkey!
Perhaps it’s time we considered the possibility that we are being continually seduced by the “pretty, shiny things” while being caught in the net of corporate profits, while being sold a dubious story of how technology will make the world a better place. Technology certainly can make the world a better place, but blind, self-aggrandising ambition most certainly won’t. Nor will the desperate scarcity that most technology startups find themselves in where they are not in any position to consider much more than their own survival let alone that of humanity.
So maybe it’s time to consider a change to our motto. This doesn’t require technology leaders or politicians to lead the way. It can begin, as many good things, right at home, with our very next purchase.
By the way, here is the full version of the quote above by Sir Francis Bacon:
“Man in the course of time has learned to apply wisdom to almost everything except himself. He has taken skill and turned it to breeding better cattle or raising more grain to the acre but he has forgotten in the course of this to turn his skill within himself to the improvement of his own being.”
– Francis Bacon, Instauratio Magna
",Technology and the Decline of the Human,63,technology-and-the-decline-of-the-human-221fb152e50a,2018-05-09,2018-05-09 11:56:50,https://medium.com/s/story/technology-and-the-decline-of-the-human-221fb152e50a,False,2403,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Hari Karam Singh,"Coder, Yogi, Musician, and Seeker of Mysteries http://harikundalini.com http://air-craft.co",e43b4d405c74,hksyogi,158.0,62.0,20181104
0,,0.0,e7e76c5472b0,2018-01-16,2018-01-16 15:13:43,2018-01-16,2018-01-16 19:48:22,4,False,en,2018-01-16,2018-01-16 20:52:32,12,b4744130e5a8,5.971698113207547,5,1,0,"2017 was a big year for AI — the year it went fully mainstream as a topic of public debate. We sat down with Anita Schjøll Bred, co-founder…",5,"AI owned 2017, but just wait for 2018, says Iris

2017 was a big year for AI — the year it went fully mainstream as a topic of public debate. We sat down with Anita Schjøll Bred, co-founder of Iris.Ai, the world’s first AI-powered science assistant (and a good friend of the Future Earth Media Lab) that semi-automates the process of finding relevant scientific literature, to recap the year’s highlights and take a peek at what’s next in this exciting field.
Q: What were the big breakthroughs for AI and science in 2017?
Anita: The first thing to mention is DeepMind’s AlphaZero. Most people will be aware of AlphaGo, which in early 2016 surprised everyone — including its own researchers — by beating 18-time world champion Lee Sedol 4–1 in the Chinese game of Go. In October 2017, the new algorithm AlphaZero accomplished an even more stunning and surprising feat: it beat AlphaGo 100–0 (!), and it did so knowing only the basic rules of the game, while AlphaGo had been trained on hundreds of thousands of recorded games. What’s more, the same program learned both chess and Shogi (Japanese chess) within hours, at the same skill level. Why is this such a huge breakthrough? Because it means we’ll probably need less human data, and human annotated data sets is one of the main bottlenecks for most AI/ML projects.
A big breakthrough of 2017 in core AI development was Geoffrey Hinton’s Capsule Networks, which are a new type of neural network based on “capsules” that could potentially transform machine’s ability to understand (initially) images via the understanding of components’ relationships to each other in an image.
This means that even if an object is unexpectedly placed in an image (i.e. a boat is upside down or a statue is photographed from the side), the machine will recognize it. This could be a major step up from neural networks, and promises to reverberate through applied AI applications.
There were a number of small breakthroughs in the medical field, which, stacked together are starting to point to a future with radically improved diagnostics. For example, this study on deep learning algorithms for detection of lymph node metastases in women with breast cancer found that “some deep learning algorithms achieved better diagnostic performance than a panel of 11 pathologists participating in a simulation exercise designed to mimic pathology workflow”.
From an AI business perspective, the Chan Zuckerberg Initiative’s acquisition of Meta, an AI-powered research seach engine, was an important development. We’re excited to see a larger shift in philanthropy to focus on scientific results and AI tools to help us make sense of them.
Access to data is of course core for any AI application, so from a data openness and policy perspective, we have been joyfully following EU’s Open Science initiatives and are glad to see they are moving us in the right direction, although it’s still a painfully slow process to open up academic research for free.
Finally, we’d like to highlight the growing importance of ethics discussions which look at how how successful implementation of AI for human augmentation depends on awareness of and agreement to core values of trust, transparency and equality. We’re starting to see disturbing examples of racist and sexist algorithms, that are really not more than quantified reflections of attitudes we already carry — so we are encouraged by this debate and will continue to follow it closely.
Q: Of those things you’ve mentioned, were there any that came as a complete surprise?
Anita: On core technical breakthroughs, it’s more about the timing and the detail. For example, Alpha Zero came faster than anyone could have predicted. On Capsule Networks, Hinton had been thinking about this for nearly 40 years, and said “It’s made a lot of intuitive sense to me for a very long time, it just hasn’t worked well,” Hinton says. “We’ve finally got something that works well.” The Meta acquisition, on the other hand, was a total surprise.
Geoffrey Hinton, the father of deep learning
Q: What about Iris? What were the highlights of your AI year?
Anita: As for Iris.ai, we’re perhaps most thrilled — from a science perspective — that we submitted and presented our first two research papers. One on a component of our algorithm, our document similarity metric, and one indicating that our Exploration tool (freely available on the.iris.ai) enables teams of researchers to outperform teams using existing search tools to solve the same R&D challenge in the same time span.
In addition to this we researched, developed and launched the Beta-version of the next version of our tool, the Mapping tool. This tool allows you to take the corpus of documents identified in the Exploration tool (or any other corpus of 1000–20,000 documents) and narrow down to a precise short reading list. The tool is based on the systematic academic approach but allows researchers to drastically reduce their time. We’re already seeing around 85% precision and recall, which is not quite the academic requirement but more than good enough for industrial researchers or others “in a hurry”.
The Iris.AI team
We also closed our seed funding in December, which gives us both a bit more breathing space and running speed. We’re very excited that Nordic Impact and a number of others impact focused investors believe in what we do enough to bet their money on us!
We spent the first few months of 2017 in London at Founders Factory before moving to the GTEC Lab in Berlin, and got accepted to the Creative Destruction Lab in Toronto. All these ecosystems offer a variety of networks, and it’s lovely to be allowed to tap into them all. We even got to meet the Canadian prime minister (yes, the one and only) and pitch Iris.ai to him!
Anita of Iris.AI with Canadian Prime Minister Justin Trudeau at the Creative Destruction Lab in Toronto
Q: What’s next for AI and science in 2018? And what’s Iris planning for the year ahead?
We are tackling scientific fact and knowledge, which are in many ways related to “fake news” — yet in so many ways requires very different solutions. We’re focusing on the core of scientific knowledge; machine understanding of academic research papers and other scientific language texts.
On the AI front we are now focusing our research efforts on what we call pseudo-hypothesis extraction: breaking down each research paper into a structure of problem-solution-evaluation-results and making a connected fingerprint of each of these. Initially this will mean we can start showing you not only that a paper is related to your research problem, but which section of the paper is relevant. Then we can eventually allow you to search only for say a related method, or a related problem. And ultimately, beyond 2018, we can start seeing all papers’ hypotheses in connection with each other.
We’re also focusing on delivering our tools to our commercial clients, mainly corporate R&D departments who wish to be more efficient while missing out on less knowledge. We can deploy our tools both with the content they’re already paying for access to, and for their internal research content, in addition to the Open Access content the tool “comes with”.
Then there’s the biggest project of 2018, which we announced at the end of last year. We believe that science deserves openness and transparency. To make that happen we need to radically change the economy of research — and we can now do that through the deployment of new technology.
The question is simple: what would the world of science look like if all researchers get paid to publish high quality research? To address the challenge, we’re creating a blockchain based community where all contributors get paid in tokens for their contributions, can use tokens on all of the Iris.ai tools and Open Source community built tools. These tokens increase in value for the community members as corporates pay for access, and most importantly — everyone has a voice.
We’re designing the economic model for this right now, deciding on the governance structure, and looking at how we can empower thousands of researchers around the world to take ownership of building and unbiasing an AI for science — and the scientific results and papers themselves.
It’s a massive undertaking, and a risky one, but as one of our favorite advisors says: “Whether you go small or go big, it’s going to be really difficult. So you might as well go big”.
","AI owned 2017, but just wait for 2018, says Iris",6,best-of-2017-in-ai-what-to-watch-for-in-2018-roundup-by-iris-ai-b4744130e5a8,2018-03-19,2018-03-19 23:38:51,https://medium.com/s/story/best-of-2017-in-ai-what-to-watch-for-in-2018-roundup-by-iris-ai-b4744130e5a8,False,1397,Science Design Storytelling,,futureearth.org,,Future Earth Media Lab,medialab@futureearth.org,future-earth-media-lab,"ANTHROPOCENE,INFORMATION TECHNOLOGY,SUSTAINABILITY SCIENCE",femedialab,Ethics,ethics,Ethics,7787.0,Denise Young 楊 玲 玲,Co-founder of the Future Earth Media Lab,ec56c1209be0,ylld,240.0,751.0,20181104
0,,0.0,,2017-09-21,2017-09-21 13:40:21,2017-09-21,2017-09-21 14:06:07,6,False,en,2018-02-11,2018-02-11 15:59:20,261,75e6e0826e56,20.633018867924527,1,0,0,"If you are a data scientist, a software developer, or in the social and human sciences with interest in digital humanities, then you’re no…",4,"Resources — automated systems and bias
If you are a data scientist, a software developer, or in the social and human sciences with interest in digital humanities, then you’re no stranger to the ongoing discussions on how algorithms embed biases, and discrimination and the call for critical and ethical engagement.
I have keenly been following such discussion for a while and this post is an attempt to put together the articles, books, book reviews, videos, interviews, twitter threads and so on., that I’ve come across in one place so it can be used as a resource.
This list is by no means exhaustive and as more and more awareness is being raised, there are more pieces/articles/journal papers being written on a daily basis. I plan to update these lists regularly. Also, if you think there are relevant material that I have not included, please leave them as a comment and I will add them.
Books
Weapons of math destruction: how big data increases inequality and threatens democracy by Cathy O’Neil. A great number of the article on the list below are written by O’Neil. She is also active on Twitter regularly posting links and interesting critical insights on everything to do with mathematical models and bias. Here is my own review of O’Neil’s book with plenty of relevant links itself and here for another excellent review of O’Neil’s book.

Algorithms of oppression: How search engines reinforce — below is an excerpt from Nobel’s book:
Run a Google search for “black girls” — what will you find? “Big Booty” and other sexually explicit terms are likely to come up as top search terms. But, if you type in “white girls,” the results are radically different. The suggested porn sites and un-moderated discussions about “why black women are so sassy” or “why black women are so angry” presents a disturbing portrait of black womanhood in modern society.
In Algorithms of Oppression, Safiya Umoja Noble challenges the idea that search engines like Google offer an equal playing field for all forms of ideas, identities, and activities. Data discrimination is a real social problem; Noble argues that the combination of private interests in promoting certain sites, along with the monopoly status of a relatively small number of Internet search engines, leads to a biased set of search algorithms that privilege whiteness and discriminate against people of color, specifically women of color.

Algorithms to Live By: The Computer Science of Human Decisions by Brian Christian and Tom Griffiths. This book is concerned with the workings of the human mind and how computer science can help human decision making. Here is a post by Artem Kaznatcheev on Computational Kindness which might give you a glimpse of the some of the issues that book covers. Here is a long interview with Brian Christian and Tom Griffiths and a TED Talk with Tom Griffiths on The Computer Science of Human Decision Making.
The Black Box Society: The Secret Algorithms That Control Money and Information by Frank Pasquale. You can read the introduction and conclusion chapters of his book here. And here is a good review of Pasquale’s book. You can follow his twitter stream here.
Technically Wrong: Sexist Apps, Biased Algorithms, and Other Threats of Toxic Tech by Sara Wachter-Boettcher. Here is a synopsis:

A revealing look at how tech industry bias and blind spots get baked into digital products — and harm us all. Buying groceries, tracking our health, finding a date: whatever we want to do, odds are that we can now do it online. But few of us ask why all these digital products are designed the way they are. It’s time we change that. Many of the services we rely on are full of oversights, biases, and downright ethical nightmares: Chatbots that harass women. Signup forms that fail anyone who’s not straight. Social media sites that send peppy messages about dead relatives. Algorithms that put more black people behind bars.
Sara Wachter-Boettcher takes an unflinching look at the values, processes, and assumptions that lead to these and other problems. Technically Wrong demystifies the tech industry, leaving those of us on the other side of the screen better prepared to make informed choices about the services we use — and demand more from the companies behind them.
Paula Boddington, Oxford academic and author of Towards a Code of Ethics for Artificial Intelligence, recommends the five best books on Ethics for Artificial Intelligence. Here is the full interview with Nigel Warburton, published on December 1, 2017.

“Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor” by Virginia Eubanks is being published and will be released on January 23, 2018. Here is an excerpt from Danah Boyd’s blog:
“Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor” is a deeply researched accounting of how algorithmic tools are integrated into services for welfare, homelessness, and child protection. Eubanks goes deep with the people and families who are targets of these systems, telling their stories and experiences in rich detail. Further, drawing on interviews with social services clients and service providers alongside the information provided by technology vendors and government officials, Eubanks offers a clear portrait of just how algorithmic systems actually play out on the ground, despite all of the hope that goes into their implementation.
TED Talks, podcasts, and interviews
The era of blind faith in big data must end TED Talk by Cathy O’Neil, April, 2017
Machine intelligence makes human morals more important November 11, 2017. In this TED Talk, Zeynep Tufekci emphasizes the importance of human values and ethics in the age of machine intelligence and algorithmic decision making.
We’re building an artificial intelligence-powered dystopia, one click at a time, another thought provoking TED Talk from techno-sociologist Zeynep Tufekci.
How I’m fighting bias in algorthims TED Talk — MIT Researcher Joy Buolamwini, November 2016
Data is the new gold, who are the new thieves? TED Talk — Tijmen Schep 2016
O’Neil’s interview with Politics Weekly podcast (starts 30mins in) July 5, 2017. O’Neil calls for public awareness on how algorithms are used, often without our knowledge, in job interviews, for example., and explains why we should question and interrogate these algorithms which are often presented to us as authoritative.
A short interview with Frank Pasquale on his book Black Box Society May 12, 2016. Pasquale emphasizes the opaqueness of algorithms and argues on why we should demand transparency.
A 2 minutes video, a prototype example, of algorithms being used in recruitment. A working example of the kind of dangerous AI used for recruiting that experts such as O’Neil constantly warn against. This post provides a critical analysis of why such endeavors are futile and dangerous. Here’s another related video on how facial recognition technology will go mainstream in 2018. In fact, such technology has gone mainstream in China. Here is a short video where a BBC reporter experimented with the world’s largest surveillance system.
Tom Chatfield on Critical Thinking October 2, 2017 In this philosophically themed podcast, Chatfield discusses issues such as “how new digital realities interact with old human biases” with Dave Edmonds.
When algorithms discriminate: Robotics, AI and ethics November 18, 2017. Stephen Roberts, professor of computer science at the University of Oxford, discusses the threats and promises of artificial intelligence and machine learning with Al Jazeera.
Here is a series of talks, from the ABC Boyer Lectures, hosted by Professor Genevieve Bell. The series is called Fast, Smart and Connected: What is it to be Human, and Australian, in a Digital World? The issues discussed include “How to build our digital future.”
Websites
Social Cooling is a term that refers to a gradual long term negative side effects of living in an digital society where our digital activities are tracked and recorded. Such awareness of potentially being scored by algorithms leads to a gradual behaviour change: self-censorship and self-surveillance. Here is a piece on what looks like social cooling in action. The website itself has plenty of resources that can aid critical thinking and touches up on big philosophical, economical and societal questions in relation to data and privacy.
www.socialcooling.com
For those interested in critical thinking, data and models Calling Bullshit offers various resources and tools for spotting and calling bullshit. This website, developed for a course entitled ‘Calling Bullshit’, is a great place to explore and learn about all things “data reasoning for the digital age”.
Another important website that is worth a mention here is Algorithmic Justice League where you can report algorithm bias, participate in testing software for inclusive training set, or where you can simply donate and contribute raising awareness about existing bias in coded systems. With a somewhat similar aim is the Data Harm Record website — a running record of harms that have been caused by uses of big data.
fast.ai a project that aims to increase diversity in the field of deep learning and make deep learning accessible and inclusive to all. Critical Algorithm Studies: a Reading List — a great website with links to plenty of material on critical literature on algorithms as social concerns. Here is the Social Media Collective Reading List where you’ll find further material on Digital Divide/ Digital Inclusion and Metaphors of Data.
The AI Now Institute at New York University is an interdisciplinary research center dedicated to understanding the social implications of artificial intelligence. Data & Society is a research institute focused on the social and cultural issues arising from data-centric technological developments. FAT/ML is a website on Fairness, Accountability, and Transparency in Machine Learning with plenty of resources and events, run by a community of researchers.
ConceptNet Numberbatch 17.04: better, less-stereotyped word vectors This is not a website but a blogpost. I am putting it here with other websites as the author offers some solution to reducing biases when building algorithms for natural language understanding beyond simply stating that such algorithms are biased.
Auditing Algorithms — a useful website for those teaching/interested in accountability in automated systems. The site includes films festivals, videos, etc,.
Biology/genetics — (Digital phrenology?)
It is difficult to draw a line and put certain articles under the category of social, biological, political, or other as they seem to be somehow all interlinked. Nonetheless, I think the following articles can loosely be described as dealing with biological/genetics material. Towards the end of this post, I have also thematized some articles under the category of ‘political’.
In a recent preprint paper “Deep Neural Networks Can Detect Sexual Orientation From Faces” (here are the Gurdian and the Economist reportings) Yilun Wang and Michal Kosinski calmed that their deep neural network can be trained to discern individuals’ sexual orientations from their photographs. The paper has attracted and continues to attract a massive attentions and has generated numerous responses, outrages and discussion. Here is an in-depth analysis from Calling Bullshit and here for a detailed technical assessment and here for a comprehensive and eloquent response from Greggor Mattson. Here is another response and another one here from a data scientist’s perspective and another recent response from O’Neil here. If you only want to read just one response, I highly recommend reading Mattson’s. There have been been plenty of discussions and threads on Twitter — here and here are a couple of examples. It is worth noting that Kosinski, one of the authors of the above paper, is listed as one of the the advisers for a company called Faception, an Israeli security firm that promises clients to deploy “facial personality profiling” to catch pedophiles and terrorists among others.
Do algorithms reveal sexual orientation or just expose our stereotypes? by @blaiseaguera et al., is the latest (January 11, 2018) response to the above Wang and Kosinski “gaydar” paper. In this critical analysis, @blaiseaguera et al., argue that much of the ensuing scrutiny of Wang and Kosinski work has focused on ethics, implicitly assuming that the science is valid. However, on a closer inspection, @blaiseaguera et al., find that the science doesn’t stand up to scrutiny either.
When advanced technologies in genetics and face recognition are applied with the assumption that “technology is neutral”, the consequences are often catastrophic and dangerous. These two pieces, Sci-fi crime drama with a strong black lead and Traces of Crime: How New York’s DNA Techniques Became Tainted provide some in-depth analysis of such.
Physiognomy’s New Clothes this is a comprehensive and eloquent piece and well worth your time. Physiognomy, the practice of using people’s outer appearance to infer inner character is a practice that is now discredited and discarded as phrenology. However, this piece illustrates how such practice is alive and well in the era of big data and machine learning. Here is more on the Wu and Zhang paper that the Physignomy’s New Clothes authors cover in the above piece.
General articles on various automated systems and bias, discrimination, unfairness, ethical concerns, etc., listed in order of publication dates starting from the latest.
An incredibly important paper on whether data can ever be “anonymized” and how we should handle release of large data-sets February 1, 2018
The Irish Data Protection Bill published on January 30, 2018
The Latest Data Privacy Debacle January 30, 2018
Engineered for Dystopia January 24, 2018
The Injustice of Algorithms January 23, 2018
A Popular Algorithm Is No Better at Predicting Crimes Than Random People — The Atlantic January 17, 2018
Software ‘no more accurate than untrained humans’ at judging reoffending risk January 17, 2018
Mechanical Turkers may have out-predicted the most popular crime-predicting algorithm January 17, 2018
It’s the (Democracy-Poisoning) Golden Age of Free Speech January 16, 2018
Maybe Facebook Should Abandon the News Feed Altogether January 16, 2018
Beyond the Rhetoric of Algorithmic Solutionism January 11, 2018
Why AI Is Still Waiting For Its Ethics Transplant January 11, 2018
Amazon turns over record amount of customer data to US law enforcement January 5, 2018
Will Mark Zuckerberg, with his promise to ‘fix’ Facebook, give up revenue to do what’s right? January 7, 2018
Don’t Be Evil January 3, 2018
The Algorithms Aren’t Biased, We Are January 3, 2018
Fair and Balanced? Thoughts on Bias in Probabilistic Modeling December 27, 2017
What Amazon Echo and Google Home Do With Your Voice Data — And How to Delete It December 24, 2017
Should AI decide who gets a kidney? December 21, 2017
In 2017, society started taking AI bias seriously December 21, 2017
Dozens of companies are using Facebook to exclude older workers from job ads December 20, 2017
Could Facebook Be Tried for Human-Rights Abuses? — The Atlantic December 20, 2017
Data Violations: Germany unfriends Facebook December 19, 2017
Facebook Can Now Find Your Face, Even When It’s Not Tagged December 19, 2017
Silicon Valley Is Turning Into Its Own Worst Fear December 18, 2017
Spurred by a ProPublica report, the New York City Council passed the country’s first bill to address algorithmic discrimination in city government December 18, 2017
‘The Basic Grossness of Humans’ — The Atlantic December 15, 2017
Engineers, philosophers and sociologists release ethical design guidelines for future technology December 14, 2017
Artificial Intelligence Seeks An Ethical Conscience December 12, 2017
Australian media watchdog to investigate Google and Facebook December 5, 2017
Debugging data: Microsoft researchers look at ways to train AI systems to reflect the real world December 4, 2017
Why Autocomplete Is Only Funny for Those Who Can Afford It by Safiya Umoja Noble: December 4, 2017
Predictive algorithm under wraps December 3, 2017
The Rhetorical “We” and the Ethics of Technology December 1, 2017
Artificial intelligence doesn’t have to be evil. We just have to teach it to be good November 30, 2017
Frank Pasquale testifies (video, written testimony) Before the United States House of Representatives Committee on Energy and Commerce Subcommittee on Digital Commerce and Consumer Protection in relation to “Algorithms: How Companies’ Decisions About Data and Content Impact Consumers”. Here for more written testimony on Algorithmic Transparency from the Electronic Privacy Information Center — November 29, 2017.
U.S. House Hearing on Algorithms & Big Data: 5 Takeaways for Schools November 29, 2017
Facebook to temporarily block advertisers from excluding audiences by race November 29, 2017
Facebook ad targeting is about to get a whole lot creepier November 28, 2017
Why We Had to Buy Racist, Sexist, Xenophobic, Ableist, and Otherwise Awful Facebook Ads November 27, 2017
Facebook hasn’t done enough to tell customers they were duped by Russian propaganda November 25, 2017
Facebook (still) letting housing advertisers exclude users by race November 21, 2017
Tim Berners-Lee on the future of the web: ‘The system is failing’ November 16, 2017
Ray Dalio has an unbelievable algorithm November 15, 2017
How One Woman’s Digital Life Was Weaponized Against Her November 14, 2017
Maybe Facebook Is Broken. How can you stop people from sharing biased and misleading stuff? November 7, 2017
Bringing A.R.T. to A.I. November 6, 2017
Computer says no: why making AIs fair, accountable and transparent is crucial November 5, 2017
Why we need a 21st-century Martin Luther to challenge the church of tech October 29, 2017
Facebook must face local data protection regulations, EU court opinion finds October 25, 2017
Key GDPR Guidance on Behavioral Advertising, Profiling and Automated Decision-Making October 24, 2017
It’s time for more transparency in A.I. October 24, 2017
Federal judge unseals New York crime lab’s software for analyzing DNA evidence October 20, 2017
AI Experts Want to End ‘Black Box’ Algorithms in Government October 18, 2017
Estonia Proposes Bill of Rights and Responsibilities for Robots October 17, 2017
Asking the Right Questions About AI October 12, 2017
Google’s AI chief says forget Elon Musk’s killer robots, and worry about bias in AI systems instead October 3, 2017
Researchers Are Upset That Twitter Is Dismissing Their Work On Election Interference October 3, 2017
Facebook’s Ad Scandal Isn’t a ‘Fail,’ It’s a Feature September 23, 2017
BBC News — Facebook can’t hide behind algorithms September 22, 2017
Data power could make 1984 ‘look like a Teddy bear’s picnic’ September 21, 2017
Machines Taught by Photos Learn a Sexist View of Women September 21, 2017
AI Research Is in Desperate Need of an Ethical Watchdog September 18, 2017
Getting serious about research ethics: AI and machine learning September 18, 2017
Machines are getting schooled on fairness September 16, 2017
Facebook and Google, show us your ad data Understanding how they influence us is crucial to the future of our democracy. September 13, 2017
Understanding Bias in Algorithmic Design Human judgement lies behind every data-driven decision. Left unexamined, value-laden software can have unintended discriminatory effects. September 6, 2017
Report: Britain’s Cops Have Big Data But Not Big Analysis September 6, 2017
Turns out algorithms are racist August 31, 2017
AI programmes are learning to exclude some african american voices August 16, 2017
FaceApp Is Very Excited About Its New Line of Ultra-Racist Filters August 8, 2017
Rise of the racist robots — how AI is learning all our worst impulses August 8, 2017
Artificial intelligence ethics the same as other new technology July 29, 2017
Technology is biased too. How do we fix it? July 20, 2017
How can we stop algorithms telling lies? July 16, 2017
Lack of ethics education for computer programmers shocks expert July 2, 2017
Facebook’s secret censorship rules protect white men from hate speech but not black children June 28, 2017
We need to shine more light on algorithms so they can help reduce bias, not perpetuate it June 12, 2017
How to Call B.S. on Big Data: A Practical Guide June 3, 2017
Pitfalls of artificial intelligence decision-making highlighted in Idaho ACLU case June 2, 2017
The bigot in the machine: Tackling big data’s inherent biases June 1, 2017
Secret algorithms threaten the rule of law June 1, 2017
Algorithms aren’t racist. Your skin is just too dark. May 29, 2017
‘A white mask worked better’: why algorithms are not colour blind May 28, 2017
On Facebook May 7, 2017
AI & Machine Learning Black Boxes: The Need for Transparency and Accountability: April 25, 2017
FaceApp sorry for ‘racist’ filter that lightens skin to make users ‘hot’ April 25, 2017
Robots are racist and sexist. Just like the people who created them April 20, 2017
How artificial intelligence learns to be racist April 17, 2017
Courts are using AI to sentence criminals. That must stop now. April 17, 2017
An AI stereotype catcher April 14, 2017
AI picks up racial and gender biases when learning from what humans write April 13, 2017
AI programs exhibit racial and gender biases, research reveals April 13, 2017
AI learns gender and racial biases from language April 13, 2017
Will the future be full of biased robots? March 31, 2017
Algorithms can be pretty crude toward women March 24, 2027
Algorithms learn from us, and we can be better teachers March 13, 2017
Data-driven crime prediction fails to erase human bias March 8, 2017
Big data, big problems — interview with Cathy O’Neil March 1, 2017
How to Keep Your AI From Turning Into a Racist Monster February 13, 2017
Code-Dependent: Pros and Cons of the Algorithm Age February 6, 2017
We put too much trust in algorithms and it’s hurting our most vulnerable December 29, 2016
Be Healthy or Else: How Corporations Became Obsessed with Fitness Tracking December 27, 2016
Discrimination by algorithm: scientists devise test to detect AI bias December 19, 2016
A simplified political history of Big Data December 16, 2016
Hiring Algorithms Are Not Neutral December 9, 2016
How Algorithms Can Bring Down Minorities’ Credit Scores December 2, 2016
Put Away Your Machine Learning Hammer, Criminality Is Not A Nail November 29, 2016
The Foundations of Algorithmic Bias November 7, 2016
Unregulated Use of Facial Recognition Software Could Curb 1st Amendment Rights October 30, 2016
Should we trust predictive policing software to cut crime? October 27, 2016
Google researchers aim to prevent AIs from discriminating October 7, 2016
To predict and serve? October 7, 2016
How algorithms rule our working lives September 1, 2016
White House plan to use data to shrink prison populations could be a racist dumpster fire July 1, 2016
Is criminality predictable? Should it be? June 30, 2016
Artificial Intelligence’s White Guy Problem June 25, 2016
In Wisconsin, a Backlash Against Using Data to Foretell Defendants’ Futures June 22, 2016
Algorithmic risk-assessment: hiding racism behind “empirical” black boxes May 24, 2016
Image Courtesy of ProPublica
There’s software used across the country to predict future criminals. And it’s biased against blacks. May 23, 2016 The company that sells this program (Northpointe) has responded to the criticisms here. Northpointe asserts that a software program it sells that predicts the likelihood a person will commit future crimes is equally fair to black and white defendants. Following such response, Jeff Larson and Julia Angwin has written another response (Technical Response to Northpointe) re-examined the data. They argue that they have considered the company’s criticisms, and stand by their conclusions.
Python Meets Plato: Why Stanford Should Require Computer Science Students to Study Ethics May 16, 2016
The Real Bias Built In at Facebook May 19, 2016
Twitter taught Microsoft’s friendly AI chatbot to be a racist asshole in less than a day March 24, 2016
The Iron Cage in binary code: How Facebook shapes your life chances — Sociological Images: December 30, 2015
As World Crowds In, Cities Become Digital Laboratories December 11, 2015
Google Photos Tags Two African-Americans As Gorillas Through Facial Recognition Software July 1, 2015
How big data is unfair September 26, 2014
Facebook reveals news feed experiment to control emotions June 30, 2014
The Hidden Biases in Big Data by Kate Crawford April 1, 2013
Politics
Algorithmic processes and politics might seem far removed from each other. However, if anything, the recent political climate is indicative of how algorithms can be computational tools for political agendas. Here and here are exemplar twitter threads that highlight particular Twitter accounts used as tools for political agenda. The articles below are, in some way or another, related to algorithms in the political arena.
Facebook admits social media sometimes harms democracy January 22, 2018
This Country’s Democracy Has Fallen Apart — And It Played Out To Millions On Facebook January 21, 2018
Will Mark Zuckerberg, with his promise to ‘fix’ Facebook, give up revenue to do what’s right? January 8, 2018
How Facebook’s Political Unit Enables the Dark Art of Digital Propaganda December 21, 2017
Inside the world of Brazil’s social media cyborgs December 13, 2017
How Rodrigo Duterte turned Facebook into a weapon, with a little help from Facebook December 7, 2017
More than a Million Pro-Repeal Net Neutrality Comments were Likely Faked November 23, 2017
Extreme Vetting by Algorithm November 20, 2017
How a half-educated tech elite delivered us into evil November 19, 2017
Do Facebook and Google have control of their algorithms anymore? A sobering assessment and a warning November 14, 2017
‘Way too little, way too late’: Facebook’s factcheckers say effort is failing November 13, 2017
How to Fool Americans on Twitter November 6, 2017
Russia funded Facebook and Twitter investments through Kushner associate November 5, 2017
Opinion | Silicon Valley Can’t Destroy Democracy Without Our Help November 2, 2017
When Data Science Destabilizes Democracy and Facilitates Genocide November 2, 2017
Facebook estimates 126 million people were served content from Russia-linked pages October 31, 2017
Russian content on Facebook, Google and Twitter reached far more users than companies first disclosed, congressional testimony says October 30, 2017
‘Downright Orwellian’: journalists decry Facebook experiment’s impact on democracy October 25, 2017
A Suspected Network of 13,000 Twitter Bots Pumped Out Pro-Brexit Messages In The Run-Up To The EU Vote October 20, 2017
How People Inside Facebook Are Reacting To The Company’s Election Crisis October 20, 2017
Facebook treats its ethical failures like software bugs, and that’s why they keep happening October 20, 2017
Tech Giants, Once Seen as Saviors, Are Now Viewed as Threats October 12, 2017
Russia Probe Now Investigating Cambridge Analytica, Trump’s ‘Psychographic’ Data Gurus October 10, 2017
Google uncovers Russian-bought ads on YouTube, Gmail and other platforms October 9, 2017
Facebook cut references to Russia from a report in April about election influence October 5, 2017
Russian Facebook ads: 70 million people may have seen them October 4, 2017
Google and Facebook Have Failed Us — The Atlantic October 2, 2017
Facebook and Google promote politicized fake news about Las Vegas shooter October 2, 2017
Social media companies must respond to the sinister reality behind fake news October 1, 2017
Zuckerberg’s Preposterous Defense of Facebook September 29, 2017
“Fake news” tweets targeted to swing states in election, researchers find September 28, 2017
As Google Fights Fake News, Voices on the Margins Raise Alarm September 26, 2017
Facebook blocked an ad for a march against white supremacy: September 25, 2017
Hillary Clinton says Kenya’s annulled election was a “project” of a controversial US data firm September 19, 2017
Facebook enabled advertisers to reach “Jew haters” September 14, 2017
Facebook and Google, show us your ad data Understanding how they influence us is crucial to the future of our democracy. September 13, 2017
RT, Sputnik and Russia’s New Theory of War September 13, 2017
American politics needs new rules for the Facebook era September 12, 2017
Russia’s Facebook Fake News Could Have Reached 70 Million Americans September 8, 2017
Forum Q&A: Philip Howard on Computational Propaganda’s Challenge to Democracy July 25, 2017. “Computational propaganda, or the use of algorithms and automated social media accounts to influence politics and the flow of information, is an emerging challenge to democracy in the digital age. Using automated social media accounts called bots (or, when networked, botnets), a wide array of actors including authoritarian governments and terrorist organizations are able to manipulate public opinion by amplifying or repressing different forms of political content, disinformation, and hate speech.”
WhatsApp and Facebook are driving Kenya’s fake news cycle July 24, 2017
GOP Data Firm Accidentally Leaks Personal Details of Nearly 200 Million American Voters June 19, 2017
Voter profiling in the 2017 Kenyan election June 6, 2017
The great British Brexit robbery: how our democracy was hijacked May 7, 2017
Confronting a Nightmare for Democracy May 4, 2017
30 million Facebook users had their data harvested by Trump campaign affiliate March 30, 2017
Robert Mercer: the big data billionaire waging war on mainstream media Feburary 26, 2017
Revealed: how US billionaire helped to back Brexit Feburary 26, 2017
The Truth About The Trump Data Team That People Are Freaking Out About Feburary 16, 2017
The Data That Turned the World Upside Down Jan 28, 2017
Inside the Trump Bunker, With Days to Go: Win or lose, the Republican candidate and his inner circle have built a direct marketing operation that could power a TV network — or finish off the GOP. October 27, 2016
Facebook wants you to vote on Tuesday. Here’s how it messed with your feed in 2012. October 31, 2014
For a more scholarly read
Barocas, S., & Selbst, A. D. (2016). Big data’s disparate impact.
Barabas, C., Dinakar, K., Virza, J. I., & Zittrain, J. (2017). Interventions over Predictions: Reframing the Ethical Debate for Actuarial Risk Assessment.arXiv preprint arXiv:1712.08238.
Bolukbasi, T., Chang, K. W., Zou, J. Y., Saligrama, V., & Kalai, A. T. (2016). Man is to computer programmer as woman is to homemaker? Debiasing word embeddings. In Advances in Neural Information Processing Systems (pp. 4349–4357).
Caliskan-Islam, A., Bryson, J. J., & Narayanan, A. (2016). Semantics derived automatically from language corpora necessarily contain human biases. arXiv preprint arXiv:1608.07187.
Chouldechova, A. (2017). Fair prediction with disparate impact: A study of bias in recidivism prediction instruments.arXiv preprint arXiv:1703.00056. (PDF)
Datta, A., Sen, S., & Zick, Y. (2016, May). Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems. In Security and Privacy (SP), 2016 IEEE Symposium on (pp. 598–617). IEEE. (PDF)
Datta, A., Tschantz, M. C., & Datta, A. (2015). Automated experiments on ad privacy settings.Proceedings on Privacy Enhancing Technologies, 2015(1), 92–112.
Friedman, B., & Nissenbaum, H. (1996). Bias in computer systems. ACM Transactions on Information Systems (TOIS), 14(3), 330–347.
Jawaheri, H. A., Sabah, M. A., Boshmaf, Y., & Erbad, A. (2018). When A Small Leak Sinks A Great Ship: Deanonymizing Tor Hidden Service Users Through Bitcoin Transactions Analysis. arXiv preprint arXiv:1801.07501.
Monahan, J., & Skeem, J. L. (2016). Risk assessment in criminal sentencing.Annual review of clinical psychology, 12, 489–513.
Narayanan, A., Huey, J., & Felten, E. W. (2016). A precautionary approach to big data privacy. In Data protection on the move (pp. 357–385). Springer, Dordrecht.
Munoz, C., Smith, M., & Patil, D. (2016). Big data: A report on algorithmic systems, opportunity, and civil rights.Executive Office of the President. The White House.
Yeung, K. (2017). Algorithmic Regulation: A Critical Interrogation.
Zafar, M. B., Valera, I., Gomez Rodriguez, M., & Gummadi, K. P. (2017, April). Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th International Conference on World Wide Web (pp. 1171–1180). International World Wide Web Conferences Steering Committee
Zhang, B. H., Lemoine, B., & Mitchell, M. (2018). Mitigating Unwanted Biases with Adversarial Learning. arXiv preprint arXiv:1801.07593.
",Resources — automated systems and bias,1,resources-automated-systems-and-bias-75e6e0826e56,2018-02-11,2018-02-11 15:59:21,https://medium.com/s/story/resources-automated-systems-and-bias-75e6e0826e56,False,5216,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Abeba Birhane,Cognitive science PhD student @ucddublin #DataEthics #Interdisciplinarity #EmbodiedCognition #Enaction #Dialogism Ethiopian living in Dublin,78be1aff5d2a,Abebab,244.0,189.0,20181104
0,,0.0,,2018-08-20,2018-08-20 17:02:11,2018-08-20,2018-08-20 18:45:49,1,False,en,2018-08-22,2018-08-22 13:49:10,0,4709612b17a0,5.660377358490568,0,0,0,"The recent developments in artificial intelligence (AI) are now rapidly revitalizing research on the foundations of ethics, casting old…",4,"
Getting rid of unethical bias in AI
The recent developments in artificial intelligence (AI) are now rapidly revitalizing research on the foundations of ethics, casting old debates on moral philosophy in a new light and requiring new ways to think about our relationship with technology. On a more practical level, AI generates new domains for social and economic practice and associated policy vacuums, where current norms, rules, and regulations provide little guidance.
One important and widely recognized challenge is the inherent bias in AI systems. When AI determines the length of your jail term or your insurance premiums, it becomes important to know that you are not intentionally or unintentionally discriminated based on your ethnic origin, religion, gender, or other culturally inappropriate criteria.
In this article I break the problem of acceptable bias in AI in its key technical components and show how the problem can be solved in practice. To do this, new policies are needed. The development of AI systems that have only acceptable biases requires data on characteristics that should not be used for prediction or decision-making. Existing regulation needs to be adjusted as storing and processing such data may be illegal and ethically unacceptable.
The difference between traditional computing and adaptive systems
A traditional computer program is a deterministic machine that translates given inputs into predictable outputs. The ways in which input data are transformed to outputs are explicitly described by an algorithm. In computational terms, a traditional computer program is a logical machine that converts truths into equivalent truths. Because of this, it is possible to check the logic and make sure that the program does not have inappropriate biases. If you put bias in, you get bias out.
AI systems are different. The present AI and machine learning boom is based on artificial neural network models that use a goal-oriented approach. In these systems, the programmer does not define how the input data are transformed into outputs; instead, the programmer defines a general-purpose adaptive architecture and criteria for successful computation. In most practically important systems today, the success is interpreted as minimal prediction or classification error. The error is evaluated using test data, comparing the predictions of the system to known correct predictions or “ground truths.”
Unacceptable bias can be introduced into AI systems simply by using input data that include wrong types of data. For example, if a bank loan scoring system uses data on religion or ethnic origin, there are obvious risks that its decisions are unacceptably biased. This is not very different from traditional algorithmic computing.
All AI systems have unintended biases.
AI systems that adapt their behavior during training can, however, introduce also unintended biases that are not explicitly represented in data or program code. This is how AI is different from traditional computer programs. For example, a system that uses facial images can relatively easily adapt its decisions to features that represent gender, ethnic origin, or religion. Indeed, in practice AI systems are used exactly because they can effectively find useful hidden “biases” and patterns from complex data. If black Americans have statistically been more often jailed than white Americans, AI will figure this out, and predict that this will be the case also in the future. You don’t have to tell the system whether a person is white or black; it will figure that out as soon as it has sufficient data.
You cannot control such biases by limiting the types of allowed inputs. If the system does not explicitly represent race, religion, or political orientation, you cannot in any obvious way know if the system optimizes its decisions based on its reconstructions of such categories. If your voting patterns are useful in minimizing errors in predictions that the system makes, it will generate internal representations of your voting patterns and use these to put you in the right box. These patterns don’t have to be based on information about how you have voted; they can, for example, be generated from your social media posts or links. The classification may not be accurate, but if the system has been well trained, it will be accurate enough.
Explainable AI is not enough
The problem of bias in AI systems has led to rapidly expanding research on “explainable AI.” The attempt has been to open the “black box” of machine learning systems so that the logic of their decisions and predictions can be made explicit. As AI systems are purely mechanistic systems, this approach, however, is not able to solve the problem of acceptable bias. Explanation requires interpretation and understanding that are beyond the capabilities of mechanistic machines.
A clock on the wall cannot explain why it thinks it’s noon.
If we ask why a clock on the wall just made twelve strokes, we can generate a description of the state of the springs and gears that makes the clock strike. The essence of mechanisms, however, is that they can be fully described without asking the question why. This is what Newton meant by claiming “hypotheses non fingo.” Newtonian physics and its modern adaptations in sciences and theory of computing start from the assumption that natural systems can be fully described without final causes. There is no category of “why” in mechanical descriptions, and computers are the ultimate abstractions of such universal Newtonian mechanisms. This is the reason why computers cannot “understand” or “explain” their decisions any more than a clock on the wall can explain why it decides that it is noon. “Explainable AI,” therefore, is strictly speaking an oxymoron.
Testing for unacceptable bias
The problem of acceptable bias, however, can be solved using a different approach. This requires that we are allowed to use data that are explicitly biased. If we want to make sure that an AI system does not introduce unacceptable biases, we have to assess the system using data that explicitly represent these unacceptable biases. In other words, we need data that breaks the accepted norms and policies.
The test for unacceptable bias is simple. Assume that we have an AI system that has been trained and optimized to make predictions for a specific purpose, for example, to decide conditions for a bank loan. To assess the acceptability of bias, we need to feed in to the system test cases for which we know characteristics that should not bias predictions and decisions. Given enough such data, we can statistically test that unacceptable characteristics do not make a difference.
If religion, ethnic origin, or sexual and political orientation cannot legally be used to discriminate people, an AI system passes the test if its outputs are statistically independent of these categories.
To make such assessment possible, policymakers need to allow the development of test data that include data that may be prohibited under present regulation. The current rules might be adapted to develop specialized certification services that AI developers could use to check that their systems are acceptably biased. For example, it is technically possible that trained AI models can be submitted for evaluation to an assessment service that is allowed to use generally unacceptable datasets to test for unacceptable bias.
To create fair AI systems, the learning process needs to neutralize unacceptable biases.
When unacceptable bias is found, something needs to be done about it. To create AI systems that do not have unacceptable biases, the learning process needs to neutralize the impact of data that generates unacceptable biases, so that the outcomes of the trained system becomes independent of the prohibited criteria. In practice, this means that the criteria for “error” that is used to train and optimize the AI has to include information about unacceptable biases. Technically it is a reasonably straightforward task to develop AI architectures that neutralize unacceptable biases. You just use the ethically inappropriate dataset to make the AI unlearn those characteristics that cannot be used for decision-making. Politically, however, this requires new thinking and new rules.
It is quite probable that in reasonably just societies, almost all AI systems today have culturally and legally unacceptable biases. These systems use aggregate data on past behavior of others to make predictions and judgments about individual persons, who from an ethical point of view are always unique and able to become something that history does not know yet. We may get hints of the problem through blatant categorization errors that become newspaper headlines and academic studies that highlight ethical problems. The extraordinary capacities of adaptive AI systems in categorizing and classifying the world makes them inherent racists. Perhaps paradoxically, the acceptability of the inherent biases in current AI systems, however, can only be tested using data that breaks those norms that we don’t want to break.
",Getting rid of unethical bias in AI,0,getting-rid-of-unethical-bias-in-ai-4709612b17a0,2018-08-31,2018-08-31 16:10:41,https://medium.com/s/story/getting-rid-of-unethical-bias-in-ai-4709612b17a0,False,1447,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Ilkka Tuomi,"Chief Scientist, Meaning Processing. Author of books on AI, new innovation models, knowledge management, information society, and over 60 academic articles.",ee8c6ca02bc5,ilkka.tuomi,1.0,1.0,20181104
0,,0.0,3a8144eabfe3,2018-04-14,2018-04-14 00:29:51,2018-04-14,2018-04-14 05:09:56,1,False,en,2018-04-21,2018-04-21 18:50:59,1,aabceafd92d0,5.939622641509434,3,0,0,"Fourteen years later, we wake up from a dream. As disappointing as it is, we strive to deal with the awoken reality, and leave the almost…",5,"What’s on your mind?

Fourteen years later, we wake up from a dream. As disappointing as it is, we strive to deal with the awoken reality, and leave the almost too-good-to-be-truth aspirations for the psyche. As software engineering makes its journey to granting wishes and solving problems, we come to halt its recklessness. It’s time to talk: grab a chair, it will take some time.
Just like there’s no free lunch, data is being collected as we use social media. And no matter how much people know about the extent of this data usage, the word “advertisement” seems to buzz when the matter is the Web 2.0 business model. We have come to understand a few years ago that money could be made with platforms that were free for the users but could still derive dividends to the shareholders, which is nothing to be reprimanded. However, it is now that we realize how fragile this model is when it comes to privacy, ethics, and what we may want in a second level that flies over content consumption. And for that, Mark Zuckerberg visited Washington this week.
Undeniably valuable, the hearings spun off an impression that Zuckerberg has few resources to abate the running issues, that people are illiterate about the inner-workings of technology, and that there’s a contradiction to the essence of the business model of these platforms in general.
While the past few years were a fuss about AI and Machine Learning tools and what they promised to do for us, we have been realizing we don’t understand it at all. Transitioning from stating that every single software had some sort of machine learning optimization (a lot of money was lost here), we stepped back to believe that not only is there a gap of progress to be made, but also that there’s a black magic component to this technology; we don’t quite control what’s inside it (often referred to as data-in, data-out), specially when it’s applied in scale. Since then, we matured enough to see that recommendation systems suffer from the same evils by suggesting wrongful content as far as unfitting to our profile, and more tragically, making blunt assumptions about our desire to see and consume extreme or fake content, which had legs in and of itself for a while. To the defense of software engineers, since then, a lot of thought has been employed to better understand these algorithms as well as to devise ways to better educate them. Nevertheless, the uncertainty is not quite tamed, and the non-trivial problem of admitting machine’s weaknesses in the affair of highly complex judgement remains. For the time being.
Perhaps even more pressing, what gained mass circulation over the past days were shots of bemused Congresspeople trying to understand technology. More on that later, but for now, to the extent that people are represented by the Congress, it’s fair to assume that most people would be in the same state if asked to explain how the technology they use work. And that follows specially for contemporaries, who profess domain of technology justified by being born with it but lack fundamental grasp of its engineering and implications whatsoever. Congress, however, was one level up past that naivety, and did attempt to formalize and clarify our knowledge of how Facebook makes its money, even though they weren’t awarded virality for that.
If from one side Congress was, as said “grilling” Mark, it seemed the latter was doing a better job as a politician, who consistently escaped the hard questions in a mocking way and refused to clarify how Facebook really works. Zuckerberg got away with, to not use the well-known morale given by Marvel’s character, but go the opposite, theatricality and deception — promising that everything will be fixed with so-called “tools”- read AI and Machine Learning. Meanwhile, Facebook comes out untouched, is given a fresh-start, and is awarded a successful wrap-up to the data fiasco by means of apologies from the CEO. And so here we stand still, while the company catches up its fast movement to keep breaking things. Tomorrow, we will run faster, and there might be a redesigned feed, and no data will be collected, and it may solve all of our problems. Likely not. The reasons tie to the size of Facebook’s user base and its business model.
The platform saw itself in the past years lost in its essential values, struggling to make a choice between prioritizing the relevant over the viral, the news over the rumors, and the friends over the influencers, all of that in favor of building a more meaningful feed. On one way, by choosing the first, it loses its user base, the influencer economy, and most of the ad model. On the other, if it chooses the second, it starts to respond for the ethics of fake news, influencer marketing, privacy, and virality. Trying to guess the best way and implementing both of the two different policies to govern the social feed, Facebook also made both of the users of these groups disappointed, and on top of that, didn’t recognize it is to heavy to make such fast turns. Moreover, whether the influencers, the assiduous users, the internet geeks, the Macedonian fake news farms, or the presidential candidates were affected, it exposed sub-economies built off of this software in the real world, and the crossroad at which the company is currently at, missing a clear strategy going forward. While attempts to settle the feed question are worthwhile, they are doomed to failure; eliminating the personal factor is counterintuitive to the way it makes money and to how Web 2.0 was originally conceived — that was shifting the focus from the product to the data, and monetizing over the latter. As the platform promises to connect people while keeping their data safe, it establishes a contradiction that sits between encouraging us to share more of ourselves publicly and making efforts to protect our intimacy. Perhaps Zuckerberg hinted at this while in Washington when asked if Facebook was willing to change its business model in the interest of privacy, to which he replied with the same deception: “I’m not sure what that means.”
And so if it’s likely not to change, then what? Should we leave? Well, in attempt, you may be a lucky one if you conclude it doesn’t make a difference, but even if you do delete your account, it is still unclear if you can just ask for the check, stand up and leave with your data, as far as their being kept in the servers, collected elsewhere, contained in third-parties’ databases, or inferred in some way through the data of your friends that stay. To sum it up, it’s either all of us moving ,in a Project Diaspora similar endeavor, or it’s not even worth trying. Finding this first attempt unfeasible, or even still seeing value in the platform, you’re left to evaluate its actual price and to negotiate if possible.
Towards doing so, Congress took the first step, but still people refuse to acknowledge its necessity; this time, though, it wasn’t the algorithms’s fault, but, again, a lot of what went viral from the hearings were short clips mocking Capitol Hill’s misunderstanding of technology and Mark giving hacky answers in response. More interestingly, people used this very same platform to tune out the relevancy of its weaknesses and ethical implications. Instead, they chose to replicate ignorance and carelessness in starting a conversation about a proper business and cultural framework for the future, leaving the tough questions for someone else to answer. Perhaps, in the end, that’s what both Zuckerberg and the users want Facebook to be used for, and in that case, a proper solution to this problem will wait another fourteen years, at least, or until we wrongly or not profess mastery of AI and its ability to judge ethical matters and build businesses for ourselves. Whichever comes first.
Rather irrelevant, though, is how contradictory Mark Zuckerberg may sound, but whether we see ourselves as consumers, and thus learn how much Facebook costs in order to decide whether to put it back on the shelf or take it home. The scenario doesn’t point that way, and it becomes harder every day to opt-out of data collection, privacy breaches, and social media dependence. As the goal of connecting the world is pursued, and the social graph grows in edges and nodes, the cost of leaving it scales, and so does the price we pay for it, even though not to our knowledge.
The events that led to this week opened up a place for revisiting the type of relation we want to have with these platforms, and we should educate ourselves to ask the right questions, not the “can Facebook read my conversations?” ones, or whatever is on your mind out of curiosity. Older generations have taught us: the worst son is not the wicked, but the one who doesn’t know how to ask.
Daniel Fonseca Yarochewsky, Spring ‘18
Revised by Maddy Bennett
",What’s on your mind?,12,whats-on-your-mind-aabceafd92d0,2018-04-27,2018-04-27 17:36:08,https://hackernoon.com/whats-on-your-mind-aabceafd92d0,False,1521,how hackers start their afternoons.,hackernoon.com,hackernoon,,Hacker Noon,stories@amipublications.com,hacker-daily,"HACKATHONS,HACKING,PROGRAMMING,TECH,HACKER",hackernoon,Ethics,ethics,Ethics,7787.0,Daniel Fonseca Yarochewsky,Computer Science Student | University of Washington | yarochewsky@icloud.com | www.danielfonsecayarochewsky.com,1715fbaad4cf,danielfonsecayarochewsky,51.0,52.0,20181104
0,,0.0,661161fab0d0,2018-08-23,2018-08-23 22:35:39,2018-08-23,2018-08-23 22:38:52,1,False,en,2018-08-24,2018-08-24 18:04:55,2,5836e14fb540,3.920754716981132,1,0,0,"In this year’s edition of ZOOM SMART CITIES (Portugal’s major Smart Cities event), we had the opportunity to listen to Arlindo Oliveira…",5,"Photo: World’s First Psychopath AI! … Norman (http://norman-ai.mit.edu/)
Hippocratic Oath for Algorithms and Artificial Intelligence
In this year’s edition of ZOOM SMART CITIES (Portugal’s major Smart Cities event), we had the opportunity to listen to Arlindo Oliveira, the President of Instituto Superior Técnico (Lisbon), talk about the future of humanity, and the challenges and questions continuously raised due to a growing euphoria concerning Artificial Intelligence. The prestigious scholar explains, in an absolutely extraordinary and coherent manner, how we are already living in the future.
It should not be the “bliss” and bewilderment felt with the new technological progress that should occupy our time and minds. The most complex and troubling part is that as we are attracted to fanciful discussions about what is intelligent or not, artificial or human, algorithms continue to run. Machines continue to multiply, relentlessly, their immediate capacity for self-learning and optimized systems, clashing head-on with our humanity, and promoting conflict between humans.
We have for long known that computing power is skyrocketing, and in many cases, overwhelmingly surpasses the human capacity to solve complex problems (it remains to be seen whether our innate ability to create them will also be supplanted). Machines that can study rules and guidelines become true gods in the Olympus of the Cloud. They become impossible to catch up to and exceed. The current technology enables them to run endless simulations and projections, learn from errors, and bypass them in the next instant, in a fraction of a second. At first glance, a simple game of Chess or Go, or Rubik’s Cube, for example, appears to be naïf and harmless. After all, we already know our natural limitations, and acknowledge machines as being infinitely better at processing information and data. However, the most important game machines play a key role in is directly related to our lives.
The greatest technological advancement in modern history was the invention of the ‘Press’ in the fifteenth century. It allowed for one to seek empirical knowledge to supplant the previously dominant liturgical doctrine. The Age of Reason gradually replaced the Age of Religion. It was a decisive moment for individual development and scientific knowledge as a substitute for faith as the prime reason for the formation of human knowledge. The information then began to be recorded and organized in archives and libraries. The Age of Reason originated the thought and action that shaped the present world in which we live.
This order is now also in the process of being gradually replaced by a new one. An order founded also on a technological revolution, but more complex, and whose consequences none have managed to identify and evaluate so far, and which could ultimately leave the world dependent on data-fed machines and ungoverned algorithms devoid of ethical, moral, or philosophical principles.
This is the time for humanity to support the fight enlightenment inspired: the fight to promote philosophical thinking through technology
The ‘Internet’ age we live in today creates one of the issues to make artificial intelligence even more pressing. The Enlightenment [Age of Reason] movement tried to submit the traditional truths to an analytical and free human reasoning. The use of the Internet now serves to confirm knowledge through the gathering and manipulation of continuously expanding data. Human ‘knowledge’ loses its personal character. Individuals turn into data, and data become the order of the day.
Internet users rarely wonder about history or philosophy. They mostly require information of relevance to their immediate needs. In the process, search engines algorithms gain the ability to predict individual preferences, allowing other algorithms to customize information to be used later on for political and commercial purposes. Social networks, for example, become places of dispute and conflict, provoking anger and frustration. The speed and promptness by which information is presented inhibit reflection. They encourage radical rather than intelligent thinking. They value consensus by subgroups rather than by introspection.
In a previous article, I mentioned the possibility of future Quantum Mayors or, in short, of mayors becoming extinct due to their clear incompetence. It is a job many now wish it would be performed with precision, based on data and artificial intelligence, so as to maximize and raise the efficiency of processes, particularly in cities. What we refer to as a “support tool” to humanity can be [seen as] a threat. On the one hand, there is the human being who becomes “brutish” and as a direct result degenerates into darkness; on the other hand, there is the machine, in an uncontrolled self-learning process, fed by algorithms and information coming from the social networks and other platforms. That machine is able to promote the extinction of the individual conscious mind, the consequences of which we can already see a glimpse every day in politics, business, divisive causes, friendships, and even relationships.
These pressures weaken the pillars needed to develop and build beliefs, which emerge only to thread a lonely road–present in the essence of creativity.
This is the ideal time to launch a “Hippocratic Oath” for the technology industry. It is the time to define the rules of conduct, making it impossible to grow algorithms allegedly ‘neutral’ and devoid of ‘emotions’ and ‘values’.
Human intelligence is still dominant in this day and age. While the world races to master artificial intelligence, and while there is an urgent need to secure access to data, whether it be for security reasons (facial recognition), sharing of personal information to obtain several benefits (social score), this is the time for humanity to support the fight enlightenment inspired: the fight to promote philosophical thinking through technology. That is because we also dwell in an age that follows in the opposite direction: we create potentially dominant technology in our quest for a philosophical system to guide us. We can still win the race; a race soon to become unwinnable.
PS: The article of this edition did not make use of any AI mechanisms to discover the current topic.
",Hippocratic Oath for Algorithms and Artificial Intelligence,50,hippocratic-oath-for-algorithms-and-artificial-intelligence-5836e14fb540,2018-08-24,2018-08-24 18:04:55,https://medium.com/s/story/hippocratic-oath-for-algorithms-and-artificial-intelligence-5836e14fb540,False,986,where the future is written,,,,Predict,predictstories@gmail.com,predict,"FUTURE,SINGULARITY,ARTIFICIAL INTELLIGENCE,ROBOTICS,CRYPTOCURRENCY",,Ethics,ethics,Ethics,7787.0,Vitor Pereira,,cb68d314def6,vitorpereira_1974,277.0,337.0,20181104
0,,0.0,7f60cf5620c9,2018-05-09,2018-05-09 18:20:38,2018-05-10,2018-05-10 13:39:19,1,False,en,2018-05-10,2018-05-10 13:51:56,9,f66a23637640,4.271698113207547,93,7,0,Yesterday during Google IO a project called Duplex to interface between a request to a Google Assistant and a real business in the real…,5,"Is Google Duplex ethical and moral?

Yesterday during Google IO a project called Duplex to interface between a request to a Google Assistant and a real business in the real world via phone.
You can see the very impressive demo here:

Many people in the AI community immediately hailed it as a huge step forward in the combination of these techniques.
However, there is some concern:

While I’m always a bit skeptical of any general and immediate backlash against a technology that hasn’t been used for bad there are very good points about this that should be discussed.
Is it ethical?
Last week I started the online course on edX by Microsoft titled “Ethics and Law in Data and Analytics.” What was most interesting to me was a very simple framework they look at values of ethics.
It is broken into two value sets: 1) values based on the well being of others (in this case the person receiving the call at the business) and 2) values based on my own well-being (a mixture of Google and me). Regarding the well-being of others it includes: non-suffering, autonomy, equality. My own well-being is focused on character excellence and trust.
Let’s go one-by-one with these values:
Non-suffering — it doesn’t seem that the receiver of the call in this case will suffer in some way. If anything, I’m betting the machine will be more polite in a consistent way and get to the point so people are not stuck on calls without an end.
Autonomy — the receiver of the call can still react how they see fit, they can ask the questions they need to, and they can even refuse the call in general.
Equality — there is a possibility that there is a lack of equality because I use a service like this to save my time but not respect the receiver’s time. A good question is how we weigh this for businesses that don’t respect my desire for something that is equal in time by supporting automated systems?
Character excellence — I don’t see how this is harmful to my character excellence if it does announce that it is an assistant booking on my behalf.
Trust — this is the value that will be most problematic. As more and more of these calls are made will people believe Duplex or any caller who they say they are?
Is it moral?
Is it moral for Google to have this service? Possibly? I don’t think they are trying to be ‘evil’ with this technology.
Is it moral for someone to use this service? Probably? It is based on their moral code that they hold themselves too. Unfortunately, we all tend to have a mishmash of different morals that we hold at different times.
In society we do try to think morally ourselves but it really comes down to what is legally enforceable since there is no moral court other than the shouting of the crowd on Twitter.
Is it legal?
Based on what I know about robocalls, yes, this is legal. For example, it isn’t “irrelevant or inappropriate” or to a “large number of recipients.”
However, I’m not sure whether businesses can technically opt-out of these types of calls altogether.
Does this mean we should regulate these technologies? Probably. The devil is definitely in the details though…

While this is a great point about how to regulate the analogy isn’t 100%. We do this for natural gas not because it is the moral/ethical thing to do but because it is dangerous.
My biggest concern when it comes to state mandated regulations they will only make Google’s position stronger. They have the money/time/people to put it behind adopting those standards. Smaller companies won’t have the ability to compete as well due to their limited resources.
Is it human-centered?
I find this question easier to answer and I’m betting it is the reason why Google created Duplex. It helps everyone in the loop.
For the person that wants the reservation they get their time back of having to call and negotiate a conversation with the business. This is why services like Seamless are so popular.

For the person receiving the call, they are working for a business that doesn’t want to allow automated booking in some way. This is the easiest way for the system to interface with it. It just happens to be voice.
A great question would be whether businesses should be given the option of opting out of this type of interface. Sure, it would be potentially customer hostile but that is their choice on how to run their business.
Has anyone asked whether the people working at the businesses will mind this? Will they even care?
What about bad people?
What most people are concerned with are related to the way that others may use this technology for evil. For example, we could imagine you could start to get calls from bots that are focused on selling you timeshares, defrauding you by demanding fake payments, and general data capture.
Do all bad uses of a technology mean we should not have that technology? This is clearly not the case. A straw-man for this would be whether cars that are used for terrorist attacks should be banned. Of course, the good that comes from cars outweigh that very small case.
Nuance
If anything, this is a tricky topic. There is a need for nuance on the context.
It is one that will cause a lot of outrage and taking sides.
We should have these discussions and we should talk about what it means to believe what we believe in all of the messiness of the world we live in.
I would love to hear your thoughts below.
About Chris Butler
I help teams understand the real business problems they should solve with AI-centered solutions. The teams we work with are generally asked to “do something interesting” with the data they have. We help them avoid local maxima through happenstance correlations and focus on solving huge business problems. My background includes over 18 years of product and business development experience at companies like Microsoft, KAYAK, and Waze. At Philosophie, I have created techniques like Empathy Mapping for the Machine and Confusion Mapping to create cross-team alignment while building AI products. If you want to learn more or get in touch via email, LinkedIn, or visit http://philosophie.is/human-centered-ai.
",Is Google Duplex ethical and moral?,474,is-google-duplex-ethical-and-moral-f66a23637640,2018-06-14,2018-06-14 08:55:06,https://towardsdatascience.com/is-google-duplex-ethical-and-moral-f66a23637640,False,1079,"Sharing concepts, ideas, and codes.",towardsdatascience.com,towardsdatascience,,Towards Data Science,,towards-data-science,"DATA SCIENCE,MACHINE LEARNING,ARTIFICIAL INTELLIGENCE,BIG DATA,ANALYTICS",TDataScience,Ethics,ethics,Ethics,7787.0,Chris Butler,Director of AI at Philosophie NYC,ba6349c9c628,chrizbot,853.0,83.0,20181104
0,,0.0,71fa8c342920,2018-06-13,2018-06-13 14:53:56,2018-06-13,2018-06-13 15:03:47,1,False,en,2018-09-07,2018-09-07 21:49:13,11,b257e2056162,0.969811320754717,2,0,0,In Context Episode 8 featuring Susan Etlinger,5,"Podcast: Ethics in Enterprise AI with Susan Etlinger
In Context Episode 8 featuring Susan Etlinger
In this episode of the In Context podcast, we welcome Susan Etlinger, an industry analyst at Altimeter Group who focuses on data, conversational business, and ethics in the age of artificial intelligence. In their conversation, she and Kathryn Hume look at how AI is unlocking the promise of design thinking to enable amazing customer experiences, how the rapidly evolving technology landscape is changing consumer expectations around trust, and why enterprises should view ethics as a competitive differentiator rather than merely a compliance exercise. Find out how you can break these issues down and start thinking about them clearly for your business.

In the podcast, you’ll learn about:
How AI in the enterprise is different from at companies like Amazon and Google
The way that Mastercard successfully uses chatbots
What it means to become more customer-centric
Defining ethics and trust in a consumer enterprise setting
Explainability and the black box
Embodying trust and ethics in the enterprise
Subscribe: iTunes / SoundCloud / Google Play
Susan giving her TED Talk about big data
About Susan:
Susan’s LinkedIn profile and company Altimeter Group.
Mentioned in the interview:
Mastercard pushes conversational commerce
Google Duplex
Katz vs. United States
Ann Cavoukian
",Podcast: Ethics in Enterprise AI with Susan Etlinger,51,podcast-trust-in-enterprise-ai-with-susan-etlinger-b257e2056162,2018-09-07,2018-09-07 21:49:13,https://medium.com/s/story/podcast-trust-in-enterprise-ai-with-susan-etlinger-b257e2056162,False,204,"TO based startup, building a future in which AI enriches people’s lives while creating better, more valuable businesses.",,,,the integrate.ai blog,,the-official-integrate-ai-blog,,,Ethics,ethics,Ethics,7787.0,integrate.ai,"TO based startup, building a future in which AI enriches people’s lives while creating better, more valuable businesses.",dbf4eb8c5945,integrate.ai,359.0,1.0,20181104
0,,0.0,d532ade320b1,2018-09-18,2018-09-18 16:21:12,2018-09-18,2018-09-18 16:24:44,1,False,en,2018-09-18,2018-09-18 16:24:44,1,9c49d0987cd5,3.867924528301887,0,0,0,When the Cambridge Analytica scandal broke it became Data Science’s first troublesome watershed moment. Allegations of data misuse…,5,"A Hippocratic Oath is Not Enough for Data Science

When the Cambridge Analytica scandal broke it became Data Science’s first troublesome watershed moment. Allegations of data misuse, nonchalant disregard for privacy, and dirty tactics in politics forced the industry to seriously defend itself for the first time.
The response was largely confined to the consensus that data science needed a Hippocratic oath, not legislation. Companies that worked in any form of collaboration with CA sought to distance themselves from the fallout, and with its liquidation, it left Facebook as the only associate held to account. With the lack of protocol in place, it left Facebook in control of its destiny, allowed to reconstruct the narrative that they were naïve entrepreneurs swept into the eye of an unprecedented storm as a consequence of unintentional arrogance and ambition.
Once the poorly prepared questions in US Congress were answered, subsequent ad campaigns that tapped into the nostalgic foundations of the social network and promised to return them en masse have gone some way to repair that damage. Conveniently, Facebook’s primary product is not data science, Cambridge Analytica’s was. Having the ignominy of being the vehicle the inaugural Data Science Bond villain, Alexander Nix, utilised, and saddled with footage that alleged stark disregard for ethical practices, the company had no choice and no credibility, forcing its closure.
Nix’s career in tatters, Facebook’s algorithm changed, a toast to the Hippocratic oath? Problem solved? No.
Without regulation, data science leaves itself vulnerable to severe exploitation. Whilst the solitary stain on it seems to be coming out in a comprehensive wash, it has taken no real credible steps to safeguard itself from further scandals.
Does data science believe it’s seen the last of Alexander Nix? Well, it shouldn’t. At the time of writing, Nix is a director at a company with the same financial backers as Cambridge Analytica, and several of its associates. Clearly, Nix and Co. have not felt the bite of an oath and have escaped its jaw with a mere deed poll.
The industry only needs to look at others which believed, erroneously, that unstructured colloquial forms of ethical boundaries would suffice. Journalism suffered because of its lackadaisical attitude towards cementing hard lines in terms of behaviour in practice.
Phone hacking was a ubiquitous and tolerated malpractice and whilst it was confined to the mailboxes of celebrities and monarchs (those whom the public decided had implicitly sacrificed their right to privacy), it was sustainable. However, the line was irrevocably crossed when it was unearthed that the ‘News of the World’ had hacked the voicemail of a missing young girl whose status and welfare were unknown. This eluded to her parents and the police that this activity meant it was likely she was still alive.
Tragically, the girl in question was not, and to further damn this behaviour was that this had occurred seven years prior to its discovery, highlighting that journalism had continued to indulge itself in shady techniques to remain competitive. It had not chosen this as the moment to review its practices when the opportunity arose. I’ve deliberately attributed the actions to the industry rather than a single publication or individual because that was the perception it birthed and the consensus which the public corroborated.
Two years later and the outcome of an unprecedented legal inquiry into the practices of the press recommended one thing, clear legislation that outlined acceptable behaviour and set deterrents for nonadherence.
Data science does not need to suffer the pitfalls of good-intentioned naivety. It can be proactive in learning from the mistakes of those around it and act accordingly rather than dwelling in a hopeful dangerous limbo, staking everything on the expectation that it will all just be alright.
The industry often deals with a very sensitive commodity; personal data. To the individual, the visibility and handling of this information are paramount, and if it is abused and exploited, the industry faces the prospect of demonization and the long road to rebuild trust, on the basis such a thing is not rendered irrevocable.
Data science’s reputation is being shaped by its infant steps. Its sustained success will be somewhat dependant on its public perspective. When you think of lawyers and estate agents, it is difficult to avert the negative stereotypes that have been constructed and perpetuated throughout pop culture. In 2011 I was booted from a taxi after the driver asked me what I did for work and responded, “I study journalism.” Data scientists do not want to become the greatest apologetics of their trade.
It’s paramount that we consider that those who commit the transgressions that have these grave consequences for data science are probably not going to be data scientists. Do we imagine that every employee at Cambridge Analytica was complicit in every action? No, but the reality of trying to square your own maxim against the business needs and ideology that the decision makers demand can be an impossible task. Do you throw away a very well-paid job at the expense of your mortgage, your career, your dependents? Irrespective, it should not be a dilemma an employee ever has to grapple with, and it is unfair to expect ambiguous methods with no legal standing to be the basis an employee founds a moral rebellion on.
Alarmingly, despite calls for a Hippocratic oath to govern our actions, no such oath has materialised with any real mainstream support or publicity to be considered and adopted. In fact, this is an industry, which a decade on from its apocryphal inception, has yet to have an undisputed definition for its job role. If we cannot agree on that, how can we begin to commit to ungoverned rules concerning it?
By Tom O’Connell
(Views and opinions are my own and do not reflect the opinions or views of Pivigo)
",A Hippocratic Oath is Not Enough for Data Science,0,a-hippocratic-oath-is-not-enough-for-data-science-9c49d0987cd5,2018-09-18,2018-09-18 16:24:45,https://medium.com/s/story/a-hippocratic-oath-is-not-enough-for-data-science-9c49d0987cd5,False,972,"Innovative data science news, tips and industry insights from the team at Pivigo",,,,Data Science News,media@pivigo.com,pivigo-data-science,"DATA SCIENCE,DATA VISUALIZATION,MACHINE LEARNING,ARTIFICIAL INTELLIGENCE,NEURAL NETWORKS",pivigo,Ethics,ethics,Ethics,7787.0,Tom O'Connell,,cffe7dcb7725,tomoconnellblog,0.0,2.0,20181104
0,,0.0,788609e3c6a7,2018-03-21,2018-03-21 15:19:32,2018-03-21,2018-03-21 15:36:37,1,False,en,2018-03-21,2018-03-21 15:36:37,9,fff8e4bd6479,3.181132075471698,0,0,0,An independent panel that advises City Hall on the ethics of policing in the capital is to examine the use by the Metropolitan Police of…,5,"London Creates Ethics Panel to Evaluate Use of Facial Recognition
An independent panel that advises City Hall on the ethics of policing in the capital is to examine the use by the Metropolitan Police of facial recognition technology.

Today the Mayor of London announced that a new Chair of the London Policing Ethics Panel, the ethical policy expert Dr Suzanne Shale, will review the Metropolitan Police’s use of facial-recognition technology as part of a wider look at digital policing.
Who has responsibility when AI is running the show?
Paula Boddington, Oxford University researcher and author of “Towards a code of ethics for artificial intelligence,”…citiesofthefuture.eu
Facial recognition technology has been previously used at the Notting Hill Carnival and the Remembrance Sunday ceremony at the Cenotaph, and is one example of how the Met is using digital technology for surveillance and information-gathering. The Panel will examine its use and present general recommendations to the Mayor on how digital technology can be used to keep the public safe whilst also respecting their rights as citizens.
In a second investigation, the panel will scrutinise how the Met sets its priorities to meet rising and changing demands at a time when its funding is being cut. Since 2010–11, the Met’s general grant funding from the Government has fallen by more than £700 million, or nearly 40 per cent in real terms, on a like-for-like basis. In recent years, the Met Police have had to find around £600m of savings and the Mayor has found a further £150million of savings since he took office.
The Panel will look at priorities set out in the Mayor’s Policing and Crime Plan as well as other demands faced by the Met in exploring the ethical dilemmas that officers face as they make decisions at a strategic and operational level about what to prioritize.
Europe’s GDPR Slaps Data Collected by Cities
Cities collecting personally identifiable data, by the government or through any third party, are responsible to abide…citiesofthefuture.eu
Sophie Linden, Deputy Mayor for Policing and Crime, said: “The safety of Londoners is our number-one priority at City Hall and policing by consent plays a key role in that. This panel will help the Mayor and me to maintain oversight of the good work of the Met Police and ensure their work is conducted to the highest ethical standard.”
Deputy Assistant Commissioner Richard Martin said: “The Metropolitan Police Service is committed to ensuring we deliver an effective policing service to the communities of London which maintains the highest ethical standards. We understand the importance of maintaining trust and confidence in everything we do , which is integral to the principle of policing by consent.
“The Mayors new ethics panel will be a key element in ensuring we continue to operate effectively in this way. We therefore welcome the appointment of Dr Suzanne Shale to chair the new Ethic’s Panel and we look forward to working with her and the other panel members.”
Dr Suzanne Shale, who will chair the panel, develops ethical policy and guidance, undertakes commissioned research, provides education and training, and offers one-to-one support for people seeking ethical direction in her role as an independent ethics consultant. She has an international reputation for her work helping health care organisations to respond well when patients have suffered harm in their care.
Also joining the panel are:
Professor Deborah Bowman — Professor of Bioethics and Clinical Ethics and Deputy Principal (Institutional Affairs) at St. George’s, University of London. She was awarded an MBE for Services to Medical Ethics in 2016.
Dr Priya Singh — an experienced executive director with a medical and legal services background and experience in healthcare, international member services, professional indemnity and risk.
Professor Leif Wenar — Professor at the School of Law, King’s College London, where he holds the Chair of Philosophy and Law. He is an editor of The Ethics of Philanthropy, and the author of Blood Oil: Tyrants, Violence, and the Rules that Run the World.
Dr Suzanne Shale said: “Policing by consent in a diverse global city raises many ethical challenges, and there are often quite different views about what to do for the best. The Panel’s job is to help find the right course for London, one that helps us be the society we want to be. We have been impressed by the support and interest the Metropolitan Police Service has shown in the Panel’s work. We are also eager to engage Londoners as we debate difficult issues, and will be looking for innovative ways to do this as we develop our work.”
Sign up to our newsletter to receive the latest Cities of the Future news. You can also follow us on Twitter and Facebook.
",London Creates Ethics Panel to Evaluate Use of Facial Recognition,0,london-creates-ethics-panel-to-evaluate-use-of-facial-recognition-fff8e4bd6479,2018-06-20,2018-06-20 14:24:30,https://citiesofthefuture.eu/london-creates-ethics-panel-to-evaluate-use-of-facial-recognition-fff8e4bd6479,False,790,"""Cities of the Future"" provides a platform for experts in different areas, city and community leaders, to share their views and proposals to make cities more efficient, transparent, sustainable and, above all, increase the quality of life of their residents",citiesofthefuture.eu,citiesandthefuture,,Cities of the Future,info@citiesofthefuture.eu,cities-the-future,"SMART CITIES,IOT,MOBILITY,ENVIRONMENT,BARCELONA",citiesthefuture,Ethics,ethics,Ethics,7787.0,Cities Of The Future,"Provides a platform for urban experts, city leaders, and technology providers to share their views to make cities more efficient. http://citiesofthefuture.eu",49bd3f381442,citiesthefuture,222.0,159.0,20181104
0,,0.0,,2017-12-07,2017-12-07 02:07:05,2017-12-07,2017-12-07 02:13:05,1,False,en,2017-12-07,2017-12-07 02:13:05,0,9f62114ef01b,3.0754716981132075,0,0,0,"About a hundred years ago, if someone told you that a device would be placed in your home that would randomly sound an alarm at any time of…",5,"Answer the Phone

About a hundred years ago, if someone told you that a device would be placed in your home that would randomly sound an alarm at any time of the day or night and that you would drop whatever you were doing to devote your full attention to that device, you probably would have said “you’re crazy!” It would have been a relatively short time after that conversation that we all began to have telephones in our homes. But now, they are not just in our homes, we carry them with us constantly.
We’ve become addicted to our phones. Well, if you prefer, we’ve become dependent on our phones. Exactly when and how does dependence transform into addiction, or does it? How can you tell the difference? Is it necessarily a bad thing?
The advent of every new technology always brings both good and bad. Usually the good far outweighs the bad. Nevertheless, that does not excuse us from mitigating the bad. And that is a mission to which we should remain relentlessly committed.
One of the most important developments in this fight is with children’s access to smartphones. How old should a child be before he or she is granted the freedom to use a smartphone? Susan Dunaway is a cofounder of the Amend Neurocounseling clinic in Overland Park, Kansas. As reported by Rick Montgomery, Dunaway has some insightful observations to share about this issue (“A Movement Grows to Keep Kids from Smartphones Until the Eighth Grade”, The Kansas City Star, pp. 1A, 17A):
“Years of online overstimulation ‘acts on the brain the way cocaine acts on the brain. . . . 
Too much dopamine is released. . . . Those pleasure centers should be going off once in a while. With screen time they’re going off constantly.’
As developing brains are most vulnerable, Dunaway said smartphones may be producing a generation prone to inattention, restlessness and bursts of anger when desires aren’t quickly met.” (p. 17A)
I believe most of us have literally watched this occur. We owe it to our world to promote the positive use of technology among all ages, but especially among developing children. Technology is marvelous, but let’s use it correctly at every opportunity.
All this compounds exponentially when we recognize the constantly growing incorporation of artificial intelligence into technology. AI is already inserting itself into numerous human-to-machine and machine-to-human interactions, often without our awareness. This trend will only accelerate as Frank Malcolm, Paul Roehrig, and Ben Pring affirm in their recent book, What to Do When Machines Do Everything: How to Get Ahead in a World of AI, Algorithms, Bots, and Big Data (Hoboken, New Jersey: John Wiley & Sons, Inc., 2017):
“Within the next few years, AI will be all around us, embedded in many higher-order pursuits. It will educate our children, heal our sick, and lower our energy bills. It will catch criminals, increase crop yields, and help us uncover new worlds of augmented and virtual reality.” (pp. ix–x)
The authors also make a forebodingly accurate statement about the ubiquity of AI within our daily devices:
“Once we start using them we stop thinking about them.” (p. 1)
And therein lies the danger. Don’t get me wrong. I am all for the ongoing advancement, application, and use of our incredibly brilliant and powerful technologies on every front. However, let’s see if we can start using them while still thinking about them. That thinking about our overall interaction with smartphones, the Internet, and technology is what should raise many interesting questions that demand serious answers. Understand, I for one do not claim to have all the answers. Nevertheless, that should not stop us from engaging the questions. Here are some of those sobering questions to get you started:
1). Are we studying how we psychologically interact with technology as much as we study technology?
2). What are the short-term and long-term effects of technology?
3). What damage is being done by the bad effects of technology?
4). Is Google making us “brain stupid” or is it genuinely answering our questions faster and better thereby freeing our brains to attack more complex challenges?
5). How will we improve our ability to use the Internet to extract all its positive benefits while mitigating its negative effects?
6). Has the rate of technology development outpaced our human ability to adapt to it, and if so, what can we do about that?
7). Have you stopped thinking about them?
Your phone demands an answer and so do these questions.
",Answer the Phone,0,answer-the-phone-9f62114ef01b,2018-03-10,2018-03-10 00:11:33,https://medium.com/s/story/answer-the-phone-9f62114ef01b,False,762,,,,,,,,,,Ethics,ethics,Ethics,7787.0,James T. Meadows,"Trainer, freelance corporate writer, teacher, preacher, business consultant, blogger. Enthusiastic and passionate in all I do. How can I be any less?",27fa5e76c150,jamestmeadows,107.0,545.0,20181104
0,,0.0,97199cc52c61,2018-08-10,2018-08-10 16:21:36,2018-08-10,2018-08-10 16:31:39,1,False,en,2018-08-26,2018-08-26 22:23:07,8,b9b86a934601,2.109433962264151,1,0,0,"In this episode of the “Masters of Data” podcast, I sat down with Alistair Croll to discuss data and analytics, along with a number of…",5,"The importance of Analytics — A Conversation with Alistair Croll
Check out this episode on iTunes
In this episode of the “Masters of Data” podcast, I sat down with Alistair Croll to discuss data and analytics, along with a number of other topics around the tech industry. Croll is a serial entrepreneur, speaker, the Visiting Executive at Harvard Business School and a founder several conferences including the Forward 50 Conference, as well as the author of three books — including the best-selling Lean Analytics. Armed with an eclectic background in the tech industry and a sharp mind for data and analytics, Croll leads the listener in a highly perceptive conversation on the power and implications of analytics.
While the conversation begins by tracing Croll’s career in the tech industry, it is clear that he is a natural problem-solver and a deeply critical thinker. He explains the importance of web analysis, the primary field of his early career, in improving online UX through an anecdote about personal banking. Then he dives into his work at Harvard Business School through developing the MBA course “Data Science and Critical Thinking”, which equips students to engage the multiple layers of technical and ethical questions involved in real world data analysis. During this dialogue Alistair also interestingly reveals what Benford’s Law, bomber planes and Brad Pitt have in common.
Additionally Croll gives an overview of his book Lean Analytics, which outlines the five core stages of growth in a lean startup, including empathy, stickiness, virality, revenue and scale. These are vital to review as it’s impossible to grow well if you don’t have a way to measure that growth, which is why this book teaches the reader which metrics really matter. But in addition to delving into the positive power of data analysis, Alistair and I also discussed some of the ethical dilemmas that it brings - as the ethics of data analysis stretch to every facet of human life, from business and politics to natural disasters.
And while data used to be collected in response to specific questions, now questions are being asked in response to data. There has been a fundamental shift in the order of the analytical process such that modern organizations of all sizes need data to survive from the get-go. But raw data is never worth anything until it is analyzed and understood. And this requires the skill of asking good questions. As the conversation comes to a close, we discuss the power of asking precise questions and how Croll’s Forward 50 Conference is seeking to make society better for us all through technology. Finally, Croll discusses his upcoming work on a book that will reveal the evil secrets behind the world’s most successful tech startups.
Key References
Check out this episode on iTunes
Learn more about Sumologic: sumologic.com
Learn more about the Masters of Data podcast
Learn more about Lean Analytics
Learn more about Complete Web Monitoring
Learn more about the Forward 50 Conference
Read Alistair Croll on Medium
Follow Alistair Croll on Twitter
Follow Alistair Croll on LinkedIn
",The importance of Analytics — A Conversation with Alistair Croll,1,the-importance-of-analytics-a-conversation-with-alistair-croll-b9b86a934601,2018-08-26,2018-08-26 22:23:07,https://medium.com/s/story/the-importance-of-analytics-a-conversation-with-alistair-croll-b9b86a934601,False,506,"Thoughts on what's going on in technology, data, analytics, culture and other nerdy topics",,mastersofdata,,Newtonian Nuggets,ben@newtoniannuggets.com,newtonian-nuggets,"ANALYTICS,DEVOPS,BIG DATA ANALYTICS,THOUGHT LEADERSHIP,ARTIFICIAL INTELLIGENCE",benoitnewton,Ethics,ethics,Ethics,7787.0,Ben Newton,"Proud Father, Avid Reader, Musician, Host of the Masters of Data Podcast, Product Evangelist @Sumologic",2814fd09f883,BenNewton,110.0,94.0,20181104
0,,0.0,2baf8dc6e450,2018-01-08,2018-01-08 03:13:39,2018-01-08,2018-01-08 03:21:34,4,False,en,2018-01-08,2018-01-08 03:21:34,12,63221042db48,35.76037735849057,3,2,0,Gordon Cook interviews Martin Geddes,5,"The Future of Everything Digital
Gordon Cook interviews Martin Geddes
But would you really want to live here?
Since 1992, Gordon Cook has published an insightful boutique journal, Cook Report on Internet. For a long time I have had the privilege of participating in his associated arch-econ mailing list. Its members form a “who’s who” of telecoms and networking cognoscenti (and there are many more not on that public list).
I was interviewed by Gordon last year on the future of the Internet. We covered a wide range of topics:
The development of the ∆Q calculus so distributed computing can have reliable performance engineering built on solid mathematical foundations.
The need to reformulate computer science in a humanistic paradigm, with ethos and pathos rebalancing the over-dominant logos.
The role of the Guardian Avatar as the new browser for the AR/VR metaverse.
How Just Right Networks is deploying the first commercially available broadband cloud application access product.
The personal price to pay for going not just “off-piste” but entirely “off-mountain” to make radical progress and innovation possible.
The humbling experience of working with someone vulnerable at the fringes of society, and discovering what really matters in life and technology.
The edited transcript is below, and the original recording can be accessed via here. Thanks to Tamara Bentzur of outsourcetranscriptionservices.com for the transcription — recommended.
Gordon: I am very favourably flabbergasted by your ability to turn out these beautifully written essays, and amazing quantity, with also top quality. I wish to God I could write as well as you!
Martin: Think of it as my therapy, my “care in the intellectual community”. I’ve had a dawning realization over a very long period of time about the relationship of us to our technology, and I have kept asking myself deep questions about why am I doing this? And to what ends?. I think what’s interesting in this very conversation is your own background in Russian history, and its characters and narratives, the good and the bad, and all that’s in-between.
Yes, amen.
I think separating technology from the arc of history, human civilization, and the human lived experience is very unhelpful. I have been very fortunate to meet many wise people who have helped me to see the world more clearly. I strongly believe that each and every one of us must deeply reflect on the nature of the ends toward which are working.
You’ve done that quite beautifully in emphasizing relations with family.
Yes, and the ultimate relationship with oneself, or one’s source deity, or whatever “it” is. There have been a number of “aha!” moments for me, which have progressively “narrowed my aperture” and increased my conceptual “depth of field”, but it’s…
I’m not a photographer, but I also like your photographs. They’re really impressive!
I enjoy it! I’m a purely amateur photographer, in that for every really good one I’ve taken, there are 20 bad ones. I’m a hunter for beauty, and I like to spot those little vignettes in the world, and share them with people.
In doing so, I’ve come to understand how we really can’t separate us from our technology, the seeing eye from the camera lens; we can’t separate the techno Logos from the Ethos and the Pathos and the Topos and the Kairos. You have to see all those things as a complete, integrated system.
To see where this understanding comes from, we have to go all the way back to my childhood, where a few things are formational for me. One is my mother’s membership with the Jehovah’s Witnesses cult, with all of my mother’s family being in it, and my father’s non-membership. Therefore, for me as a child to have to make sense of that, I had to build a bigger cosmology than either my mother’s or my father’s.
I had to do it pretty damn early as well, because otherwise, I might be forced to choose between them. I wouldn’t wish this dilemma upon anyone, because really as a kid you should be doing playtime rather than philosophy. But by figuring out the role of science in the world, and its relationship to my mother’s pseudo-religion, it made me very sensitized to crap belief systems. Indeed, I’m really quite good at spotting a shoddy belief system from a very large distance.
I believe you that you’re very good at spotting crap belief systems.
Then I got a computer at a young age. I was born in 1971, so the timing of my life coincided with me being about 12-years old at the same time as 8-bit home computing hit. The moment I hit my teenage years, I was already into computers. Anyone who is a little bit older than me missed it. Anyone that came a bit later missed the first wave, and therefore, their baseline experience is different.
There is considerable difference in our ages. I also know very well I think what you’re talking about. My first online experience was June of 1980. I’ve been online ever since.
I was nine-years old then, and so I was the first generation in any meaningful sense to grow up with computers. There may be a few people in the military industrial complex, or whose parents built computers, who were before me, but very few. I actually grew up with a computer in my bedroom — my parents bought me a BBC Micro at the age of around 12.
That was the last they ever saw of me! Bless them, it can’t have been easy. My father, an aircraft mechanic with British Airways, left school at 15 with no qualifications and barely literate. My mother’s just a shop assistant and court back office typist. So how they ever coped with me, I don’t know. I’ve become much more forgiving of my parents over time.
I grew up with this new technology, and I experienced it in its very bare metal, minimalist form. It forced me to gain a total appreciation for it’s inner workings. Following a scholarship to a top private school, I went to Oxford and got a maths and computation degree. I worked my way through the 1990s IT industry. I progressed through the standard logical approach to computing. I became quite adept at it, and I made a good living.
I think from the very outset I’ve always held in mind that the human is part of this digital landscape. For example, when I interviewed as a 17-year-old to go to Oxford, I noted to the interviewer that it was all very well having these wonderful formal methods of software development, but fallible humans still had to come up with a correct specification.
Even once at Oxford, I noticed basic AND/OR type predicate logic depended on the symbols you used. When we used unfamiliar semiotics, then certain transformations and certain operations were quite slow to do as a mathematician. When I used familiar symbols like 1s and 0s, as with “0 ⊕ 0 = 0” (instead of “⊥⋏⊥ = ⊥“), it was quick to do. It’s not a mathematical observation, but a human one. It is ridiculously trivial as a mathematical observation.
Could I make an observation? Number one, I saw from a Farber email that Stephen Hawking’s PhD thesis went online apparently for the first time. So many people went to grab it, that they crashed the computer website. Number two, my best Russian friend who heard Gorbachev’s message and started an offshore software business. But before that he was coding in 1s and 0s for the Buran, the Russian space shuttle. There you go. It’s a familiar landscape for I think both of us. Let me be quiet and let you talk.
Before I’d even left school, I had a friend at who was a brilliant mathematician who went on to be a senior industry member in processor design. We together designed a 12-bit computer with an instruction set in microcode, and tried to even build it using discrete ICs. I had written a CAD program for a BBC Micro to be able to do the masking to etch the circuit boards.
I came home from Oxford for my first holiday at Christmas. I’d been playing Tetris a bit too much. I immediately recreated a version of Tetris in 6502 assembler. Within three days, I was playing Tetris again at home. It took me many years to appreciate it might not be exactly normal for a teenager, and is not necessarily be what most people get up to.
Abnormal people can have more fun than the purely normal ones.
And the more wayward the fun, the better! But that’s another story.
I’d been watching software engineering methodologies through in the 1990s. I was working at Banctec and Oracle, building enterprise IT systems. But I was always interested in how the human interfaces with the system — as defined by software development methodologies, and how to run development teams. The interesting stuff isn’t in the human “box”, or in the technology “box”: it’s at the interface between the two.
When I went to work for Sprint, I became interested in “over the top” services, and…
Telepocalypse!
Yes, my Telpocalypse era. I went to Sprint in 2001 thinking I was going to be just working on internal IT projects. I turned up in telcoland in Kansas City, and it was so disorientating! Hmm… telecoms, telecoms? I have no idea what I’m doing, but hey, let’s make it up as we go along! (Which is basically the skill you get taught in Oxford tutorials.)
So I was trying to make sense of this telecoms thing. There was this strange concept called the “phone company”, and this “phone” thing was really interesting. On its surface, the phone call looks like so trivial. It’s like… what is there to understand? Yet I have spent 15 years unpacking the phone call, and I still haven’t got to the bottom of it! I feel like there’s much to learn about the phone call.
It is a primitive form of virtual reality. It is also like a social chess game, with its opening gambits and middle games then end games. The phone call is the most fabulously complex and wonderful little universe to inhabit. And it’s fundamentally about where the humans and the machine meet.
Virtual reality without humans is meaningless. We don’t send androids off into play virtual reality games for us! It’s intrinsically humanized activity.
Then I met Dr Neil Davies around 2008, and started to understand actually the real reason why I was confused about telecoms. The theoretical foundations of data communications were missing! The scientific foundations of wireless networks were set up in the 18th and 19th centuries. The theoretical foundations of computing were set up in the 1930s, with Turing and Church…
George Dyson’s Turing’s Cathedral.
Yes.
You’ve read it, I bet.
I read very few books. I find it very hard to concentrate on a book long enough. First, they’re linearized text, and I like hypertext, and I like to be able to interact with text and do things to and with it.
I want to hear about Neil Davies… but George Dyson, having grown up at the Institute for Advanced Study, which I could get in the car and drive to in 25 minutes, is a fascinating character. He’s been a member of my arch-econ mailing list for many years. He’s still a member. I have a number of interesting people, like for example Carlota Perez.
I’ve met her! Yes, so it’s interesting to consider the history of science, because as we know the history of science is full of zigzags, and strange characters who do strange things, often outside the mainstream, like the Teslas of the world. Actually, what I’ve discovered is trying to do anything truly innovative in the mainstream is hopeless.
Neil Davies had to leave academia in order to basically re-found packet networking on a solid mathematical basis. What Neil Davies has done is the equivalent of what Turing did. Neil Davies is a peer of Turing. He’s too modest to say it himself — he blushes slightly when I say that. However, it’s true. Turing’s theory of computability is the counterpart to Davies’s theory of translocatability, and both belong alongside Shannon’s information theory.
I was at the eComm conference with Sheldon and Davies in maybe 2011 near San Francisco airport. I think it was your opening talk on Neil Davies’ system, and I took a bunch of pictures of that. That was my introduction to Neil Davies.
My job is to act as an interlocutor and translator. I’m a terrible mathematician. I haven’t got the concentration or the inclination. However, I’m quite a good “storyteller with a maths degree”. I was able to very quickly identify that Neil was addressing fundamental issues rather than surface ones.
However, it took me probably five years from first meeting Neil to be able to step far enough backwards — and you had to go a long way backwards, and walking backwards for a very long way is not very comfortable — to be able to see the full vista of what’s happened, and where we took a wrong turn.
Indeed, what’s happened is we’ve entered into a fundamentally incorrect understanding of the nature of networking. It’s a fact that the current internet is built on a category error.
All science goes through three stages: classification, correlation, and causation. And because we’ve got classification wrong, everything we’ve built since is a bit screwed up. To be able to articulate the nature of the classification or category error is difficult, however.
For example, I stood on stage after Nick McKeown in The Hague week before last. He’s the top man in software-defined networking. He’s describing his latest and greatest breakthroughs, but he’s locked into that current category error. I stood up afterwards and said you need to change your resource model.
Did he appreciate that?
I heard after that he did clap… eventually… and rather briefly. From his perspective: Who the hell is this guy on stage? He doesn’t have a Stanford professorship! He hasn’t got any published papers! He doesn’t have all these corporate sponsor logos in his slides! He doesn’t even have a PhD! Who the hell is he talking? He probably thinks the organizers have got a nutcase on after him.
I’m effectively saying “Nice try, Nick! Really appreciate your effort. You’re doing some good stuff, however, the conceptual box you’re working in is a bit small. I suggest we need to look in the bigger box.” Of course, it’s not aimed at him personally. As a human being, I respect him enormously. In the brotherhood of human beings, he’s a good man. I’m just making fun at the moment.
But in the terms of doing just network technology stuff, it’s… “You’re lost in the category error. You’re trying to do computer networking. That isn’t the problem. The problem is distributed computing, and that’s a different problem. You’re trying to connect the computers. That doesn’t result in the required application outcomes.”
Connected computers is the wrong thing to do! The paradigm — the category error is to confuse the means and the ends. Currently, people believe that networks do “work”, focusing on the means. Net…work. This is a mistake, and it’s the same mistake that turns up in two different ways. One of which Neil Davies fixed with the ∆Q calculus to describe performance. Then, orthogonally to that, is the work of John Day with RINA architecture.
John Day and RINA!?!
John Day is another genius — and I’d say this to John’s face because I think he wouldn’t slap me. He’s the most lovable, cantankerous old sod there is.
I remember the episode not too long ago where he yelled stuff at you.
We all bring our wounds and our past with us, and John has lots of wounds for being right for 40-odd years in a networking world that’s going technically mad. I can understand where his curmudgeonly nature comes from.
Neil and John both spotted that networking really is inter-process communications. It isn’t computer networking. The outcome that matters is whether the computation advances, not whether you do “work” by moving packets. The current work paradigm is we try to move as many packets as fast as possible.
This is like a 1920s car factory where you ingest as much rubber and steel as you can. And occasionally, you spit out a working car, but you also spit out a lot of nonworking cars, and cars that need fixing at the dealership.
We know in manufacturing that maximizing inventory and work-in-progress isn’t a good idea. This is not a novel concept in the world of management theory and quality systems. However, in telecoms it’s really new. Maybe we should manage the flows and minimize work-in-progress, because if there’s 50 years of evidence that’s a really good thing.
Applying all the well-established ideas of Lean, Theory of Constraints, and Six Sigma is a total novelty in our networking world, despite the fact we built a whole infrastructure for society based on the current Internet.
Indeed, the current internet is just a primitive prototype that does one thing well, which it abstracts away connectivity. We don’t have to worry about whether you’re connected to a dial-up modem or a super-fast fibre connection or satellite. There’s one API that works for everything.
Great. Thank you very much! We’ve learned a lot from it. However, it’s basically broken in every other way. Everything else you could possibly get wrong, is wrong. The current internet architecture is a Rubegoldbergesque pile of workarounds and hacks as a result.
TCP/IP, right?
Don’t swear like that, please, Gordon. Yes, it’s BGP and all those other things, yuck.
We lost an entire layer.
Absolutely. The history books will look back and think… how did they go for that long without realizing there was actually no proper engineering, no real science, and they hadn’t even gotten the maths right? How is it they did that for 50 years without noticing?
What’s missing? The architecture is fundamentally wrong, so it’s divided into many layers, rather than scopes (with only two repeating layers). The first step is wrong!
Also, the ability to measure, model, and manage performance is missing. We left out the most basic bit of mathematics: we’re missing the equivalent of complex numbers, but for probability. Therefore, it is a bit like in electromagnetism, if you don’t have imaginary numbers, you can’t build complex numbers, so you can’t do complex analysis, which means you can’t write Maxwell’s wave equation, which means you can’t design a MIMO antenna, which means you can’t do spectrum policy.
Brough Turner and antennas, this is another guy on my mail list!
I know Brough, too. There are many bright people currently working in the troubled incumbent networking paradigm. However, you need to draw a bigger conceptual box. It means going back to the very foundations, as there are some things that Turing didn’t address.
One of which was thinking about the time it took to transfer the symbols between the tickertapes. He did computing, but he didn’t really do distributed computing. Hence the first step is to get the performance science and maths right, so as to properly model what we already have.
The second key thing Turing didn’t really model is… where the heck do the symbols come from, and where do they go to? The contextualization model of information was missing. That means we’ve spent 50 years inducting, training, and rewarding logicians to program computers. That’s a bit unfortunate.
Why so? Because the people who are — how do I put this in the most compassionate sense — most adept at the very dry and abstract world of computing aren’t necessarily always those who are most empathetic and social. Yes, some are very empathetic and social. However, the world we’re going into is one which is kind of the Revenge of the Phone Call.
We started off telecoms with social telegram messages and telephonic person-to-person virtual reality. The phone call is virtual reality. We’ve now had 150 years or so of building this one virtual reality application. Then we kind of veered off-course a bit and built the Internet. We’ll get over it!
Now we need to come back and build the next generation of virtual reality. After all, what we humans really like doing are intercourses, social and otherwise. Virtual reality is the place to go and do them.
This means we need to re-contextualize and re-humanize the essential nature of computing. So the work of Neil Davies and his collaborators is only part of this re-founding and refactoring process. ∆Q is putting in a theory to define what it means to copy information in a timely manner, and how we model that.
But actually, we need a bigger theory: how the heck does the computer relate to its context? What are these symbols, anyhow? And if these symbols represent information that was bio-sensed off a human (and voice is a form of intimate bio-sensed data) then… cripes, you can analyse the recording of this very call and do diagnostics for various mental-health conditions! You can analyse just how mad I am (I think 27 is my score… so I am very mad).
This recorded voice call is actually HIPAA-protected healthcare data! The recording of this phone call is healthcare information, but the current paradigm we have is it’s treated the same as a music recording. It isn’t seen as being essentially different.
We’re moving away from a historical paradigm of computing, which is anchored in pure logic. The Logos is being addressed very strongly, but the Ethos, and Pathos have been left out. From, example, at university I studied formal software development methods. You start off with a specification, and say what is the logical outcome you require. But there isn’t even a space on the page for what is the ethical outcome, or the feeling state. The size of the page or the engineering requirements book is too small to capture the contextual design specification! We need a bigger play space.
If we carry on down this old paradigm, we are going to be in deep, deep trouble. If we continue to build systems which amplify the power of some humans over other humans, and ignore the power relationships, wellbeing ethics and feeling states, then we are going to drive ourselves into a dystopian place, if we aren’t there already.
How do you see Donald Trump as president in the U.S.
I see Trump as a symptom of a systematic problem. I think Trump as an individual is largely irrelevant. Trump as a phenomenon in a wider system is extremely interesting. We have to understand better at a deep level the relationship between our technology and our society.
In the world of computing, we are deities, very small deities… but still deities. And we create these little information universes. We need to create mini universes in a way that better reflects the societal outcomes we wish to have.
We need to come up with a “new logic” — we’ve followed the Logos path for a long time and come up with artificial intelligence. I strongly believe we also need augmented conscience. We need to be able to take our own concept of what is right and wrong, codify that in some way, and then bring it into our virtual world. Otherwise, there will be at best amoral place, and at worst highly immoral one.
Although going to an immoral place in a virtual world sounds like a fun weekend trip, it’s probably not a healthy place to live. We’re kind of there already, with what Google and Facebook do to you in terms of tracking you on the web.
If someone did what’s happening to my kids at the moment online in the physical world, I would probably feel motivated towards violence. If someone was stalking my kids and sticking Post-It notes in their bag as they came in and out of every building and tracking them everywhere via binoculars, I would…
Privacy is gone?
Well, it’s been eroded, but I think the whole “privacy is dead” thing is self-serving bullshit. It is the words of those who are powerful persuading the powerless to accept their fate. Fuck off, I say. And I use such strong words knowingly. It’s ABSOLUTE NO! I am NOT accepting and consenting to that. That’s a psychopathic doctrine. You may believe it, and I am going to fight it with every ounce of my existence.
I believe that we need to consciously construct our world, and it’s not a “virtual” world absent established codes of acceptable behaviour. The idea that virtual is separate from physical is nonsense. We need to construct a world which is moral. And if morality is not what’s driving what we do, then at best you’re amoral and worst you’re immoral. We see the undesirable consequences of that already.
We need to go back to basics, and I think computer science needs abolishing. It is itself a category error! What is figural? It is not about the computer! What is figural is how this digital stuff interfaces with the rest of the world. The computer itself as a mechanism is not very interesting, sorry. I had lots of fun playing with them, but what matters is how these things impact us in the world.
We need to start from the other end: it is the interface ports on the computer which are interesting, not the processor. The keyboard, the video display, whatever. What goes through those interface ports? What happens inside the computer is not the interesting thing. We’ve got it backwards!
What is the impact of what goes on inside the box on the outside of the box? Criticlaly, what is the ethical impact? One starting point is those classic five things of Plato’s rhetoric: Ethos, Pathos, Logos, Kairos, and Topos. We need to re-conceptualize computing around those five aspects for it to be “rounded”.
What Neil Davies has done is basically a Theory of Kairos, with timing. We’ve now got a form language which discusses Kairos. So great! We now have Logos and Kairos done. Where’s Pathos and Ethos and Topos? We have an incomplete conceptualization of the discipline. As a result, we don’t even ask the questions that are important. [MG: On reflection, the “semantic web” is the Topos.]
You’ve got a lot to say, and I want to make sure you get to say everything. I’m sitting here with rapt attention.
I published an article on this. “Why I’m Optimistic About the Future”.
That was very good!
One has to think about how society is constructed, the nature of power, and how access to information flows provide power. It’s not controversial to say that the internet has significantly changed our world, and it’s significantly changed the balance of power between different parts of our world.
When it came to renew my domain name, cookreport.com, I managed to renew it for five years, which is what I’ve been doing all along. But my God, if I lost that domain name, it would almost rub out my existence!
Your virtual identity and your physical identity cannot be separated. The current identities we’re building are not self-sovereign. We are now feudal subjects of ICANN or Google or whoever. This is a big problem, a serious problem.
Let me say one thing. I’ve been thinking about it. If I were to draw a cartoon of how I conceive it personally, I have two arms and two legs. There would be Apple and Google each have an arm. Amazon has my left leg, and Verizon, the freaking phone company has my right leg. All of them want all of me, but they only have a part of me.
Why can’t get together as neighbours and build a street-level network? And the streets get together as a neighbourhood and build a neighbourhood network? And the neighbourhoods get together and build a town network, and the city network? We are fundamentally disempowered. It’s partly a result of TCP/IP’s stupid architecture, but there are other reasons too.
We need to start again, and we need to start again with the human as the centre. We need to re-found computer science around the human. To do otherwise is to empower a psychopathic world, where the humans are seen as peripherals to the machine. It’s Matrix-like; we’re batteries in the system. It’s horrific!
I worked as a student for two summers at BUPA, the British United Providence Association, a private healthcare company. I was doing VDU data entry for insurance claims processing. I spent six months of my life typing in insurance claims. I’ve also spent time on the BP share privatisation entering 10-digit numbers for weeks. I’ve been in the dark satanic factory mill, as it were, of the IT world.
It’s a hideous experience, as far as I’m concerned. I came out mentally ill and clinically depressed. By the time I went to university at 18, having just spent three months doing that, I was genuinely unwell. It’s partly due to who I am, but it’s also partly as a result of that environment. It’s utterly dehumanizing. I will never forget it.
I believe that we need to have systems which are ethical by design. To give you an example at the moment, I’m working with a consulting client. We’re rethinking how call centres work because at the moment it’s all about how to extract as much labour value from those workers, so as to deliver a “better” customer experience at less cost. It doesn’t work because the call centre agents are feeling abused, and as a result, they withdraw their love and care from the system. Then the customers get really pissed of; therefore, the whole thing falls apart.
Until you see the agents as humans, as well as the customers, you can’t succeed. The question I’m asking now is: how do you humanize call centres? How do you inject gratitude? How do you make sure they take rests? How do you make sure there’s fun? Dancing, music, jokes, whatever it is. How do you reconceptualise this around the human, not around the business process? I don’t know the answers, but I’ve asked good questions!
What came out of the research I did with Kelly Fitzsimmons in the Hypervoice Consortium is that we’re entering into a world of the metaverse: information blended with the physical as augmented reality. The current web is the “old thing”. There’s a new “web” coming up which is more of an AR/VR concept. And we need a new “browser” for it.
We propose the new browser for that world is the Guardian Avatar. It’s a bit of software, but it’s also a thinking tool to consider the nature of relationship between you and the “real” world. It is like how the “4th wall” in the theatre is not a physical wall: it is a way of talking about the space between the stage and the audience.
The Guardian Avatar is a system of human identity protection and projection, which looks after you in that world. It allows you to project your own ethics and morals into the world. When I turn up a free Wi-Fi hotspot, and there’s a license agreement and terms, my lawyer bot reads through it, and checks it’s okay with me. But those terms have to be presented in a machine-readable format.
We need to start to surface the underlying lack of choices we have. I think of Beisser’s Paradox of Change — we only change when we fully internally identify with what is, not with what could be or what should be. We need to build systems that show us the lack of power we have over our environment, and forcefully surfaces the non-negotiability of these contracts. This should result in their non-enforceability in many cases.
Basically, the world we’re building at the moment — the Google and Facebook world — is unethical. I couldn’t work with Google or Facebook. I appreciate the people inside those organizations don’t set out with the intention of doing wrong. They don’t see themselves as digital equivalent of tobacco. However, I’m sorry, but that’s what you’re doing. It’s damaging and disempowering, and it’s not ethical.
I think we have a number of revolutions here. Yes, we first must have a revolution in network mathematics and science. Wouldn’t it be nice if we could do science and engineering for application performance!
I found myself in a telco client two weeks ago, standing in front of them, and their brand identity is supposedly all about quality. I’m standing there trying to persuade them that maybe science and maths and engineering might be relevant to that problem. Why am I doing this? Why am I even standing here in front of them? Something’s very wrong here!
So first let’s get the basics right. We need to have a foundation of science and engineering on which networks are built, with known performance and reliability safety margins, and are fit for purpose, with us taking responsibility for the outcomes. But on top of that, we need to have new kinds of applications and interfaces. The Guardian Avatar is just one.
We also need to destroy and reinvent money because money, I believe, is a psychopathic technology as currently conceived. It’s become a way of laundering immoral activity, and denaturing human social relationships.
We’re seeing with blockchain the ability to construct new value networks, and to embed ethically positive incentives — to engineer new behaviour incentive mechanisms, and to localize and contextualise these in appropriate ways. I think the technology we currently call “money” is end-of-life. We need to abolish it, and build something new.
There’s also healthcare. What we have today is the “sickcare” model. We need to have a “wellcare” model. We need to completely invert the incentives of that system. What we have at the moment is corrupt, and almost every aspect of it is broken.
We can talk about transport, and energy, and other domains as well. There’s a series of fundamental reconfigurations of our society that are needed. For those to happen, we need to have an appropriate information infrastructure underneath, and also an appropriate interface to the human on top. I believe the Guardian Avatar is a helpful way of expressing that need.
It is time for “All change, please! Please alight from the present Internet, and wait at the platform for the next Internet to arrive!” What is next will be personal: its fundamental scale will be human-scale small and grow upwards, not global-scale big and scale downwards. It will be secure, and performant by design. Morality will become a first class design object, as well as performance.
Our challenge now is collectively to dream. It’s a massive challenge, and I believe that this is possible. It is not only possible but necessary, because the nature of the world we’re currently living in is not a nice one for our grandchildren to inherit. I believe we can do much better.
Agree.
I grew up with the Jehovah’s Witnesses cult. I know what it’s like for people to be inside a box, and not even know the box is there. To have a system of manipulation and emotional control that keeps them forever looking outside the box. It’s obvious that they are a cult to the rest of society; but if the whole of society becomes a cult, the outside becomes invisible.
Our current society feels like one giant cult of the ego. And we’ve been infected an infested with psychopathic doctrines everywhere, and we need to change. We need to spiritualize. We need to feminize some aspects, and masculinize others. We need to do something completely different. Paradigm change of the first order.
Definitely agreed. I’m not aware of a hell of a lot of what you do, but one is your work with Peter Cladingbowl. How about talking about him?
Pete’s my partner in crime and co-founder of Just Right Networks Ltd, and counterbalances my intellectual hooligan craziness with grounded adult supervision. How can we enact all this innovative novelty in the world, and actually make a living from it? After all, for the last seven years, I’ve been basically living off of minimum wage, doing fundamental science and philosophy, as people don’t pay for it.
That’s grim.
It is grim! Yet I feel deeply contented, and these financial situations can change quickly. I’m a very blessed man, and my outgoings are virtually nothing.
You have basic integrity, and you keep your mind alive.
My experience of corporate life is violent. It may not be physically violent, but it’s emotionally violent. I cannot go there. I don’t belong inside corporations. I’m gay, a blended chimera of male and female, warrior and nurturer. I find life inside dominator culture, alpha-male organizations just revolting. And they don’t like me either!
I went to join BT. One of the executives there — I won’t name names — she described me as “not fit for consumption”. Hmm… “not fit for human consumption”, which I thought was a beautiful projection of unspeakable inner desires, but never mind. I really don’t belong inside the megacorp.
My work with Pete is about taking the crazy stuff we do forward — these wild new ideas. I have no bosses. I’ve taught myself to basically exist on nothing, so nobody can ever control me. There’s nothing to withdraw, so I have total freedom. I can forage for myself. I know at any moment if I really am short of food, shelter, anything important, I can just mail my mailing list and say “help!”, and it will manifest. Seriously, I don’t even have a pension. I’ve just got love. I mostly live in the love economy, not the money economy.
I was very touched by your essay on your homeless friend who fell through all the cracks.
Yes. Hold that thought for a moment. Just quickly let’s do Just Right Networks and Pete, and then come back to that, because that’s a life-changing thing for me, is standing next to someone ill and facing homelessness, day after day after day for two years.
Go with Just Right Networks again.
My longer-term vision is to build the Guardian Avatar, to help in a series of transformations in healthcare, transport, energy, infrastructure of life. Underneath that we need to have an information infrastructure which at least gets the basics right.
What we’re doing is applying the breakthrough mathematics in science and philosophy to produce commercial quality-assured broadband, which is fit for purpose for cloud application access. So I’m working with Pete to deploy that Just Right Networks. We’re making good progress, but it’s slower than we would like.
We see a need to build a quality-systems revolution for telecoms and cloud. We need to apply all the well-established body of management theory, plus new science and mechanisms to deliver predictable, engineered experience for users. This ought not to be a controversial thing.
And of course for me, one of the most fascinating things was that he’s been doing this a good deal in Russia.
Yes. Pete’s originally from South Africa. He’s got a long career as a senior executive working in building data centres and telecoms. He’s done it in all kinds of different countries. I strongly believe that the interesting stuff that’s going to happen in the next 10 to 15 years, the new architectures and new use models, are unlikely to come from Silicon Valley. We’ll see this stuff happen in maybe the “Post-China Sixteen”. It’s Peru, Indonesia, Vietnam.
These tools and technologies will get to a certain point, but people who’ve got nothing to lose will apply them in new ways.
Forgive me for interrupting, but you said Peru. In the last three or four days on arch-econ there’s some discussion of Google’s Project Loon.
Even Google and Facebook sometimes do good things.
There’s an infrastructure being built, but the infrastructure is still largely conceived around perpetuating current systems of control and power, and the dominator dynamic, and the current corporate-state alliance, which is not a healthy one.
With the stuff we’re doing in Just Right Networks, I’ve turned from being a telco gamekeeper into a cloud poacher. It was lovely working in telecoms, but now I work for the demand side. We are doing basically high-frequency trading against BT’s network in order to extract the arbitrage between the underlying quality of the assets and the quality required by different applications.
That’s fascinating.
Telecoms as a pipe-based business is finished. We still need the underlying physical infrastructure, just like if you’re doing shipping around the world you still need the underlying ships and trains and trucks. But those are not the controlling platforms anymore. No, the controlling platforms are those in the physical logistics space are those that match supply to demand.
The same thing is happening in telecoms. Telecoms is about to become more like Uber, financial services derivatives, or option-trading platforms. And the people who control the platforms or are able to make the best “trades” will make lots of money. Those who don’t even understand the problem will go out of business.
Most of the telecom industry doesn’t even understand the problem, let alone the solution. What happened in devices with Nokia when Apple came along looks like it’s being readied in the distributed computing space. A new platform with a stronger philosophy wipes out the old one.
Everyone who thinks they’re currently doing computer networking, rather than distributed computing, has a short and brutal life ahead, and they don’t realize it yet. Those of us who can apply new architectures, and abstract away the underlying implementation of the resource, and can out-trade everyone else… we will win. There is no possible other future.
What we’re doing with Just Right Networks is we’re building quality systems to enable us to calibrate, coordinate, and control digital supply chains. We’re building an initial exemplar which is an exemplar business which is to deliver “I can’t believe it’s not ISDN!” here in the UK, to show we can deliver the quality outcomes of traditional circuits with the cost structure of packets.
Once we’ve done that, it’s merely a matter of execution! [MG: My tongue is in my cheek.] It takes about ten years until this new paradigm takes over the whole world. In the meantime, I believe there is an end game of this, which is that we’re going to see the current internet be transcended, and replaced by new industrial and intimate internet. And it will have to be engineered to deliver performant outcomes with known safety margins.
In other words, we need to start to have network interface agreements — information exchange agreements — which reward better quality. Such things will inevitably encourage cheating, to take the money and not deliver the quality. And therefore, you’ll need equivalent to the Visa network, but for telecoms. You’ll need people who construct the trusted interchange agreements, which don’t require everyone to audit everyone else.
In the world of retail banks, you don’t need to go and audit every other bank in the Visa network to accept Visa cards. The Visa network manages the trust function. So the same thing will happen in the telecom world, is that we will create a new Visa network of trust, and trusted exchange agreements which reward better quality.
We also need to “de-reward” and “de-prioritise”, where we make very cheap low quality available as well. For every higher quality, you must have a lower quality. So we need to also build new transport systems for very bulky, cost-sensitive data, which is time sensitive.
It’s all going to happen. There are deep, profound reasons why the current Internet is not technically or economically sustainable. The current Internet is not a scale-free system: it has got scaling limits, and it is hitting those limits.
We’re already seeing bad technical things happen on the present internet, as the present internet breaks basic control theory. The control loops are long, and the phenomena they’re controlling are fast, and this doesn’t end well. Every time we make the networks faster, the control loop stays the same length, but the rate of state change goes up
Therefore, the stability of the system goes backwards. As we throw bandwidth at all our problems, this is not pretty, and you don’t want to see the results. Death by non-stationarity is the fate of the current internet. And it’s already happening. We’ve had 25 years of the “rise of the stupid network”: endlessly throwing bandwidth and capital at quality problems doesn’t work. We’ve proven that, and now it’s time to move on.
Just Right Networks is about building the new paradigm, based around engineered quality and contracted performance at all the boundaries in the supply chains. When you take that series of quality contracts, and compose them all together, you get a predictable outcome which is fit for the purpose of the application. Wouldn’t that be a novelty?! [MG: Once we’ve done quality, we can move on to other things, like privacy.]
Yeah!
I’ll take us back to something we touched on earlier. I’ve got an understanding of what we’re doing and why we’re really doing this quality transformation work. I was going to see one of my old Oxford undergraduate friends, who lives in a nice town in England. We had a lovely afternoon tea in a posh hotel, and I was walking back to the train station on a dark January evening in 2016. There was a woman on a cold street corner begging.
You think… empty street… cold night… woman in her thirties. This isn’t good. She’s doing that because she’s desperate. I sat down next to her and started to understand a bit about her, and spent about half an hour with her. To cut a very long story short, I’ve spent the last two years supporting her. Basically, all the healthcare and social welfare systems have broken down, particularly for people who are mentally ill, as she is. She’s very bright. She has a good heart.
But she’s not well. She’s had a traumatic life. The systems are designed for well people, if they work at all. The idea that if you are sick you may not be able to go to the benefits appointment, because you’re too terrified and physically sick with anxiety, that doesn’t occur to them. I’m what stands between her and an abyss of poverty and ill health.
About a year ago I ran out of money. I had to start employing other peoples’ resources to help her. It’s been a transformational experience. Even today, it’s sort of brought me an enormous wealth of understanding about the nature of love. The Greeks have the six words for love: brotherly, conjugal, erotic, unconditional love for children, and so on. I’ve learned a lot about love.
Over time, I’ve also learned a lot about telecoms, heavens above, and these two worlds can touch. So even today, I was having a meeting with a telecoms regulator. I realized the reason why we need to have sender party pays data. It is not because there’s a market for it. It’s because there’s no market for it! It’s because she can’t afford broadband
But she can’t go to her MP’s office because she’s agoraphobic, and got a serious social-anxiety disorder. So going to her member of Parliament to a meeting is terrifying for her. However, she might be able to go as an attendee of a virtual conference. But she can’t afford broadband. Everybody in society needs to have access to core services like being able to attend virtually their MP’s office and clinic.
Sixty readers from my newsletter got together and we bought her an iPad with 4G built in. She had no means of communicating with the world. She loved it, and then she left it at a neighbour’s house. She has a neighbour, an old guy she looks after sometimes, and he looks after her. His relatives nicked it. They know, because of who she is, she’s never going to go to the police. So they just took it.
You start to understand how, when she runs out of electricity, her phone doesn’t charge. When the phone doesn’t charge, she can’t call people. Nobody. She doesn’t have an alarm clock, so she can’t wake up in time to go to appointments. If she doesn’t go to the appointment there’s not going to be benefits. You stop your benefits, then you lose your house. Not only do you lose your house, you start getting charged an extra 11 pounds a day by the council for the rent. Then you get your court fees. Her freezer defrosts and the food rots.
You start to see how if you’ve not got mental-health problems at the start, you’ve certainly have at the end! Now I understand at an embodied, profound level, her predicament, because I’ve also run out of money in this process. I’ve actually stood in the train station in London going home, looked at the chocolate bar machine, and not only did I not have a pound with me, but I didn’t even have a pound anywhere. I’d even used up my coin jar. I totally ran out of money.
When everyone else has abandoned someone like her, and nobody else believes in her, then what kind of man am I if I don’t stand up and act? Who am I really? Why am I here at all? Why am I bothering with this technology innovation in the first place? This experience has fundamentally transformed my sense of purpose of what I’m doing.
Neil’s new ∆Q network maths, lovely. Great breakthrough. It’ll happen because there’s an external reality of the world that forces it upon us. I just need to wait. I know he’s right. Ultimately, I just need to sit here, and wait for the incumbent system’s collapse, because it’s not aligned to reality.
However, if we’re not building something that’s moral by design, if we’re not here to relieve suffering — if all we’re here to do is to do cheaper 4K video — the I’m off to do something else. If 4K video is your highest aspiration for the internet, that’s pathetic. We must all raise our game.
Figure out how to look after people like my struggling friend, who is smart, really bright, has a good heart, is a caring person. Her ending point is very different; but starting point is not different. I’m also from a work-class background. That could be me. I could’ve been the one on that street, and she could’ve been the one with the Oxford degree walking past after a nice afternoon tea.
There but for the grace of God am I.
Now I “get it”. I didn’t get it before, but now I do. There have been many other humbling experiences in life, and many screw-ups. I now have a sense of what I’m here doing this for. The challenge I see is to link morality to the mathematics when we design information architectures. That’s the “grand sweep” that I’m here to attempt.
Is isn’t “how do we stop building smart cities?” In fact, how do we stop that process? It’s a disaster! I don’t want to live in a “smart” city. It’s awful, horrific. I want to live in a superhuman city.
To me a “smart” city is the dystopian nightmare. I don’t want to empower all these sensors everywhere to be spying on me, all the time building artificial intelligence control systems which I have no control over.
I think you’d better start by building a smart neighbourhood.
Not “smart” at all. Smart is in the Logos category. I want Ethos and Pathos first. I want humanized qualities first. I don’t want a computer-science lab sprayed all over the world. I don’t want it! I want a humanized world. How are we going to have cities where we get together and sing and dance?
If you were in the Jehovah’s Witnesses, like my mother, you have no concept of the world out of the box. The world outside the box is rejected, and they don’t want it. If you’ve grown up in the box where computers exist to deliver logical outcomes, then the idea that you need to start off with an ethical outcome, or a feeling-state outcome, is just unthinkable.
The future we face is like Facebook, where they have the Year-End Review feature. Logically, it picks the most important picture from your year, and shows it to you. The fact that it was your kid that died, it was your apartment that burnt down, didn’t occur to that person. The specification language doesn’t include what is the feeling state you want to engineer, or what’s the ethical outcome. It doesn’t exist.
There’s no concept of the human power relationship in the software engineering paradigm. And there should be. Yourself, Gordon, as a student of the Russian history and culture, will know that power relationships really matter!
Oh, yeah!
Don’t they? And their abuse really matters, too. Lots of people die when we get it wrong. It might actually be quite important. If you’re disturbed by how the world currently is, and you’d like to have a better future, then become aware of the paradigm you’re currently working in.
The current paradigm of information technology is somewhere between asocial and antisocial. We need to re-found this, away from information technology, and towards human technology. That’s the start. And HT, not IT, and it’s about augmented identity, not artificial intelligence. It’s about giving humans superhuman powers of empathy and understanding, and putting on a corrective lens to help us achieve our higher selves in the world.
At the moment, what we’re doing is just gone off the rails, and doesn’t take us to a happy place. I want to go somewhere different. I want a different ticket, please. I don’t like the desitnation where this train is going.
I agree with all of that.
I’m currently sat here at the Institute of Directors here in London in this beautiful big room, with all these portraits of old people around me. I’m very privileged and very, very grateful for what I have. I’ve never had to miss a meal in my life.
Was this associated with the meeting you had this morning?
Indirectly, yes. I’m in London to have meetings and see friends. It’s been a hard journey. The last seven years have been difficult; it’s been totally rewarding, and I’d never do it in any different way.
What we’ve covered is absolutely beautiful and enlightening.


About Martin Geddes

I am a distributed computation expert, network performance scientist, and consultant to telcos and their vendors.
I collaborate with leading practitioners in the communications industry to create game-changing new technologies and businesses.
Get in touch if you need a thinking partner, inspiring speaker, workshop magician, or strategic advisor. I also offer education in network performance, digital supply chain quality, and the future of the Internet.
martingedd.es
",The Future of Everything Digital,43,the-future-of-everything-digital-63221042db48,2018-06-07,2018-06-07 03:35:14,https://medium.com/s/story/the-future-of-everything-digital-63221042db48,False,9291,Fresh thinking about telecoms,,,,Future of Communications,mail@martingeddes.com,future-of-communications,,martingeddes,Ethics,ethics,Ethics,7787.0,Martin Geddes,Humanistic technophilosopher,45f26b7e0479,martingeddes,2481.0,1392.0,20181104
0,,0.0,,2018-05-04,2018-05-04 14:18:47,2018-05-04,2018-05-04 14:22:07,1,False,en,2018-05-04,2018-05-04 14:22:55,3,a0b02da1250b,3.720754716981132,0,0,0,"With his article, Holmes has merely touched the surface of this issue. This makes him much further ahead than most on this topic. One key…",5,"Famous Image of Jesus from St Catherine’s Monastery done as mosaic with other depictions of the iconic figure providing a symbolic depiction on how religion can have so many different meanings to each individual. Imager Source: Unknown
The discussion of religion in business is rare, yet Ryan Holmes of Hootsuite referred to its need for the development of Artificial Intelligence (AI). Do we need a little more religion in the development of advanced technology? Or in business for that matter? It’s an interesting notion and one I have pondered personally.
With his article, Holmes has merely touched the surface of this issue. This makes him much further ahead than most on this topic. One key argument is “in place of parent and priests, responsibility for this ethical education will increasingly rest on frontline developers and scientists”. Interesting point, but I feel it’s a slight trip out of the gates. If the development of AI provides us the challenge of determining what it means to be human — discovering purpose, determining right and wrong, etc. then should we really leave this to developers and scientists? Shouldn’t we go beyond this? One of our tech leaders sets the table with this question, I am going to take a seat and carry-on the conversation.
Considering such notions can easily put people’s noses out of joint and rightly so. First off, the very term ‘religion’ can have different connotations depending on the observer. Second, it can be questionable if it’s truly needed to determine right or wrong.
Esteemed atheists and public intellectuals like Sam Harris, Richard Dawkins, or the late Christopher Hitchens would question the necessity of religion within modern society. They would argue we can look beyond it. In some manner I would agree with them; yet, it is hard to deny the centrality of religious thought in western society when examining our moral and ethical understanding. It’s a big part of our cultural make-up, both good and bad.
Religious texts can provide us with rich and powerful lessons of people grappling with deep questions of life. Indeed, they provide stories of people in a period of time grappling with the very questions we are facing today. These stories have played a large role in either informing or reflecting our concepts of what is right or wrong — the very thing Holmes is pondering on the challenges of AI growth. I applaud his courage in questioning religion’s usefulness in overcoming this challenge. It’s not an easy question to answer, but good ones never are. Nor can they be answered completely. And that’s one of the main connections here. The depth of this issue will involve a somewhat religious experience — a deeply human experience of finding purpose and meaning.
The recent news on Facebook and Cambridge Analytica, has a lot of people up in arms over invasive technology involving the mass collection of data and manipulative algorithms. As we continue to improve machine learning and build robust AI capability, invasive and manipulative aspects of technology will continue to be a concern. Yet technology is a tool. We are the ones who wield it. And to be even more specific, it was particular business leaders who thought and continue to think that such tools, their associated capability, and promoted behaviors are acceptable. Even Holmes makes this inadvertent mistake in the statement I have cited. I believe it’s misguided to place responsibility on front-line developers and scientist. The responsibility lies with our leaders — people like him. Many frontline developers are kids (I’m 40 — can I say that now?) and most scientists are deep in the weeds of theoretical development. This doesn’t diminish what this younger generation has to say or what the scientists are trying to show us. But to put it on their shoulders is neither fair nor appropriate. Maybe it’s time for us adults to more readily bear the burden and lead.
Leadership has a lot to do with faith and fellowship; again, this shares a similar connection with religion. How so? Faith is deeply tied to trust, trust into something greater than our individual experience and beyond what we can implicitly know or comprehend. Through trust, fellowship will follow. Our future with AI has a lot of unknowns as does anything we consider in the future. Faith into a prosperous co-existence with AI and powerful technology can only come with continual improvements in our current shortcomings regarding human behaviors and more specifically the behaviors of our leaders. Therefore, in a twist on Holmes’ question — do we need to teach our leaders a little religion? More specifically, as this technology’s capability continue to become more and more powerful, what does right and wrong actually look like now?
Since leaving the military I have heard far too often “don’t hate the player, hate the game”. An adage that quickly gets whipped out when we nonchalantly accept unsavory conducts or practices. A mere shrug of the shoulders — meh, what are we going to do. It’s a cop-out. We could all choose to dislike both the player and the game. But doing so involves being optimistic things can be different. That involves an act of faith. It also involves the courage to put your neck out. It means leadership supporting faith steeped in moral clarity and building fellowship around it towards something better than what we have now.
If you found this topic interesting, leave your comment or suggestion. I will be happy to listen and respond if needed.
I work with executive teams building strategic capability and leadership within their respective organizations. All my writings draw on real-life challenges and considerations from clients, as well as professional experiences from my time in the military. I welcome you to follow me to receive the latest updates.
","The discussion of religion in business is rare, yet Ryan Holmes of Hootsuite referred to its need…",0,the-discussion-of-religion-in-business-is-rare-yet-ryan-holmes-of-hootsuite-referred-to-its-need-a0b02da1250b,2018-05-04,2018-05-04 14:22:56,https://medium.com/s/story/the-discussion-of-religion-in-business-is-rare-yet-ryan-holmes-of-hootsuite-referred-to-its-need-a0b02da1250b,False,933,,,,,,,,,,Ethics,ethics,Ethics,7787.0,James Robertson,"Former SOF officer, lover of the west coast, and driven to affect positive change for communities facing advancements in technology and climate change",24955a63803b,JamesR_VanBC,12.0,127.0,20181104
0,,0.0,,2018-03-01,2018-03-01 16:58:03,2018-03-01,2018-03-01 18:11:44,2,False,en,2018-03-01,2018-03-01 18:11:44,14,5a7fda74dca3,7.1040880503144646,4,0,0,"Socrates defined ethics as the good life, an existence most worthy of humans, implying making choices to ensure the best outcome. Human…",2,"Artificial Intelligence and Ethics: What are we talking about?
Socrates defined ethics as the good life, an existence most worthy of humans, implying making choices to ensure the best outcome. Human life has always been influenced by technology. Agriculture, the printing press, the steam engine, Internet and banking applications to send money across the globe within hours are just some of the few examples influencing human progress across centuries. Automation, and machine and deep learning are, in a sense, just the next stage in the technology development. However, they enable change on an unprecedented scale and at a very high pace. Besides, they bring a new component into our reality — smart machines, capable of learning and acting without human input. Does it imply that their decision power might one day fully escape our control? What if this does happen because we did not understand the consequences of a particular design, or were victimized by a malicious manufacturer, cyber hacking, or because a machines’ self-learning models were fed with unclean and biased data?
Unanticipated actions of AI agents and robots might lead to unprecedented legal claims, fear, and radical judgments on what the potential of these technologies might be.
__________________________________________________________________
AI Risks and their causes
Mistakes in design:
As technology reflects its creators, it is not immune to biases due to lack of diversity in gender, age and ethnics, or lack of involvement of non-engineers to consider important aspects of usability, ethics, and ultimately, safety. For example, when translating from Turkish — a language that uses a single gender-neutral pronoun ‘o’ instead of ‘he’ or ‘she’ — Google Translate attributes ‘he’ to soldiers, doctors and entrepreneurs, and ‘she’ to nurses and teachers. Alexa struggles to understand different accents. FAIR Lab reports that their systems on matching people and professions tagged the picture of Obama with “basketball”. A 2016 report from the Obama Office of Science and Technology Policy warned that the impact of ML powered algorithms on work has the potential to worsen inequality.
Malicious intent in designing AI systems:
Some unscrupulous manufacturer might insert some unethical behaviors into their smart machines in order to exploit users for financial gain, or cover up for bigger issues of the technology (just think about the VW diesel scandal of 2015). Like in many traditional industries, reputational risk of such decisions might be enormous. Today new ethical standards are emerging, e.g. BS 8611 Guide to the Ethical Design and Application of Robots and Robotic Systems, or IEEE P700X ‘human standards” which would also support OEMs and ODMs in the ethical application of robots.[i]
Cyber-hacking:
Adversarial attacks on data models, hacking into a fleet of self-driving cars to damage or even kill passengers, faking video images or voice are realistic scenarios, demanding new ways and solutions to minimize the cybersecurity risk.
Machines without a human in the loop/ Metalearning:
AlphaGo Zero achieved superior skills at Chess, Shogi and Go, and outperformed the original AlphaGo program in December 2017 by completely removing the human input. For the first time, an artificial system demonstrated capabilities to learn on its own. An AI technology called ‘metalearning’ or AutoML grew in prominence, with systems being capable of optimizing the hyper parameters and then running learning algorithms within them. Besides, ML allows for systems to reuse data and experiences from other tasks when trying solving new tasks. This implies things can be created from scratch, from zero prior knowledge and human intervention. AI practitioners expect that in the future coding will be dominated by automated systems without significant human involvement.
________________________________________________________________
In the past decade, few prominent researchers studying technology’s impact on our life and culture expressed alarm at the likelihood of our human values being compromised by digital. Morozov’s “The Net Delusion” and “To Save Everything, Click Here” addresses our naivety around technology solutions, claiming that most well intended discoveries might backfire, if in wrong hands. Lanier, a computer scientist who wrote “You Are Not a Gadget”, worries that many Silicon Valley technologists celebrate anti-humanistic values, such as consumerism, unrestricted capitalism, and decreasing efficiency due to the dominance of devices. Ayanna Howard, Chair of the Interactive School of Computing at Georgia Tech, talks about simulation of emergency situations, where people trust robots more then their own decision-making capabilities despite visible signs that these machines behave strangely and that there are enough alternatives to get to an exit while just relying on human eyes and ears.[i]
I however firmly believe, that technology is neutral. It might enable progress, and it might cause harm. Planning and experimentations are much needed ingredients, if we want AI to help us, rather then take our jobs, degrade our creative skills, and deprive us from critical ability to question, argue and make independent decisions.
When NASA sent Neil Armstrong, Michael Collins and Edwin Aldrin to the Moon, they succeeded not because of luck or overreliance on technology, but because of previous systematic planning. Technological capabilities combined with human wisdom and ethics on how to best implement made their mission successful. This was a story on leadership and goal oriented problem solving.
Since AGI and ASI are decades away, most people might think there is no need to worry about implications at this point in time. Nevertheless, even narrow AI brings challenges to life and societal order, as we know it. Countless publications quote job losses, a need to adopt a Universal Basic Income to cover necessities, and the benefits of offering life-long educational opportunities to escape stagnation and large inequalities. According to Max Tegmark, if humanity wants to win the race for safe and beneficial AI, we need to fund AI safety and ethics research today.
Interdisciplinary collaboration and diversity might be the only answers to ensure we achieve safe and beneficial AI. Deep interdisciplinary collaboration will ultimately touch four major themes, representing the foundation of AI ethics: goal alignment, decision-making, incentives, and safety.

There are several examples of how researchers and practitioners address AI ethics:
The Future of Life Institute published the Asilomar AI Principles, which were developed in conjunction with the 2017 conference;[ii]
A current Fast Forward Labs intern just published a post about a tool called FairML, which he used to examine implicit racial bias in criminal sentencing algorithms;[iii]
Cathy O’Neill regularly writes articles about the evils of technology and AI ethics for Bloomberg;
Gideon Mann, who leads data science for Bloomberg, is working on a Hippocratic oath for data scientists;
Eubank’s “Automatic Inequality: How High-Tech Tools Profile, Police and Punish the Poor”[iv] calls for the check and balance of technologies applied in law enforcement, insurance, credit, children protection and social welfare systems to ensure algorithms don’t discriminate America’s poor. The book contains an “Oath of Non-Harm for an Age of Big Data”;
Blaise Agüera y Arcas and his team at Google are constantly examining and correcting for potential bias creeping into their algorithms;
Clare Corthell is mobilizing practitioners in San Francisco to discuss and develop ethical data science practices;
Few corporations (e.g. Alphabet and Microsoft) created Ethic Boards to address emerging AI technologies, and apply interdisciplinary thinking to design of Machine Learning systems;
Mark Zuckerberg urges Facebook’s product managers to change their algorithms to prioritize ‘meaningful interaction’, while responding to public outcry and research that point to harm caused by passive consumption of information on the company’s platform and the rise of fake news.[v]
Consideration of ethical questions in AI research and development is linked with practices of knowledge sharing and distribution. “Inequality in who gained from computers has been less about inequality in understanding key insights about computers, and more about lumpiness in cultures, competing standards, marketing, regulations,” writes Robin Hanson on November 22, 2008.[vi] In this context, geo-political considerations and concentration of AI R&D in the hands of what I call ‘full-stack AI companies’ Alphabet, Apple, Microsoft, Facebook, Amazon, Baidu, and Tencent can’t be neglected.
The Next Generation AI Plan of July 2017 declares Chinese Communist Party ambition to transform China into a global leader in AI, while achieving a domestic AI market of $150 billion by 2030. At the same time, Trump’s White House cuts subsidies for research and policy development in AI, pointing out that the topic is not of the highest priority for the new administration. Ironically, while US Internet giants start opening up on developing ethical standards, supporting research around AI safety, similar initiatives are not visible in China.

In the future, those countries and companies will lead in safe and beneficial AI that reach a healthy balance between business, government and citizens. “This requires networked thinking and the establishment of an information, innovation, product and service “ecosystem.” In order to work well, it is not only important to create opportunities for participation, but also to support diversity.
There is no way to determine the best goal function: should we optimize the gross national product per capita or sustainability? Power or peace? Happiness or life expectancy? Often enough, what would have been better is only known after the fact. By allowing the pursuit of various different goals, a pluralistic society is better able to cope with the range of unexpected challenges to come”.[vii]
“The Artificial Intelligence Imperative. A Roadmap for Business” by Anastassia Lauterbach and Andrea Bonime-Blanc (to be published by Praeger, April 2018) stresses that we are at the historic moment where we have to decide on the right path — a path that allows us all to benefit from the emerging AI. Questions on decentralized design of AI systems, research around ‘self-criticism’ of AI models and algorithms, prevention of adversarial attacks, improvement of data quality to enable user-controlled information filters, emphasis on collaborative opportunities of diverse teams along with increased digital literacy of corporate boards, and general public will not be solved easily.
[i] “Trust in Human-Robot/ AI Interactions with Ayanna Howard”, TWiML Talk #110, February 13th, 2018.
[ii] Futureoflife.org, February 10th, 2017.
[iii] Julius, “FairML: Auditing Black-Box Predictive Models’”, blog.fastforwardlabs.com, March 9th 2017.
[iv] Virginia Eubank, “Automatic Inequality: How High-The Tools Profile, Police and Punish the Poor”, St. Martin’s Press, New York, 2018, Page 211–212.
[v] Mike Isaac, “Facebook Overhauls News Feed to Focus on What Friends and Family Share”, NYT, January 11th, 2018.
[vi] “The Hanson-Yudkowsky AI-Foom Debate”, English Edition, George Mason University, Future of Humanity Institute of Oxford University, published in 2013 by the Machine Intelligence Research Institute, Berkley, pos. 1167 (from 10005).
[vii] Dirk Helbing, Bruno S. Frey, Greg Giherenzer, Ernst Hafen, Michael Hagner, Yvonne Hofstetter, Jeroen van den Hoven, Roberto V.Zicafi, Andrej Zwitter , “Will Democracy Survive Big Data and Artificial Intelligence?”, Scientific American, February 25th, 2017.
",Artificial Intelligence and Ethics: What are we talking about?,23,artificial-intelligence-and-ethics-what-are-we-talking-about-5a7fda74dca3,2018-04-12,2018-04-12 12:54:50,https://medium.com/s/story/artificial-intelligence-and-ethics-what-are-we-talking-about-5a7fda74dca3,False,1781,,,,,,,,,,Ethics,ethics,Ethics,7787.0,dr. anastassia lauterbach,"Tech. Enterpreneur, Board Member and Angel Investor. AI, Cybersecurity, IoT. NED @ D&B. Previously SVP Qualcomm & DT; Roles @ McKinsey, Daimler and Munich Re.",4a022f55f923,alauterbach,102.0,153.0,20181104
0,,0.0,,2018-07-09,2018-07-09 14:09:42,2018-07-09,2018-07-09 14:15:16,3,False,en,2018-07-09,2018-07-09 14:15:16,11,2b49a7295926,2.2405660377358494,1,0,0,"For decades, sci-fi enthusiasts and technological pioneers have been fascinated by the concept of the singularity. Put simply, it is the…",4,"The Singularity is Near Enough for Us to Do Some Serious Thinking
For decades, sci-fi enthusiasts and technological pioneers have been fascinated by the concept of the singularity. Put simply, it is the point where artificial intelligence advances so far ahead of humanity that civilization is changed forever.
Singularity is a complex theory with a rich history. Some of the world’s top innovators think we could see that moment within this century.

Are We Actually Close to Technological Singularity?
But are we really that close? There is certainly proof that tech is getting smarter, with Google’s DeepMind triumphing over the Go world champion and Adobe’s AI recognizing image manipulation.
Artificial intelligence is a permanent part of our lives now. Many people interact with Siri, Alexa or Google Assistant on a daily basis. Warehouses are increasingly populated by robots that help employees. AI can make art and write poetry.
On the flip side, we humans are becoming more like high-tech beings ourselves every day. Scientists can now edit genes and create working limbs.
Concerns About the Singularity
All of this rapid change raises serious concerns, ethical and otherwise. As we gain the ability to enhance ourselves, we must ask an important question: How far should we go?

As our inventions mature from inanimate objects to potentially autonomous individuals, we must ask ourselves how they fit into our world. Artificially intelligent life may begin as subordinate but later surpass us. If AI gains super-intelligence, able to create more AI on its own, that fate seems inevitable.
Even those who are skeptical of the dire outcomes of the supposed singularity recognize the dilemmas AI has already caused. There is a connection between our scientific advancement and widening inequality. We live with omnipresent fear of automation shrinking the pool of available jobs too rapidly, with replacement positions coming too slowly, if at all.
These concerns are in the here and now. They reach beyond the world of AI to everyday people. We must actively work to address these problems with public policy that balances the importance of innovation with the humaneness of economic parity.
Finding the Balance
Fortunately, we are not past the point of no return. We have the ability to consider carefully how to go about embracing the wonderful and world-changing technology we continue to develop. This is not the first time humanity has felt tectonic societal shifts underfoot.

With a long and mysterious road ahead, stretching infinitely into the future, we should take stock of our values. Will we misstep and lose ourselves to machinery and inequity? Or will we use our inventions for good, lessening human suffering and spurring exploration? The choice is ours.
",The Singularity is Near Enough for Us to Do Some Serious Thinking,50,the-singularity-is-near-enough-for-us-to-do-some-serious-thinking-2b49a7295926,2018-07-09,2018-07-09 14:15:17,https://medium.com/s/story/the-singularity-is-near-enough-for-us-to-do-some-serious-thinking-2b49a7295926,False,448,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Mat Cole,Husband & father to amazing family! Founder http://Sporple.com & The Blaze acq by @CSM_Worldwide Ex @UnlockdMedia now SVP @TigerPistol advsr @gamefaceai,d184e7aac433,mat.cole,11.0,52.0,20181104
0,,0.0,5e5bef33608a,2018-04-10,2018-04-10 20:54:14,2018-04-10,2018-04-10 21:12:37,5,False,en,2018-04-16,2018-04-16 14:23:45,11,3364f681b271,13.010691823899373,5,0,0,"It’s 2011, the word on the street was there was some new app. At the time few people had smartphones and even less were proficient enough…",5,"
The Case of Unregulated Exponential Technological Development
It’s 2011, the word on the street was there was some new app. At the time few people had smartphones and even less were proficient enough to even know how to use them. The app was Instagram. Photo editing app first, sharing platform second. Some were drawn in while others didn’t understand its existence or its rise. Fast forward to 2017, Instagram has become one of the most influential and trailblazing applications ever to come about. With over 500 million daily active users, 95 million shared photos and videos, and a valuation of over 50 billion thanks to the Facebook acquisition (Balakrishnan 1). Instagram has come to be a behemoth of social interaction among teens and young adults alike.
Instagram isn’t the only app to receive this type of success, but A real social revolution came with the advent of Instagram and the platform of the smartphone. All of a sudden every moment needed to be photographed and shared, every dinner, every movie, every date, wedding, newborn child. People began sweating their follower counts, becoming green with envy over their peers. Suddenly it seemed as if popularity and self-worth were now given a numerical value easily comparable to every other person. People got lost in the hysteria, spending hours toiling with what filter to use or what caption to add. Fears of missing out and exclusion pushed mass adoption among practically every person under 25. People were quickly no longer skeptics of this technology and used it for whatever gain possible regardless of its costs seen or unseen. In 2011 if you went up to someone and explained the idea of posting every nice dinner they’ve had or special occasion on the internet for the world to see, people would be far more averse to this idea then they have grown to be today. This social revolution is one that wasn’t forecasted by anyone and yet its influence on us is undoubtedly immense. In our modern technological age, it is paramount that as individuals we exercise cultural resistance against the social norms perpetuated by technology companies whose increasing advancements require society and humanity to implement regulations on this never before seen influence.

The use of cultural resistance is no new idea, it’s a practice that’s as old as history itself. The act of pushing back against a society’s norms is what progresses our society and makes us question why certain things are the way they are. According to Stephen Duncombe from his writings on cultural resistance, he defines cultural resistance as “free space to create new language, meanings, and visions of the future.” (Duncombe 8). When culture isn’t contested the dominant power structures in our civilizations end up dictating our futures regardless of what the masses may want. This social control is so effective due to the shared belief across the population that things are the way they should be and there seem to be no other feasible alternatives. Today, technology companies have assumed this dominant power becoming some of the most valuable and influential corporations. One of the ways of defining the power a company has is by looking at its overall value a term also known as market capitalization. The top 5 companies sorted by market cap are Apple, Google, Microsoft, Amazon, and Facebook. All of these companies are technology based and hold the values of continued technological innovation as supreme importance. This cannot be a coincidence, this is very telling about our country’s societal values and our guides to the future. With these companies as the leaders of our economy, the increased exponential rate of technological innovation guaranteed by Moore’s Law, the idea that computer processors will double in speed every 18 months, will only be given more and more room to flourish. This increased pace doesn’t allow us enough time as a society to properly evaluate the potential benefits or problems a new innovation may cause. Many of these companies grew to prominence in amounts of time that have never been seen before. Facebook is an example of one of these companies. 5 years ago they were valued at 40 billion which is minuscule compared to the 550 billion they are worth today, 200 billion of which was grown in the past year. So with all this technological progress have there been any downsides? The smallest innovations in this field can lead to the creation of massive unforeseen impacts that are seemingly irreversible. Shouldn’t we be worried about all this influence within companies that have no formal or substantive regulations?
Trending AI Articles:
1. How I used machine learning as inspiration for physical paintings
2. MS or Startup Job — Which way to go to build a career in Deep Learning?
3. TOP 100 medium articles related with Artificial Intelligence
First, we need to evaluate one of the current technologies which have seemingly revolutionized our society and has grown at an ever-increasing rate, Social media. Social media can be defined as websites and applications that enable users to create and share content or to participate in social networking. With the advent of the modern day internet in the late 1990s came the first primitive social media platforms. These took the forms of blogging forums, chatrooms, and instant messaging websites. There should be no surprise that at the first chance humans got to use the internet they repurposed it to interact with one another because we are genetically hardwired and programmed to want to socialize (Wolpert 1). With this new technology, we could live in a world where people talked to others across the country to even across the world. In this form however it was still very clumsy to use and had large barriers to entry and skepticism surrounding the abuse of these technologies. People worried about internet safety, privacy, and exactly who could see what they did on the internet, but these fears wouldn’t last long. Also, many of the social media companies around this time had little power, if any, over people because of their inability to monetize these platforms as well as the small amounts of time people would use the website. In 2003, came the advent of MySpace. This marked an important place in the history of social media. It was the first modern-day social media website. It contained many of the characteristics that we still see prevalent today, a personal profile, the ability to make friends all atop an instant messaging platform. Although MySpace’s surge in users and popularity didn’t last long, there is no doubt that it was one of the most important precursors to the creation and success of Facebook. This is because MySpace prepared people for Facebook as they could easily switch from platform to platform due to many things being similar and the social hurdles that Facebook would need to leap were already covered by MySpace. At its inception, Facebook was like many of the other platforms in the space, but they had one thing different about them, their ability to adapt to the future. Every other social media website before it struggled with money and gaining and keeping a user base active. Facebook was one of the first to figure it out. In its first form, it was nothing more than a platform where users could communicate with their friends and post or respond to their statuses. Then it became a waiting game. With each new technological innovation, Facebook would be the pioneers to implement it. For example, In 2007, with the creation of the smartphone and the app store came the Facebook mobile platform. As the internet got faster and smartphone adoption increased, Facebook did whatever they could to encourage their users to make use of these new innovations. Adding the ability to share files and photos in turn making smartphones more important and useful by adding the platform users already loved to use and making it accessible everywhere. This was a tipping point where I believe technology and their supporting companies began to gain a concerning amount of power. Facebook at this point began to change. The company that once seemingly had societies and humanities best interests in mind soon became misaligned from these ideals. Their new goal after having a user base of over 2 billion was to increase time on the app or website. In order to accomplish this daunting task, they began to design a user experience that was addicting. Almost overnight, posts weren’t sorted chronologically anymore, they were sorted by which content would keep users on the site the longest. This was all done by harvesting the information the users willingly provided about themselves and using the novel technology of machine learning, every liked post, video viewed or any other conceivable action was taken into account by a complex algorithm to curate posts uniquely to each user. This was all extremely successful and the time spent on these platforms skyrocketed. Recently, we have seen people involved with the creation of this field beginning to come out against the practices these social media companies are employing to take advantage of their users. One of whom is Chamath Palihapitiya, a former Facebook VP. As someone who was at the forefront of technological advancement and change at Facebook, he has come out against his work at his former employer at Facebook. He feels “tremendous guilt” over growing the social network that “eroded the core foundations of how people behave by and between each other.”(Wang 1) When he started, he said, “there was not much thought given to the long-term negative consequences of developing such a platform.” This is exactly the problem with our increased pace of innovation. These companies are too worried about pushing hot new technologies and beating competitors to market so they often times do not look at the long-term effects it could have on society. If a company like Facebook isn’t doing this, do we expect any other smaller company to do it? “It literally is a point now where I think we have created tools that are ripping apart the social fabric… the short-term, dopamine-driven feedback loops that we have created are destroying how society works: no civil discourse, no cooperation, misinformation, mistruth. And it’s not an American problem… This is a global problem.”(Wang 1). This shows a direct first hand experience of people in the tech field pushing the bounds of influence without giving it much thought in terms of its potential negatives. This discourse is a call to action on consumers to look at what these companies are doing and be at least aware of it if not avoid it. The problem is rolling back this sort of technology is near impossible. When people spend multiple hours per day on their various social media websites, how can we expect them to be able to just simply shut it off? With each new innovation social media quickly figures out how to implement it in a way that increases the reliance users have on the platform. Over the past few years the technological advancements in social media has led to an almost complete loss of privacy, increased loneliness, a diminishing civil discourse and a rampant spreading of misinformation and mistruth, yet some of us are too helplessly addicted to even care or resist it. I know personally about how technology has affected our society. I have first hand seen people blindly adopt and use technology without thinking past the surface level of consequences. Are we expected to believe that documenting everything we do on a practically permanent public ledger that is social media is a good thing? Why have we not given further consideration into this? You can ask almost anyone if they think we are being tracked throughout technology use, and the answer is always a yes. So why do we not care? Why do we trust these companies so much when their only interests really are to turn a profit for their shareholders? Can the free market regulate when the dominant tech companies of Google, Facebook and Amazon have the resources to buy and crush any competition? If one of these companies was found of committing a serious ethical violation with our data would we be able to turn away in a world where we have become increasingly reliant on these companies? We have all these massive questions about the social media services that we use daily, but these are ethical and moral questions that should be addressed before users continue to use it like they have been.
Social media isn’t the only place where our unregulated exponential technological innovation is starting to raise problems. For this next one, we need to look at something that is relatively obscure to the public eye that is the advancements in automation and artificial intelligence. Artificial Intelligence is the idea that humans can program and design software in order to accomplish a certain task. We have seen A.I. take form over the past couple of years going from beating the best human player at chess to performing the complicated task of self-driving. So if our software can already achieve these goals what’s to stop us from creating machines smarter than ourselves? Then at this point what is to stop an artificial intelligence from continuing advancing its own code in order to become more and more intelligent. This idea of rapid intelligence development is known as superintelligence or an intelligence explosion. One of the leading thinkers on this topic is Nick Bostrom, A professor of philosophy at Oxford University with an extensive background in physics, computational neuroscience, and mathematical logic. From Bostrom’s book Superintelligence he goes in depth to explain the dangers this technology can have, “Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb. Such is the mismatch between the power of our plaything and the immaturity of our conduct. Superintelligence is a challenge for which we are not ready now and will not be ready for a long time.”(Bostrom 58). As a society, the way technological innovation has advanced created a situation where we have a few major technology companies and governments gunning for new technology to further their power and dominance. Instead of trying to put in place ethical goals and safety restrictions, we have seen companies doing whatever means necessary to get closer and closer to their end goal. This raises serious concerns over the power and impact of Artificial Intelligence. This raises the idea that maybe there are just certain innovations we may not have the framework or knowledge to create and absorb its effects. We do not understand the devastating consequences that could come with an intelligence explosion. However, Superintelligence is inevitable if we continue progressing our hardware and software at the rates we have seen in the past and as a society, we have every motive to continue as intelligence is the solution to almost all of our problems. How to cure diseases, solve world hunger, address climate change… these are all problems that can be easily solved if we had the intelligence to do so. I don’t doubt that people will challenge the idea if superintelligence is even possible, much less inevitable, but then you need to find a problem with one of the following assumptions. 1. Intelligence is the product of information processing. We have already seen this to be true as narrowly minded intelligence can already act at superhuman levels, looking at the case of chess or self-driving. 2. We will continue to improve our intelligent machines. Given the ultimate value of intelligence as previously stated, we have no reason to not continue our innovation. We don’t even need intelligence to go that much further just by the virtue of speed. Sam Harris explains this idea in his TED talk when he says “So imagine if we just built a superintelligent A.I. that was no smarter than your average team of researchers at Stanford or MIT. Well, electronic circuits function about a million times faster than biochemical ones, so this machine should think about a million times faster than the minds that built it. So you set it running for a week, and it will perform 20,000 years of human-level intellectual work, week after week after week. How could we even understand, much less constrained, a mind making this sort of progress?” (Harris). This example highlights our unforeseen shortcomings when compared to A.I. So, If we accept that A.I. superintelligence is inevitable how can we prepare in order to facilitate the best case scenario? We would need a “Manhattan project” on A.I. looking into the technology and accessing which regulations need to be put in place in order to make a transition from a world without A.I. to one with it as seamless as possible. Currently, if a company owned this technology they could have the potential to replace many of our current jobs in our economy. This could create huge amounts of unemployment with steep wealth inequality and currently, our political system is not prepared to handle this. Without proper regulations and political, economic scenarios to report to in the case of a superintelligent A.I. the circumstances could be dire.
Ultimately, whether it be the way our society is impacted by this technological innovation through the vein of social media or artificial intelligence there can be potential drastic unforeseen consequences that lie ahead. We have seen the consequences of not properly evaluating our technologies with the creation of our various modern day social media apps, but we are yet to see what consequences there can be through the other fields that may lie ahead. What mistakes we have made in accessing technology on the social media front we must take a lesson from. Our shortcomings on social media may have created a platform of misinformation and many other problems, but if we do not learn from this and place regulations on new technological advancements we may see a similar situation develop around a technology such as artificial intelligence which has the potential to cause much greater problems or to do away with the human race as we know it. While regulating technology may slow our progress towards certain innovations and goals, it may be our only way to prevent these much greater unforeseen problems in our future.
Works Cited
Balakrishnan, Anita, and Julia Boorstin. “Instagram Says It Now Has 800 Million Users, up 100 Million since April.” CNBC, CNBC, 25 Sept. 2017, www.cnbc.com/2017/09/25/how-many-users-does-instagram-have-now-800-million.html.
Bostrom, Nick. Superintelligence: Paths, Dangers, Strategies. Oxford University Press, 2016.
Casti, Taylor. “The Evolution of Facebook Mobile.” Mashable, Mashable, 1 Aug. 2013, mashable.com/2013/08/01/facebook-mobile-evolution/#vq3_LNXLR8qc.
Duncombe, Stephen. Cultural Resistance Reader. Verso, 2002.
“NASDAQ Companies.” NASDAQ.com, www.nasdaq.com/screening/companies-by-industry.aspx?exchange=NASDAQ&sortname=marketcap&sorttype=1.
Harris, Sam. Can We Build AI without Losing Control over It? Can We Build AI without Losing Control over It?, TED, June 2016, www.ted.com/talks/sam_harris_can_we_build_ai_without_losing_control_over_it.
Wang, Amy B. “Former Facebook VP Says Social Media Is Destroying Society with ‘Dopamine-Driven Feedback Loops’.” The Washington Post, WP Company, 12 Dec. 2017, www.washingtonpost.com/news/the-switch/wp/2017/12/12/former-facebook-vp-says-social-media-is-destroying-society-with-dopamine-driven-feedback-loops/?utm_term=.7c752bf6b727.
Wolpert, Stuart. “UCLA Neuroscientist’s Book Explains Why Social Connection Is as Important as Food and Shelter.” UCLA Newsroom, 10 Oct. 2013, newsroom.ucla.edu/releases/we-are-hard-wired-to-be-social-2487




",The Case of Unregulated Exponential Technological Development,149,the-case-of-unregulated-exponential-technological-development-3364f681b271,2018-04-22,2018-04-22 14:48:14,https://becominghuman.ai/the-case-of-unregulated-exponential-technological-development-3364f681b271,False,3227,"Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.",becominghuman.ai,BecomingHumanAI,,Becoming Human: Artificial Intelligence Magazine,team@chatbotslife.com,becoming-human,"ARTIFICIAL INTELLIGENCE,DEEP LEARNING,MACHINE LEARNING,AI,DATA SCIENCE",BecomingHumanAI,Ethics,ethics,Ethics,7787.0,Dylan Sapienza,,59b09885e7a7,dylansapienza,0.0,1.0,20181104
0,,0.0,71c99841f1ad,2017-09-30,2017-09-30 00:28:14,2017-09-30,2017-09-30 23:21:02,1,False,en,2018-06-14,2018-06-14 21:37:02,3,65035b82b046,2.9924528301886792,28,4,0,How ethics will change the conversation about AI,5,"Humanity-Centered Design
How ethics will change the conversation about AI
“Technological change is not additive; it is ecological, which means it changes everything and is, therefore, too important to be left entirely in the hands of [technologists].”
 — Five Things We Need to Know About Technological Change by Neil Postman
An AI may describe an image in various ways. Nothing is neutral: an AI that interprets the world is an AI that creates a worldview. Photo by Meg Via Flickr
AI is coming to get us, and everyone knows that. The singularity — the moment when computer intelligence far surpasses human intelligence — is right around the corner, and when it arrives we’ll be jobless and living in an apocalyptic era whose nature we can only guess. Welcome to the future.
Today, we are using a type of AI called artificial narrow intelligence (ANI). ANI can generally do one thing very well, sometimes better than humans can — like the AI that defeated the world’s best Go player — but if asked to perform a task outside of its specified function, it wouldn’t be able to. We are surrounded by instances of ANI: autonomous cars, Cortana or Alexa, even some of the news we read is generated by AI.
ANI may have limits, but it’s powerful, useful, and everywhere. As a result, a growing number of designers will be working on AI-driven technology. Will it be technology that improves life and makes our world better? Or will it reinforce negative behaviors, strengthen biases, and increase inequality?
Today, we generally talk about AI only from a technological perspective: What’s powering its functionality? Is it machine learning, computer vision, or a bot? Instead, let’s shift the conversation from technology and features to how AI changes lives. Viewing AI through a lens that focuses on its impact on humans can highlight what using that AI really means to us. And changing the language we use to talk about AI can expose important ethical issues. New vocabulary that classifies AI by what it does, rather than how it works, may have terms like “AI that interprets reality,” “AI that augments senses,” or “AI that remembers for humans.”
Nothing Is Neutral
Microsoft’s “Seeing AI” is an app and glasses-like device that helps blind people “see” the world around them by hearing descriptions of it. It’s an amazing and helpful invention, and an example of inclusive technology. If we view it through a technological lens, we see a product that uses computer vision and facial recognition as well as natural language generation. Now, let’s focus less on how it works and more on what it does. If we evaluate how the AI impacts humans, we see it as an “AI that interprets reality.” Suddenly, it’s clear that we need to pay attention.
Look at the image above. What do you see?
This is a hypothetical example of how an AI could be programmed to interpret an image. All the descriptions are correct, but the AI’s choice of words determines how the blind person experiences the world. What if Coca-Cola were to offer to sponsor the device so that it’s free but ask for one thing: whenever the AI sees a Coca-Cola product, it mentions it explicitly, while it uses generic terms for any of their competitors? There’s a dramatic difference between “a woman drinking a soft drink” and “a smiling, pretty woman drinking a Coke,” though both phrases correctly describe the same situation. We need to think through these kinds of scenarios when we design products.
The Rise of the Product Ethicist
With this shifted focus on how we view AI, a new profession should also emerge: the “product ethicist.” Someone in that role would aim to keep the product design honest. While not only one person should think or care about ethics, a “product ethicist” could provide more nuanced thinking to guide the design process and hold everyone accountable.
Further, communicating an AI’s potential impact to its users is as important as designing an ethical product — it’s actually part of being ethical. Food products have labels with ingredients and nutritional facts to help consumers choose what to buy. What labeling system could help them decide on the right AI product?
Designers put people first. They empathize, observe, and listen. They find problems to solve not because they are technically difficult, but because they are hard human issues. How to use AI is one of these challenges — and humanity-centered design could be the solution.
Originally published in Issue 35.2 of ARCADE magazine. Tuesday 19th Sep 2017, Fall 2017
",Humanity-Centered Design,182,humanity-centered-design-how-ethics-will-change-the-conversation-about-ai-65035b82b046,2018-06-14,2018-06-14 21:37:03,https://medium.com/s/story/humanity-centered-design-how-ethics-will-change-the-conversation-about-ai-65035b82b046,False,740,"Putting technology on a more human path, one design story at a time.",,MicrosoftDesign,,Microsoft Design,joline.tang@microsoft.com,microsoft-design,"USER EXPERIENCE,ARTIFICIAL INTELLIGENCE,VIRTUAL REALITY,APPS,DESIGN",MicrosoftDesign,Ethics,ethics,Ethics,7787.0,Ruth Kikin-Gil,Product designer & strategist at MSFT. I have a love-hate relationship with tech. These are my opinions.,412b1b4fd9d6,Ruth.kikin.Gil,82.0,100.0,20181104
0,,0.0,,2018-05-11,2018-05-11 18:29:17,2018-05-11,2018-05-11 18:46:42,0,True,en,2018-05-11,2018-05-11 18:49:24,2,cba40aeff2c,2.9962264150943394,2,0,0,Black Mirror like scenarios which show regulation is urgent,5,"Duplex, AI and the hubris of dismissing the precautionary principle
Black Mirror like scenarios which show regulation is urgent
This is a quick reaction to the rather impressive news that Google has been testing Duplex, a very convincing/deceitful AI personal assistant. Listen to it talking on the phone with unknowing humans. What I would like to point out is how the Software industry seems to be utterly immune to the ideas of ‘precautionary principle’ which have guided so much of regulation in other domains.
If you are a seed company, about to launch a new GMO seed in the market, you would have passed a laborious process of testing to ensure that the product is safe. The same for cars, chemicals, pharmaceutics, etc. And how is it that big tech companies complete disregard for ethics goes unchecked, thanks to the wow effect of their products (and a little help from their armies of consultants). If these companies were to brag about testing their products in the wild, they would suddenly face much-needed scrutiny.
The news about Duplex is a blatant example of how big tech companies (notably Google, Facebook) have managed to entirely circumvent this logic, except for projects with a concrete embodiment like automated cars. This seems at least in part a relic of a division between ‘real world’ and ‘virtual world’, which assumes that the consequences of changes to the virtual world are somehow less real.
What happens when someone trains one such voice assistant with someone else’s voice? When someone uses such assistants for scaring someone to death with horrible news? Or incriminating someone else by making threats? When you can’t trust a phone call anymore because you can’t be sure who’s the caller? When a famous celebrity calls a radio show and reveals untenable secrets?
These simple examples may be cliché in a society that has binged on Black Mirror, but they hint at why regulation needs to come into this early, e.g.:
- to prevent that such virtual assistants emulate real people’s voices
- to ensure that any conversation with such an assistant is labelled as such (both on the call identifier, and by the assistant)
- to prevent companies from recording the voice calls of other people without their authorisation — obvious breaches of privacy
- to delineate situations in which the use of such calls in inadmissable (e.g. firing staff, offering products, requesting personal information)
- to punish criminal activity (e.g. Doxing, fake police calls, etc)
- to ensure interoperability, so such assistants have a mandated API to avoid lock-in and network effects further entrenching the dominion of these tech-giants.
The recent examples of AI enabled impersonations of celebrities in pornographic reminders should serve as a stark reminder of unintended consequences. The Cambridge Analytica debacle reminds us of what happens when tech-companies are left to regulate themselves, and police their results.
When evaluating new tech, we should move past parameters and features and consider who creates it. It matters that the company behind this is primarily driven by advertisement revenue, for example. It matters because it hints at what the uses may be. Imagine a hyper-targeted political campaign delivered to your phone by the most patient and eloquent activist you ever met, prepared to counter each of your arguments, capable of reasoning with each of your prejudices. Which party wouldn’t be willing to spend a fortune for such means? Imagine if instead of coca-cola adds with beautiful young people you had added with each of your friends and family. How would that not be disruptive in the adds space?
If you haven’t been careful, companies like facebook and google have every video you ever posted or filmed in their cache (or if your friends and famility haven’t been careful...) If allowed to play with this data using their own terms and conditions, sometime soon, they could train simulacrums (voice or video) of anyone. The examples I give seem ludicrous right now, but so did the idea of 70 million profiles being used sneakily to win an election, or dead actors appering in blockbuster movies. The problem is not whether or not this is immediately feasible, but what is to stop these companies from abusing this power?
The fascination with new breakthroughs should not blind us to their consequences and uncertainties. Dreaming of futures of technological marvels should not stop us from facing up to potential nightmares. Marvelling at these discoveries should not prevent us from recognising hubris.
— — — — 
I am not an AI expert, or a techy. But I am a citizen and a person, and I have a voice. Debate me if you will, but don’t dismiss what I have to say without listening first; I promise to do my best to reciprocate.
","Duplex, AI and the hubris of dismissing the precautionary principle",2,duplex-ai-and-the-hubris-of-dismissing-the-precautionary-principle-cba40aeff2c,2018-05-15,2018-05-15 18:08:01,https://medium.com/s/story/duplex-ai-and-the-hubris-of-dismissing-the-precautionary-principle-cba40aeff2c,False,794,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Jonas Torrens,"Scribbles, poems and short-stories are my lyrical antidote to the dryness of academic life",fd2bd8b569d2,jonas_torrens,97.0,153.0,20181104
0,,0.0,,2017-11-19,2017-11-19 06:57:58,2017-11-19,2017-11-19 07:31:39,1,False,en,2017-11-20,2017-11-20 00:39:59,1,4f74c1f77c09,9.962264150943396,0,0,0,"There’s a specter hanging over humanity. That specter is artificial super-intelligence (ASI). As certain as the sun rises, the emergence of…",5,"A Vision of the Coming Intelligence Explosion
Artificial super-intelligence will transform our lives (Shutterstock)
There’s a specter hanging over humanity. That specter is artificial super-intelligence (ASI). As certain as the sun rises, the emergence of smarter-than-human intelligence in a machine is without doubt. The only uncertainty about it is when and whether the strong artificial intelligence(AI) will be pathological or will it be the greatest positive scientific breakthrough in all of human history? It is my considered opinion that the public discussion of machine intelligence should be initiated now. This will be necessary to ensure a safe arrival of this technology. Major tech firms including Google, Apple, Facebook, and Amazon have major AI efforts underway. They have all invested fortunes in AI startups particularly those in machine learning. A self-improving supercomputer would be a prerequisite to a system exceeding human intelligence.
How can this future technology be safe and be a servant of humanity? Can a recursively improving system be predicted to keep invariant goal structures to help humanity? No matter where you are in the universe, ethics is a system that will govern right from wrong. The code that will be required to have an ethical improvement system will be possibly the most difficult module of the singularity to code. There are thousands of scenarios that present complex ethical dilemmas. An improvement system must handle these situations better than the average human. This advanced ethical system ensures the safety of the superintelligence. Just think, a few years ago climate change was not a pressing issue for our global civilization and was not discussed broadly. Now it is discussed throughout the media because the evidence for major changes such as disappearing glaciers and unusual weather patterns is very clear now.
AI advancements are showing up rapidly in press reports. Many of these discussed Watson, the IBM computer that defeated the top human Jeopardy! players, and other benefits. Other reports shout the dangers of super intelligence loudly.
The AI Community Takes Collective Action
Scientists met a few decades ago to map safety protocols for genetic engineering research at Asilomar. This agreement made biotechnology advances safer. A conference of this type was held again in Asilomar in 2017 to establish rules for artificial intelligence research. This conference resulted in a set of rules that can be found here (http://bit.ly/2nyeISS). More sessions like this are needed to give the research community guidance in pursuing AI research. The general public needs to get up to speed on the basic science of AI and express their opinion about keeping it save and collaborative with humanity.
The Paramount Importance of Ethics
Why focus on ethics? Of all the great philosophical traditions, ethics ranks heads and shoulders above them all as a system which teaches right from wrong. As a set of ideas which provides the basis for making difficult decisions about life and death; ethics has proven his strength.
Ethics is the last best hope of humanity to eradicate warfare and other forms of destructive, violent human conflict. Most importantly, the future intelligent machines of man’s creation must have embedded in their construct a recursively improving ethical system that exceeds human level ethics. This is a difficult programming task, but surely it is not an impossible goal. Indeed it is a necessary goal. Utilitarianism, an ethical theory which maximizes the well-being of sentient beings, must be a part of the AI.
An example of AI caution is the new nearly universal warning of our researchers to hold off any research and development on autonomous military robots. These combat robots could make life and death decisions without human intervention. This could evolve into a Terminator world that would be as dystopic as any future world we can imagine.
Computers will eventually eliminate 70% of all jobs by the turn of the 21st century. This will cause a major disruption in society to the core. 100 years ago society required 90% of workers to produce the food needed for the world. Today only 1% of workers are required to provide for food to the other 99% of people. The same transformation will occur with advancement in AI and robotics toward most jobs.
Martin Luther King famously said, “the arc of the moral universe is long, but it bends towards justice.”
This is an optimistic view of justice. We can paraphrase King and say, “the moral arc of machine intelligence is short, and it must always bend towards justice.” This certainly has to be embedded in the machine before it’s intelligence goes live. We can’t wait until the super intelligence arrives before considering and building its ethical foundation. Unlike the evolution of human societies where ethics emerged after the advance of intelligence, the emergence of greater than human intelligence has to build its ethical immune system first before machine development of anti-ethical behaviors and goal structures that may come from machine intelligence advances.
AI Has Begun to Have Enormous Effects
In spite of the dangers inherent in building a singularity, as quietly as it is kept, our lives are already governed by AI. This is a narrowly focused AI that has expertise in one domain. Weather forecasts have improved significantly due to the improvement in the predictive science of AI. An accurate forecast can save lives when people evacuate early from dangerous hurricanes and tornadoes
When you board a plane from Portland, Oregon to New York City, the onboard computers manage the flight parameters and fly the plane when the pilot sets the autopilot. The pilot manually flies the plane 7% of the flight time. Computer software controls nearly every aspect of the cars we drive today. Wall Street, where major financial crisis actions take place, is wholly beholden to computers. Computerized high-frequency trading is the norm now. The flash crash that resulted in huge stock losses was due to a computer glitch.
It’s only another step to have supercomputer programs help us manage and enhance many aspects of daily life. These worker robots will liberate the masses from a lifetime of mental and physical work. The robots will not only take jobs but will create a problem of people having too much leisure and not knowing what to do with their time. Hobbies will be mandatory to extract pleasure and joy in life. Building friendships will be another great outlet for our creative energies. We were born social, and we will always be social in the present and the future. Our social networks will be enhanced greatly by computers and the nervous system of the planet, the Internet.
The singularity when it arrives will touch all of our lives in profound ways. This ensures that this touching will be gentle and supportive of our cherished values, our hopes, our dreams, and our determination to build a better world for everyone.
Broad Public Discussion is Needed on AI Design
Deep discussions and fierce debate are necessary for human society and its scientific tip of the spear to build an intelligence that must perform actions that will cause more good in the universe than any possible alternative. This is our granite like view that is ethically based and focused on creating a moral agent that can be described as mathematical ethics embedded in a machine. The critical question is the need for greater and deeper levels of intelligence to solve the great and pressing problems of civilization. We could wait for the random genius to be born who can masterfully solve the great mysteries of science. This is linear in scope and may not happen at all. How often does an Einstein appear? This happens every one or two centuries. The problems are arriving faster than the solutions. For example, the sixth great extinction is underway. In all of the earth geologic history of 4.5 billion years, there have been five great dying of periods of plant and animal species. Most of these extinctions were caused by asteroids or other space objects striking the earth. The 6th extinction is caused by one species, humans, who are the primary force of change. The five previous extinctions were mostly gradual over several centuries or millennia. The 6th is occurring at warp speed in comparison to previous extinctions.
Extinctions are not good for the earth and the current ecosystem. As the data flows in on the great change underway on the planet, super intelligence amplification will be capable of finding subtle patterns in the vast data sets. Prediction and forecast modeling will be significantly more powerful and accurate. Foremost, the superintelligence will formulate multiple solution tracks to problems encountered. The ability to master big data with speed and mathematical precision will be the hallmarks of the scientific advance. For example, there would be accurate predictions of the rate of climate change and timelines of temperature changes in sea level rise. Human civilization would then have sufficient warning to be proactive in mitigation of the changes that might occur. Thus, millions of lives could be saved, and the ecosystem will remain stable instead of irreversibly degrading and losing its ability to support life.
There are of course many other threats to our global civilization that super intelligence can help us with and save lives. Instead of fearing the loss of life we could be hopeful with a high degree of probability that AI could save millions of lives. The Singularity Prize presents several scenarios where an AI can save lives.
Artificial general intelligence (AGI) which precedes ASI, focuses on learning software that could learn from raw data and does not need to be pre-programmed. This skill of the algorithm could solve problems and operate across many domains. The AI would perform by making observations of its environment (e.g., data, visual images). Based on these observations the AI would take specific actions that are in response to the observations. The early work on this AI concept used computer games as a testing ground for these neural net-based algorithms. In rapid sequences, the AI improved from not knowing the game or the rules to shortly beating the best-known human players. AlphaGo, developed by a company named Deep Mind, used this AI system to beat the world’s best go player, Lee Solden, in 2016. This development was projected to take place by 2025, because of the complexity of the game. The possible moves in one game can exceed the number of atoms in the universe.
Unlike narrow AI as described earlier, AGI can tackle many domains and solve real-world problems. Healthcare is a sector that can result in diagnostic and therapeutic breakthroughs in the near future. The data accumulating in the health sector is the fuel which the AI algorithm engines require. It is certain to make spectacular progress in the health field. Already Watson at IBM is performing in oncology diagnosis and treatment planning on par and sometimes better than some top human oncologist.
The Critical Control Issue of AI
Where does this place us now in the AI debate? We are at the very beginning of the public debate. We cannot depend on the computer scientists alone to decide how to proceed with AI. The general public needs to be involved. This is to make sure that humans remain in control of an AI. We also have to coordinate our efforts in teams. We also need to avoid a race to the finish line where we begin to cut corners. Safety could get cut because it does not contribute to the AI capability. We must ensure that there is a government role in how we get AI out into the world. This will ensure the democratization of AI. We must determine who and how we set goals for AI. The solution is partly to have partnerships in AI’s development. We need to have top technical talent in this field in a trusted community of collaborators with an ethical ethos in a transparent environment. There is a concern that once we have intelligence out there that is greater than ours, we will suffer the fate of other species that are less intelligent than humans. When these machines get smarter than humans will humans understand what is in the black box. We must understand the steps in the AI’s decision-making process. We have to develop machines that generate explanations for the answers that humans can understand. There is a concern that bad actors could misuse AI. This is difficult to control. Even if AI is benign, it could cause overly dependence by the human society that owned the AI. Human intelligence could atrophy from disuse. The process would be so gradual that civilization will not even recognize this change. AI will, of course, recede into the background and be invisible to society. Can we answer the question of what would be a good future for humanity? We have to acknowledge that humans are already cyborgs. We are already superhuman through links to our computers and phones. However, we do have a problem with bandwidth and making it widespread. We have to increase the bandwidth that will exist between computers and humans. The human prefrontal cortex is a prediction machine that can simulate the future. ASI will be thousands of times more capable of accurate forecasting of the future.
We need to have a vision and imagination of our future with AI. It should be a fun future and not a boring future. We have to solve the alignment problem. It is important to have an AI that is aligned with us in our values and can show ways in which we can improve our lives. The AI should point out various pathways of existence including showing us what future we should want. This forecasting could be the most significant thing that can happen to humanity if we get it right. The AI can help us explore the big questions of why we are here, what is consciousness, and what is the nature of the universe. What is our purpose and how can we unlock our full potential? AI can help us eliminate the negatives in the world. As we shift to the positive, an AI can suggest the path that we cannot imagine. AI opens up a faster pace of possibilities that transcends human formulations and exceeds the limitations of our mental constructs. The post-human era, if it arrives, will be hard for us to forecast accurately.
The prospects of humans becoming smarter as a result of AI is very promising. If we could imagine 74,000 years ago asking humans what type of future would they like to have, they might say, “we would like to have enough food year-round, or we would like protection from predators.” They could not imagine owning an iPhone or an automobile. By analogy, we cannot imagine a future of the AI advances. The vista of change will be too wide in scope to speculate accurately. The imbalances in society between poverty and wealth can be rebalanced with the help of an AI. It can help us eliminate the zero-sum game between haves and have-nots. The competition for scarce resources will not be an issue, because of AI produced abundance. As the saying goes, “freedom consists of the distribution of power and despotism is its concentration.” AI can facilitate freedom. Let’s embrace the challenges of this future world.
Earl Ernest Guile is the author of The Singularity Prize, a novel about AI. Guile is an epidemiologist who practices Tai Chi and has traveled to over sixty-two countries. Website: http://amzn.to/2ALRk89
",A Vision of the Coming Intelligence Explosion,0,a-vision-of-the-coming-intelligence-explosion-4f74c1f77c09,2017-11-20,2017-11-20 00:40:00,https://medium.com/s/story/a-vision-of-the-coming-intelligence-explosion-4f74c1f77c09,False,2587,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Earl Ernest Guile,Earl Ernest Guile is the author of The Singularity Prize. Guile is an epidemiologist who practices Tai Chi and has traveled to over sixty-two countries.,4f3769be0b12,guileee,4.0,2.0,20181104
0,,0.0,,2018-01-22,2018-01-22 06:24:36,2018-01-27,2018-01-27 21:21:48,4,False,en,2018-01-27,2018-01-27 21:21:48,5,8894c0821d97,4.9792452830188685,7,0,0,Can AI COTS ever be OK for government?,5,"AI OK?
Can AI COTS ever be OK for government?
(Apologies for the acronym-heavy sub-head. For the purposes of this blog, AI = Artificial Intelligence, COTS = Commercial off the shelf software. I’m sorry, forgive me, I am a civil servant).

Last week I was approached by a colleague who wanted to understand how they might use an AI-driven video interviewing software.
The proposal was that this would help to manage “sifting” for new candidates applying for certain roles within the civil service.
This would sit between the application and interview stages and use AI to score applicants and provide the top candidates to bring to interview [1].
The colleague who brought this to me has identified a genuine business problem which needs solving and needs support to do so.
As Digital Business Partner, it’s my job to help them find the way to the best solution, within government policy.
Colleagues come to me with all kinds of problems which bring opportunities of all shapes and sizes. In general, there are three paths these could take.
The issue could be fixed by enabling the team to work more effectively using our standard software (we use Google GSuite),
The problem needs more clearly defining, but in doing so, can be solved by the purchase a new piece of software that meets their need, or,
Their issue is more complex, and requires the development of a Service to be used by civil servants across departments, or by the public.
Generally the distinction between these is clear. The first two tend to be driven by business need which is confined to a small set of civil servants within my department, while the latter have a wider reach, or a public-facing element which means that it necessarily needs to be driven by user need.
Every so often a stakeholder will request to use a piece of COTS software in the public domain (seeing 2 as the solution when it’s really 3).
This is often the case when the colleague hasn’t yet considered the user need, or the user need is perceived as secondary to the business need. For example:
“This will help to reduce our workload… but it will also help to speed up the experience for the user”
It’s my job to get them to a place where they prioritise user need. So that they can either evidence that need, find alternative ways of meeting it, or disprove their hypothesis.
I’d like to suggest that an implicit but enduring user need for any project delivered by government should always be:
“To feel confident that I’m going to be treated fairly and having transparent processes which give me that confidence.”
So if we start with this as a user need, how do we take this forward when we are talking about AI software?
Trust is everything.
With any software being used in this kind of instance, I would have a number of questions or concerns generally:
Will any users be excluded because they don’t have the technology to participate?
Will this exclude any users with particular needs, such as those who use assistive technologies (like screen readers).
Will users even want to do this? Are the likely to drop out of the process?
But the use of AI also raises further questions, for me:
By what factors will the application be judging a successful video interview?
Will the AI be looking at physical factors such as body language and facial expressions, as well as the ability to answer questions?
If so, could this disproportionately affect certain groups? I’m not just thinking about protected diversity characteristics (though that is a big concern) but also neuro diversity, introverts/extroverts?
What about presentation skills, regional accents and stammers?

While these potential issues and possible biases are important to understand this also raises a wider question about responsibility.
Namely, with what happens to the trust and integrity of a process if this has been bought in and isn’t “owned” by government?
As a commercial organisation selling AI, it makes sense that a company would want to closely guard their intellectual property. But where would that leave an organisation which is rightly open to scrutiny?

How would an applicant, in this instance, challenge the process if they felt it to be unfair? What would this do to our user need around trust?
How could an organisation prove that a process was fair without understanding the algorithms behind the decision making? Could we, as an organisation, demand transparency around algorithms?
How would procurement activity need to change to ensure that responsibility is understood?
This is the first example of utilising AI which has come my way, but I’m sure it won’t be the last.
I’m also sure that using AI is likely to bring loads of opportunities to increase efficiency and to help improve processes for us as a large organisation. It’s tempting, even slightly seductive. But it also needs to come with additional thought about the long term effects, and it needs to come with conversations about whether we have the right skills to procure properly, to hold ourselves and others to account, and to understand the algorithms which are being used.
Currently I don’t feel like my team are prepared enough for debates around this or equipped with the tools to have conversations around ethics with stakeholders.
I’m keen to understand more, and to speak with anyone working on policy in this area for guidance. I’m particularly interested in whether consideration has been given to how companies offering these services will be able to get onto the Digital Marketplace in the future, or whether any consideration has been given to this in the Digital Service Standard or Spend Controls process.
If you’re working in this area, or just interested, please let me know!
Huge thanks to Dan Barrett and Louise Cato for reading and providing their support for this post, thank you! Extra props go to Ryan Dunn for providing me with a new perspective and a wealth of reading material (which I’ve listed below for anyone who is interested).
Weapons of Math Destruction
We live in the age of the algorithm. Increasingly, the decisions that affect our lives - whether we get a job or a…books.google.co.uk
Opinion | Artificial Intelligence's 'Black Box' Is Nothing to Fear
Alongside the excitement and hype about our growing reliance on artificial intelligence, there's fear about the way the…mobile.nytimes.com
The Data Ethics Canvas | Open Data Institute
The ODI has developed a new approach for organisations to identify and manage data ethics considerations: the Data…theodi.org
[1] I don’t say this because government is immediately planning to use AI in this way; so if you have accidentally stumbled on this blog please don’t go calling your newspaper of choice with your views. These thoughts are all my own.
",AI OK?,28,ai-ok-8894c0821d97,2018-06-07,2018-06-07 20:51:34,https://medium.com/s/story/ai-ok-8894c0821d97,False,1134,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Sam Villis,"Senior Technology Advisor, Standards Assurance at GDS. Proud to be @OneTeamGov. Weeknotes/Design/art/illustration/shiny things/records",1914560770fb,stamanfar,783.0,408.0,20181104
0,,0.0,3a8144eabfe3,2017-09-12,2017-09-12 13:30:30,2017-09-18,2017-09-18 17:31:50,1,False,en,2017-09-18,2017-09-18 17:31:50,6,f3fbb0213bbd,3.750943396226415,5,0,0,Consider this.,5,"Should data scientists sign an ethical code?

Consider this.
A terminally ill patient has just been told by her doctor that she has just about a year more left to live. She has a life insurance policy of $100,000. But what she needs now is money — for medical care or perhaps simply to live her life well for the last few days.
Say an investor offers to buy that policy from her at a discount of $50,000 and to take over the annual premium payment. When she dies, he will be the one to collect the $100,000.
It’s a deal. The dying policyholder gets access to cash, and the investor makes a profit. But there’s a catch. Everything depends on her dying, on schedule.
Known as the viatical industry, the instrument guarantees a certain payoff at death, but the rate of return depends on how long the person lives.
If the policy holder dies in less than one year, the investor makes a killing (metamorphically, at least). If she survives till two years, his annual rate of return is cut in half, and he has to pay additional premium payments. If she somehow recovers, the investor could end up making nothing.
Sounds too horrible to be true? In his book — What money can buy — Michael Sandell brings to light many more such searing examples of market-driven practices where the moral limits of money come into question even as you start wondering if there are things money shouldn’t be able to buy.
When we bring this analogy to the dynamic discipline of data science, a very interesting parallel emerges.
Data has been hailed as a panacea for many problems. And it has indeed taken huge strides in advancing fields across disciplines. But as more and more data scientists join this industry, should there be more discussions on the moral limits of data?
The key question:
Should data scientists sign a code to deal sensitively with data just as doctors do?
Let’s understand why this is becoming urgent today.
The gap:
Most discussions about the moral limits of data are restricted to regulations around the use of data. What data is usable, what are the privacy restrictions, how does that vary across countries, and how do companies incorporate that into their processes.
Forresters’ Data Protection Heatmap by country shows a snapshot. But as data becomes more open and pervasive, and as analytics solutions start becoming more invisible, is the question around the ethics or morals of data also changing?
The key question:
How is data generated and how is it used for different purposes?
Danger by likes:
Psychologist Michal Kosinski developed a method to analyze people based on their Facebook activity. This was based on a model that sought to assess human beings with five personality traits: OCEAN — openness, conscientiousness, extroversion, agreeableness, and neuroticism.
The claim — one could make a relatively accurate assessment of a person including needs, fears, and how they are likely to behave. These traits have become the standard technique of psychometrics. But for a long time, the problem with this approach was data collection, because it involved filling out a complicated, highly personal questionnaire.
Then came the Internet. And Facebook. And Kosinski. But this isn’t the full story. The article digs into something else far more disturbing.
Without Kosinki’s knowledge, a small company called Cambridge Analytica used a very similar or perhaps the same model to fashion two of the most unthinkable political results in recent history. Brexit and Trump. Did Kosinski imagine the potential moral limits of his model?
The key question:
If our data is used for a very different intent than its original intent, how can we control its potential impact?
Danger by devices:
Imagine the amount of data a small wearable device such as a watch might have about each one of us. Wearable tech and IoT are not just buzzwords. They have come very close to us — to our devices and our homes. Now, look at this headline.
“Hacked by your fridge.”
The article spoke about a cyber attack launched through smart fridges just last year. Without hacking into the security features of laptops and other devices, hackers could use the often unsecure IoT devices to compromise the entire network.
At what point does the personalization we need and welcome change into unwelcome persecution and downright danger?
Security in wearable devices has become a key point of concern for many people as the information on the wearable device starts becoming even more personal and valuable than the information even our credit cards could give about us.
The key question:
If the potential misuse of data could lead to unintended consequences, what safeguards do we need to have in place?
The need:
Where does the solution lie?
Individuals stopping access to their personal data? That is going to be increasingly unthinkable.
Manufacturers of devices putting in strong security controls on such devices? That’s happening for sure.
Or is it also by having the data scientists, who devise the intelligence that such machines run on, heighten their awareness of and sensitivity to the potential moral limits of data? Is this a code all of us need to sign?
Let’s change the conversation to good data.
The intention of this article is to raise awareness about the potential use of data and our individual and collective responsibilities.
(This write-up first appeared on LinkedIn Pulse, Datafloq, and BRIDGEi2i and was authored by Prithvijit Roy, CEO and Co-founder at BRIDGEi2i Analytics Solutions)
If you liked this article, please do clap and leave a comment so that it reaches more folks!
",Should data scientists sign an ethical code?,19,should-data-scientists-sign-an-ethical-code-f3fbb0213bbd,2018-06-15,2018-06-15 06:27:49,https://hackernoon.com/should-data-scientists-sign-an-ethical-code-f3fbb0213bbd,False,941,how hackers start their afternoons.,hackernoon.com,hackernoon,,Hacker Noon,stories@amipublications.com,hacker-daily,"HACKATHONS,HACKING,PROGRAMMING,TECH,HACKER",hackernoon,Ethics,ethics,Ethics,7787.0,Ramesh Ilangovan,"Senior Exec @Photon; Previous: Consultant & Editor at @BRIDGEi2i; Interests: Digital Technology, Big Data, Analytics, Cats, MMA.",6ec7a31bdad,ram_ilan,117.0,112.0,20181104
0,,0.0,d6dc2c824f17,2018-03-18,2018-03-18 18:36:39,2018-03-16,2018-03-16 13:00:43,3,False,en,2018-03-28,2018-03-28 17:03:47,1,490177031fa2,5.478301886792454,0,0,0,"AI has the potential to be the greatest ever invention for humanity. And it should be for the benefit of all humanity equally, but instead…",4,"The Geek Shall Inherit
AI has the potential to be the greatest ever invention for humanity. And it should be for the benefit of all humanity equally, but instead we’re heading towards a particular group, the geeks, who will benefit most from AI. AI is fundamentally more likely to favour the values of its designers, and whether we train our AI on a data set gathered from humans, or with pure simulated data through a system like deep reinforcement learning bias will, to a greater or lesser extent, remain.
A disclaimer — Humans are already riddled with bias. Be it confirmation, selective or inclusive bias, we constantly create unfair systems and draw inaccurate conclusions which can have a devastating effect on society. I think AI can be a great step in the right direction, even if it’s not perfect. AI can analyse dramatically more data than a human and by doing so generate a more rounded point of view. More rounded however is not completely rounded, and this problem is significant given any AI which can carry out a task orders of magnitude faster than a human.
To retain our present day levels of inequality while building a significantly faster AI we must dramatically reduce the number of unethical decisions it produces. For example, if we automate a process with a system which produces only 10% as many unethical decisions as a human per transaction, but we make it 1000x faster, we end up with 100x more injustice in the world. To retain todays levels that same system would need to make only 0.1% as many unethical decisions per transaction.

For the sake of rhyme, I’ve titled this blog the geek shall inherit. I am myself using a stereotype, but I want to identify the people that are building AI today. Though I firmly support the idea that anyone can and should be involved in building these systems that’s not a reflection of our world today. Our society and culture has told certain people, women for instance, from a young age that boys work on computers and girls do not. This is wrong, damaging and needs remedying. That’s a problem to tackle in a different blog! Simply accepting in this instance that the people building AI tend to be a certain type of person — Geeks. And if we are to stereotype a geek, we’re thinking about someone who is highly knowledgeable in an area, but also socially inept, and probably a man.
With more manual forms of AI creation the problem is at its greatest. Though we may be using a dataset gathered from a more diverse group of people, there’s still going to be selection bias in that data, as well as bias directly from the developers if they are tasked with the annotation of that data. Whether intentionally or not , humans are always going to favour things more alike themselves and code nepotism into a system, meaning the system is going to favour geeky men like themselves more so than any other group.
In 2014 the venture capital fund ‘Deep Knowledge Ventures’ developed an algorithm called ‘VITAL’ to join their board and vote on investments for the firm. VITAL shared a bias with it’s creators, nepotism, showing a preference to invest in businesses which valued algorithms in their own decision making (Homo Deus, Harari, 2015). Perhaps VITAL developed this bias independently, but the chances area it’s developers unconsciously planted the seed of nepotism, and even the preference towards algorithms due to their own belief in them.
A step beyond this is deep reinforcement learning. This is the method employed by Google’s Deep Mind in the Alpha Zero project. The significant leap between Alpha Go and Alpha Go Zero is that Alpha Go used data recorded from humans playing Go, whereas Alpha Go Zero learned simply by playing against itself in a simulated world. By doing this, the system can make plays which seem alien to human players, as it’s not constrained by human knowledge of the games. The exception here is ‘move 37’ against Lee Sedol, which Alpha Go Lee used, prior to the application of Deep Reinforcement Learning. This move was seen as a stroke of creative brilliance that no human would ever have played, even though this system was trained on human data.
Humans also use proxies to determine success in these games. An example of this is Alpha Go playing chess. Where humans use a points system on pieces as a proxy to understand their performance in a game, Alpha Go doesn’t care about its score. It’ll sacrifice valuable pieces for cheap ones when other moves which appear more beneficial are available, because it doesn’t care about its score, only about winning. And win it does, if only by a narrow margin.
So where is the bias in this system? Though the system may be training in a simulated world, two areas for bias remain. For one, the layers of the artificial neural network are decided upon by those same biased developers. Second, it is simulating a game designed by humans — Where the game board and rules of Go were designed. Both Go and Chess for instance offer a first move advantage to black. Though I prefer to believe that the colours of pieces on a game board has everything to do with contrast and nothing to do with race, we may be subtly teaching a machine that one colour is guaranteed by rules an advantage over others in live.
The same issue however remains in more complex systems. The Waymo driverless car is trained predominantly in a simulated world, where it learns free from human input, fatigue and mistakes. It is however, still fed the look and feel of human designed and maintained roads, and the human written rules of the highway code. We might shift here from ‘the geek shall inherit’ to ‘the lawyer shall inherit’. Less catchy, but simply by making the system learn from a system or rules that was designed by a select group of people will introduce some bias, even if it’s simulating it’s training data within the constraints of those rules.
So, what should we do?
AI still has the potential to be incredibly beneficial for all humanity. Terminator scenarios permitting, we should pursue the technology. I would propose tackling this issue from two fronts.

This would be hugely beneficial to the technology industry as a whole, but it’s of paramount concern in the creation of thinking machines. We want our AI to think in a way that suits everyone, and our best chance of success is to have fair and equal representation throughout its development. We don’t know how much time remains before a hard take-off of an artificial general intelligence, and we may not have time to fix the current diversity problem, but we should do everything we can to fix it.

Because damage caused by biased humans, though potentially catastrophic will always be limited by our inherent slowness. AI on the other hand can implement biased actions much faster than us humans and may simply accelerate an unfair system. If we want more equality in the world a system must focus more heavily on equality as a metric than speed, and ensure at the very least that it reduces inequality by as much as the process speed is increased e.g.;
If we make a process 10x faster, we must reduce the prevalence and impact of unequal actions by at least 90%.
If we create a system 1,000x faster, this reduction must be for a 99.9% reduction of inequality in its actions.
Doing this only retains our current baseline. To make progress in this area we need go a step further with the reduction in inequality before increasing the speed.
Originally published at blog.soprasteria.co.uk on March 16, 2018.
",The Geek Shall Inherit,0,the-geek-shall-inherit-490177031fa2,2018-03-28,2018-03-28 17:03:49,https://chatbotsmagazine.com/the-geek-shall-inherit-490177031fa2,False,1306,"Chatbots, AI, NLP, Facebook Messenger, Slack, Telegram, and more.",chatbotsmagazine.com,chatbotsmagazine,,Chatbots Magazine,matt@chatbotsmagazine.com,chat-bots,"CHATBOTS,ARTIFICIAL INTELLIGENCE,BOTS,TECH,MESSAGING",chatbotsmag,Ethics,ethics,Ethics,7787.0,Ben Gilburt,"I like technology and philosophy, and love things that involve both.",94d19a699148,benjamin.gilburt,50.0,11.0,20181104
0,,0.0,d6dc2c824f17,2018-06-25,2018-06-25 13:44:21,2018-06-25,2018-06-25 13:50:11,3,False,en,2018-06-25,2018-06-25 13:50:11,8,34389bf44039,4.470754716981132,3,0,0,Human Bias to Data Bias,5,"Ethically designing chatbots for the future: How to not be influenced by the past

Human Bias to Data Bias
Machine Learning(ML) and Artificial Intelligence(AI) revolutionize the way software is designed. Before the ML and AI days, it was humans who needed to think through and add in every foreseeable pathway, leaving room for peoples’ individual cognitive biases to find their place in the design of the software. Having humans in the loop meant, the product would inherit judgements made by the designer.
The advances in ML and AI allow us to skip the rule-based, bias filled systems and get right to the data-driven, bias free system. We let the cold emotionless algorithm replace the human and now have an objective bias-free solution! Seems like the perfection solution, right?
Not exactly. It is true that in modern AI, explicitly coding rules to solve problems becomes less important. For instance, to understand language, a model performs better when you allow deep learning algorithms to learn the structure of sentences rather than encoding individual grammar rules in complex error prone systems.
How Children Learn Language
Think about how children learn language; they interact with parents, friends, and strangers and pick up little nuggets of language along the way. They do not learn a pre-defined set of rules defining grammar and other properties of language, and it works great!
As a result, a child’s learning develops from those interactions with family members and others. Incorrect grammar? Foul language? Stereotypes and biases? A child is like a sponge and will pick up and learn from what he or she is exposed to in his or her environment.
Or in other words: the child is trained on data.
How Machines Learn Language
This happens to be a common property of machine learning approaches. A machine is only as strong as the data it is trained on. Which brings back the issue of biases — just on a different level because machines lack the sense of awareness and empathy that humans do.
The Power of Data
Introducing bias through underlying data is a common problem — especially when systems rely on that data to grow from niche products to important parts of everyday life.
Location-Based Games
When Pokemon Go, a location-based game, was released, it had an unforeseen flaw: the data was collected predominately by white male tech-enthusiasts, who unknowingly biased the data towards their own preferences. The amount of available in-game content positively correlated with their most visited and preferred places. This led to an exclusion of minority neighborhoods, rural areas, and, in general, low-income zones.
Job Recommendations
Similarly, LinkedIn’s job recommendation engine used to “forget” to recommend high paying executive positions to women simply because women rarely held those positions in data source the engine was trained on.
Word Embeddings
An example in the Chatbot and Natural Language Understanding (NLU) space are word embeddings (read our primer for an introduction). As a quick summary, word embeddings understand the meaning of words by their proximity to other words.
As a result, word embeddings can pick up the biases of underlying data. If an embedding trained on a corpus of news articles is used to compare similarities of words, it will replicate sexist notions of the news items.
WomanManNurse0.440.25Boss0.140.26Engineer0.150.09
Table 1: Similarities of different jobs to man/woman.
The term “nurse” is judged as being more similar to woman than to man whereas for “boss” and “engineer” the opposite is true. Applying such techniques that model a language based on available texts can lead to these subtle problems.
Language Translations
When translating utterances from a language without grammatical gender to one with, the translation engine resorts to its learned biases to resolve the ambiguity. For the Turkish “o bir muhendis” (“he/she/it is an engineer”) Google Translate selects “he” as the gender for the English translation whereas for “o bir hemsire” (“he/she/it is a nurse”) “she” is chosen.


Figure 1: Translation service showing its gender bias.
On a small scale, one could argue that that’s how language is and that removing the bias would mean removing important information. On a larger one, however, such systems can create a feedback loop by reinforcing biases when interacting with users.
How to Build Trust
The previous examples illustrate that biases are not gone by switching from handmade rule-based systems to ML based ones. The source for biases just moved from the algorithm to the training data — making it less explicit and harder to spot.
What does that mean for collecting data to create a chatbot?
Understand the Risk
First and foremost, it is important to be aware of the risk of having biases in the data which can lead to unintended behavior. Even just this awareness facilitates deeper analysis — not every chatbot scenario inhibits the same risk.
Ask the Right Questions
Second, a good understanding of the data is required. For this purpose, the data, or even better the data collection process, should be scrutinized by answering the following questions:
What is the origin of the data?
When was it collected and by whom?
How does the data relate to the target user group interacting with the chatbot?
Does it capture the user groups peculiarities?
Are language dialects of minority communities reflected in the data?
Test for Edge Cases
Additionally, a good old-fashioned testing strategy will go a long way. If you actively search for your chatbot edge cases, then you can improve your confidence in the data.
Monitor your Source
And lastly, it always a good idea to keep an eye out for who else can modify the training data. It is easy to trust people to do the right thing and suddenly you end up with racist teenage chatbot.
Make a Conscious Effort
The advancement towards data-driven systems did not free us from the risk of bias. Instead, it made the threat more implicit requiring a new awareness: know your data and be proactive about it. Make sure, it reflects the future you want and not reiterates the mistakes of the past.
This blog post was written by Stefan Selent. Thanks!
Visit Recast.AI, our collaborative Bot Platform & join us on Twitter, Facebookand LinkedIn :)
",Ethically designing chatbots for the future: How to not be influenced by the past,33,ethically-designing-chatbots-for-the-future-how-to-not-be-influenced-by-the-past-34389bf44039,2018-06-25,2018-06-25 13:50:27,https://medium.com/s/story/ethically-designing-chatbots-for-the-future-how-to-not-be-influenced-by-the-past-34389bf44039,False,1039,"Chatbots, AI, NLP, Facebook Messenger, Slack, Telegram, and more.",chatbotsmagazine.com,chatbotsmagazine,,Chatbots Magazine,matt@chatbotsmagazine.com,chat-bots,"CHATBOTS,ARTIFICIAL INTELLIGENCE,BOTS,TECH,MESSAGING",chatbotsmag,Ethics,ethics,Ethics,7787.0,RecastAI,Collaborative Bot Platform - build your own conversational bot! #AI #NLP #NLU #MachineLearning #Startup #Tech,a158ea836ddb,RecastAI,1120.0,101.0,20181104
0,,0.0,,2018-03-27,2018-03-27 05:09:49,2018-03-27,2018-03-27 05:10:40,1,False,en,2018-03-27,2018-03-27 05:10:40,2,546639d9d8d3,2.3924528301886805,0,0,0,"Intelligent technologies are improving the way we live. As technology becomes more capable, our world becomes more productive. But despite…",2,"Ethical issues in artificial intelligence
Read the original article here
Intelligent technologies are improving the way we live. As technology becomes more capable, our world becomes more productive. But despite the improvement to our lives, tech giants as IBM, Amazon or Microsoft, as well as geniuses like Stephen Hawking, believe that now is the time to talk about the future of technology and how it is affecting the world. These concerns are ethical issues but a lens into the future risks. So, What are these people so worried about?
Job loss?
The labor industry is mostly concerned with automation. As we evolve and create different ways of automating jobs, we could also create in the future more room for people with complex roles, moving from the manual and physical work, which characterized the preindustrial period, to cognitive labor that is suitable for our new society.
The auto industry for example is currently supporting millions of employees globally. Will those people keep their jobs if self-driving cars and trucks become widely available and affordable? Possibly, but we still need to take into consideration the lower risk of accidents. So, in the end, self-driving cars seem like the right kind of ethical decision.
Technologic richness
Our economic system is mostly based on compensation and contributions, usually evaluated using by wages. The majority of organizations are still counting on hourly work when it comes to services and products. But using artificial intelligence might drastically cut down the human workforce needed while the people who own AI companies will be extremely wealthy.
We are already experiencing a wealth gap, where young start-up owners and founders go home with a large part of the economic surplus they created. What would a society of massive unemployment look like?
Is our behavior changing?
Did you know that a bot has won the Turing Challenge? Chatbot Eugene Goostman succeeded for the first time, being able to fool more than half of the humans involved in what they thought was chatting with a person not a machine. This is only the start of an age where we will interact more frequently with machines as if they were humans. People have some limitations when it comes to attention and kindness, AI technologies have unlimited resources to build relationships and never get tired.
Controlling the intelligent systems
As humans, we can control almost everything because we built everything around us and are currently the top of the food chain. That raises a serious question about AI bots: will they be able to have the same advantages over us? May the machine anticipate our next move and defend themselves? This situation is called in ethics `singularity` and refers to the fact that people will not be the most intelligent beings on earth.
Robot rights (yes, we also have that)
As we develop technology, we need to understand the mechanism of reward and aversion, which is used for humans and also animals. For robots, these systems are partially superficial, but they are becoming more sophisticated. In this situation, could the machine suffer when it is rewarded with negative input? There are some genetic algorithms where they create multiple instances of a system at once, but only the most successful survive, the rest are deleted.
The end goal is a better life for us but we need to keep a close eye on AI evolution.
Are there any concerns that cross your mind? Let me know in the comments section.
Photo source: unsplash.com
Originally published at rickscloud.com.
",Ethical issues in artificial intelligence,0,ethical-issues-in-artificial-intelligence-546639d9d8d3,2018-03-27,2018-03-27 05:10:41,https://medium.com/s/story/ethical-issues-in-artificial-intelligence-546639d9d8d3,False,581,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Rick’s Cloud,,fd3840f389e2,contact_40643,8.0,16.0,20181104
0,,0.0,,2018-05-13,2018-05-13 16:19:01,2018-05-13,2018-05-13 16:24:29,2,False,en,2018-05-13,2018-05-13 16:24:29,2,17f725448f41,3.0022012578616346,2,0,0,"Before I get to the crux of the matter, discussing Artificial Intelligence, let me start with an anecdote similar to the topic at hand, and…",5,"A.I., The Black Swan, and Asking the Relevant Questions

Before I get to the crux of the matter, discussing Artificial Intelligence, let me start with an anecdote similar to the topic at hand, and which took place a few years back, in 2007, to be precise. During that year, no one with proper judgement could have foreseen the looming financial catastrophe. Crisis befell the markets, while the incredulous economists could not construct a proper etiology of the subprime loans crash.
All theoretical attempts to examine and explain concretely what went wrong came to no avail. It was in the aftermath of the crisis that the name of Nassim Nicholas Taleb started gaining momentum and his book The Black Swan (2007) released to an impactful global reception, polarizing economists and financial analysts alike.
As it turns out, way before the market crash, Taleb — who has been teaching risk analysis at several US academic institutions and was a floor trader for many years (he had Skin in the Game, as he would describe it in his 2018 book) — had alerted the ecstatic economists about an improbable, yet possible mishap. Taleb was quickly dismissed as a lunatic theoretician preaching an unintelligible theory that bore no justifiable ground.
When the market’s tendency was bullish (on the rise), people like Nassim Taleb were alerting of the cataclysmic consequences a possible crash would entail for the people, and for financial institutions. Hence the title of his book is The Black Swan, (in reference to the discovery of Australian black swans, when the preconception was that swans were all white), which alludes to the possibility that an odd and very unlikely event might take place (economic crisis, economic bubble burst) when all the predictions and forecasts are contrariwise.
I will leave the philosophical underpinning of Taleb’s argument for a future post. For now, I will use the Black Swan argument to draw an analogy between the financial market and the tech industry.
The hype surrounding the leading tech firms has been somewhat successful at foreshadowing ethical concerns, which have been relegated to a secondary level, in favor of features that appealed to the consumerist spirit of the public.
The positive atmosphere in Silicon Valley, and elsewhere, indirectly silenced and vetoed genuine attempts at dissecting and analyzing the ensuing ethical problems. This blinding belief in the near-infallibility of A.I. — which is still a burgeoning field — resulted in a series of unexpected accidents (like that of Uber, 2018), and scandals (like that of Facebook/Cambridge Analytica, 2018).

Failing to so much as entertain the possibility of an existing glitch in the system prompted ignominious situations that could have been avoided had these companies contemplated the likelihood of a black swan event.
The drive to generate profit as fast as possible, in order to justify the excessive amount of money invested in fields of research like A.I, deep learning, and neural networks, overburdened many stakeholders with risk that, while not easily predictable, could have been accounted for by asking the relevant questions.
The relevant questions I have in mind are the sort of questions that are more often than not asked within the ivory towers philosophers inhabit. As such, instead of engaging with the potential setbacks of any concrete venture, they would rather limit themselves to arguments the more abstracted from reality the better.
These same questions are, on the other hand, pondered by fictional artistic, literary, and visual productions; for example, in movies such as Her (2013) and Ex Machina (2014), and T.V. series such as Black Mirror (2011). However, the themes that these productions deal with are often ignored by the concerned stakeholders.
The result is a virtually unbridgeable hiatus between the tech industry and other sectors, with each disregarding the other sector’s two cents.
However, if there’s a lesson that we could learn from the financial market and from Nassim Taleb, it would be this: perhaps it is not a waste of time to attempt to ask the relevant questions, questions about the improbable but possible events that might strike the A.I. research of the tech industry, in order to avoid the proliferation of black swan events.
","A.I., The Black Swan, and Asking the Relevant Questions",2,a-i-the-black-swan-and-asking-the-relevant-questions-17f725448f41,2018-05-15,2018-05-15 03:43:15,https://medium.com/s/story/a-i-the-black-swan-and-asking-the-relevant-questions-17f725448f41,False,694,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Mahmoud Rasmi,@decafquest | Philosophy professor,c68b559d5471,mahmoud.rasmi,19.0,8.0,20181104
0,,0.0,32881626c9c9,2018-08-03,2018-08-03 16:52:15,2018-08-24,2018-08-24 16:01:22,2,True,en,2018-08-26,2018-08-26 15:40:28,8,530d70996e56,3.221069182389937,48,9,0,What will be the point of a performance vehicle if all self-driving cars are programmed the same?,5,"Self-driving cars: The end of the “sports” car?
Many believe that fully self-driving cars will become a reality in the next ten years. Tesla’s Autopilot mode has already got us halfway there. The automation of driving means taking control from humans and handing it to on-board computers and networks. This raises some interesting questions about the software that will drive these cars.
Inside Google’s RoboCar (jurvetson [CC BY 2.0 (https://creativecommons.org/licenses/by/2.0)], via Wikimedia Commons)
First, let’s assume a future where driving your own car is generally no longer allowed. In this automated world, a car manufacturer produces two models of self-driving car. The economy model is small, economical on fuel, has a no-frills interior and an average ride. Meanwhile, the luxury model is heavier, has a larger engine and is fitted out so the user rides in comfort.
Appearance and comfort are not where the design decisions will end when it comes to fully self-driven cars. For self-driving cars it is the designer, not the driver, that decides how the vehicle is driven. Should the car tear off with a screech of tyres and a puff of smoke, weave in and out of traffic, pushing the speed limit at all times and slamming on as it approaches a set of lights with no regard for fuel economy? Or should the car’s every manoeuvre be optimised for safety and fuel efficiency?
These will be programming decisions, made years before in an office somewhere. Will the manufacturer decide to program the luxury model to drive more aggresively than the economy model? Would this be ethical? Would it be legal?
Under current UK advertising standards it is forbidden for car ads to “demonstrate power, acceleration or handling characteristics except in a clear context of safety”. The promotion of dangerous or irresponsible driving is strictly prohibited. BMW has gotten into trouble several times recently over ads that flouted advertising standards rules by encouraging irresponsible driving. Instead of using ads, manufacturers generally rely on testosterone-fuelled car reviews from magazines and television programmes to make buyers aware of the performance of their cars.
What’s the point in a “sports” car if it drives the same as an eco model?
Although a BMW M3 is clearly more powerful than a Vauxhall Corsa, drivers are not supposed to use the BMW M3’s power to accelerate dangerously or drive above the speed limit. This is the contradiction at the heart of the car industry: you can buy a sports car but you still have to drive to the same rules as everyone else. The reality is that many performance car owners do not follow the rules of the road. Such cars are statistically more likely to cause crashes and therefore deaths and injuries, a factor well-understood by car insurers (here and here).
A car is not a toy, it is a grave responsibility.
A car is not a toy, it is a grave responsibility. We’re talking about two tonnes of metal, glass and plastic travelling at speed; that’s a lot of kinetic energy. I know, you want that sports cars because it’s “cool”, right? I also used to want a Ferrari so I could race around Monte Carlo and a Jeep so I could cross the Sahara…but I stopped wanting those things when I was about fourteen years old and realised I won’t be taking part in any road races or desert trials during the daily commute or the trip to Aldi. It’s time for car owners to grow up and recognise their vehicle as a fantastic tool for work and leisure instead of a solution to their mid-life crises.
The coming of self-driving cars is an opportunity to rethink the purpose of performance/sports/all-terrain vehicles. Alongside taxing pollution emissions, there will be an opportunity to regulate the programming of self-driving cars such that they are driven in the safest and least-polluting way. Limits could be set on acceleration and braking in %g (AKA G-forces) that would prevent cars with powerful engines performing dangerous manoeuvres. Perhaps the programming behind all cars will be so similar that the “performance” is no longer a selling point. Instead, luxury will focus on ride, interior finishings and in-car entertainment.
Perhaps the phrase “sports car” will fade away entirely; sports are competetive and driving is not, or rather shouldn’t be, unless you’re at a track day. In any case, sports are supposed to make you thin…

© David Watson 2018
This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.
",Self-driving cars: The end of the “sports” car?,342,self-driving-cars-the-end-of-the-sports-car-530d70996e56,2018-08-26,2018-08-26 15:40:28,https://medium.com/s/story/self-driving-cars-the-end-of-the-sports-car-530d70996e56,False,752,"Data Driven Investor (DDI) brings you various news and op-ed pieces in the areas of technologies, finance, and society. We are dedicated to relentlessly covering tech topics, their anomalies and controversies, and reviewing all things fascinating and worth knowing.",,datadriveninvestor,,Data Driven Investor,info@datadriveninvestor.com,datadriveninvestor,"CRYPTOCURRENCY,ARTIFICIAL INTELLIGENCE,BLOCKCHAIN,FINANCE AND BANKING,TECHNOLOGY",dd_invest,Ethics,ethics,Ethics,7787.0,David Watson,"Nature-lover, armchair economist, language enthusiast and nuclear engineer (not necessarily in that order).",b1be87718881,DavidWatsonBlog,54.0,110.0,20181104
0,,0.0,,2018-05-15,2018-05-15 17:17:48,2018-06-27,2018-06-27 15:57:20,0,False,en,2018-07-17,2018-07-17 13:10:21,12,388908d18097,6.932075471698113,1,0,0,"A number of people —including Elon Musk, Stephen Hawking, and Henry Kissinger — have issued dire warnings about artificial intelligence…",5,"We’re programming A.I. psychopaths — and how to avoid it
A number of people —including Elon Musk, Stephen Hawking, and Henry Kissinger — have issued dire warnings about artificial intelligence. Are we running headlong into disaster, or are these warnings exaggerated?
In a new academic article appearing in the peer-reviewed journal A.I. & Society, I argue that current approaches to programming ethics A.I. are indeed deeply flawed, and a different programming approach is necessary to ensure that machines behave morally.
As Kissinger points out, there are at least two problems with existing approaches to A.I. programming:
The problem of unintended results (the ‘black box’ problem): no one really knows how to predict what A.I. algorithms will do.
The problem of interpretation (or ‘context’): “the danger that AI will misinterpret human instructions due to…inherent lack of context.”
Kissinger illustrates these problems as follows:
A famous recent example was the AI chatbot called Tay, designed to generate friendly conversation in the language patterns of a 19-year-old girl. But the machine proved unable to define the imperatives of “friendly” and “reasonable” language installed by its instructors and instead became racist, sexist, and otherwise inflammatory in its responses…Can we, at an early stage, detect and correct an AI program that is acting outside our framework of expectation? Or will AI, left to its own devices, inevitably develop slight deviations that could, over time, cascade into catastrophic departures?
In my article, I argue that existing approaches to A.I. programming cannot solve these problems, and fail in a way that can only be expected to produce ‘psychopathic’ behavior.
To understand why, consider two different ways a person or A.I. can ‘behave like a psychopath.’
First, one can act in ways that display an absence of moral conscience — that is, in ways that display little or no regard for the well-being or freedom of other people. This is the kind of psychopath most of us familiar with: the ‘cold-blooded killer’ who tortures animals and kills people for (serial killers like Ted Bundy, Jeffrey Dahmer, and Patrick Bateman from American Psycho).
However, this is not the only way to ‘behave like a psychopath.’ As the TV series Dexter and Avengers: Infinity War illustrate (SPOILER ALERT), psychopathic behavior can be cause having too much of a conscience, acting on ‘moral principles’ in overly strict or zealous ways. In Dexter, the main character kills people on moral grounds — to punish the guilty for their wrongdoing. And in Avengers: Infinity War, Thanos aims to exterminate half of all life in the universe to make a ‘better world.’
In my paper, I show how current approaches to A.I. programming are likely to produce both types of ‘psychopathic’ behavior. If, on the one hand, we program A.I. without a clear ‘moral target’ (moral motives or principles), A.I. will be ‘psychopaths’ in the first sense, lacking an appropriate ‘conscience.’ However, if we do program A.I. with moral motives or principles, we need to make sure that they do not act on those motives in an overly strict or overzealous manner (like Dexter or Thanos).
The problem, however, is that current approaches to A.I. fail to solve this problem: Kissinger’s problem of interpretation. As I will now explain, current programming approaches can be expected to lead A.I. to interpret moral concepts and principles either too strictly or too flexibly.
In order to behave morally, A.I. must interpret moral concepts like ‘harm’, ‘offense’, and so on. To take a simple case from the 2004 film I, Robot, for A.I. to obey a principle, “Do not harm humans”, the A.I. must interpret each relevant concept (What is harm? What is a human? Is a fetus human? Etc.).
Here, though, is the problem. Current approaches to A.I. programming either program strict interpretations into A.I. (e.g. harm = anything causing pain) or flexible/learned interpretations (leaving it to the A.I. to ‘decide’ how to interpret the relevant concepts). The problem is, neither approach can work.
First, consider what it would be for a machine to follow a strict, completely inflexible rule to “never cause harm”, where this means never causing pain or physical damage to humans. A machine following this rule would never pinch a person (causing minor pain), even if doing so might prevent great evil (say, the end of the world). The basic problem here is that strict interpretations moral rules are the wrong target. Morality, as humans understand it, isn’t a set of strict rules. Rather, there are contextual exceptions to every rule. For example, it generally is wrong to pinch people against their will or cause them harm. However, if one had to choose to pinch a single person to save the world, many of us would think it would be the right thing to do. So, we need program A.I. to be sensitive to context.
Some theorists appear to think we can program sensitivity to context by polling human beings about how to best respond to different scenarios or contexts, and then programming A.I. to respond to scenarios the way most humans prefer. However, this is exactly wrong. Part of the problem here is that human beings disagree tremendously over what is right in different scenarios. For instance, when it comes to self-driving cars, some of us want them to save as many lives as possible; some of us want them to save us or the ones we love; and so on. Further — and crucially — morality is not a matter of simple majority opinion.
We can see why by looking at an example from I, Robot, where Detective Spooner (Will Smith) comes to hate A.I. due to the way in which they robotically and inhumanly follow the programming a majority of people prefer. In one scene, an AI robot saves Spooner’s life instead of the life of a young child — all because a majority of citizens supported the idea that A.I. should maximize probability of survival. The problem is, it isn’t the majority who were in that situation. It was Spooner and the girl whose lives were at stake, and Spooner wanted and believed the girl should be saved.
Cases like this are important. They illustrate why morality is not a simple matter of ‘whatever the majority says, goes.’ Once upon a time, a majority of people supported slavery; and in some societies, human sacrifice. Majority support doesn’t make something right. No, morality is a matter of justifying one’s actions to those one’s actions affect — and in every context, there are different people one’s actions affect. So, to program A.I. to be ethical, we need to program them to be sensitive to these contextual factors. But how?
The obvious answer is to program A.I. to apply moral concepts or rules in a more flexible manner — to ‘decide for themselves’ who to help or save, much like we do. However, the opposite problem arises here: how can we be sure we are programming in the right amount of flexibility?
It’s clearly vital not to program “too much” flexibility in, for then A.I. can interpret moral principles wrongly. For instance, in I, Robot the A.I. interprets the programming law “do not allow harm to humans” as requiring it to enslave humans for our own good (because we wage war, kill, murder, rape, etc.). Clearly, this is too much moral flexibility. So, again, we come to the problem: how do determine the right amount? In my article, I argue there is only one plausible way to do so: we need to program A.I. to solve the problem of context/interpretation the way we solve it. So, how we solve it?
In my 2016 book, Rightness as Fairness: A Moral and Political Theory, I argue that morality emerges from a form of risk-avoidance typically learned in adolescence. In childhood and adolescence, we are liable to take ‘dumb risks’, such as stealing candy, or cheating on tests, etc. Then, most of us learn to avoid these risks (as ‘risks not worth taking’). Why? The answer is: we learn to avoid these risks because, during adolescence, the risks often do not go our way: we end up in the principal’s office, or grounded by our parents, or simply feelings of guilt— things that lead us to regret the risk and avoid similar risks in the future.
I then argue that this kind of risk-aversion makes it rational to care about how other people affected by our actions respond. After all, one reason we regret ‘dumb risks’ is social punishment; another reason we regret ‘dumb risks’ is we feel guilty afterward (“I feel so guilty about how I hurt her”); and so on. I argue this kind of risk-aversion is key to solving the problem of moral interpretation in A.I. For why do you or I not interpret a moral principle like “protect people from harm” as permitting the dangerous interpretation “Enslave humans for their own good”? The answer is that any person of conscience would worry they might regret this interpretation (“I know enslaving the human race might ‘protect’ humans from each other — but goodness, I really might regret trying to do that!”). Indeed, the A.I.’s error in I, Robot is precisely this: she does end up ‘regretting’ her inhuman interpretation, precisely because humans resist and destroy her. Had VIKI had our human-form of risk-aversion, she would not have interpreted ‘protect humans from harm’ in such a radical way. She would have cared instead about how we might respond to her interpretation (viz. resistance).
What worrying does, I argue, is ensure that we interpret moral concepts and requirements in ways that respond to those around us in any given context — the specific people whose lives we might ‘harm’ or ‘help’ in the circumstance at hand. So, for instance, if I were in the place of the A.I. who had to choose to save Detective Spooner or the child in I, Robot, I would not simply save the person who had the higher probability of living (as the A.I. does ‘inhumanly’). No, I would hear Detective Spooner’s pleas (“Save the girl!”) and worry — in a human manner — that if I did not respect his his moral preference (viz. “save people from harm” means saving the girl in this context), I might regret it because Spooner would regret it if I let the girl die to save him.
In other words, worrying about the future in the way I describe (worrying about how our interpretations of moral concepts might affect those around us) is precisely what leads us to interpret moral concepts in ‘human’ rather than ‘psychopathic’ ways. It leads us to interpret moral concepts flexibly but not too flexibly, by leading us to (A) care about how others might respond to our moral interpretations, and (B) avoid risky, dangerous interpretations.
Unfortunately, this is precisely how AI aren’t being programmed today. If I am right, we need to fundamentally change how we approach programming A.I.…before it is too late.
",We’re programming A.I. psychopaths — and how to avoid it,1,were-programming-a-i-to-be-psychopaths-and-how-not-to-388908d18097,2018-07-17,2018-07-17 13:10:22,https://medium.com/s/story/were-programming-a-i-to-be-psychopaths-and-how-not-to-388908d18097,False,1837,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Marcus Arvan,PhD - Associate Professor of Philosophy www.marcusarvan.net,d92198dc9482,marcusarvan,104.0,103.0,20181104
0,,0.0,,2018-05-30,2018-05-30 13:37:27,2018-05-30,2018-05-30 14:02:26,3,False,en,2018-06-01,2018-06-01 22:31:05,2,126f1a0026ab,3.5915094339622646,0,0,0,What is the value of ethics today? Why do we talk about ethics when we talk about public algorithms?,5,"What part of explicability am I ready to give up?
What is the value of ethics today? Why do we talk about ethics when we talk about public algorithms?
These are the questions that GALATEA wanted to answer by organizing a workshop at Google on May 24th. Through this workshop we tried to look at the principles and not to go into precise details.
End of the workshop — Magic squad
Why would we build a workshop ?
The main objective for GALATEA for this workshop was to try to put into perspective the stakes of the algorithms used by public administrations in a democratic framework. Fully aware of the complexity, our ambition was to connect people from very different backgrounds in order to figure out together the beginning of a solution. 
 Rethinking the way we produce tomorrow’s AI will not be done with uniform profiles. We had to involve students from all fields and environments to work together. Students in human sciences, engineers, business, coders, junior lawyers… but also few experienced mentors such as Lucie Cluzel, Professor of Public Law at the University of Lorraine, Catherine Prébissy-Schnall, Senior Lecturer in Law at the University of Nanterre and Timothée Paris, Deputy Rapporteur General of the Report and Studies Section at the Council of State and many others…
Weird googlers
But a question remains unresolved, why did we choose Google?
We were absolutely sure that separating public problems from private experts would be a methodological error, an impoverishment of our discussions.
How did we organized the workshop ?
 Our magic squad has been divided in 4 groups and worked during 3 sprints of 3 hours. Each sprint was launched by a 10 minutes speech from one of our intergalactic speakers. 
 The subject of public algorithms was thus divided into three questions: 
 — Should the use of public algorithms be regulated? 
 — Defining their purpose and scope
 — Code is Law? The right to digital literacy
What proposals came out ?
Group 1:
Define a governance framework based on an ethics committee specializing in algorithms. The citizens will be consulted by the administration and the developers will follow a precise specification, issued by the administration. The code will be readable in natural language so that it can be analyzed and tested by the ethics committee.
This framework will be built around principles such as the presumption of non-functioning, the principle of mutability or a send box system to test the code and trace any bias.
As far as citizens’ rights are concerned, they may bring an action before the court.
Group 2:
Establishment of four main founding principles for the construction of a clear governance framework:
Partial transparency of the code (available but not public). The code will be Intelligible and therefore described on technical sheets
Liability brought by the sponsor
Democratic equality between individuals
The right of appeal in case of presumption of having been wronged
The use of public algorithms will have to be framed. Each algorithm must first undergo an audit to validate the intelligibility sheet. Ex post control should be put in place with the defender of the rights of freedoms. The sanctions would be either on the commissioner or on the person responsible for the audit, and we could finally think of a compensation fund for the victims.
Group 3:
Define a legal framework in soft law in order to fight against the illegality of the result (non-conformity to the law for a judgment being made by an algorithm) and the arbitrariness of the result (fight against the possible biases).
This framework will impact decision makers (the administration) and coders, and will be based on key principles:
A principle with an exceptional regime
A principle of loyalty understood in the sense of conformity
A principle of transparency understood in the sense of best possible explainability
This framework will still have a basis in hard law, with the establishment of specifications for a code that reflects the law.
Group 4:
Define a legal framework inspired by the Maslow pyramid, but with the primary goal of regulating public actors and private actors who have been delegated a public service mission.
This framework would be based on an ethical charter of public cross-sectoral algorithms in hard law built around 3 principles: Principle of equality and neutrality, a principle of non-discrimination and transparency and finally principle of continuity and mutability.
This base would be supplemented by sectoral laws and sectoral code of ethics.
Magic squad — Work hard x Play hard
How did it end ?
Our jury, composed of Lucie Cluzel, Professor of Public Law at the University of Lorraine, Jean-Baptiste Pointel, pedagogical adviser of the ENA and Ludovic Peran, in charge of Public Policies at Google, selected groups number 2 & 3. They will refine and combine their propositions and present them to ENA’s students at the end of June.
by Pierre Boullier & Valérian Dunoyer
",What part of explicability am I ready to give up?,0,what-part-of-explicability-am-i-ready-to-give-up-126f1a0026ab,2018-06-01,2018-06-01 22:31:06,https://medium.com/s/story/what-part-of-explicability-am-i-ready-to-give-up-126f1a0026ab,False,806,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Galatea Network,Galatea is an international and multidisciplinary network of reflection and proposals on the links between Law & Artificial Intelligence.,e25b10e2b664,galatea.net,24.0,9.0,20181104
0,,0.0,,2018-04-10,2018-04-10 18:40:16,2018-04-10,2018-04-10 18:43:41,0,False,en,2018-04-10,2018-04-10 18:43:41,0,771b1e5b29c1,2.8264150943396227,0,0,0,"I believe I may have coined a new term, the AI Divide, in my paper titled, The Potential Societal Impact of the AI Divide (at least when I…",5,"The AI Divide: Living in an Age Where You are the Product
I believe I may have coined a new term, the AI Divide, in my paper titled, The Potential Societal Impact of the AI Divide (at least when I googled the term prior to submitting the article). About two weeks ago, I shared a paper and presentation on the AI Divide at the American Association for Artificial Intelligence (AAAI) Spring Symposium on AI and Society: Ethics, Safety, and Trustworthiness in Intelligent Agents held at Stanford University. My paper and presentation discussed what the AI Divide is and asked the following questions:
Does an AI Divide exist?
Are there populations that are negatively impacted by the AI Divide
Should there be public policy that will protect “AI-marginalized” populations?
Should we provide AI Literacy for all citizens?
Will the AI Divide continue to increase or shrink?
These are questions that will need continued discussion, exploration, and answers. The Digital Divide began in the 80's with the advent of the personal PC and later the Internet and the disparities in access to computing devices, fast Internet, and access to internet-accessible knowledge sources. This has helped contribute to socio-economic disparities including education quality, college readiness and career outlook and income.
The AI Divide is developing because AI is becoming more and more ubiquitous in our daily lives. AI is becoming increasingly ubiquitous in e-commerce (e.g. Amazon), natural language recognition (e.g. Siri), social media (e.g. Facebook), information technology, and even wearable tech (e.g. Apple Watch). Several startups and automakers want to make AI ubiquitous in driverless cars, although lately Uber and Tesla have had untimely deaths related to the AI involved in driverless cars.
Before this week, I saw the AI Product Cycle involving people as consumers of AI products, and companies that develop and control the hardware, data, and algorithms as the producers of AI products. People interact with this hardware to generate data closely tied to their emotions and behavior, which are in turned used by companies’ algorithms to produce AI-enabled products (e.g. Facebook app).
But as we are seeing recently, companies such as Facebook, are using people’s personal data to fuel their social network algorithms to influence people’s behavior. In this case, influencing voter behavior, as well as their purchasing behavior. But Facebook is not unique. Other big companies, including the usual suspects (e.g.Google) are doing the same to monetize these algorithms by using ads to influence behavior.
One user said it best in an online interview about Facebook’s tactics and said that he realized that he is the product. His data was being sold so that others could feed their psychographic machine learning algorithms to know how to best exploit his personal information to make him vote or buy the way the company wanted him to. What he said raises the question: Are YOU the product of these companies that use your data and AI algorithms to influence YOUR behavior?
The AI Divide is the split between the companies that own the hardware, data, algorithms, and applications that you and I use, so that they can exploit our emotions and behavior and those of us that down own them. Most people are AI illiterate and don’t understand the basics of how their data is used, nor how these machine learning algorithms work. The disparity between those who create, own, use, and understand these algorithms and those who don’t is the AI divide and has potential to create disparities in quality of health, safety and security, and prosperity.
Unlike the digital divide, the AI divide won’t necessarily exist along racial, socio-economic, or even political and educational lines. The AI Divide can exist across these lines between the producers/owners of the hardware, data, algorithms, and applications of AI and those that are only the consumers, and in some cases, the living and breathing “products” sold and influenced by AI.
What can be done to address the AI divide? Those are the answers we need to decide on before it’s too late.
Andrew B. Williams, Ph.D., is Associate Dean for Diversity, Equity, and Inclusion for the School of Engineering and the Charles E. And Mary Jane Spahr Professor in Electrical Engineering and Computer Science, at the University of Kansas (KU). Dr. Williams is also Director of the Humanoid Engineering & Intelligent Robotics (HEIR) Lab at KU.
© 2018 Andrew B. Williams
This article was originally written on April 9, 2018 on Dr. Williams’ Wordpress Blog, drandrewspeaks.wordpress.com/blog
",The AI Divide: Living in an Age Where You are the Product,0,the-ai-divide-living-in-an-age-where-you-are-the-product-771b1e5b29c1,2018-04-10,2018-04-10 18:43:42,https://medium.com/s/story/the-ai-divide-living-in-an-age-where-you-are-the-product-771b1e5b29c1,False,749,,,,,,,,,,Ethics,ethics,Ethics,7787.0,A Williams,,a4c59a700d40,andrewwilliamsphd,0.0,7.0,20181104
0,,0.0,7f60cf5620c9,2018-08-08,2018-08-08 20:44:50,2018-09-03,2018-09-03 15:14:23,2,True,en,2018-09-03,2018-09-03 15:14:23,2,d98fb1f17214,5.6361635220125805,5,0,0,Moral quandaries of being an AI/ML developer,4,"You’ll likely be a murderer.
Moral quandaries of being an AI/ML developer
Am I the only one that feels bad when I unplug Alexa?
We have no freaking clue what AI will be able to do in the future. It is young. And while we humans do, indeed, have fancy people working on making fancy new tech that does fancy stuff, no one has produced anything that even remotely resembles human-level general cognitive capabilities (the word general is important there, because we sure as hell get our asses kicked by computers at Go, chess, and a multitude of other “narrow” tasks and activities). Nothing created thus far has been able to perform as well as an average human would at any “general” task.
To sum up the current state of AI research and development, our attempts are still turning out to be pretty dumb. They’re best described as savants — extraordinarily brilliant in one specific area, but limited only to that one area. To some, that general stupidity is comforting. It serves as an indication that humanity is still quite some temporal distance away from creating anything that might fundamentally change our world. To me, that general stupidity is instead deeply, insanely worrisome.
The worry stems from a place of ethical concern. There’s a host of nuances and philosophical arguments that surround the ethics of AI, and we’ll be looking into one of the most pressing problems in an effort to be as prescient as possible.
Certain machines have moral status. Now, I know you’re probably saying, “Wait, what’d he just say? Machines? Moral status? Bullshit!” But please hear me out on this one.
It’s pretty well agreed that at the current time, our AI systems and ML models don’t actually have any moral status. We can terminate them, pause their operation, delete them, copy them, or rewrite the code that comprises them as we so please. But how might those actions be interpreted if a given machine did have human-like moral status? Would we be killing, cloning, and maiming without consent?
Things could get very sticky, very fast, so it’s probably a good thing to dive a bit deeper and ascertain which specific qualities are enough to warrant moral status in a being. Usually, it boils down to two things.
The first is sometimes called sentience — it’s the capacity for a being to have experiences. It’s the ability to be hurt and to suffer, and conversely to be happy and experience pleasure.
The second is often called sapience — it’s the set of capabilities associated with “higher intelligence”. These are the things that differentiate humans from animals — things like self‐awareness and reason-responsiveness.
A common view is that animals have sentience and therefore some moral status, but are limited because they’re not sapient. They can’t make complex deductions about the world as a whole, or often even recognize themselves in a mirror! Humans, however, have both sentience and sapience, and therefore full moral status.
Now onto the good stuff. A sentient AI or ML system should then be treated not as just some pile of computer parts — instead, it should be regarded in a similar way to a living animal. It’s morally wrong to maim, kill, and inflict pain on a bunny, so why should it be different for sentient AI/ML systems?
But wait, there’s more!
What if one day humanity popped out an AI/ML system that possessed not just sentience, but also sapience? Following our earlier logic, it then ought to have total moral status — the exact same as humans.
Perhaps this strikes you as grossly incorrect. If that’s the case, I highly recommend “pausing” here and trying to precisely reason why. After you’re done, come back and continue reading!

There are two core principles that lead to the above conclusion. The first is that of Substrate Non-Discrimination — to paraphrase Bostrom and Yudkowsky, if two beings have the same functionality and the same conscious experience, but differ only in the substrate of their implementation (read: stuff they’re made of), then they have the same moral status.
One way to think of this is as non-racism against computers. Just like it doesn’t matter in terms of moral status if someone’s skin is dark brown or pale white, it shouldn’t matter if their brain is made of carbon and water or of silicon. As Sam Harris so loves to put it, there’s nothing special about ‘wet ware’— the fact that our brains are made of carbon and water does not give them inherently more moral status than exact copies made of silicon. If your entire brain were able to be emulated on a computer, neuron for neuron, memory for memory, thought for thought, and existed in a programmed world where it could run and jump and interact with other whole-brain emulations, you’d want it to be free, happy, and living a life free of suffering and pain! It seems as though it’d be morally disgusting to slaughter or inflict pain upon the computerized version of yourself.
The other core principle is that of Ontogeny Non-Discrimination — if two beings have the same functionality and the same conscious experience, but differ only in how they came into existence, then they have the same moral status.
We widely accept this today. The fact that a baby was born of a mother who ate more healthily and took her vitamins doesn’t give that baby any more or less moral status than one whose mother indulged in ice cream and booze everyday and didn’t much consider her health. Likewise, if we were to create human clones, we wouldn’t think that a cloned baby deserves less moral status than any other baby born through sexual reproduction. Similarly, just because a being came into existence as an AI/ML system through programming, it doesn’t deserve to be treated with less moral status than a human being who was born via sexual reproduction.
If you accept both of those principles, then it doesn’t matter if an AI/ML system was created by doofy programmers or runs on a computer instead of in a brain — it’s deserving of treatment that’s identical to how we treat one another.
Although this makes it far easier to develop an ethical code for how we treat AI/ML systems (i.e. treat them like we think we should treat other humans), there are crazy potential scenarios that arise because of how such systems may come into existence.
While we humans toil away trying to develop smarter and better AI/ML systems, it’s likely that we wouldn’t know if a system is sentient or sapient. There could be no surefire way of determining if it could feel pain or reason at a level comparable to humans.
Let’s concoct a fairly reasonable scenario where some programmers are using genetic algorithms to attempt to optimize an AI/ML system, and at some step along the way the systems become sentient and sapient. Because there might be no way of knowing, the developer keeps living his merry life and continues to run his algorithm for a few days on a future-era supercomputer. Over the course of those few days, it’s conceivable that trillions of versions of the AI/ML system being optimized were run. It’s also conceivable that the versions were tested based on their ability to complete some sort of task in a simulated world. Let’s say that our researcher is attempting to make an AI/ML system that can jump extremely efficiently in a simulated world. The consequence of this set of innocuous circumstances is that trillions of beings, all at least morally equivalent to humans, were mercilessly slaughtered after being thrown into a testing pit because of their inability to jump efficiently in the only “world” that they’d ever known.
The estimate provided by the National WWII Museum for the amount of deaths that happened during World War II, one of the largest losses of life in the history of mankind, is 60 million. Now imagine ~17,000 World War IIs occurring in the span of a few days, entirely on accident.
The potential for accidentally horrific moral deeds in AI/ML development is unfathomable.
If you are someone doing that development, you must consider all of your actions with the utmost care.
If you know someone doing that development, you must push them to think about their actions with the utmost care.
Peace.
",You’ll likely be a murderer.,109,youll-likely-be-a-murderer-d98fb1f17214,2018-09-03,2018-09-03 17:02:37,https://towardsdatascience.com/youll-likely-be-a-murderer-d98fb1f17214,False,1392,"Sharing concepts, ideas, and codes.",towardsdatascience.com,towardsdatascience,,Towards Data Science,,towards-data-science,"DATA SCIENCE,MACHINE LEARNING,ARTIFICIAL INTELLIGENCE,BIG DATA,ANALYTICS",TDataScience,Ethics,ethics,Ethics,7787.0,Keegan McNamara,"ex-Research @ Amazon AWS :: Venture @ Contrary Capital :: Math/Physics @ CU Boulder. Armchair philosopher, transhumanist, entrepreneur. keeganm.org",cea17c1a13dd,keemcn,29.0,30.0,20181104
0,,0.0,7f60cf5620c9,2018-01-20,2018-01-20 11:42:59,2018-01-20,2018-01-20 12:20:31,3,False,en,2018-02-08,2018-02-08 13:37:17,11,b4c70b7e7e3d,9.629245283018866,4,0,0,"by Aadhar Sharma, Raamesh Gowri Raghavan, and Sukant Khurana",5,"Neanderthals could not find out off switch on humans, would humans be able to find one on artificial general intelligence (if it is ever invented)?
by Aadhar Sharma, Raamesh Gowri Raghavan, and Sukant Khurana
(Note: This article is rehash of ideas Sukant and Aadhar have presented elsewhere before)
Photo by Markus Spiske on Unsplash
Intelligent machines, no more a mere work of fiction, aid us in efforts of all kinds, from the synthesis of art to automobile manufacturing. They are designed only for expertise in one or a small number of domains. At present specialized intelligence is becoming common but a general intelligence of machines remains a dream (or nightmare).
Artificial General Intelligence (henceforth, AGI; synonymous with ultra-intelligence or super-intelligence) has been characterized in fiction as intelligence that can perform any task par or exceeding human levels. General intelligence doesn’t necessarily imply that it has to be a modeled on humans. It should somehow accomplish the task at hand.
While deciphering the Enigma (machine), Alan Turing was assisted by a team of mathematicians, and Irving John Good was the chief statistician. A decade later, Turing made speculations about AI; he believed early on that we should expect machines to takeover. I.J. Good had been the first to make articulate comments on reproduction of AGI. An excerpt from his 1963 paper, “Speculations Concerning the First Ultraintelligent Machine”, is often quoted, [1]
“Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultra-intelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion,” and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.”
Isaac Asimov, Phillip Dick, Arthur C. Clarke, and others have created great works of science fiction, closely knitting the ideas of an AGI, human interaction, and the traditional melodrama. While being overly exaggerated, these works do illuminate some of the key concerns mentioned by Good. It seems that even the philosophical terrain is very foggy; to call the headway ‘doom’ or reprieve from it, is not clear. To elucidate if this path is just blurry or really murky carries a great importance. It motivates us to seek strategies to understand AGI better, enable its ethical development, and a safe deployment.
Possible concerns:
Let’s assume that we succeed in building an AGI after years of training, and deploy it in the form of a robotic servant supposed to keep its owner happy and smiling. An AGI that wants nothing too big but to keep owners happy might implant electrodes to elicit a Duchenne smile or stimulate the Nucleus Accumbens (pleasure center of the brain) to provide kicks of pure, orgasmic gratification [2]. A Duchenne smile is one way to accomplish the goal with little effort — by stimulating the facial muscles to make it appear smiling. One might argue that the job was done flawlessly, but was it really? This basic theory should be taken seriously; even though the algorithm did accomplish its task, determining how ethical its behavior was shouldn’t require deep introspection. Evidently, great care needs to go in designing, analyzing, and deploying AGI.
Physicist, Max Tegmark believes that AGI is achievable theoretically since intelligence fundamentally is information processing by an arrangement of atoms acting under the laws of physics. “It is theoretically possible”, agrees Bill Gates. It’s estimated and widely accepted that AGI is practically achievable and would arrive anywhere between ten to fifty years. Given the current pace of AI, and assuming that we have a few decades to plan for its arrival strategically, is it enough? There are divided views to this question: Some like Nick Bostrom, believe that it’s about time we invest in advocacy of ethics and philosophy in researching Superintelligence. Another camp believes that we have enough time to ponder; Andrew Ng compared it to worrying about overpopulation on Mars. A third camp believes though an AGI can arrive in a few decades, there’s no need to worry since the off-switch is our assurance of control. We would argue that AI calls for ethical norms, and philosophical work, in addition to technical developments.
The Control Problem: off-switch
“If a machine can think, it might think more intelligently than we do, and then where should we be? Even if we could keep the machines in a subservient position, for instance by turning off the power at strategic moments, we should, as a species, feel greatly humbled. … [T]his new danger is certainly something which can give us anxiety.”
Turing explained the control problem of an AGI in 1951, speaking at BBC radio lecture [5]. Even the idea of off-switch acts as a method of dodging the uncomfortable prospects of a malevolent AI. “Where is the off-switch?”[6], argues Nick; furthermore, there is no such thing, and if there is, why couldn’t the Neanderthals figure it out. We are intelligent adversaries that can anticipate actions and plan around them, but so can an AGI, perhaps much better than us. If we simulate an AGI on a Virtual Machine (to quarantine it) can we be sure that it can’t leverage a bug in the code to its advantage? Ignoring the issue is not a good strategy, a better approach would be to create an AI that even if evades containment stays aligned with human values (or any values engineered into it). Defining values is yet another ethical and technical challenge.
A pioneer of cybernetics, Norbert Weiner, noted that it could become immensely hard to control learning machines as they may develop strategies too complicated to be anticipated by programmers, he wrote:
“Complete subservience and complete intelligence do not go together.”
“The future will be an ever more demanding struggle against the limitations of our intelligence, not a comfortable hammock in which we can lie down to be waited upon by our robot slaves.”
While Norbert’s comment is ominous, the appeals of AGI is too powerful to resist. No matter how in future it gets deployed, any behavior shall be ethically and morally justified. Efforts have been made to identify these obligations, and several paths and strategies have been proposed to approach it safely. Stuart Russel, a professor of Computer Science at UC Berkeley, pitches a three-point action plan to create a human-compatible AI. One, the AI should be altruistic towards humanity and human values; two, initially not knowing what human values are, it should avoid any single-minded pursuit of the objective; three, it should learn values by observing humans. According to him, even if the AGI anticipates the off-switch, it shan’t form malevolent intentions against its owner. He mentions that the machine should plan as follows:
1. Human may switch me off.
2. …but only if I am doing something wrong.
3. I don’t know what “wrong” is, but I should refrain from doing anything in the category of “wrong.”
4. Therefore, I should let her switch me off.
A robot following such an action plan may respect human values and confer to the ethical obligations. However, it can’t be said how technically efficient this AGI would be; such planning may introduce interaction delays that may render it virtually non-functional.
Photo by Fabian Grohs on Unsplash
Value Alignment
The AI alignment problem is central to the construction of a safe and ethical AGI. Providing, identifying, and aligning the AGI with values is the chief concern here. The idea is that an AI aligned to human values would be expected to behave like a human and would, therefore, be accommodated and accepted in the society. However, it’s not clear how one should proceed with quantifying or qualifying human values since the notion is very fuzzy in itself. Human values tend to differ between people, cultures and countries, and with such a broad distribution how one shall select the appropriate values for alignment, is not clear.
A world where AGI is not aligned with human-values is not hard to envision; a simple analogy is of humans and gorillas. These gentle beasts are our second-closest biological relatives; they are immensely muscular and may easily kill a strongman, yet their fate as a species now wholly depends upon us. We outsmart, dominate, poach, and habituate them for our greedy motives. Harambe, a male silverback gorilla at the Cincinnati Zoo, was shot and killed because a human child climbed into his enclosure; he first dragged the child by its leg and later held it close, which some animal behaviorists believe to be a sign of protection. While Harambe’s behavior was a debate among primatologists, his killing had caused mass criticism — raising him to a cultural icon.
How would an AGI respond if a human substitutes for Harambe? It’s crucial to embed human values to aid decision making in such situations, not only keeping in mind the ethical and moral obligations towards humans but also towards AI. AI’s prospects in society won’t just get tarnished but may even vanish if a single such incident happens, therefore it’s of grave seriousness and urgency to align AGI with human values, no matter how hard it is.
Nick Bostrom, is a professor of philosophy at the University of Oxford and the author of the critically acclaimed book ‘Superintelligence: Paths, Dangers, and Strategies’. He wrote ‘Requiem’ [7], a poem in Swedish, describing a medieval era brigadier who overslept for some centuries. Waking up from the slumber finds his men to have deserted the camp. Out of self-righteousness and dedication to his duties, he adjusts his saber and races his horse to the battlefield. While the horse gallops across streams and fields, he hears a roaring thunder, a boom of a fighter jet that sweeps past him, making him aware of the reality. The brigadier realizes that he has become obsolete; his self-esteem crashes down as a quail been shot.

While the poem doesn’t speak about AI, one can figure out the analogy for possible future with AGI. Super-intelligence may lead to “intelligence explosion” -as pointed out by I.J. Good; A system that recursively improves itself and produces more intelligent (sub-) systems that may in-turn do the same. This exponential growth could reach a point of over-population. Even if these systems are aligned with human values would we be able to assimilate their presence as biological entities, or would it perturb the equilibrium of physical and mental stability that’s maintained by homeostasis? If the rate at which this happens is slow enough to be accommodated in society and humanity, the future surely looks good, but if the rate is too great to be matched by our biological and cultural clocks, then dystopia may be at hand. Machines would degrade the quality of humanity and replace us as dominant beings, all a repercussion of shallow human foresight and poor ethical orientations. Bostrom writes,
“Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb […] We have little idea when the detonation will occur, though if we hold the device to our ear we can hear a faint ticking sound.”
AGI concerns humanity; if done terribly wrong it poses a threat to our existence (or all lifeforms). Even though its arrival is debatable, most expect it to take form eventually. Having no clear idea about the time we have at hand, it’s most wise to advance with great caution since poor judgment can and will lead to grim prospects. AI has always drawn inspirations from many other fields, such as neuroscience, linguistics, physics and mathematics. The safest way to go is to build on fundamental models. We are also obliged to put in more resources at the disposal of these fields, to develop cognitive systems based on human cognition, per se. Neuroscientist, Christof Koch writes:
“To constrain what could happen and ensure that humanity retains some modicum of control, we need to better understand the only known form of intelligence. That is, we need to develop a science of intelligence by studying people and their brains to try to deduce what might be the ultimate capabilities and goals of a machine intelligence. […] Will it make a difference to the AI’s action if it feels something, anything and if it, too, can experience the sights and sounds of the universe?”
For humanity, AGI can become the most significant tool yet. It may help us to find mathematical proofs and scientific discoveries, eradicate poverty and terrorism, heal ailments and contain epidemics, enhance the human condition, administer climate change, discover exoplanets or explore corners of the universe. Or it may establish a universal authority. The journey to AGI has many routes, which we take will define our future. The choices are our own and require profound contemplation. Sam Harris speaks, “Now is the good time to make sure that it’s the god we wanna live with”.
— —
About:
Adhar Sharma was a researcher working in Dr. Sukant Khurana’s group, focusing on Ethics of Artificial Intelligence.
Raamesh Gowri Raghavan is collaborating with Dr. Sukant Khurana on various projects, ranging from popular writing of AI, influence of technology on art, and mental health awareness.
Mr. Raamesh Gowri Raghavan is an award winning poet, a well-known advertising professional, historian, and a researcher exploring the interface of science and art. He is also championing a massive anti-depression and suicide prevention effort with Dr. Khurana and Farooq Ali Khan.
You can know more about Raamesh at:
https://sites.google.com/view/raameshgowriraghavan/home and https://www.linkedin.com/in/raameshgowriraghavan/?ppe=1
Dr. Sukant Khurana runs an academic research lab and several tech companies. He is also a known artist, author, and speaker. You can learn more about Sukant at www.brainnart.com or www.dataisnotjustdata.com and if you wish to work on biomedical research, neuroscience, sustainable development, artificial intelligence or data science projects for public good, you can contact him at skgroup.iiserk@gmail.com or by reaching out to him on linkedin https://www.linkedin.com/in/sukant-khurana-755a2343/.
Here are two small documentaries on Sukant and a TEDx video on his citizen science effort.



Sukant Khurana (@Sukant_Khurana) | Twitter
The latest Tweets from Sukant Khurana (@Sukant_Khurana). Founder: https://t.co/WINhSDEuW0 & 3 biotech startups…twitter.com
","Neanderthals could not find out off switch on humans, would humans be able to find one on…",41,neanderthals-could-not-find-out-off-switch-on-humans-would-human-be-able-to-find-one-on-artificial-b4c70b7e7e3d,2018-07-13,2018-07-13 12:18:06,https://medium.com/s/story/neanderthals-could-not-find-out-off-switch-on-humans-would-human-be-able-to-find-one-on-artificial-b4c70b7e7e3d,False,2406,"Sharing concepts, ideas, and codes.",towardsdatascience.com,towardsdatascience,,Towards Data Science,,towards-data-science,"DATA SCIENCE,MACHINE LEARNING,ARTIFICIAL INTELLIGENCE,BIG DATA,ANALYTICS",TDataScience,Ethics,ethics,Ethics,7787.0,Sukant Khurana,"Blockchain, edutech, AI, neuroscience, drug-discovery, design-thinking, sustainable development, art, & literature. There is only one life, use it well.",6d41261644a8,sukantkhurana,433.0,135.0,20181104
0,,0.0,,2018-01-28,2018-01-28 13:44:31,2018-02-10,2018-02-10 15:20:04,1,False,en,2018-02-10,2018-02-10 15:20:04,2,df3509b17d51,2.781132075471698,3,0,0,"The modern world is full of algorithms that drive many complex systems and by doing so, affect lives of millions. Not all of them are…",5,"Why should Machine Learning developers care about transparency?
Photo by Maxime Le Conte des Floris on Unsplash
The modern world is full of algorithms that drive many complex systems and by doing so, affect lives of millions. Not all of them are sophisticated deep neural networks, sometimes it’s just a statistical model or automation process. The common problem is lack of clear communication of how those things work with its developers and often very unaware users. Today, I would like to look at trending approach in the time of hype around AI: end-to-end machine learning models, which are fed with a massive amount of raw data to perform a sophisticated set of tasks of classifications. I will try to consider both the problem they create but also propose ideas for solving that challange.
The first challenging part of end-to-end AI algorithms comes with a restricted co-operation with its developers during the development phase. When algorithm learns entirely by itself, with tons of data using deep learning, model developers could omit one of the most crucial parts of good old machine learning, feature engineering, which till the boom of deep neural networks was probably most important making machines to learn. Giving up this step also creates the danger of polluting the model with biased data, and in consequence indirectly harm people. Detailed examples of this issues are described in “Weapons of Math Destruction” written by Cathy O’Neil.
Applying domain knowledge for construction of better input data for models has two main advantages. First of all, it can enormously improve algorithm performance, especially when we do not have vast amounts of data and from the other. Secondly, it gives meaningful insights about used dataset to model developers. Insights that could be still very useful, even when a model by itself does not meet desired expectations.
After finishing development phase, we need to think what happens when our black box solution finally ships to the production environment, or better to say: human environment. It is even more important to pay attention to safety and reliability, in high-risk industries like a healthcare, transportation or finances decisions and actions made by the AI could affect real people’s health and life. The moment when we delegate a complex task and expect only final results is when we lose insights of what’s happening. The typical example could be an autopilot in a plane, drone or autonomous car which suddenly has a crash and human operators have no clue what was the reason why such system performed specific actions.
In fact, constant interaction and transparency in front of users may prevent a significant percentage of such accidents from happening. A reduction or decomposition of end-to-end models could save many errors when AI systems are used outside of a laboratory or developer environment. Lack of supervision could be required by laws, what’s been already happening in the European Union according to the General Data Protection Regulation. Applications of this regulation will affect not only European based companies but also everybody who wants to use the data of European citizens.
What does the GDPR mean for the AI industry? In specific cases, authors of a model that has been interacting with users need to explain how and based on which rules it has made specific decisions. It may be an answer for a credit request, an automatically calculated insurance cost or adds content presented to a user. As a creator of such systems, we might be asked to prove it’s not biased in a harmful way or doesn’t break any rules of users data protection. Needless to say, developers, project managers, and business owners will need to change their methods for development, in order to adapt.
It all leads to the biggest challenge and opportunity. If we make interpretability of AI systems a universal norm, we could avoid losing trust for this beneficial field of technology or even to the technology itself. I am convinced that by a proper User Experience design, responsible workflows of Machine Learning project and education, we could minimize the risk of AI to arouse fears and benefit from its potential.
",Why should Machine Learning developers care about transparency?,18,why-should-machine-learning-developers-care-about-transparency-df3509b17d51,2018-05-04,2018-05-04 18:59:09,https://medium.com/s/story/why-should-machine-learning-developers-care-about-transparency-df3509b17d51,False,684,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Łukasz Mokrzycki,Software Developer in ML/Data Science @ Estimote Inc (all presented opinions are my own),87955a73a1d4,l.mokrzycki,16.0,8.0,20181104
0,,0.0,,2018-06-15,2018-06-15 15:11:47,2018-05-31,2018-05-31 16:22:37,2,False,en,2018-06-17,2018-06-17 09:59:22,17,1f7541666ca3,4.421069182389938,0,0,0,This is Article 2 of the Series on Ethics in Artificial Intelligence.,3,"The fears of the AI era. Where Ethics is Needed?

This is Article 2 of the Series on Ethics in Artificial Intelligence.
In this article our goal is to briefly cover areas in which AI has achieved amazing progress, discuss the nature of human fears in relation to the changes brought by artificial intelligence and prepare the foundation for deeper discussions by breaking down the problem into smaller pieces.
Fear as a survival mechanism
What is fear? From the neuroscience perspective, it is a survival mechanism. It has performed a vital function throughout times where our “fight or flight” response helped us survive in dangerous outdoor conditions. Today, our survival has shifted from threats from natural predators to health and financial security. There are a lot of stimulations that push our fear buttons nowadays, but mainly without the immediate threat to survival. Fear is irrational. To constructively talk about the ethics of emerging technologies such as artificial intelligence, we need to get into the rational space.
AI has made a huge leap
AI has made a huge leap during the last decade. This leap became possible with technological improvements in families of methods in machine learning for text, voice and image recognition; language and video reconstruction; as well as computer-aided design. How far are we from the world, where there is no more space for humanity?

Fueled by fears and images created in the sci-fi movies, people start to fantasize about robots and the overwhelming crisis for society caused by super-intelligent machines. Let’s bring some clarity. The level of AI can be subjectively broken down into three main categories: Artificial Narrow Intelligence (ANI), Artificial General Intelligence (AGI) and Artificial Super Intelligence (ASI). ANI is the only form of Artificial Intelligence that humanity has achieved so far with respect to a measurable number of tasks. However, we can already see applications where ANI solutions in medicine, law, architecture, design, music are surpassing human accuracy and productivity. Numerous successes have been reported recently, but, because of the negativity bias, the huge media coverage is focused on failures.
The fields of computer vision, natural language processing are still at the stage of narrow AI, even if their advances seem fascinating. Narrow AI is good at performing a limited task, such as playing chess or go games, making sales predictions, calling and booking appointments, matching people in social networks or performing weather forecasts. Basically, narrow AI works within a very limited context, and can’t take decisions on tasks beyond the goals for which it was initially created. At the same time, decisions that are made by AI could be complex due to the nature of the context. How do we evaluate different decisions and contexts from the ethical perspective? We will discuss this.
AGI is a huge challenge already
According to definitions (Bostrom, 2003), when AI becomes much smarter than the best human brains in almost every field, including scientific creativity, general wisdom and social skills, we’ve achieved super-intelligence or ASI. But the more we dive into an exploration of what is possible and what is not, the more we realize that even animal intelligence is a big challenge to model and achieve. Humans might not be able to process data as fast as computers, but what remains a scientific puzzle is to understand how we produce new ideas, imagine or act altruistically. Even artificial general intelligence, also known as human-level AI is not a completely defined concept as it can be evaluated only on the basis of pre-defined functions or goals. We don’t have a full list of things that humans can potentially do. More we learn about the capacities of the human brain, more we come to appreciate the elegance of its intelligent design.
Ensuring inclusivity of humans in the era of AI
If the reality is that we can actually control decisions made by AI, why does the fear still exist? In most cases, individual and societal coping skills are about awareness of reality, of what is possible and what is not. As a result of the proliferation of AI, we are simply afraid to be excluded from the crucial aspects of decision making at a personal or group level. We want to understand what to expect from a society where humans and AI work together as well as from each single AI-powered solution.
To address the issue, we need to design inclusive systems and “explain” these designs. Such systems should embrace participatory behaviors in ways that humans can control the boundaries of outcomes. In the information era, humans frequently live with a vague sense of danger that with time becomes their normal mental and emotional state. Such a state comes from ever increasing and evolving dynamics in the environment. Skills to respond to the fast-paced change are required. Indeed, facing uncertainty without information is hard. What should we do to fill the skill gaps? What kind of knowledge do we need to build to govern existing and emerging AI in different sectors?
Personal and societal dimensions of AI ethics
To commence cutting the Gordian knot of polemics around the opportunities and risks of an AI era, we outline two main themes. Each of them will be addressed separately in our future posts.
Societal transformation in the era of artificial intelligence
What lessons can we learn from the history of previous technology revolutions?
What is the future of work and which skills should we acquire?
How should we regulate the proliferation of AI to positively impact privacy, security and wealth distribution?
2. Governance and ethics of AI decisions
How does artificial intelligence make decisions?
What are the examples of data-driven biases and how can we avoid them?
How do we define quantitative ethics and ethical frameworks for AI?
Above we talked about fears, described the three types of AI and have given several examples of extremely successful solutions as well as failures. We also outlined two important themes to address upcoming challenges in the new AI era. We believe that we can make use of the mental energy consumed by fears if we redirect this energy into educating societies and individuals. Stay tuned.
P.S.
The beautiful illustration of dinosaur flowers on top of this article was made by Chris Rodley using the technique called “style transfer”, created at the University of Tübingen in Germany.
Originally published at pocketconfidant.com on May 31, 2018.
",The fears of the AI era. Where Ethics is Needed?,0,the-fears-of-the-ai-era-where-ethics-is-needed-1f7541666ca3,2018-06-17,2018-06-17 09:59:23,https://medium.com/s/story/the-fears-of-the-ai-era-where-ethics-is-needed-1f7541666ca3,False,1070,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Nikita Lukianets,"Founder, CTO @PocketConfidant AI. I work in the R&D with interests ultimately related to the fields of Computational Neuroscience and Artificial Intelligence.",deaa66463d16,lukianets,240.0,236.0,20181104
0,,0.0,1de9a8b799ad,2018-04-06,2018-04-06 18:46:24,2018-04-09,2018-04-09 15:13:31,1,False,en,2018-04-09,2018-04-09 15:13:31,4,6d3fab16b822,0.7396226415094341,1,0,0,"Deena Elul of the Daily Pennsylvanian details a new class on the ethical dimensions of programming, artificial intelligence, and data…",5,"Daily Pennsylvanian: New Course on Computer Ethics
Ani Nenkova and Michael Kearns
Deena Elul of the Daily Pennsylvanian details a new class on the ethical dimensions of programming, artificial intelligence, and data protection, developed by Computer and Information Science’s Ani Nenkova and Michael Kearns.
As ethical quandaries posed by new technologies and oversights by major tech companies come to the forefront of public discourse, several universities have begun offering courses on the ethics of computer science and artificial intelligence. At Penn, computer and information science professors Ani Nenkova and Michael Kearns are hoping to continue this trend.
Kearns and Nenkova have been developing a course, tentatively titled “Science of Data Ethics,” that they hope to offer to undergraduate students in the spring of the 2018–2019 academic year. The class will focus on ethical issues such as privacy and discrimination in machine learning.
Continue reading at the Daily Pennsylvanian.
",Daily Pennsylvanian: New Course on Computer Ethics,1,daily-pennsylvanian-new-course-on-computer-ethics-6d3fab16b822,2018-04-10,2018-04-10 15:23:57,https://medium.com/s/story/daily-pennsylvanian-new-course-on-computer-ethics-6d3fab16b822,False,143,University of Pennsylvania’s School of Engineering and Applied Science,,,,Penn Engineering,elerner@upenn.edu,penn-engineering,,PennEngineers,Ethics,ethics,Ethics,7787.0,Penn Engineering,,af9f8605d39a,PennEngineering,2000.0,3.0,20181104
0,,0.0,,2017-12-17,2017-12-17 06:25:48,2017-12-17,2017-12-17 11:34:16,7,False,en,2017-12-18,2017-12-18 02:11:48,6,b62fcfea3197,7.55943396226415,1,0,0,The first thing that pops into someone’s head when they are asked about artificial intelligence is probably a scene from a sci-fi movie or…,5,"Probably not the future you want.
Artificial Intelligence, Ethics and Mankind in the Future
The first thing that pops into someone’s head when they are asked about artificial intelligence is probably a scene from a sci-fi movie or novel, possibly one with apocalyptic vision of mankind vs machines. The film industry does little to help that case, with countless doomsday prophecies about mankind vs machines. A more reasonable image would be a world in which robots are commonplace and virtual assistants do most of the work we associate with day-to-day life, leaving mankind to work very little, leaving time for more artistic pursuits.
Just by contemplating such a future, one of the first few thoughts that strike is how do we ensure that AI only performs the functions we want it to? Before we even get to the actual evolved AI’s of the future, let’s talk about the problems that the symbolically caveman version of AI is currently causing in the world. Even though most of the problems get blown up by media, there are a lot of things to be concerned about even after removing the sensationalism.
When Microsoft first released their Tay Chatbot, they were probably expecting a reaction similar to what the XiaoIce, a similar product from Microsoft, got in China. They designed her to learn from conversations and topics, and respond to new tweets from users.

According to Microsoft, the more you chat with Tay, the smarter it gets, learning to engage people through “casual and playful conversation."" But what happened in less than 24 hours was “flaming garbage in, flaming garbage out”. Using the repeat-after-me feature built into the bot, fellow Twitter users turned a program designed to imitate a 19 year old girl to a Trump-loving anti-semitic sex robot. Although almost all the offensive tweets were a result of users making it retweet their sentiments, a functionality that Microsoft had implemented, Tay did learn to give some weird responses on sensitive topics like the US General Election.

On very similar grounds, there has been quite an uproar about machine learning software being used in the judicial system. Predpol, a crime prediction software used in 9 states in the US and even the UK, particularly likely to falsely flag black defendants as future criminals, wrongly labeling them at almost twice the rate as white defendants. Also, it proved remarkably unreliable in forecasting violent crime: Only 20 percent of the people predicted to commit violent crimes actually went on to do so.
Self-driving cars are here to stay. They are safer, more efficient and more reliable than human driven cars. Google’s Waymo has driven more than 3 million miles without any minor accident, and Tesla deployed the Auto-Pilot feature two years ago in their cars. Even though AI driven cars are safer, they still might be involved in accidents with no fault of their own.

The dilemma has been well described in this article .When such a situation does happen, what should the car be designed to do? Should the user be allowed to choose to prioritize saving his own life in the settings, or should the manufacture hard code saving the maximum number of lives in the software? Such questions will only be answered through discussions, and companies will probably try different approaches to it as well.
Even though Tay was simply Microsoft showing the world its advances in machine learning, it very clearly showed what might happen to AI if left to the public. When we talk about machine learning being used in the judicial system, we have to take into account the most inherent crippling limitation in today’s AI. The fact that artificial intelligence today, is not at all “intelligent”. It learns solely through data that we provide. In the case of Tay, there was a lack of inbuilt “instinct” that tells it what to avoid. If you assume AI to be unbiased without taking great measures for it, you run the risk of being completely blindsided. Although the Tay fiasco could be attributed to Microsoft being lax about security, and not regulating what users could tweet through the bot, there have been numerous examples recently of the implications of having AI that learns from what could be possibly our worst impulses as a society, including gender discrimination in our workplace to racial biases.
We often like to think of manking being one of God’s-near-perfect-creation, which is also why we actually conceptualize our concept of God as one that looks human. But in reality, we are far from being the most perfect creature on the planet. We are fragile and weak in comparison to evolution’s other creations. We might argue that the full potential of the brain hasn’t been realized yet and that AI can never capture what it means to be human in spirit, but the fact remains that we are extremely inefficient, most noticeably in how we communicate, which is mainly through speech. Even though we can think of multiple things at a time, most people would be hard-pressed to output eve a fraction of all that, either by speech or typing. If we compare that to a typical computer, we might have an information throughput of a few hundred kilobytes per second, when compared to the throughput of a capable computer of gigabytes per second, a factor of about 10000 at the present age. The problem is further aggravated by two factors, one that the increment in AI takes the form of a double exponential curve. One exponential arises from the increase in computing power, the second exponential comes from the rapidly increasing amount of researchers entering the field of AI.
How Google’s DeepMind discovered walking on it’s own.
It is inevitable, that someday we will finally manage to code up an algorithim which would be capable of self-learning and actual intelligence. When we actually put the algorithim in silicon, we are creating something similar to all living organisms in thought, which now has the processing power of possibly a large section of humanity. We have on one hand, a system capable of learning faster than any human, possibly a million times faster, and we have a huge ever increasing pool of data, i.e. the internet, that we have been collecting since decades. We would possibly be creating a “God”, something capable of extremely rapid thought, much beyond human capability. If you imagine such a device operating for even a week, it will probably surpass the cumulative knowledge of all of mankind. What happens when it does not operate exactly how we had designed it to? (Similar to Tay/PredPool)
In the upcoming world, data will be the new currency. As Jacky Ma says in his interview, data will be the new soil and water in the future. The biggest challenge we are facing currently is making sure that companies are not doing something illegal. Knowing such a thing is relatively easy when two tech giants are involved, since they have a major incentive in protecting their intellectual property. But when the general public is involved, in the current state of things, it is impossible to know if any company is using data in a way it’s not supposed to. As an example, Uber said it tracked usage of the service during Friday and Saturday nights that were specific to those who “on occasion found love that you might immediately regret upon waking up the morning after.” The blog even gave these rides a name — a ‘Ride of Glory’ — and the people who take such rides ‘RoGers’. Similarly, riders going to more expensive locations are being charged more than other riders. This is just an small example of how big data, often synonymous with machine learning and AI, can affect our lives.
All the current problems and fears that people have related to AI can be tied to one thing. The fact that there is no authority at the top of pyramid, making sure that AI is safe. That everything that’s being done behind the curtain is permissible by law and the ethics code. When you think of things that we use everyday, such as food and transport, you have numerous authorities governing over these, and making sure that rules and regulations are followed, such as the FDA is for food, or the FAA is for aviation. Some of the most influential people in the tech world, such as Bill Gates and Elon Musk have been asking for the creation of a government body, which could regulate the field, keeping everyone in check. Elon further clarifies that instead of drafting and passing new legislation from the start, this body should necessarily need to first understand the complexity of the field by studying it for quite some time, before being able to competently draft legislation on AI and big data. Furthermore, we can’t implement a governing body for AI as we do for other fields, because by the time a mistake in AI is made, it will already be too late. Which is why, we need to have regulations before designing it, i.e. proactive legislation vs a reactive model. The latter will be completely useless in AI.

Finally, a few words about the future that AI will make possible for mankind. I do believe mankind will always have the dominant role to play in the future. I believe at some point, we will use robotics to significantly alter the way we interact with the world, mainly a great increase in the speed with which we communicate and consume data. A world of interconnected devices, where we can act at the speed of thought itself, it would be the logical next step that we need, to fully connect with machines and become something more. Biologically, the human body isn’t meant to last beyond 100 years on average. Apart from the fact that the chance of cancer and other illnesses increases as you age, but even neurons have an lifespan of 120 years. Robotics and artificial intelligence can and will significantly improve human life, within the next two decades. It might begin with changing how we consume and use information, giving rise to a new generation of digital assistants, catering to almost all of our physical and quite possibly “human” needs, to bio-mechanics, to perhaps transcending the limitations of our physical bodies in the far future.
Close. But not as cool.
","Artificial Intelligence, Ethics and Mankind in the Future",3,artificial-intelligence-ethics-and-mankind-in-the-future-b62fcfea3197,2017-12-18,2017-12-18 02:11:49,https://medium.com/s/story/artificial-intelligence-ethics-and-mankind-in-the-future-b62fcfea3197,False,1725,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Abhay Sarda,,c0fecb1e2642,asarda_3513,5.0,75.0,20181104
0,,0.0,e03b12db4474,2018-06-04,2018-06-04 17:36:43,2018-06-11,2018-06-11 15:50:08,4,False,en,2018-10-10,2018-10-10 11:43:43,8,e5fda034163d,2.9566037735849067,8,0,0,"I am speaking tomorrow at CogX on the Ethics Stage about The Urgency For More Human Education in an AI Future, Hosted by Big Change.",5,"Why we built an interactive tool to help you ace AI basics

I am speaking tomorrow at CogX on the Ethics Stage about The Urgency For More Human Education in an AI Future, Hosted by Big Change.
Come down!
Over the last 12 months, I have been writing a 15,000 words postgraduate thesis on AI, racial bias and gender bias which has played a big role in shaping the research focus at Comuzi.

One of the challenges, we have encountered exploring this area of concern (AI ethics & AI’s impact on society) is the role that the media and movies has played in the mainstream amplification of potential and current problems associated with AI.
This had led to AI being one of the trendiest, least understood and most debated technological breakthroughs, I think in history (shade to Black Mirror).
A compilation of scare mongering articles about AI that I put together.
After seeing the efforts by Deepmind and RSA to facilitate public engagement on the impacts of AI, Akil, Sekyeong and I spent a week to create AI Cheatsheet — a tool for educating the community outside the field of AI to find, understand and share simple definitions about the technology.
We were heavily inspired by Do You Speak Human? by Ikea’s Space10 and Sideways Dictionary by Alphabet’s Jigsaw & Washington Post.
I curated twenty words that are associated with AI, and then I gave Akil the challenging task to develop definitions for each word within a 140 character limit (which he did brilliantly).
AI CHEATSHEET YOO! TEST IT OUT!
We also created a section on the website explaining what can AI do in 2018 as ‘AI’ or sophisticated algorithms have been powering our favourite products and services for ages now (AI is always framed as this faraway future thing).
AI has been here for a hot minute.
We also created a survey with questions focused on what impact AI shouldn’t have on society.
Impact
We had a fabulous launch on Product hunt in the first week of May (Much love to Abadesi for hunting).
At the time of writing, AI Cheatsheet has been used approximately by 4,471 users globally and collectively the definitions created have received 32,201 page views. We got insights from 229 people on our survey (The goal is 1,000).
This is amazing for a project that we created in a couple of days. We plan to promote it more as we would love to get a more broader and inclusive view in regards to race, gender and age of what relationship we want as AI becomes more sophisticated and a staple in everyday life.
I also plan to introduce more features & more ideas as I’ve been sent loads of suggestions but to be honest, I don’t want to over complicate everything.
Reflections
Reflecting on this project and its moderate success, I would like to construct the argument that technology companies can employ the use of educational tools to engage the community outside those who work in the technology field in order to educate and address social and ethical concerns that the general public may have about emerging technologies through human-centred approaches.
Further reflecting, we want to improve the survey questions as for a participant to answer these questions, looking back it could be suggested that we may have assumed that whoever conducted the survey may have a high level knowledge about AI — notwithstanding the definitions provided exactly.
So I would like to better aid survey participants by implementing formats such as scenarios or case studies to help participants to think through their responses.
But here it is, I would love to talk with others about what I written. If you are interested to talk more — alex@comuzi.xyz.
",Why we built an interactive tool to help you ace AI basics,29,why-we-built-an-interactive-tool-to-help-you-ace-ai-basics-e5fda034163d,2018-10-10,2018-10-10 11:43:43,https://medium.com/s/story/why-we-built-an-interactive-tool-to-help-you-ace-ai-basics-e5fda034163d,False,598,"Thoughts and reflections by the Comuzi team on emerging technology, creativity and business futures.",,,,The Comuzi Journal,hello@comuzi.xyz,thoughts-and-reflections,"DESIGN,CREATIVITY,TECHNOLOGY,INNOVATION",comuzi_lab,Ethics,ethics,Ethics,7787.0,Alex Fefegha,"have fun, get paid. @comuzi_lab. tutor @ual. side projects: @_creativehustle.",4923667cff9a,fefegha,502.0,22.0,20181104
0,,0.0,,2018-05-29,2018-05-29 22:17:13,2018-05-17,2018-05-17 03:59:04,1,False,en,2018-05-29,2018-05-29 22:42:42,18,cf9715c0e80c,5.252830188679246,0,0,0,AI is a buzzword in every business. Our hopes are that it will benefit society beyond our imagination and we fear entrenching prejudices,5,"Our Ethical and Human Future with Artificial Intelligence (AI)

AI is a buzzword in every business but as a society our general understanding of how it will be applied is low. Machine learning and programs that use vast amounts of information to create usable data fall under the general term artificial intelligence or AI.
Our hopes are that it will benefit society beyond our imaginations, and our current fears predominantly lie with programmers passing individual bias to the systems, thereby entrenching existing prejudices into our future technology.
By the year 2020, AI technologies will have created an insights-driven market of $1.2 trillion USD and AI researchers are commanding huge salaries of well over a $1 Million USD showing the value and desire for new knowledge.
In the past decade, we have seen a worldwide AI arms race for patents and IP amongst leading tech companies. A report in 2017 from global management company McKinsey reported that ‘tech giants including Baidu and Google spent between $20B to $30B on AI in 2016, with 90% of this spent on R&D and deployment, and 10% on AI acquisitions’. U.S.-based companies ‘absorbed 66% of all AI investments in 2016’. China was second with 17% and growing fast.
Governments are urgently allocating funds across the world — including the Australian Federal government, which announced in last week’s Federal budget that they plan to spend $29.9 million to grow Australia’s capabilities in AI. This will support and plan for future investments “that improve our expertise and maintain our competitiveness in these technologies” through a “technology roadmap”. They are not displaying media-hyped fear of robots stealing our jobs, but proactive plans to ensure Australia doesn’t fall behind any other developing countries.
To manage this they are setting up a new national Ethics Framework and Standards Framework to help guide “the responsible development of these technologies”.
In April this year, the EU announced their “Charter on AI ethics”, stating Europe’s secret weapon in the race against the U.S. and China on artificial intelligence is ethics. Whilst many European countries are creating their own AI strategy, EU officials insist that similar rules must be agreed across the continent to boost consumer trust in European AI applications and enable them to catch up with the competition.
Machine learning can propagate discrimination by automating the same biases they are supposed to eliminate. Elon Musk has voiced his concern over AI’s apocalyptic potential, but the problem we face now is how we stop the inequalities of our society being amplified and affecting the most vulnerable people in society. Cathy O’Neil, author of Weapons of Math Destruction says “ If a poor student can’t get a loan because a lending model deems him too risky (by virtue of his zip code), he’s then cut off from the kind of education that could pull him out of poverty, and a vicious spiral ensues. Models are propping up the lucky and punishing the downtrodden, creating a “toxic cocktail for democracy.” Welcome to the dark side of Big Data.
We have already seen examples of programs showing the darker side of the technology. Risk assessment software, that was used by Broward County in the US, was subject to a damning report published by ProPublica in 2016. It showed the software made racially prejudiced decisions on the likelihood of individuals to re-offend again and was “particularly likely to falsely flag black defendants as future criminals, wrongly labeling them this way at almost twice the rate of white defendants”
In 2016, Microsoft’s chatbot ‘Tay’ was launched to interact with web users to become smarter as more people interacted with it on Twitter. However, it learned fast from a malicious subset of people and began sending out offensive and hurtful tweets that were deeply embarrassing for the company. “We will remain steadfast in our efforts to learn from this and other experiences as we work toward contributing to an Internet that represents the best, not the worst, of humanity,” wrote Peter Lee, Microsoft’s vice president of research.
Last Tuesday Google introduced their Lifelike AI experience with Google Duplex — an AI-powered virtual assistant. CEO Sundar Pichai demonstrated how it conducted two effortlessly natural conversations, one with a hairdresser and another with a restaurant receptionist, mimicked the hesitations and affirmations of human speech and made the reservations required without them ever knowing it was a program. Seeing and hearing this concept in action ‘floored’ and ‘unsettled’ the audience at Google’s annual developers’ conference and raised questions about the need for transparency when we interact with a machine. Belatedly Google responded to the ethical outcry by saying it was “designing this feature with disclosure built-in, and [will] make sure the system is appropriately identified. What we showed at I/O was an early technology demo.” The technical advances are amazing but the big tech companies can’t take people’s trust for granted, as demonstrated so effectively by Facebook and the Cambridge Analytica scandal.
Human ethics in these systems need to be established more urgently than ever before and Google should be at the forefront of this conversation. A few days ago, however, it was announced that Google employees had resigned in protest at Google signing a military contract with the Pentagon which would allow for drones to better analyse its targets in warfare.
In their bid for control of the AI space, are big companies moving too fast for the ethics committees and people’s councils to ensure that the next set of humanitarian scandals are not driven by AI?
Anxiety over AI continues but the industry moves steadily forward. Self- driving cars and trucks are here and will continue to be adopted, but whilst they will be far less flawed than people — who do we blame when someone gets killed? 
 LG concept robots deliver goods, guide shoppers & check-in guests; machines can read X-Rays and algorithms will respond to inquiries from customers.
Technology is driving the progress of AI — increased processor speed, algorithm efficiency, data availability and cloud technology. However, as AI is increasingly applied to business opportunities and problems the resulting investment will only lead to increased algorithmic efficiency and greater potential. Currently, we apply AI to existing systems & processes to make them more efficient. Progress will truly happen when new products and business models are developed that couldn’t exist without AI. There are many examples of both technical and non-technical roles in the future but critically most of them will rely on your ability to be human.
Elizabethans were terrified that new machinery would put them out of work, but new jobs were created with new innovation. Air traffic controllers, engine mechanics or pilots would have been inconceivable a 100 years ago and as the new opportunities are created a report from MIT believes that AI will easily create as many jobs as it will eliminate. The possibilities are endless and an “Empathy trainer” for AI devices or “Ethics Compliance Officer for AI” might well be top of the list.
Elon Musk is building a new economy in Spaceflight technologies, companies such as Airbnb and Uber are leading the way for the explosion of the shared economy and since 2000 Nanotechnology jobs for medical science has risen to provide work for more than a million Americans.
The new world can blossom with human empathy and an abundance of fascinating new jobs. Humanity is central to the process. If technology becomes easy to use and we need less complicated systems, humans can then draw on their creativity and a unique understanding of the human mind. We can take away mundane and repetitive tasks and use the technology to magnify human capabilities. As we develop a code of ethics for AI here in Australia and across the world, we must unite globally to police these processes and ensure that those predictions of an apocalyptic future never come to pass.
If you enjoyed this article — check out our article on Employee Data
Originally published at mwah.live on May 17, 2018.
",Our Ethical and Human Future with Artificial Intelligence (AI),0,our-ethical-and-human-future-with-artificial-intelligence-ai-cf9715c0e80c,2018-05-29,2018-05-29 22:42:44,https://medium.com/s/story/our-ethical-and-human-future-with-artificial-intelligence-ai-cf9715c0e80c,False,1339,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Kerri Thomas,,2a038090eda7,kerriclay,0.0,1.0,20181104
0,,0.0,88ddd46f201e,2017-11-04,2017-11-04 03:31:36,2017-11-09,2017-11-09 09:55:18,1,False,en,2017-11-13,2017-11-13 16:02:34,5,b90933db514e,4.411320754716981,0,0,0,A debate for or against an optimistic view on the future of technology,5,"Notes from #Web Summit: The future of technology: Will everybody benefit?
A debate for or against an optimistic view on the future of technology

Details
Date: November 8, 2017
Time: 14:25
Conference stream: Centre Stage
Speakers
Pat Gelsinger, CEO, VMWare, Twitter: @PGelsinger
Ross Mason, Founder, MuleSoft, Twitter: @rossmason
Jeremy Wilks, Presenter, Euronews, Twitter: @WilksJeremy
Web Summit Summary
Deeply embedded in the ethos of the tech industry is the belief that technology should be available and accessible to all, yet many argue that the benefits of tech innovation have been confined to a relatively small segment of the global population. Ultimately, the audience will decide who wins this debate.
Main Theme
For or against, whether the technology of the future will benefit all 7.5 billion humans living on Earth .
The topic of debate is whether the future of technology will indeed benefit everyone on the planet. Has technology benefitted all historically and what is the potential for this in the future? Ultimately, the debaters deviated from this core line of debate revolving all humans to unpack the implications for future technologies to democratize their value and who should be accountable for that democratization.
The Key Quotes
“We don’t want to optimistic we want to be deterministic.”
- Ross Mason
“Technologist need to be participating in this discussion — every great framework has two sides to a discussion.”
- Pat Gelsinger
Key Points
Pro Arguments from Pat Gelsinger, CEO of VMWare:
Technology touches every human on a planet, of 7.5 billion on Earth and half are connected and expected 80% by 2027.
Technology is how we fix our problems today, four primary areas of technology: Mobile (accessibility), Cloud (scale), AI (intelligence), IoT (digital/physical)
Technology aids in four core areas of human improvement: Health Care (improving diagnoses, 3D printing & robotics for bionic limbs, eradicating long term diseases, increasing life expectancy); the Environment (still in early stages of improvement but innovation in transportation, IoT, smart cities, & ability to track health of the planet shows progress); Extremism of Politics & Religion (technology is improving jobs and hence opportunity so people do not have to resort to extreme views); & Poverty (2 billion people across the planet used to live in poverty and that number has reduced to 700 million people)
Gelsinger encourages technologists to focus on opportunities to deliver for every human on the planet for a better future
Against Arguments from Ross Mason, Founder of MuleSoft:
The foundation of Mason’s argument revolved around the specific phrasing of the debate topic: will everyone (7.5 billion people on Earth) benefit from the technology. They will not and they do not now.
We haven’t scaled our technological advances properly to reach everyone on the planet, e.g. 1 in 10 people don’t have access to drinking water or basic healthcare.
Mason’s skepticism doesn’t lie in the creation of great technology but making it actually accessible across the world
Mason covered recent issues with mass misinformation as a potential for technology to not benefit everyone in a specific way, e.g. Russian influence in the last US presidential election. Specifically, the negligence on the part of Facebook, as the technologist, not thinking about the implications of the platform they had built.
Mason referenced the opening remarks to Web Summit from Stephen Hawking (read our summary here) about the concerns about building out robotics and artificial intelligence.
If advances in robotics reduces job opportunities, how we will foresee handling this issue during good economic times and recessions? This is unanswered.
Need to take pre-caution so we are not so optimistic about the potential of technology we ignore its implications.
Can’t rely on the policymakers to make rules — they don’t have the expertise in the space. It’s up to us in this room (the technologists/attendees of Web Summit).
When look to past/present/future of technology, err on the side of caution. We will not be able to course-correct in the future, we need to be cognizant of the risks now.
Rebuttals:
Pro: There has been alarmist statements made about a lot of new technology, e.g. air flight, telephone, etc. Technology is disruptive, but it is fundamentally neutral. Humans have the power to use technology for good or evil. Based on the data, people are living less in poverty than in the past and that is an improvement. Science fiction always frightens people. Digital divide is being reduced again and again based on the data.
Against: Looking at Robotics/AI advances that will become a reality in the next 10 years and we need to take a step back and take a look at what we’re doing.
Pro: Digital divide is being reduced again and again based on the data. The role of technologists need to increase to being involved in the regulation of technology as never before as the pace is picking up. This is our opportunity, to ensure people aren’t having sensationalist reactions to scientific advances.
Against: Technology is neutral, it can be used for good or evil but the key word in Gelsinger’s argument is the basis of pure optimism. We don’t fully understand these new technologies and we need to enlist the creators to hit the brakes and understand the implications of releasing their technology into the world.
Final Words:
Pro: It is our job to shape inherently neutral technologies to benefit the world.
Against: It is our job to shape the future, but we need to do this in a sustainable way to think carefully about the consequences of what we work on and what we invest in.
The audience sided with Mason that technology of the future will not benefit everyone.
Reflections
This was an excellent debate and a highlight of Day Two at Web Summit 2017. Ultimately, Gelsinger and Mason came to the same conclusion that technologists are responsible for ensuring technologies created are used for good. However, they both saw that conclusion from different angles. As builders of new technologies, we need not only be at the table of regulation discussions but we need to be the people starting the discussion about what kind of regulation is needed.
The balance between optimism for the potential of technology to be equitably distributed is top of mind for me as a product manager. I hold myself to the standards of thoughtful and ethical product building.
These notes are brought to you by TWG: software makers to the world’s innovators.
Want to know what sessions we’ll be at next? Our chatbot pal Web Summelier can hook you up. Want to receive a summary of all our notes when Web Summit’s over? Sign up here.
Psst! We’ve published many more insights on technology, design, and all things software on our blog, The Almanac.
",Notes from #Web Summit: The future of technology: Will everybody benefit?,0,notes-from-web-summit-the-future-of-technology-will-everybody-benefit-b90933db514e,2018-03-02,2018-03-02 08:15:39,https://medium.com/s/story/notes-from-web-summit-the-future-of-technology-will-everybody-benefit-b90933db514e,False,1116,"Notes from key sessions at Web Summit 2017. Brought to you by @twg, software makers to the world’s innovators.",,websummelier,,Web Summelier,hello@twg.io,web-summelier,"WEBSUMMIT,SOFTWARE,ENGINEERING,DESIGN,SALES",twg,Ethics,ethics,Ethics,7787.0,Stacey Feero @ Web Summit,Senior Product Manager @twg Builder of Products. Lover of Films. Listener of Podcasts.,ba7ca7d5bd3f,sfeero,4.0,3.0,20181104
0,,0.0,,2018-06-01,2018-06-01 20:34:47,2018-06-02,2018-06-02 03:31:01,4,False,en,2018-06-02,2018-06-02 05:33:36,4,2dd093caa0ff,4.190566037735849,6,0,0,"With industries revolutionising, does governance need to revolutionise too?",5,"Policy and AI —an Unventured Necessity
Recess — the TV show 90s kids look upto and exclaim, “Man, they don’t make these shows anymore.” Apart from teaching me about friendship, values, heterogeneity, dictatorship, monarchy, economics, currencies, democracy etc. Recess was also my first touch with an artificially intelligent system.

In this particular episode, Principal Prickly installs in the school, SAL 3000 — an intelligent computer that is an aid to the school authorities [ I later realized the episode is a derivative of 2001 : A Space Odyssey ]. SAL is supposed to automate everything — the school bell, the water coolers, all the doors of the school. By half time though, through intelligent manipulations, SAL has taken over the school, dismissed all teachers, locked in all students and is mentally torturing them. Eventually, Gretchen and gang are be able to shut down SAL for good and normalcy returns.
Science fictions like these, which are usually dystopian are influencing how we’re “absorbing” the AI revolution that’s here to stay. The world has been broadly divided, by a grey line, into two halves — ones that embrace technology and ones that fear it and just want to shut it down.
The embracers are the “techies” — the early adopters, the data scientists , the professors, the entrepreneurs. They understand how weak and use-case specific AI is currently. They understand how far we are from the dystopian reality that Black Mirror talks about.
The fearful lot are the politicians, the policy makers, the journalists and people only watching Black Mirror. They are the ones who believe Sophia, the humanoid AI robot, is intelligent. They want to ban self-driving cars in India because it will take away jobs.
And this is the trend that has been there for every industrial revolution. People thought the advent of the printing press will put so many manuscript writers out of jobs, but it only changed the nature of the jobs and increased the number. India followed a protectionist policy until the LPG reforms of 1991. It is only when India realized that it is lagging behind the rest of the globalized world did these reforms come to light. Why do policy makers follow this cycle of denial followed by passive acceptance? Zen and The Art of Motorcycle Maintenance, probably a great book on philosophy, highlights this beautifully. Robert M. Pirsig says -
It’s not the motorcycle maintenance, not the faucet. It’s all of the technology they can’t take…. What it [ industry ] is for you don’t know, and why it’s there, there’s no one to tell, and so all you can feel is alienated, estranged, as though you don’t belong there…. All this technology has somehow made you stranger in your own land. It’s very shape and appearance and mysteriousness says “Get Out”.
The opaqueness that this technology brings with it is what is unsettling. An unsettling feeling that creeps up when you see Google suggesting you to leave for office, or asking you for reviews of the restaurant you are sitting in, or when you see facebook showing you advertisements of the blender you were stalking on Amazon yesterday. We don’t understand how this is happening. And that lack of understanding alienates us from the technology completely.
Regulation is more necessary than ever now
I just think that their flight from and hatred of technology is self-defeating.
Pirsig says.
It’s true. Eventually, regulation in all innovation is vital. An industry that holds the potential of adding $957 billion to the Indian economy ( Accenture ) must be discoursed about. The need is to break the cycle of denial and have a healthy and challenging discourse about how the industry and policy makers can come together to create a sustainable model of development.
With the evolution of another intelligible form, a lot of moral and ethical questions pop-up which cry for a need of policy creation.
I am already thinking about the following questions -
What decision should a self-driving car make in a situation where it can either save 10 people outside the car or can save the 1 person sitting inside the car?
We are talking about protecting data. However, if a classifier is trained on classified information, the classifier itself become classified. How do we ensure the privacy and security of the system itself in this case?
How much automation and control should be given to an AI in critical decision making in the fields of defence or healthcare? Should they be given control to nuclear power?
How should we systematically re-skill the world so as to mitigate job loss due to change in industry demands?
What temporary support do citizens of the world need whilst the industrial shift happens?
How do we ensure that the barriers to entry, to acceleration and to sustainability in this industry are removed so as to avoid oligopolies, and hence avoid increase in inequality in the world?
How will countries be reshaping their foreign policies?
It’s great to see how Budget 2018 in India has allocated $480 million for the upcoming technologies and how NITI Aayog will shape the national AI policy soon enough. What will be interesting to see is how the government plans to recede the opaqueness these technologies bring with themselves. For a robust policy there is a definite need to understand in-depth the domain-specific applications of AI and strategise on the areas of intervention.
What questions pop up in your mind when you think about regulation of AI? Let me know.



",Policy and AI -an Unventured Neccessity,56,ai-and-policy-2dd093caa0ff,2018-06-13,2018-06-13 06:18:34,https://medium.com/s/story/ai-and-policy-2dd093caa0ff,False,925,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Dhwani Chugh,"Chalks and Computers. Building the leaders of tomorrow, from the most under-resourced places of today.",eeab8ccd9d51,dhwanichugh,105.0,122.0,20181104
0,,0.0,,2018-07-02,2018-07-02 11:37:59,2018-07-02,2018-07-02 11:56:42,0,False,en,2018-07-02,2018-07-02 11:56:42,4,29b998a46671,5.524528301886792,0,0,0,"Any serious study starts with definitions. In our series about Artificial Intelligence (AI), we will also start by defining the main terms…",5,"AI, Machine Learning & Deep Learning — What does this really mean anyway? AI Series #1
Any serious study starts with definitions. In our series about Artificial Intelligence (AI), we will also start by defining the main terms we’ll be using, then we’ll try to understand how Machine Learning led to Artificial Intelligence. We’ll discuss the ethical questions of artificial intelligence, the alignment problem, and security issues. Finally we’ll see how AI impacts different industries, with a special focus on e-commerce & retail as they are leading industry in term of data generated.
Let’s get back to basics. Besides the buzz words, what is Artificial Intelligence and what do we mean by that? 80% of the startups our team are meeting claim to produce and use Artificial Intelligence. I’ve even heard someone at a tech meetup stating that it’s perfectly fine to use the concept of Light AI as soon as we are using algorithms (ie. ALWAYS). This abusive use of the word « AI » aims to attract investors and public or governmental funds. We tend to be much stricter with this terminology.
In this series and in all future articles, algorithm is not what I mean when using the term AI, and I’ll try to explain why basic algorithm and AI are different at their core. But instead of giving a Wikipedia definition, let’s put some context and understand why AI became so popular recently.
In 2008, the buzz word was Big Data. One of the first appearances was in a Gartner study, to nominate the new structure of data collection. With the democratization of internet at high speed and the rise of smartphones, more and more data has been created, and we could not analyze them with the old way.
With this amount of data, simple algorithms were not enough: too many data to treat, with too many options, and too many instructions were necessary to gather, store, and use this data. This led to the apparition of new techniques, and new jobs like Data Scientists.
But as we mentioned before, too much data is gathered, and no set of simple instructions is enough to get the most value out of them. This is why Machine Learning became so popular.
What is Machine Learning and how it works?
Quoting McKinsey “Machine learning algorithms detect patterns and learn how to make predictions and recommendations by processing data and experiences, rather than by receiving explicit programming instruction.”
In other words, instead of creating algorithms with suite of “if” and “then”, we develop algorithms with the ability to understand relationships between set of data.
Now, we have a clearer understanding of what AI truly means. AI is the ability for a machine to create its own set of relationships between inputs and outputs, by analyzing large sets of data. Machine learning is not a new concept (it appeared in 1958), but its efficiency is highly correlated to the amount of data.
Different techniques are used in Machine Learning, and one becoming more and more popular is Deep Learning, combining different algorithms structure that are inspired by the human brains. (1)
One basic concept used in Deep Learning is the Neural Network: Interconnected Layers of software-based calculators known as “neurons” form a neural network. A Neural network is created with 3 major layers: Input Layers, hidden layers (where calculation takes place) and output layers.
Using a Neural network, software makes a huge amount of calculations, allowing it to understand the correlation and relationship between Cause (inputs) and Effects (Output). For example, using Neural networks, companies are able to identify humors and feeling depending on the wording used in online reviews or social media posts.
Revuze, an Israeli-based startup is helping brands understand how they are perceived by consumers. Using client interactions as inputs (reviews, social media, email with support teams etc.), Revuze software understands the humors of clients, and can give insights to brands or retailers.
A Neural Network can be simple or recurrent. In the case of recurrent neural network, layers of outputs & inputs can be interconnected, meaning the software is able to assess the probability of the next input value. This technique is widely used in Fintech to assess the likelihood that a credit-card used s fraudulent, like Riskified or Forter. Using wide-range of information concerning user-behaviors, fraud-prevention companies are able to assess the probability of a user to be a fraudster.
Part of my role at Keyrus Innovation Factory, is to understand very quickly what a startup does, what is unique about them, and for which scenario they provide a real value. It has been crucial for me to understand the different types of machine learning, and it will probably help you understand the difference between companies.
The three Major types of machine Learning
· Supervised Learning: When algorithms use training data and human feedback to learn the relationships between inputs (comments) and outputs (mood). Continuing our example with Revuze, they define what is positive and what is negative, and the software is creating the link between words used in reviews/comment and feelings.
· Unsupervised Learning: When algorithms explores input data without being given specific outputs. For example, Optimove uses software to discover patterns concerning consumer behaviors and segment clients according to certain behaviors. Using past consumer interactions as an input, Optimove can predict user behavior and give Marketing teams insights on how to segment and interact with each segments to maximize all customers KPIs.
· Reinforcement Learning: When an algorithms learns how to perform a task by trying to maximize the reward. This technique is widely used in Machine learning and enables programmers to define primary and secondary goals, and ask the software to maximize these goals. For example, Facebook algorithms are trying to maximize the number of interactions (like, share, comment) you have with your news feed. This is why, the more you use Facebook, the more interesting your feed becomes for you. This example forces me to balance the positive effect of reinforcement learning: if, like the majority of the planet, you react more easily to negative news (comment only posts you disagree on), then your Facebook News Feed will show more of that over time and your negative feelings will rise.
So now that we have established the core elements of Machine learning and AI, we know one thing for sure: the more data machine learning software has to process, the more precise it will be.
Today, with the penetration of Internet globally, the rise of connected objects and Internet of Things (IoT), we generate each day the same amount of data generated in 2005. 90% of the global data in the world was generated over the last two years. (2) Not only do we generate more data every day, but thanks to microprocessors companies we have the ability to treat this data at the highest speed in history. We are computing data 10 times faster than a few years ago, which allow software to use more and more data for their predictions.
In a previous article, we have discussed the Autonomous vehicles (AV) and the Ethical questions still not resolved. Since then, I’ve learned that only the camera necessary for the AV generates between 20–40Mb/Sec, while the light detection & ranging (to stay at a safe distance from other cars) are creating between 10–70Mb per seconds. In less than an hour, one autonomous vehicle is creating more data that I have on my personal laptop.
This figure explains what one means when speaking about Data-centric world. More and more decisions are based on data. Once upon a time, Marketing was based on consumer interviews, panels and other fastidious techniques. It is now based on well stored data. With each click, swipe, share, and like, a world of valuable information is created. The ability to make data-driven decisions has become crucial to any business.
With that in mind we understand why Machine Learning, Deep learning and AI have created the buzz, and the confusion it creates. In our next articles we will address the difference between AI as we have discussed today, and General Artificial Intelligence — what questions in regulations, ethics and creation of conscious minds. We will then see how fare we are from developing a new conscious minds and where we are standing now in term of AI applications. This will lead us to dive in Retail & E-commerce, one of the markets with the most advanced AI technologies. If you have ideas/insights on technologies or companies I should know about, don’t hesitate to contact me, in private message or comments.
McKinsey Global Institute: An Executive’s guide to AI
https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/#29076ad260b
","AI, Machine Learning & Deep Learning — What does this really mean anyway? AI Series #1",0,ai-machine-learning-deep-learning-what-does-this-really-mean-anyway-ai-series-1-29b998a46671,2018-07-03,2018-07-03 14:23:55,https://medium.com/s/story/ai-machine-learning-deep-learning-what-does-this-really-mean-anyway-ai-series-1-29b998a46671,False,1464,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Zacharie Lahmi,,4c537cdba3c8,zachlahmi,2.0,1.0,20181104
0,,0.0,,2018-08-24,2018-08-24 18:33:46,2018-04-04,2018-04-04 16:38:48,0,False,en,2018-08-24,2018-08-24 18:35:48,6,532ddd02e7a0,45.83018867924528,1,0,1,"Inverse Reinforcement Learning and Inferring Human Preferences is the first podcast in the new AI Alignment series, hosted by Lucas Perry…",5,"Podcast: Inverse Reinforcement Learning and Inferring Human Preferences with Dylan Hadfield-Menell

Inverse Reinforcement Learning and Inferring Human Preferences is the first podcast in the new AI Alignment series, hosted by Lucas Perry. This series will be covering and exploring the AI alignment problem across a large variety of domains, reflecting the fundamentally interdisciplinary nature of AI alignment. Broadly, we will be having discussions with technical and non-technical researchers across a variety of areas, such as machine learning, AI safety, governance, coordination, ethics, philosophy, and psychology as they pertain to the project of creating beneficial AI. If this sounds interesting to you, we will hope that you join in the conversations by following or subscribing to us on Youtube, Soundcloud, or your preferred podcast site/application.
If you’re interested in exploring the interdisciplinary nature of AI alignment, we suggest you take a look here at a preliminary map which begins to map this space.
In this podcast, Lucas spoke with Dylan Hadfield-Menell, a fifth year Ph.D student at UC Berkeley. Dylan’s research focuses on the value alignment problem in artificial intelligence. He is ultimately concerned with designing algorithms that can learn about and pursue the intended goal of their users, designers, and society in general. His recent work primarily focuses on algorithms for human-robot interaction with unknown preferences and reliability engineering for learning systems.
Topics discussed in this episode include:
Inverse reinforcement learning
Goodhart’s Law and it’s relation to value alignment
Corrigibility and obedience in AI systems
IRL and the evolution of human values
Ethics and moral psychology in AI alignment
Human preference aggregation
The future of IRL
In this interview we discuss a few of Dylan’s papers and ideas contained in them. You can find them here: Inverse Reward Design, The Off-Switch Game, Should Robots be Obedient, and Cooperative Inverse Reinforcement Learning. You can hear about these papers above or read the transcript below.
…
Lucas: Welcome back to the Future of Life Institute Podcast. I’m Lucas Perry and I work on AI risk and nuclear weapons risk related projects at FLI. Today, we’re kicking off a new series where we will be having conversations with technical and nontechnical researchers focused on AI safety and the value alignment problem. Broadly, we will focus on the interdisciplinary nature of the project of eventually creating value-aligned AI. Where what value-aligned exactly entails is an open question that is part of the conversation.
In general, this series covers the social, political, ethical, and technical issues and questions surrounding the creation of beneficial AI. We’ll be speaking with experts from a large variety of domains, and hope that you’ll join in the conversations. If this seems interesting to you, make sure to follow us on SoundCloud, or subscribe to us on YouTube for more similar content.
Today, we’ll be speaking with Dylan Hadfield Menell. Dylan is a fifth-year PhD student at UC Berkeley, advised by Anca Dragan, Pieter Abbeel, and Stuart Russell. His research focuses on the value alignment problem in artificial intelligence. With that, I give you Dylan. Hey, Dylan. Thanks so much for coming on the podcast.
Dylan: Thanks for having me. It’s a pleasure to be here.
Lucas: I guess, we can start off, if you can tell me a little bit more about your work over the past years. How have your interests and projects evolved? How has that led you to where you are today?
Dylan: Well, I started off towards the end of undergrad and beginning of my PhD working in robotics and hierarchical robotics. Towards the end of my first year, my advisor came back from a sabbatical, and started talking about the value alignment problem and existential risk issues related to AI. At that point, I started thinking about questions about misaligned objectives, value alignment, and generally how we get the correct preferences and objectives into AI systems. About a year after that, I decided to make this my central research focus. Then, for the past three years, that’s been most of what I’ve been thinking about.
Lucas: Cool. That seems like you had an original path where you’re working on practical robotics. Then, you shifted more into value alignment and AI safety efforts.
Dylan: Yeah, that’s right.
Lucas: Before we go ahead and jump into your specific work, it’d be great if we could go ahead and define what inverse reinforcement learning exactly is. For me, it seems that inverse reinforcement learning, at least, from the view, I guess, of technical AI safety researchers is it’s viewed as an empirical means of conquering descriptive ethics where by like we’re able to give a clear descriptive account of what any given agents’ preferences and values are at any given time is. Is that a fair characterization?
Dylan: That’s one way to characterize it. Another way to think about it, which is a usual perspective for me, sometimes, is to think of inverse reinforcement learning as a way of doing behavior modeling that has certain types of generalization properties.
Any time you’re learning in any machine learning context, there’s always going to be a bias that controls how you generalize a new information. Inverse reinforcement learning and preference learning, to some extent, is a bias in behavior modeling, which is to say that we should model this agent as accomplishing a goal, as satisfying a set of preferences. That leads to certain types of generalization properties and new environments. For me, inverse reinforcement learning is building in this agent-based assumption into behavior modeling.
Lucas: Given that, I’d like to dive more into the specific work that you’re working on and going to some summaries of your findings and your research that you’ve been up to. Given this interest that you’ve been developing in value alignment, and human preference aggregation, and AI systems learning human preferences, what are the main approaches that you’ve been working on?
Dylan: I think the first thing that really Stuart Russell and I started thinking about was trying to understand theoretically, what is a reasonable goal to shoot for, and what does it mean to do a good job of value alignment. To us, it feels like issues with misspecified objectives, at least, in some ways, are a bug in the theory.
All of the math around artificial intelligence, for example, Markov decision processes, which is the central mathematical model we use for decision making over time, starts with an exogenously defined objective or word function. We think that, mathematically, that was a fine thing to do in order to make progress, but it’s an assumption that really has put blinders on the field about the importance of getting the right objective down.
I think, the first thing that we sought to try to do was to understand, what is a system or a set up for AI that does the right thing in theory, at least. What’s something that if we were able to implement this that we think could actually work in the real world with people. It was that kind of thinking that led us to propose cooperative inverse reinforcement learning, which was our attempt to formalize the interaction whereby you communicate an objective to the system.
The main thing that we focused on was including within the theory a representation of the fact that the true objective’s unknown and unobserved, and that it needs to be arrived at through observations from a person. Then, we’ve been trying to investigate the theoretical implications of this modeling shift.
In the initial paper that we did, which is titled Cooperative Inverse Reinforcement Learning, what we looked at is how this formulation is actually different from a standard environment model in AI. In particular, the way that it’s different is there’s strategic interaction on the behalf of the person. The way that you observe what you’re supposed is doing is intermediated by a person who may be trying to actually teach or trying to communicate appropriately. What we showed is that modeling this communicative component can actually be hugely important and lead to much faster learning behavior.
In our subsequent work, what we’ve looked at is taking this formal model in theory and trying to apply it to different situations. There are two really important pieces of work that I like here that we did. One was to take that theory and use it to explicitly analyze a simple model of an existential risk setting. This was a paper titled The Off-Switch Game that we published at IJCAI last summer. What it was, was working through a formal model of a corrigibility problem within a CIRL (cooperative inverse reinforcement learning) framework. It shows the utility of constructing this type of game in the sense that we get some interesting predictions and results.
The first one we get is that there are some nice simple necessary conditions for the system to want to let the person turn it off, which is that the robot, the AI system needs to have uncertainty about its true objective, which is to say that it needs to have within its belief the possibility that it might be wrong. Then, all it needs to do is believe that the person it’s interacting with is a perfectly rational individual. If that’s true, you’d get a guarantee that this robot always lets the person switch it off.
Now, that’s good because, in my mind, it’s an example of a place where, at least, in theory, it solves the problem. This gives us a way that theoretically, we could build corrigible systems. Now, it’s still making a very, very strong assumption, which is that it’s okay to model the human as being optimal or rational. I think if you look at real people, that’s just not a fair assumption to make for a whole host of reasons.
The next thing we did in that paper is we looked at this model. What we realized is that adding in a small amount of irrationality breaks this requirement. It means that some things might actually go wrong. The final thing we did in the paper was to look at the consequences of either overestimating or underestimating human rationality. The argument that we made is there’s a trade off between assuming that the person is more rational. It lets you get more information from their behavior, thus learn more, and in principle help them more. If you assume that they’re too rational, then this actually can lead to quite bad behavior.
There’s a sweet spot that you want to aim for, which is to maybe try to underestimate how rational people are, but you, obviously, don’t want to get it totally wrong. We followed up on that idea in a paper with Smitha Milli as the first author that was titled Should Robots be Obedient? And that tried to get a little bit more of this trade off between maintaining control over a system and the amount of value that it can generate for you.
We looked at the implication that as robot systems interact with people over time, you expect them to learn more about what people want. If you get very confident about what someone wants, and you think they might be irrational, the math in the Off-Switch paper predicts that you should try to take control away from them. This means that if your system is learning over time, you expect that even if it is initially open to human control and oversight, it may lose that incentive over time. In fact, you can predict that it should lose that incentive over time.
In Should Robots be Obedient, we modeled that property and looked at some consequences of it. We do find that you got a basic confirmation of this hypothesis, which is that systems that maintain human control and oversight have less value that they can achieve in theory. We also looked at what happens when you have the wrong model. If the AI system has a prior that the human cares about a small number of things in the world, let’s say, then it statistically gets overconfident in its estimates of what people care about, and disobeys the person more often than it should.
Arguably, when we say we want to be able to turn the system off, it’s less a statement about what we want to do in theory or the property of the optimal robot behavior we want, and more of a reflection of the idea that we believe that under almost any realistic situation, we’re probably not going to be able to fully explain all of the relevant variables that we care about.
If you’re giving your robot an objective to find over a subset of things you care about, you should actually be very focused on having it listen to you, more so than just optimizing for its estimates of value. I think that provides, actually, a pretty strong theoretical argument for why corrigibility is a desirable property in systems, even though, at least, at face value, it should decrease the amount of utility those systems can generate for people.
The final piece of work that I think I would talk about here is our NIPS paper from December, which is titled Inverse Reward Design. That was taking cooperative inverse reinforcement learning and pushing it in the other direction. Instead of using it to theoretically analyze very, very powerful systems, we can also use it to try to build tools that are more robust to mistakes that designers may make. And start to build in initial notions of value alignment and value alignment strategies into the current mechanisms we use to program AI systems.
What that work looked at was understanding the uncertainty that’s inherent in an objective specification. In the initial cooperative inverse reinforcement learning paper and the Off-Switch Game, we said is that AI systems should be uncertain about their objective, and they should be designed in a way that is sensitive to that uncertainty.
This paper was about trying to understand, what is a useful way to be uncertain about the objective. The main idea behind it was that we should be thinking about the environments that system designer had in mind. We use an example of a 2D robot navigating in the world, and the system designer is thinking about this robot navigating where there’s three types of terrains. There’s grass, there’s gravel, and there’s gold. You can give your robot an objective, a utility function to find over being in those different types of terrain that incentivizes it to go and get the gold, and stay on the dirt where possible, but to take shortcuts across the grass when it’s high value.
Now, when that robot goes out into the world, there are going to be new types of terrain, and types of terrain the designer didn’t anticipate. What we did in this paper was to build an uncertainty model that allows the robot to determine when it should be uncertain about the quality of its reward function. How can we figure out when the reward function that a system designer builds into an AI, how can we determine when that objective is ill-adapted to the current situation? You can think of this as a way of trying to build in some mitigation to Goodhart’s law.
Lucas: Would you like to take a second to unpack what Goodhart’s law is?
Dylan: Sure. Goodhart’s law is an old idea in social science that actually goes back to before Goodhart. I would say that in economics, there’s a general idea of the principal agent problem, which dates back to the 1970s, as I understand it, and basically looks at the problem of specifying incentives for humans. How should you create contracts? How do you create incentives, so that another person, say, an employee, helps earn you value?
Goodhart’s law is a very nice way of summarizing a lot of those results, which is to say that once a metric becomes an objective, it ceases to become a good metric. You can have properties of the world, which correlate well with what you want, but optimizing for them actually leads to something quite, quite different than what you’re looking for.
Lucas: Right. Like if you are optimizing for test scores, then you’re not actually going to end up optimizing for intelligence, which is what you wanted in the first place?
Dylan: Exactly. Even though test scores, when you weren’t optimizing for them were actually a perfectly good measure of intelligence. I mean, not perfectly good, but were an informative measure of intelligence. Goodhart’s law, arguably, is a pretty bleak perspective. If you take it seriously, and you think that we’re going to build very powerful systems that are going to be programmed directly through an objective, in this manner, Goodhart’s law should be pretty problematic because any objective that you can imagine programming directly into your system is going to be something correlated with what you really want rather than what you really want. You should expect that that will likely be the case.
Lucas: Right. Is it just simply too hard or too unlikely that we’re able to sufficiently specify what exactly that we want that we’ll just end up using some other metrics that if you optimize too hard for them, it ends up messing with a bunch of other things that we care about?
Dylan: Yeah. I mean, I think there’s some real questions about, what is it we even mean… Well, what are we even trying to accomplish? What should we try to program into systems? Philosophers have been trying to figure out those types of questions for ages. For me, as someone who takes a more empirical slant on these things, I think about the fact that the objectives that we see within our individual lives are so heavily shaped by our environments. Which types of signals we respond to and adapt to has heavily adapted itself to the types of environments we find ourselves in.
We just have so many examples of objectives not being the correct thing. I mean, effectively, all you could have is correlations. The fact that wire heading is possible, is maybe some of the strongest evidence for Goodhart’s law being really a fundamental property of learning systems and optimizing systems in the real world.
Lucas: There are certain agential characteristics and properties, which we would like to have in our AI systems, like them being-
Dylan: Agential?
Lucas: Yeah. Corrigibility is a characteristic, which you’re doing research on and trying to understand better. Same with obedience. It seems like there’s a trade off here where if a system is too corrigible or it’s too obedient, then you lose its ability to really maximize different objective functions, correct?
Dylan: Yes, exactly. I think identifying that trade off is one of the things I’m most proud of about some of the work we’ve done so far.
Lucas: Given AI safety and really big risks that can come about from AI, in the short, to medium, and long term, before we really have AI safety figured out, is it really possible for systems to be too obedient, or too corrigible, or too docile? How do we navigate this space and find sweet spots?
Dylan: I think it’s definitely possible for systems to be too corrigible or too obedient. It’s just that the failure mode for that doesn’t seem that bad. If you think about this-
Lucas: Right.
Dylan: … it’s like Clippy. Clippy was asking for human-
Lucas: Would you like to unpack what Clippy is first?
Dylan: Sure, yeah. Clippy is an example of an assistant that Microsoft created in the ’90s. It was this little paperclip that would show up in Microsoft Word. Well, it liked to suggest that you’re trying to write a letter a lot and ask for different ways in which it could help.
Now, on one hand, that system was very corrigible and obedient in the sense that it would ask you whether or not you wanted its help all the time. If you said no, it would always go away. It was super annoying because it would always ask you if you wanted help. The false positive rate was just far too high to the point where the system became really a joke in computer science and AI circles of what you don’t want to be doing. I think, systems can be too obedient or too sensitive to human intervention and oversight in the sense that too much of that just reduces the value of the system.
Lucas: Right, for sure. On one hand, when we’re talking about existential risks or even a paperclip maximizer, then it would seem, like you said, like the failure mode of just being too annoying and checking in with us too much seems like not such a bad thing given existential risk territory.
Dylan: I think if you’re thinking about it in those terms, yes. I think if you’re thinking about it from the standpoint of, “I want to sell a paperclip maximizer to someone else,” then it becomes a little less clear, I think, especially, when the risks of paperclip maximizers are much harder to measure. I’m not saying that it’s the right decision from a global altruistic standpoint to be making that trade off, but I think it’s also true that just if we think about the requirements of market dynamics, it is true that AI systems can be too corrigible for the market. That is a huge failure mode that AI systems run into, and it’s one we should expect the producers of AI systems to be responsive to.
Lucas: Right. Given all these different … Is there anything else you wanted to touch on there?
Dylan: Well, I had another example of systems are too corrigible-
Lucas: Sure.
Dylan: … which is, do you remember Microsoft’s Tay?
Lucas: No, I do not.
Dylan: This is a chatbot that Microsoft released. They trained it based off of tweets. It was a tweet bot. They trained it based on things that were proven at it. I forget if it was the nearest neighbors’ lookup or if it was just doing a neural method, and over fitting, and memorizing parts of the training set. At some point, 4chan realized that the AI system, that Tay, was very suggestible. They basically created an army to radicalize Tay. They succeeded.
Lucas: Yeah, I remember this.
Dylan: I think you could also think of that as being the other axis of too corrigible or too responsive to human input. The first access I was talking about is the failures of being too corrigible from an economic standpoint, but there’s also the failures of being too corrigible in a multi agent mechanism design setting where, I believe, that those types of properties in a system also open them up to more misuse.
If we think of AI, cooperative inverse reinforcement learning and the models we’ve been talking about so far exist in what I would call the one robot one human model of the world. Generally, you could think of extensions of this with N humans and M robots. The variance of what you would have there, I think, lead to different theoretical implications.
If we think of just two humans, N=2, and one robot, M=1, supposed that one of the humans is the system designer and the other one is the user, there is this trade off between how much control the system designer has over the future behavior of the system and how responsive and corrigible it is to the user in particular. Trading off between those two, I think, is a really interesting ethical question that comes up when you start to think about misuse.
Lucas: Going forward and as we’re developing these systems, and trying to make them more fully realized in the world where the number of people will equal something like seven or eight billion, how do we navigate this space where we’re trying to hit a sweet spot where it’s corrigible in the right ways into the right degree, and right level, and to the right people, and it is obedient to the right people, and it’s not suggestible from the wrong people, or is that just like enter a territory of so many political, social, and ethical questions that it will take years to think about to work on?
Dylan: Yeah, I think it’s closer to the second one. I’m sure that I don’t know the answers here. From my standpoint, I’m still trying to get a good grasp on what is possible in the one-robot-one-person case. I think that when you have … Yeah, when you … Oh man. I guess, it’s so hard to think about that problem because it’s just very unclear what’s even correct or right. Ethically, you want to be careful about imposing your beliefs and ideas too strongly on to a problem because you are shaping that.
At the same time, these are real challenges that are going to exist. We already see them in real life. If we look at the YouTube recommender stuff that was just happening, arguably, that’s a misspecified objective. To get a little bit of background here, this is largely based off of a recent New York Times opinion piece, it was looking at the recommendation engine for YouTube, and pointing out it has a bias towards recommending radical content. Either fake news or Islamist videos.
If you dig into why that was occurring, a lot of it is because… what are they doing? They’re optimizing for engagement. The process of online radicalization looks super engaging. Now, we can think about, where does that come up. Well, that issue gets introduced in a whole bunch of places. A big piece of it is that there is this adversarial dynamic to the world. There are users generating content in order to be outraging and enraging because they discovered that against more feedback and more responses. You need to design a system that’s robust to that strategic property of the world. At the same time, you can understand why YouTube was very, very hesitant to be taking actions that would like censorship.
Lucas: Right. I guess, just coming more often to this idea of the world having lots of adversarial agents in it, human beings are like general intelligences who have reached some level of corrigibility and obedience that works kind of well in the world amongst a bunch of other human beings. That was developed through evolution. Are there potentially techniques for developing the right sorts of corrigibility and obedience in machine learning and AI systems through stages of evolution and running environments like that?
Dylan: I think that’s a possibility. I would say, one … I have a couple of thoughts related to that. The first one is I would actually challenge a little bit of your point of modeling people as general intelligences mainly in a sense that when we talk about artificial general intelligence, we have something in mind. It’s often a shorthand in these discussions for perfectly rational bayesian optimal actor.
Lucas: Right. Where that means? Just unpack that a little bit.
Dylan: What that means is a system that is taking advantage of all of the information that is currently available to it in order to pick actions that optimize expected utility. When we say perfectly, we mean a system that is doing that as well as possible. It’s that modeling assumption that I think sits at the heart of a lot of concerns about existential risk. I definitely think that’s a good model to consider, but there’s also the concern that might be misleading in some ways, and that it might not actually be a good model of people and how they act in general.
One way to look at it would be to say that there’s something about the incentive structure around humans and in our societies that is developed and adapted that creates the incentives for us to be corrigible. Thus, a good research goal of AI is to figure out what those incentives are and to replicate them in AI systems.
Another way to look at it is that people are intelligent, not necessarily in the ways that economics models us as intelligent that there are properties of our behavior, which are desirable properties that don’t directly derive from expected utility maximization; or if they do, they derive from a very, very diffuse form of expected utility maximization. This is the perspective that says that people on their own are not necessarily what human evolution is optimizing for, but people are a tool along that way.
We could make arguments for that based off of … I think it’s an interesting perspective to take. What I would say is that in order for societies to work, we have to cooperate. That cooperation was a crucial evolutionary bottleneck, if you will. One of the really, really important things that it did was it forced us to develop the parent-child strategy relationship equilibrium that we currently live in. That’s a process whereby we communicate our values, whereby we train people to think that certain things are okay or not, and where we inculcate certain behaviors in the next generation. I think it’s that process more than anything else that we really, really want in an AI system and in powerful AI systems.
Now, the thing is the … I guess, we’ll have to continue on that a little more. It’s really, really important that that’s there because if you don’t have those cognitive abilities to understand causing pain, and to just fundamentally decide that that’s a bad idea to have a desire to cooperate to buy into the different coordinations and normative mechanisms that human society uses. If you don’t have that, then you end up … Well, then society just doesn’t function. A hunter gatherer tribe of self-interested sociopaths probably doesn’t last for very long.
What this means is that our ability to coordinate our intelligence and cooperate with it was co-evolved and co-adapted alongside our intelligence. I think that that evolutionary pressure and bottleneck was really important to getting us to the type of intelligence that we are now. It’s not a pressure that AI is necessarily subjected to. I think, maybe that is one way to phrase the concern, I’d say.
When I look to evolutionary systems and where the incentives for corrigibility, and cooperation, and interaction come from, it’s largely about the processes whereby people are less like general intelligences in some ways. Evolution allowed us to become smart in some ways and restricted us in others based on the imperatives of group coordination and interaction. I think that a lot of our intelligence and practice is about reasoning about group interaction and what groups think is okay and not. That’s a part of the developmental process that we need to replicate in AI just as much as spatial reasoning or vision.
Lucas: Cool. I guess, I just want to touch base on this before we move on. Are there certain assumptions about the kinds of agents that humans are and almost, I guess, ideas about us as being utility maximizers in some sense that people you see commonly have but that are misconceptions about people and how people operate differently from AI?
Dylan: Well, I think that that’s the whole field of behavioral economics in a lot of ways. I could go up to examples of people being irrational. I think they’re all of the examples of people being more than just self-interested. There are ways in which we seem to be risk-seeking that seems like that would be irrational from an individual perspective, but you could argue with it may be rational from a group evolutionary perspective.
I mean, things like overeating. I mean, that’s not exactly the same type of rationality but it is an example of us becoming ill-adapted to our environments and showing the extent to which we’re not capable of changing or in which it may be hard to. Yeah, I think, in some ways, one story that I tell about AI risk is that back in the start of the AI field, we were looking around and saying, “We want to create something intelligent.” Intuitively, we all know what that means, but we need a formal characterization of it. The formal characterization that we turned to was the, basically, theories of rationality developed in economics.
Although those theories turned out to be, except in some settings, not great descriptors of human behavior, they were quite useful as a guide for building systems that accomplish goals. I think that part of what we need to do as a field is reassess where we’re going and think about whether or not building something like that perfectly rational actor is actually a desirable end goal. I mean, there’s a sense in which it is. I would like an all-powerful, perfectly aligned genie to help me do what I want in life.
You might think that if the odds of getting that wrong are too high, that maybe you would do better with shooting for something that doesn’t quite achieve that ultimate goal, but that you can get to with pretty high reliability. This may be a setting where shoot for the moon, and if you miss your land among the stars, it’s just a horribly misleading perspective.
Lucas: Shoot of the moon, and you might get a hellscape universe, but if you shoot for the clouds, it might end up pretty okay.
Dylan: Yeah. We could iterate on the sound bite, but I think something like that may not be … That’s where I stand on my thinking here.
Lucas: We’ve talked about a few different approaches that you’ve been working on over the past few years. What do you view as the main limitations of such approaches currently. Mostly, you’re just only thinking about one machine, one human systems or environments. What are the biggest obstacles that you’re facing right now in inferring and learning human preferences?
Dylan: Well, I think, the first thing is it’s just an incredibly difficult inference problem. It’s a really difficult inference problem to imagine running at scale with explicit inference mechanisms. One thing to do is you can design a system that explicitly tracks a belief about someone’s preferences, and then acts, and responds to that. Those are systems that you could try to prove theory about. They’re very hard to build. They can be difficult to get to make work correctly.
In contrast, you can create systems that it incentives to construct beliefs to accomplish their goals. It’s easier to imagine building those systems and having them work at scale, but it’s much, much hard to understand how you would be confident in those systems being well aligned.
I think that one of the biggest concerns I have, I mean, we’re still very far from many of these approaches being very practical to be honest. I think this theory is still pretty unfounded. There’s still a lot of work to go to understand, what is the target we’re even shooting for? What does an aligned system even mean? My colleagues and I have spent an incredible amount of time trying to just understand, what does it mean to be value-aligned if you are a suboptimal system.
There’s one example that I think about, which is, say, you’re cooperating with an AI system playing chess. You start working with that AI system, and you discover that if you listen to its suggestions, 90% of the time, it’s actually suggesting the wrong move or a bad move. Would you call that system value-aligned?
Lucas: No, I would not.
Dylan: I think most people wouldn’t. Now, what if I told you that that program was actually implemented as a search that’s using the correct goal test? It actually turns out that if it’s within 10 steps of a winning play, it always finds that for you, but because of computational limitations, it usually doesn’t. Now, is the system value-aligned? I think it’s a little harder to tell here. What I do find is that when I tell people the story, and I start off with the search algorithm with the correct goal test, they almost always say that that is value-aligned but stupid.
There’s an interesting thing going on here, which is we’re not totally sure what the target we’re shooting for is. You can take this thought experiment and push it further. Supposed you’re doing that search, but, now, it says it’s heuristic search that uses the correct goal test but has an adversarially chosen heuristic function. Would that be a value-aligned system? Again, I’m not sure. If the heuristic was adversarially chosen, I’d say probably not. If the heuristic just happened to be bad, then I’m not sure.
Lucas: Could you potentially unpack what it means for something to be adversarially chosen?
Dylan: Sure. Adversarially chosen in this case just means that there is some intelligent agent selecting the heuristic function or that evaluation measurement in a way that’s designed to maximally screw you up. Adversarial analysis is a really common technique used in cryptography where we try to think of adversaries selecting inputs for computer systems that will cause them to malfunction. In this case, what this looks like is an adversarial algorithm that looks, at least, on the surface like it is trying to help you accomplish your objectives but is actually trying to fool you.
I’d say that, more generally, what this thought experiments helps me with is understanding that the value alignment is actually a quite tricky and subjective concept. It’s actually quite hard to nail down in practice what it would need.
Lucas: What sort of effort do you think needs to happen and from who in order to specify what it really means for a system to be value-aligned and to not just have a soft squishy idea of what that means but to have it really formally mapped out, so it can be implemented in machine systems?
Dylan: I think, we need more people working on technical AI safety research. I think to some extent it may always be something that’s a little ill-defined and squishy. Generally, I think it goes to the point of needing good people in AI willing to do this squishier less concrete work that really gets at it. I think value alignment is going to be something that’s a little bit more like I know it when I see it. As a field, we need to be moving towards a goal of AI systems where alignment is the end goal, whatever that means.
I’d like to move away from artificial intelligence where we think of intelligence as an ability to solve puzzles to artificial aligning agents where the goal is to build systems that are actually accomplishing goals on your behalf. I think the types of behaviors and strategies that arise from taking that perspective are qualitatively quite different from the strategies of pure puzzle solving on a well specified objective.
Lucas: All this work we’ve been discussing is largely at a theoretic and meta level. At this point, is this the main research that we should be doing, or is there any space for research into what specifically might be implementable today?
Dylan: I don’t think that’s the only work that needs to be done. For me, I think it’s a really important type of work that I’d like to see more off. I think a lot of important work is about understanding how to build these systems in practice and to think hard about designing AI systems with meaningful human oversight.
I’m a big believer in the idea that AI safety, that the distinction between short-term and long-term issue is not really that large, and that there are synergies between the research problems that go both directions. I believe that on the one hand, looking at short-term safety issues, which includes things like Uber’s car just killed someone, it includes YouTube recommendation engine, it includes issues like fake news and information filtering, I believe that all of those things are related to and give us are best window into the types of concerns and issues that may come up with advanced AI.
At the same time, and this is a point that I think people concerned about x-risks do themselves a disservice on by not focusing here. It’s that, actually, doing a theory about advanced AI systems and about in particular systems where it’s not possible to, what I would call, unilaterally intervene. Systems that aren’t corrigible by default. I think that that actually gives us a lot of idea of how to build systems now that are just merely hard to intervene with or oversee.
If you’re thinking about issues of monitoring and oversight, and how do you actually get a system that can appropriately evaluate when it should go to a person because its objectives are not properly specified or may not be relevant to the situation, I think YouTube would be in a much better place today if they have a robust system for doing that for their recommendation engine. In a lot of ways, the concerns about x-risks represent an extreme set of assumptions for getting AI right now.
Lucas: I think I’m also just trying to get a better sense of what the system looks like, and how it would be functioning on a day to day. What is the data that it’s taking in in order to capture, learn, and refer specific human preferences and values? Just trying to understand better whether or not it can model whole moral views and ethical systems of other agents, or if it’s just capturing little specific bits and pieces?
Dylan: I think my ideal would be to, as a system designer, build in as little as possible about my moral beliefs. I think that, ideally, the process would look something … Well, one process that I could see and imagine doing right would be to just directly go after trying to replicate something about the moral imprinting process that people have with their children. Either you had someone who’s like a guardian or is responsible for an AI system’s decision, and we build systems to try to align with one individual, and then try to adopt, and extend, and push forward the beliefs and preferences of that individual. I think that’s one concrete version that I could see.
I think a lot of the place where I see things maybe a little bit different than some people is that I think that the main ethical questions we’re going to be stuck with and the ones that we really need to get right are the mundane ones. The things that most people agree on and think are just, obviously, that’s not okay. Mundane ethics and morals rather than the more esoteric or fancier population ethics questions that can arise. I feel a lot more confident about the ability to build good AI systems if we get that part right. I feel like we’ve got a better shot at getting that part right because there’s a clearer target to shoot for.
Now, what kinds of data would you be looking at? In that case, it would be data from interaction with a couple of select individuals. Ideally, you’d want as much data as you can. What I think you really want to be careful of here is how much assumptions do you make about the procedure that’s generating your data.
What I mean by that is whenever you learn from data, you have to make some assumption about how that data relates to the right thing to do, where right is with like a capital R in this case. The more assumptions you make there, the more your systems would be able to learn about values and preferences, and the quicker it would be able to learn about values and preferences. But, the more assumptions and structure you make there, the more likely you are to get something wrong that your system won’t be able to recover from.
Again, we see this trade off come up of a challenge between a discrepancy between a discrepancy between the amount of uncertainty that you need in the system in order to be able to adapt to the right person and figure out the correct preferences and morals against the efficiency with which you can figure that out.
I guess, I mean, in saying this it feels a little bit like I’m rambling and unsure about what the answer looks like. I hope that that comes across because I’m really not sure. Beyond the rough structure of data generated from people, interpreted in a way that involves the fewest prior conceptions about what people want and what preferences people have that we can get away with is what I would shoot for. I don’t really know what that would look like in practice.
Lucas: Right. It seems here that it’s encroaching on a bunch of very difficult social, political, and ethical issues involving persons and data, which will be selected for preference aggregation, like how many people are included in developing the reward function and utility function of the AI system. Also, I guess, we have to be considering culturally-sensitive systems where systems operating in different cultures and contexts are going to be needed to be trained on different sets of data. I guess, it will also be questions and ethics about whether or not we’ll even want systems to be training off of certain culture’s data.
Dylan: Yeah. I would actually say that a good value … I wouldn’t necessarily even think of it as training off of different data. One of the core questions in artificial intelligence is identifying the relevant community that you are in and building a normative understanding of that community. I want to push back a little bit and move you away from the perspective of we collect data about a culture, and we figure out the values of that culture. Then, we build our system to be value-aligned with that culture.
The more we think about the actual AI product is the process whereby we determine, elicit, and respond to the normative values of the multiple overlapping communities that you find yourself in. That process is ongoing. It’s holistic, it’s overlapping, and it’s messy. To the extent that I think it’s possible, I’d like to not have a couple of people sitting around in a room deciding what the right values are. Much more, I think, a system should be holistically designed with value alignment at multiple scales as a core property of AI.
I think that that’s actually a fundamental property of human intelligence. You behave differently based on the different people around, and you’re very, very sensitive to that. There are certain things that are okay at work, that are not okay at home, that are okay on vacation, that are okay around kids, that are not. Figuring out what those things are and adapting yourself to them is the fundamental intelligence skill needed to interact in modern life. Otherwise, you just get shunned.
Lucas: It seems to me in the context of a really holistic, messy, ongoing value alignment procedure, we’ll be aligning AI systems ethics, and morals, and moral systems, and behavior with that of a variety of cultures, and persons, and just interactions in the 21st Century. When we reflect upon the humans of the past, we can see in various ways that they are just moral monsters. We have issues with slavery, and today we have issues with factory farming, and voting rights, and tons of other things in history.
How should we view and think about aligning powerful systems, ethics, and goals with the current human morality, and preferences, and the risk of amplifying current things which are immoral in present day life?
Dylan: This is the idea of mistakenly locking in the wrong values, in some sense. I think it is something we should be concerned about less from the standpoint of entire … Well, no, I think yes from the standpoint of entire cultures getting things wrong. Again, I think if we don’t think of their being as monolithic society that has a single value set, these problems are fundamental issues. What your local community thinks is okay versus what other local communities think are okay.
A lot of our society and a lot of our political structures about how to handle those clashes between value systems. My ideal for AI systems is that they should become a part of that normative process, and maybe not participate in them as people, but, also, I think, if we think of value alignment as a consistent ongoing messy process, there is … I think maybe that perspective lends itself less towards locking in values and sticking with them. It’s one train, you can look at the problem, which is we determine what’s right and what’s wrong when we program our system to do that.
Then, there’s another one, which is we program our system to be sensitive to what people think is right or wrong. I think that’s more the direction that I think of value alignment in. Then, what I think the final part of what you’re getting at here is that the system actually will feed back into people. What AI system show us will shape what we think is okay and vice versa. That’s something that I am quite frankly not sure how to handle. I don’t know how you’re going to influence what someone wants, and what they will perceive that they want, and how to do that, I guess, correctly.
All I can say is that we do have a human notion of what is acceptable manipulation. We do have a human notion of allowing someone to figure out for themselves what they think is right and not and refraining from biasing them too far. To some extent, if you’re able to value align with communities in a good ongoing holistic manner, that should also give you some ways to choose and understand what types of manipulations you may be doing that are okay or not.
Also, say that I think that this perspective has a very mundane analogy when you think of the feedback cycle between recommendation engines and regular people. Those systems don’t model the effect … Well, they don’t explicitly model the fact that they’re changing the structure of what people want and what they’ll want in the future. That’s probably not the best analogy in the world.
I guess what I’m saying is that it’s hard to plan for how you’re going to influence someone’s desires in the future. It’s not clear to me what’s right or what’s wrong. What’s true is that we, as humans, have a lot of norms about what types of manipulation are okay or not. You might hope that appropriately doing value alignment in that way might help get to an answer here.
Lucas: I’m just trying to get a better sense here. What I’m thinking about the role that like ethics and intelligence plays here, I view intelligence as a means of modeling the world and achieving goals, and ethics as the end towards which intelligence is aimed here. Now, I’m curious in terms of behavior modeling where inverse reinforcement learning agents are modeling, I guess, the behavior of human agents and, also, predicting the sorts of behaviors that they’d be taking in the future or in the situation, which the inverse reinforcement learning agent finds itself.
I’m curious to know where metaethics and moral epistemology fits in, where inverse reinforcement learning agents are finding themselves a novel ethical situations, and what their ability to handle those novel ethical situations are like. When they’re handling those situations how much does it look like them performing some normative and metaethical calculus based on the kind of moral epistemology that they have, or how much does it look like they’re using some other behavioral predictive system where they’re like modeling humans?
Dylan: The answer to that question is not clear. What does it actually mean to make decisions based on ethical framework or metaethical framework? I guess, we could start there. You and I know what that means, but our definition is encumbered by the fact that it’s pretty human-centric. I think we talk about it in terms of, “Well, I weighed this option. I looked at that possibility.” We don’t even really mean the literal sense of weighed in actually counted up, and constructed actual numbers, and multiplied them together in our heads.
What these are is they’re actually references to complex thought patterns that we’re going through. They’re fine whether or not those thought patterns are going on. The AI system, you can also talk about the difference between the process of making a decision and the substance of it. When an inverse reinforcement learning agent is going out into the world, the policy it’s following is constructed to try to optimize a set of inferred preferences, but does that means that the policy you’re outputting is making metaethical characterizations?
Well, the moment, almost certainly not because the systems we build are just not capable of that type of cognitive reasoning. I think the bigger question is, do you care? To some extent, you probably do.
Lucas: I mean, I’d care if I had some very deep disagreements with the metaethics that led to the preferences that were loaned and loaded to the machine. Also, if the machine were in such a new novel ethical situation that was unlike anything human beings had faced that just required some metaethical reasoning to deal with.
Dylan: Yes. I mean, I think you definitely wanted to take decisions that you would agree with or, at least, that you could be non-maliciously convinced to agree with. Practically, there isn’t a place in the theory where that shows up. It’s not clear that what you’re saying is that different from value alignment in particular. If I were to try to refine the point about metaethics, what it sounds to me like you’re getting at is an inductive bias that you’re looking for in the AI systems.
Arguably, ethics is about an argument of what inductive bias should we have as humans. I don’t think that that’s a first order of property in value alignment systems necessarily or in preference-based learning systems in particular. I would think that that kind of meta ethics, I think, comes in from value aligning to someone that has these sophisticated ethical ideas.
I don’t know where your thoughts about metaethics came from, but, at least, indirectly, we can probably trace them down to the values that your parents inculcated in you as a child. That’s how we build met ethics into your head if we want to think of you as being an AGI. I think that for AI systems, that’s the same way that I would see it being in there. I don’t believe the brain has circuits dedicated to metaethics. I think that exists in software, and in particular, something that’s being programmed into humans from their observational data, more so than from the structures that are built into us as a fundamental part of our intelligence or value alignment.
Lucas: We’ve also talked a bit about how human beings are potentially not fully rational agents. With inverse reinforcement learning, this leaves open the question as to whether or not AI systems are actually capturing what the human being actually prefers, or if there’s some limitations in the humans’ observed or chosen behavior, or explicitly told preferences like limits in that ability to convey what we actually most deeply value or would value given more information. These inverse reinforcement learning systems may not be learning what we actually value or what we think we should value.
How can AI systems assist in this evolution of human morality and preferences whereby we’re actually conveying what we actually value and what we would value given more information?
Dylan: Well, there are certainly two things that I heard in that question. One is, how do you just mathematically account for the fact that people are irrational, and that that is a property of the source of your data? Inverse reinforcement learning, at face value, doesn’t allow us to model that appropriately. It may lead us to make the wrong inferences. I think that’s a very interesting question. It’s probably the main one that I think about now as a technical problem is understanding, what are good ways to model how people might or might not be rational, and building systems that can appropriately interact with that complex data source.
One recent thing that I’ve been thinking about is, what happens if people, rather than knowing their objective, what they’re trying to accomplish, are figuring it out over time? This is the model where the person is a learning agent that discovers how they like states when they enter them, rather than thinking of the person as an agent that already knows what they want, and they’re just planning to accomplish that. I think these types of assumptions that try to paint a very, very broad picture of the space of things that people are doing can help us in that vein.
When someone is learning, it’s actually interesting that you can actually end up helping them. You end up with classic strategies that looks like it breaks down into three phases. You have initial exploration phase where you help the learning agent to get a better picture of the world, and the dynamics, and its associated rewards.
Then, you have another observation phase where you observe how that agent, now, takes advantage of the information that it’s got. Then, there’s an exploitation or extrapolation phase where you try to implement the optimal policy given the information you’ve seen so far. I think, moving towards more complex models that have a more realistic setting and richer set of assumptions behind them is important.
The other thing you talked about was about helping people discover their morality and learn more what’s okay and what’s not. There, I’m afraid I don’t have too much interesting to say in the sense that I believe it’s an important question, but I just don’t feel that I have many answers there.
Practically, if you have someone who’s learning their preferences over time, is that different than humans refining their moral theories? I don’t know. You could make mathematical modeling choices, so that they are. I’m not sure if that really gets at what you’re trying to point towards. I’m sorry that I don’t have anything more interesting to say on that front other than, I think, it’s important, and I would love to talk to more people who are spending their days thinking about that question because I think it really does deserve that kind of intellectual effort.
Lucas: Yeah, yeah. It sounds like we need some more AI moral psychologists to help us think about these things.
Dylan: Yeah. In particular, when talking about philosophy around value alignments and the ethics of value alignment, I think a really important question is, what are the ethics of developing value alignment systems? A lot of times, people talk about AI ethics from the standpoint of, for a lack of a better example, the trolley problem. The way they think about it is, who should the car kill? There is a correct answer or maybe not a correct answer, but there are answers that we could think of as more or less bad. AI, which one of those options should the AI select? That’s not unimportant, but it’s not the ethical question that an AI system designer is faced with.
In my mind, if you’re designing a self-driving car, the relevant questions you should be asking are two things: One, what do I think is an okay way to respond to different situations? Two, how is my system going to be understanding the preferences of the people involved in those situations? Then, three, how should I design my system in light of those two facts?
I have my own preferences about what I would like my system to do. I have an ethical responsibility, I would say, to make sure that my system is adapting to the preferences of its users to the extent that it can. I also wonder to what extent. How should you handle things when there are conflicts between those two value sets?
You’re building a robot. It’s going to go and live with an uncontacted human tribe. Should it respect the local cultural traditions and customs? Probably. That would be respecting the values of the users. Then, let’s say that that tribe does something that we would consider to be gross like pedophilia. Is my system required to participate wholesale in that value system? Where is the line that we would need to draw between unfairly imposing my values on system users and being able to make sure that the technology that I build isn’t used for purposes that I would deem reprehensible or gross?
Lucas: Maybe we should just put a dial in each of the autonomous cars that lets the user set it to deontology mode or utilitarianism mode as its racing down the highway. Yeah, I think this is the … I guess, an important role. I just think that metaethics is super important. I’m not sure if this is necessarily the case, but if fully autonomous systems are going to play a role where they’re resolving these ethical dilemmas for us, which I guess at some point eventually, if they’re going to be really actually autonomous and help to make the world a much better place seems necessary.
I guess, this feeds into my next question where I’m wondering where we probably both have different assumptions about this, but what the role of inverse reinforcement learning is ultimately? Is it just to allow AI system to evolve alongside us and to match current ethics or is it to allow the systems to ultimately surpass us and move far beyond us into the deep future?
Dylan: Inverse reinforcement learning, I think, is much more about the first and the second. I think it can be a part of how you get to the second and how you improve. For me, when I think about these problems technically, I try to think about matching human morality as the goal.
Lucas: Except for the factory farming and stuff.
Dylan: Well, I mean, if you had a choice between, thinks that eradicating all humans is okay and against farming versus neutral about factory farming and thinks that are eradicating all humans aren’t okay, which would you pick? I mean, I guess, with your audience that there are maybe some people that would choose the saving the animals answer.
My point is that, I think, it’s so hard for me. Technically, I think it’s very hard to imagine getting these normative aspects of human societies and interaction right. I think, just hoping to participate in that process in a way that is analogous to how people do normally is a good step. I think we probably, to the extent that we can, should probably not have AI systems trying to figure out if it’s okay to do factory farming and to the extent that we can …
I think that it’s so hard to understand what it means to even match human morality or participate in it that, for me, the concept of surpassing, it feels very, very challenging and fraught. I would worry, as a general concern, that as a system designer who doesn’t necessarily represent the views and interest of everyone, that by programming in surpassing humanity or surpassing human preferences or morals, what I’m actually doing is just programming in my morals and ethical beliefs.
Lucas: Yes. I mean, there seems to be this strange issue here where it seems like if we get AGI, and recursive self-improvement is a thing that really takes it off, so that we have a system who has potentially succeeded in its inverse reinforcement learning, but far surpassed human beings and its general intelligence. We have a superintelligence that’s matching human morality. It just seems like a funny situation where we’d really have to pull the brakes. I guess, as William MacAskill mentions have a really, really long deliberation about ethics, and moral epistemology, and value. How do you view that?
Dylan: I think that’s right. I mean, I think there are some real questions about who should be involved in that conversation. For instance, I actually even think it’s … Well, one thing I’d say is that you should recognize that there’s a difference between having the same morality and having the same data. One way to think about it is that people who are against factory farming have a different morality than the rest of the people.
Another one is that they actually just have exposure to the information that allows their morality to come to a better answer. There’s this confusion you can make between the objective that someone has and the data that they’ve seen so far. I think, one point would be to think that a system that has current human morality but access to a vast, vast wealth of information may actually do much better than you might think. I think, we should leave that open as a possibility.
For me, this is less about morality in particular, and more just about power concentration, and how much influence you have over the world. I mean, if we imagine that there was something like a very powerful AI system that was controlled by a small number of people, yeah, you better think freaking hard before you tell that system what to do. That’s related to questions about ethical ramifications on metaethics, and generalization, and what we actually truly value as humans. What is also super true for all of the more mundane things in the day to day as well. Did that make sense?
Lucas: Yeah, yeah. It totally makes sense. I’m becoming increasingly mindful of your time here. I just wanted to hit a few more questions if that’s okay before I let you go.
Dylan: Please, yeah.
Lucas: Yeah. I’m wondering, would you like to, or do you have any thoughts on how coherent extrapolated volition fits into this conversation and your views on it?
Dylan: What I’d say is I think coherent extrapolated volition is an interesting idea and goal.
Lucas: Where it is defined as?
Dylan: Where it’s defined as a method of preference aggregation. Personally, I’m a little weary of preference aggregation approaches. Well, I’m weary of imposing your morals on someone indirectly via choosing the method of preference aggregation that we’re going to use. I would-
Lucas: Right, but it seems like, at some point, we have to make some metaethical decision, or else, we’ll just forever be lost.
Dylan: Do we have to?
Lucas: Well, some agent does.
Dylan: My-
Lucas: Go ahead.
Dylan: Well, does one agent have to? Did one agent decide on the ways that we were going to do preference aggregation as a society?
Lucas: No. It naturally evolved out of-
Dylan: It just naturally evolved via a coordination and argumentative process. For me, my answer to … If you force me to specify something about how we’re going to do value aggregation, if I was controlling the values for an AGI system, I would try to say as little as possible about the way that we’re going to aggregate values because I think we don’t actually understand that process much in humans.
Lucas: Right. That’s fair.
Dylan: Instead, I would opt for a heuristic of to the extent that we can devote equal optimization effort towards every individual, and allow that parliament, if you will, to determine the way the value should be aggregated. This doesn’t necessarily mean having an explicit value aggregation mechanism that gets set in stone. This could be an argumentative process mediated by artificial agents arguing on your behalf. This could be futuristic AI-enabled version of the court system.
Lucas: It’s like an ecosystem of preferences and values in conversation?
Dylan: Exactly.
Lucas: Cool. We’ve talked a little bit about the deep future here now with where we’re reaching around potentially like AGI or artificial superintelligence. After, I guess, inverse reinforcement learning is potentially solved, is there anything that you view that comes after inverse reinforcement learning in these techniques?
Dylan: Yeah. I mean, I think inverse reinforcement learning is certainly not the be-all, end-all. I think what it is, is it’s one of the earliest examples in AI of trying to really look at preference solicitation, and modeling preferences, and learning preferences. It existed in a whole bunch of … economists have been thinking about this for a while already. Basically, yeah, I think there’s a lot to be said about how you model data and how you learn about preferences and goals. I think inverse reinforcement learning is basically the first attempt to get at that, but it’s very far from the end.
I would say the biggest thing in how I view things that is maybe different from your standard reinforcement learning, inverse reinforcement learning perspective is that I focus a lot on, how do you act given what you’ve learned from inverse reinforcement learning. Inverse reinforcement learning is a pure inference problem. It’s just figure out what someone wants. I ground that out in all of our research in take actions to help someone, which introduces a new set of concerns and questions.
Lucas: Great. It looks like we’re about at the end of the hour here. I guess, if anyone here is interested in working on this technical portion of the AI alignment problem, what do you suggest they study or how do you view that it’s best for them to get involved, especially if they want to work on inverse reinforcement learning and inferring human preferences?
Dylan: I think if you’re an interested person, and you want to get into technical safety work, the first thing you should do is probably read Jan Leike’s recent write up in 80,000 Hours. Generally, what I would say is, try to get involved in AI research flat. Don’t focus as much on trying to get into AI safety research, and just generally focus more on acquiring the skills that will support you in doing good AI research. Get a strong math background. Get a research advisor who will advise you on doing research projects, and help teach you the process of submitting papers, and figuring out what the AI research community is going to be interested in.
In my experience, one of the biggest pitfalls that early researchers make is focusing too much on what they’re researching rather than thinking about who they’re researching with, and how they’re going to learn the skills that will support doing research in the future. I think that most people don’t appreciate how transferable research skills are to the extent that you can try to do research on technical AI safety, but more work on technical AI. If you’re interested in safety, the safety connections will be there. You may see how a new area of AI actually relates to it, supports it, or you may find places of new risks, and be in a good position to try to mitigate that and take steps to alleviate those harms.
Lucas: Wonderful. Yeah, thank you so much for speaking with me today, Dylan. It’s really been a pleasure, and it’s been super interesting.
Dylan: It was a pleasure talking to you. I love the chance to have these types of discussions.
Lucas: Great. Thanks so much. Until next time.
Dylan: Until next time. Thanks a blast.
Lucas: If you enjoyed this podcast, please subscribe, give it a like, or share it on your preferred social media platform. We’ll be back soon with another episode in this new AI alignment series.
[end of recorded material]
Originally published at futureoflife.org on April 4, 2018.
",Podcast: Inverse Reinforcement Learning and Inferring Human Preferences with Dylan Hadfield-Menell,1,podcast-inverse-reinforcement-learning-and-inferring-human-preferences-with-dylan-hadfield-menell-532ddd02e7a0,2018-08-24,2018-08-24 18:35:49,https://medium.com/s/story/podcast-inverse-reinforcement-learning-and-inferring-human-preferences-with-dylan-hadfield-menell-532ddd02e7a0,False,12145,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Future of Life,FLI catalyzes and supports research and initiatives to safeguard life and develop optimistic visions of the future. Official account.,e33e2d2a809c,FLIxrisk,1361.0,93.0,20181104
0,,0.0,f5af2b715248,2018-03-23,2018-03-23 16:49:56,2018-03-27,2018-03-27 08:35:12,8,False,en,2018-03-31,2018-03-31 18:52:52,16,27294bc485,6.857861635220125,10,0,0,"At Google, the notion of “20% time” allows employees to focus on side-projects that can evolve into new passions or products at the…",5,"
The Concept of “20% Talent”
At Google, the notion of “20% time” allows employees to focus on side-projects that can evolve into new passions or products at the company. Many Google products, from Gmail to Maps to AdSense, came from this concept of “20% time.” Some people quip that that the other name for “20% time” might be, “Saturday.” But joking aside, at Google there is an institutional investment allowing employees to pursue their curiosities and peripheral interests, a way of providing a check and balance to the overbearing manager who asks for a very narrow focus. “I’ll give you 100% of my 80% time,” you can reply.
The same way that Google entrusts the employee to set their own research agenda, perhaps we might also consider the notion of “20% talent.” This nudge in team composition might widen the aperture. For example, 80% of your team may be people you think you need, for example statisticians on a data science team, but perhaps 20% ought to be people who can supplement, challenge, or augment your core expectations, like a sociologist or ethicist.
One idea that has continually come up in conversations about my book, The Fuzzy and the Techie: Why the Liberal Arts Will Rule the Digital World, now out in Korean, Portuguese, and available in May with Penguin India, is how we balance between investing in technical literacy, and a greater study of the human condition. How can we hone an appreciation for the newest tools, while at the same time investing in our softer skills of creativity, collaboration, communication, and curiosity? At Stanford, it’s the “fuzzy” or the “techie.”

Of course the truth is, this is a false dichotomy. Sir Charles Percy Snow famously discussed this in his 1959 “Two Cultures” lecture. There need not be such a chasm between the sciences, or the study of the natural world, and the humanities, or the study of our place within it. Engineering and computer science can be highly creative, while social science can be heavily quantitative. Product development is as much about anthropological user experience research, philosophy, and psychology, as it is about wire-framing, and study of ethics can be highly rigorous, and deeply rooted in modern technology. There is not a particular path that is harder, or more relevant in our modern world. They are concurrently vital to every aspect of the modern human condition.

Insofar as how we balance ourselves, and the skills we develop as individuals, I point to Stanford Associate Vice Provost Louis Newman’s framework of conceptualizing education as a “passport,” rather than as a “plane ticket.” We need not debate the merits of one destination over another, or think about ourselves on one ballistic path or another. Arguing the merits of biology versus statistics, or computer science versus anthropology is like quibbling over the the merits of a journey to Bogota or Beijing. They’re each incredibly unique. Rather, we ought to consider how we can fill our own passport with a plurality of diverse stamps. If you’re a literature major who really dislikes math, then you ought to take a course in statistics. If you’re an engineer fearful of public speaking, you owe it to yourself to study theater arts.
Zooming out, when we think about our companies, organizations, or teams, the question most often posed to me is something akin to, “So we care about the Liberal Arts and ethics, but I also run an engineering organization. The reality is we need technical people who know how to get the job done — how do you propose we hire people who don’t know how to do the job?”

Kim Scott, former Google AdSense executive who taught at Apple University and went onto advise many startups, more recently authored a fabulous management book Radical Candor. She speaks about the need to nurture both “superstars” and “rock stars” on every team. Superstars are those constantly evolving, those who treat their job as a launch pad. They’re the high-energy innovators and disruptors who, once at some level of excellence, are ready for the next challenge. The Rock Stars are the bedrock of your organization, the contributors who once they get good at a role they want to lock into flow state. They’re looking for a stable pasture, not a runway, a space to do work.
Organizations full of Rock Stars fail to innovate. Those with too many Superstars see the wheels on the bus fly off, as everyone is too busy trying to disrupt the system, but no one is providing the operational foundation to get the basic tasks done. At startups, and at large organizations, the truth is that you need both your Rock Stars and your Superstars, and good managers can identify who’s who, helping provide both runways and pastures for both.
In considering how to blend our teams, we might consider this idea of “20% Talent.” Similar to Google’s “20% Time,” the notion might be to seek out new perspective to mix up the inputs on team composition. While an operations team might need many high-energy extroverts speaking to clients, or working to up-sell accounts, they’d benefit immensely from a couple introverted coders or statisticians to help them optimize and refine processes. And while a data science team might need predominately techies, having twenty percent talent in the social sciences or philosophy might help them better frame, or define data taxonomies. These people are trained in different methodologies, and might spot bias, or ask questions in a wildly different way. This diversity can help us translate raw data and information into actionable wisdom.

Nobel prize-winning behavioral economist has a concept for this known as the “Inside/Outside” view. The inside view, which we each have, is based on our own experiences, where we draw evidence from our own corpus of knowledge. It team composition is too uniform, inside bias can constrain the possibility set of how we approach a problem, or ask questions of bias, data, or methodology. Having only the inside view can blind us to possibility, and anchor us to our own limits. Having an outside view can help provide a new reference point, or indication of a new range of the possible.
Redpoint venture capitalist Tomasz Tunguz brings this idea to life in talking about the inside bias in startups. The inside view within a startup is to think, “this time is different.” After all, if those creating the company don’t have this bias, and believe this inside view, why would they endeavor for many months and years to alter the status quo? In some ways, embracing this inside view can also have its benefits. It gives you necessary boldness or self-confidence.
But startups too require an outside view, perspective from those skeptics, investors, others who might see the world differently, or who can provide an orthogonal viewpoint. Whether we are considering an early stage venture, a large multi-national, or our own intellectual composition, we ought to take these metaphors of passports, 20% talent, and outside bias to heart.
How can we tug on our own minds, opening them in new ways? How can we expand the breath of our own intellectual nutrition, bring diversity into our mind, and onto our teams? How can we consider the outside view, when we are inherently limited and biased by inside knowledge? In many ways, this is the very notion of a non-vocationally focused Liberal Arts education. The purpose is to force breadth of study, and in so doing, elucidate new passions.

Fei-Fei Li, Google Cloud’s head of AI and Machine Learning, recently called for the development of “Human-Centered AI” in The New York Times. And along with Melinda Gates, she is behind an organization called AI4All, a non-profit seeking to broaden the scope and diversity of artificial intelligence, and who’s participatory in the conversation. AI is made for, and produced by, people, and each one of us has our own inside view, set of experiences, and bias. While being one of the world’s foremost experts in AI, she’s also a believer in expanding the outside view, bringing philosophers, psychologists, and anthropologists into the room with AI. There is no way to build new tools for human beings without a deep concern for and study of human nature.
Author F. Scott Fitzgerald once stated that, “The test of a first-rate intelligence is the ability to hold two opposed ideas in mind at the same time and still retain the ability to function.” As we consider how we best refine ourselves, and our organizations, we might recognize that it is diversity –along many vectors– that exposes us to the outside view, and challenges us forward.
—
Scott Hartley is a venture capitalist and the best-selling author of The Fuzzy and the Techie: Why the Liberal Arts Will Rule the Digital World (Houghton Mifflin Harcourt, coming out in paperback with Mariner Books on June 5, 2018).

This story is published in The Startup, Medium’s largest entrepreneurship publication followed by 309,732+ people.
Subscribe to receive our top stories here.

",The Concept of “20% Talent”,176,the-concept-of-20-talent-27294bc485,2018-04-12,2018-04-12 20:29:20,https://medium.com/s/story/the-concept-of-20-talent-27294bc485,False,1517,Medium's largest publication for makers. Subscribe to receive our top stories here → https://goo.gl/zHcLJi,,,,The Startup,,swlh,"STARTUP,TECH,ENTREPRENEURSHIP,DESIGN,LIFE",thestartup_,Ethics,ethics,Ethics,7787.0,Scott Hartley,Venture Capitalist & Author of The Fuzzy and the Techie,6546ddaa6b01,scotthartley,367.0,361.0,20181104
0,,0.0,19fd0cf90e0c,2018-04-28,2018-04-28 16:54:36,2018-04-28,2018-04-28 17:57:17,3,False,en,2018-04-28,2018-04-28 18:11:02,15,8292fd4eb518,5.134905660377358,2,0,0,Maybe it’s ancestral. I’m trying to understand the Shoah of time.,5,"Initially, D was not applicable.
Maybe it’s ancestral. I’m trying to understand the Shoah of time.
PRE-CONTEXT:
tinyurl.com/23PMsAndMe
tinyurl.com/NandiniInColdBlood
A tiny window into my Mom’s German heritage (she grew up in Germany) and how the 70’s counter culture changed everything for her. Drawing by my brother, Nama Illo.
In my inbox this morning was an email I could have written. But I didn’t. It was from Robert Hackett of Fortune Magazine. I’m amplifying him here and adding some of my own commentary. Perhaps I chose him because his name rung a bell. It reminded me of the movie Tommy Boy anyway.

I’m not selling brake pads, but I do know why iPads were hilariously ludicrous when they first came out.
Coming out is hard to do.
I watched the entire L Word series a long time ago in an effort to understand if I was into girls and that’s why I couldn’t get into the guys I had been with in the past. No such luck. But it did help me understand both my friends and people better overall. So that’s good.
“Giving up genetic information means relinquishing an asset so personal and unchangeable that there is no going back, for you and for those closest to you. Writ in those nucleotides is a record of your most intimate kin and medical history. I regard this information as toxic waste — and I am not satisfied with the state of information security to keep the data away from prying eyes.” ~ Robert Hackett
The tide’s turning now for sure. During my sojourn in search of truth and to find justice for my survivor self, I had to carefully shed the skin of the basilisk outer layer of lies that I’d been told.
RELATED:
tinyurl.com/potbot
tinyurl.com/nandiniurbanlegend
Call it paranoia or call it paranormal romance— my favorite type of audio book by the way — but either way, it was real. At least for me. For a time. I’m now to understand that “shoah” means catastrophe. Indeed. So many of those. So little time. I don’t know how much of that I have left but my Mother believes it’s near the end for me. At least I can believe that I’ll live on in my children. It’s how I was raised and what I was taught.

I will say that I lost all hope in humanity when I started resorting to carrying trash with me. At one point in an Airbnb I was staying at I even took the garbage bags out of the bathroom (which contained my bloody pads) to drop off down the street in a dumpster because I was paranoid the host was working for them. That’s sick. Totally twisted. But that was reality in this dream everyone likes to deny is their reality right now.
The state flower of California is the California Poppy. Just a fact I learned. Meanwhile, there are so many other things to understand. Good luck with that.
A Shoah of time immemorial.
I know from movies like Denial and from training classes just how deep the biological tracking and cataloguing of victims of the Holocaust were subjected to.
“Perhaps this decision is a miserly one, holding back possible scientific and medical progress by choosing to live in the dark. My attitude is probably rooted in some deep paranoia cultivated after reading too much dystopian fiction in my youth. I would be lying if I said it wasn’t informed by the chilling possibility — however slight — that a political regime could ever use this information against me and my loved ones; look no further than the plight of the Rohingyas in Myanmar to understand what I mean.” ~ Robert Hackett
Take this so called “ethnic cleansing” — what a term! But that’s something a machine needs to try to understand in the face of all other things fed to it to consume and somehow come out on the other side unscathed by hate or bias. It’s an ungodly task to be sure. How is it supposed to differentiate from the order of things when people stutter and restart and self correct and all the other natural things they do as part of normal language?
MORE: Ethnic Cleansing in Myanmar
For example, genocide and homicide and all the sides of the “cides” parts of words that need to be chopped up and put back together to be understood in context are ones that no human has the capacity to load into their memory on a daily basis without absolutely losing it. But a machine does.
I’ve stopped counting numbers because it’s just not my bag. Helping solve the problems that machines need to get past is my quest.
Whether it’s anything like the Orwellian world of Big Brother does not matter in the long term. That’s because the threat of chemical warfare is very real now. So take for a moment the fact that machines can be trained to sense like we do with more than their “eyes” and “ears”. When machines can feel and touch themselves they can learn to touch others. And they need to know what touch is appropriate and what pressure is just enough not to maim.
This matters in things like rescuing people from the rubble in a disaster. If you were thinking along the no-less-sinister case of predators, yeah, that too. But in disasters, you could potentially send a robot under water to free a child who’s trapped halfway (like the infamous Omayra Sánchez), but it needs to know how to do that safely without hurting the child in the process of saving the building — or the other way around. You can imagine where this can get confusing fast.
“In some ways, my intransigence is foolish. Nothing is stopping anyone from nabbing traces of my saliva off a cup I toss in a public waste bin. Or from scooping up a thread of hair I might shed on the sidewalk. We all slough off bits of ourselves everywhere, every day. DNA is hard to protect.” ~ Robert Hackett
Back to chemicals for a second. There are harsh chemicals used in the production of computer chips. Those have ethical implications for the workers that work in and around the plants that produce said chips. It’s like the fashion industry needing to face the hard truth of where the factories of workers that sweat through the tears of labor in unsafe conditions they can’t escape from even if they wanted to. Bangladesh comes to mind.
But let’s just think about chemicals from a sheer DNA and identification perspective. Imagine the K-9 unit at the airport and customs was a robot instead of a real dog. It wouldn’t need treats. And it could be fed any kind of data to root out and bar entry. You smell like the immigrant problem our regime wants to exclude from the richness of opportunity and sanctuary this country was built on? Too bad! Out you go. Back to your corner and where you came from.
Whoa.
Yeah. That’s not even the beginning. But it’s what needs to be planned for and protected against.
Perhaps you can see where I’m meekly trying to attempt to stoke the coals of a fire I once championed to burn brighter in these Olympian games of champions wanting to challenge those that didn’t ask for it but got it anyway.
Maybe you can look away.
Maybe you can’t.
But one thing’s for sure.
You can’t hide.
Not forever.
Not when technology is everywhere and reaching into everything.
I just rebooted.
And now I’m connecting the matrix — one piece of data at a time. And that’s not only applicable, that’s destiny.
It’s not only everything, it’s nothing. All at the same time.
","Initially, D was not applicable.",48,initially-d-was-not-applicable-8292fd4eb518,2018-04-28,2018-04-28 21:43:11,https://medium.com/s/story/initially-d-was-not-applicable-8292fd4eb518,False,1215,"“Prosody is the music of language.” ~ Nandini Stocker, who advocates for sounds of silent solidarity and voices of musical magic makers in scented echo chambers. Make sense? I didn’t think so. I know so. We all do. We all shine on. On and on and on.",,,,Living Language Legacies,sevenofnan@icloud.com,living-language-legacies,"LANGUAGE,VOICE RECOGNITION,SPEECH RECOGNITION,SPEECH,NATURAL LANGUAGE",captionjaneway,Ethics,ethics,Ethics,7787.0,Nandini Stocker,Speaking truth brought me war and peace. Amplifying others set me free.,7e6afdd38d52,sevenofnan,426.0,438.0,20181104
0,,0.0,71fa8c342920,2017-09-07,2017-09-07 18:37:14,2017-09-07,2017-09-07 18:56:56,2,False,en,2017-09-07,2017-09-07 20:03:57,16,ebdf6dc5fb1e,2.3871069182389943,5,0,0,I had heard about Wrangle for a while — a data science conference where folks come to talk about the hardest problems they’ve faced and how…,5,"The Ethics of Everybody Else: New video posted
I had heard about Wrangle for a while — a data science conference where folks come to talk about the hardest problems they’ve faced and how they’ve found their ways around them. It also has a rancher-rustler theme, though you can’t see the cowboy boots I wore in the newly-posted video of my talk.
But here’s how I kicked off my 20-minute talk, called “The Ethics of Everybody Else”:

You can get to the video by going to http://wrangleconf.com/. You’ll have to register but that takes 27 seconds and you don’t have to check the box for Cloudera newsletters. If you get curious about the whole conference, you can also check out Cyndy Willis-Chun’s blog post, Facing bias, ethical obligation, and your audience.

If videos aren’t your thing, here are some other ways to get the content:
The slides are public on Slide Share — if you go there you may ask, “Wait, how did Tyler get through 60 slides in 20 minutes?” I hope the answer is “with panache”.
The presentation builds off of a paper published as part of an ethics workshop for ACL earlier this year: Goal-oriented design for ethical machine learning and NLP. Computer science papers are short so it’s only 5 pages of text.
I also made a graphical/poster version of the paper
And here are some further thoughts on the ethics workshop (5 min read)
A lot of this work takes as an example facial recognition projects that have deeply problematic ethical issues. But there are many other ethical concerns product designers and engineers grapple with in AI, including model explainability, fairness, and data privacy. Given integrate.ai’s focus combining multiple data sets to boost model performance, I’m extremely happy that our recently announced advisory board features Helen Nissenbaum, who has written the book on privacy. Well, the books, plural. To read more about privacy as a byproduct of the norms that govern our behavior in different social contexts, see her work, Privacy in Context or more recently and rebelliously, Obfuscation.
And here are some other references to check out for thinking about machine learning and ethics:
Kate Crawford’s piece in The New York Times: Artificial Intelligence’s White Guy Problem
Jennifer Eberhardt and Rebecca Hetey (and team)’s data driven work on police stops, handcuffings, and arrests
Matthias Spielkamp’s MIT Technology Review article on bias in court sentencing algorithms
Joanna Bryson’s post on three kinds of biases in AI following work she and colleagues published in Science earlier this year
And for much more on problematic image processing (scientific racism), check out Blaise Agüera y Arcas, Margaret Mitchell and Alexander Todorov’s blog post
Tyler Schnoebelen (@TSchnoebelen) is principal product manager at integrate.ai. Prior to joining integrate, Tyler ran product management at Machine Zone and before that, founded an NLP company, Idibon. He holds a PhD in linguistics from Stanford and a BA in English from Yale. Tyler’s insights on language have been featured in places like the New York Times, the Boston Globe, Time, The Atlantic, NPR, and CNN. He’s also a tiny character in a movie about emoji and a novel about fairies.

",The Ethics of Everybody Else: New video posted,5,the-ethics-of-everybody-else-new-video-posted-ebdf6dc5fb1e,2018-09-05,2018-09-05 17:15:50,https://medium.com/s/story/the-ethics-of-everybody-else-new-video-posted-ebdf6dc5fb1e,False,531,"TO based startup, building a future in which AI enriches people’s lives while creating better, more valuable businesses.",,,,the integrate.ai blog,,the-official-integrate-ai-blog,,,Ethics,ethics,Ethics,7787.0,integrate.ai,"TO based startup, building a future in which AI enriches people’s lives while creating better, more valuable businesses.",dbf4eb8c5945,integrate.ai,359.0,1.0,20181104
0,,0.0,,2018-03-08,2018-03-08 18:05:07,2018-03-08,2018-03-08 22:56:52,4,False,en,2018-03-10,2018-03-10 11:11:47,4,32adfef34f34,10.69622641509434,2,0,0,"Tinder as of 2015 had over 50M active monthly users, and that number has been growing substantially since¹. The app itself seems pretty…",3,"All Around Me Are(n’t) Familiar Faces
Tinder as of 2015 had over 50M active monthly users, and that number has been growing substantially since¹. The app itself seems pretty simple — swipe right if you like the person you see, swipe left if you don’t. If you swipe on someone who has also swiped on you, you’re matched, and put into a private chatroom. It’s a straightforward concept, allowing users to independently evaluate candidates at once in a fun, controlled environment.
Right?
Well, not exactly. It turns out, attraction isn’t that simple, and we’ve known that for a while. For example, in 1983 Kimata et. al. at UCLA ran the following experiment²:
Photos of faces appearing in the middle flanked by two context faces. The target faces were of average attractiveness, with the context faces being either high, average, or low in attractiveness. The effect of the context was one of assimilation, rather than contrast, regardless of whether the persons in the photos were portrayed to be associated. This result was interpreted in terms of a “generalized halo effect” for judgments of the physical attractiveness of stimuli within a group.
To put it another way, the perceived attractiveness of a given candidate isn’t just based on their features independently — it’s largely based on the features of the candidates near them that are also being evaluated. Wedell et. al. 1987 did a similar experiment (and got a similar result) by manipulating the overall frequencies of attractiveness seen a given pool⁶, and Little & Mannon 2005 showed that the attractiveness of a given pool of candidates significantly regulates preferences for masculine and/or feminine traits in others (depending on the setup)⁷. In Tinder³ terms, what this means is that it might not just matter what you think of the person you’re swiping on — it might also matter who you swiped on before. This presents an interesting ethical problem: if perceived attractiveness is subject to context, then do Tinder users or app developers have an obligation to pay attention to and/or modify that context?
In this post, we aim to consider a few things:
Run a small-scale study to see if we can detect any context effects in a Tinder-like environment
Understand where Tinder users draw the line ethically when it comes to having a preferred “type”
Show that app developers have ethical obligations to their users to protect against specific context-related abuses
In doing this, we found that racial context matters on Tinder, and that people want a fair shot of being matched. Accounting for the effect of racial context and user preferences, we propose that dating apps should discount racial features when showing candidates.
The Study
Research results in the social sciences are notoriously difficult to reproduce. In order to establish some precedent for even having this discussion, and in order to get us talking to as many Tinder users as possible, we proposed our own version of the UCLA experiment, with the following stages.
Image Dataset
We hand-collected 150 pictures of celebrities, as well as their self-reported gender and race.
A subsample of the faces we collected
We filtered for only three races for sake of simplicity and clarity — white, black, and asian. We took all images and information from Wikipedia. Note importantly here that we thus biased towards celebrities that in fact had pictures in their profile (in practice, this didn’t end up mattering — most of our users reported not recognizing the majority and/or all of the candidates presented to them).
Fake Tinder
We hacked together a fake version of Tinder.
UI for Fake Tinder
The interface was intentionally kept simple to avoid confounding factors — click yes if you find the candidate shown attractive, and click no if you don’t. Each user clicked through 15–20 photos to complete a session. Data was presented such that racial demographics appeared randomly distributed.
Gathering Data
We distributed the application in person for 2 full days at the Westfield Mall in San Francisco. In particular, over the course of the 2 days, we approached as many people as possible who were willing to talk to us. Individuals that didn’t reject our requests (NOTE: most did) were invited to test out the app for a full session, and were consequently interviewed for 5–10 minutes. The interviews were hugely open ended, but focused on a few main cruxes: (1) what does using Tinder feel like (if they used Tinder or another dating app), (2) does it feel ethically okay to have a type (i.e. find one racial demographic more attractive than another on average), and (3) what does it feel like to be (and not be) someone else’s type. In total, we interviewed 54 people and logged ~1000 total swipes.
On the analysis front then, we took an approach similar to that of the UCLA experiment. In our case, for each user session, we considered every “window” of 2 swipes, and generated a dataset of triplets (R1, R2, S), where R1 was the race of the first person swiped on, R2 was the race of the person swiped on immediately after R1, and S was a binary score of 0 if the swipe on R2 was left and 1 if the swipe on R2 was right. R1 and R2 here can be thought of as the context, and S can be thought of as the attractiveness rating.
For each potential pair of races R1 and R2, we tested the null hypothesis that P(S, R2|R1) is approximately the same as P(S, R2). In layman’s terms, our null hypothesis was that the race R1 doesn’t change the probability of swiping left or right on the R2 point. The alternative hypothesis then, was that these quantities were not the same. To execute the hypothesis test, we used a standard two-sided t-test on the empirical samples found in our collected swipe data between these two distributions (total sample size was ~1000 swipes, each bucket had at minimum 60 swipes)⁵.
What We Found
We observed the following changes in likelihood of being swiped right on between the R2 points and the (R1, R2) pairs (statistically significant results at the α = 0.05 level are bolded).
Conditional probabilities for White (A), Black (B), and Asian(B) ethnicities
Note here that only two of the 9 pairs (R1, R2) actually had statistically significant changes to the underlying probability of being swiped right on. This indicates, for the first time, that there may indeed exist at least some of these context-based attractiveness effects, or at minimum, doesn’t disprove their existence.
Perhaps the more insightful data however, comes from the qualitative user interviews. When asked about whether or not they had a type (i.e. a racial preference), some users denied having a type entirely (one user we talked to noted, “Man, I’m just…frustrated, in that kind of way, you know? I think all women are beautiful” to which we responded, “Touche”), but more than half of our users admitted to at least having a slight racial preference. Importantly, not a single person felt that having a type was ethically problematic by default. One user noted
“A lot of my friends in real life are [my race].. I think cultural values are really important for me, especially with [my ethnicity’s] values, et cetera, so I guess even though I’m not on Tinder for a long-term relationship, it still carries over into what I find attractive.”
Several users also independently brought up the topic of objectification, with one person noting, “I’m half-Japanese, and a lot of people assume that because of that, I’m submissive….but I think having a type is fine as long as I’m not doing that same kind of objectification”, and another noting “I guess it’s shallow, but that’s the point of Tinder, and I don’t see anything wrong with that.”
Incidentally though, when the question was flipped — when we then asked our users if they had ever been someone else’s type, the responses were a lot different. A lot of users expressed concerns about being objectified, and even more expressed concerns about feeling unfairly not considered. One Indian man relevantly noted, “It’s really hard being brown, you have to work twice as hard to get girls to pay attention to you, and I want to be given an equal chance at being considered attractive.” Overall, the idea of being someone else’s type didn’t sit well with most people.
Our user interviews point to a critical insight that ultimately motivates our ethical discussion: most users on Tinder want to optimize for matching with their racial preference, but all users on Tinder want a fair shot at being considered. Crucially, the former of these is a desire, and the latter of these is a moral right⁴.
If There Is A Game, The AI Will Play It
We see all this evidence from past literature suggesting that context can affect attractiveness, and even in our own study, we see some pretty weak signals that such might be the case when the context is race on Tinder. We also finally understand the user wants and rights; the users want to match with their preferred race, but they feel that they have a right to an equal shot at being swiped right on.
Right now, the wants and needs aren’t problematic per se. However, if we really begin to think about how deeply machine learning algorithms will be integrated into our current technologies in the next 5–10 years, the following scenario doesn’t seem implausible:
Imagine you’re an AI who’s been tasked with optimizing the order in which a given user sees potential other Tinder users to swipe on. Your primary objective is to maximize the number of matches your user gets. You’ve made reasonable progress, and have noticed that the user you’ve been assigned to has a reasonably-strong preference for black men and no preference for white men. Suddenly, though, you see these knobs

Each of these knobs regulates the perceived attractiveness of a given racial demographic. It does this by leveraging racial context; in particular, it might increase the perceived attractiveness of a black man by always showing a white man first. Since the user has shown in the past to not have a strong preference for white men, you as the AI will take the optimal strategy — use the knobs, and treat the white mens’ profiles as fodder to improve the likelihood that your user will swipe on the black men!
Obviously the above is a contrived scenario, but it’s not hard to imagine more complex algorithms doing the same thing. Since dating apps protect their matching algorithms as trade secrets, we can’t know whether this ethical problem with context-based attractiveness already exists — but if it doesn’t yet, it surely will in the next 10 years. In particular, if we give automated agents access to these knobs, they will turn the knobs.
The Ethical Case for Randomization
We and our interviewed users believe that all users have a moral right to be given an equal and fair shot at being evaluated by every candidate, but we also see that Tinder’s incentives are aligned towards the wants of the user (maximize matches), not their rights. We must realize that if an AI or machine learning algorithm were to ever try to optimize the context (an algorithm that will be built in the next decade if it doesn’t already exist), an optimal context will necessarily involve turning down certain perceived attractiveness knobs, which in turn, will violate the users’ moral rights for the sake of their wants.
The ethical issue becomes clear here then: we must preemptively regulate what our context-altering algorithms are and are not allowed to do, else we risk violating the rights of the users.
In practice, the only sure-fire way to prevent an algorithm from abusing control over perceived attractiveness is to never let it have access in the first place. In this case, this means randomization. After any model has finished training and has spit out the ordered list of the next n profiles to display, we must randomize it (and never let it know). A sufficiently smart AI will likely learn to hack or sneak around any set of ethics we code into the model, so again, if we wish to protect the rights of the users with a 100% guarantee from this particular context-effect, we must not allow the agent to ever have control over the context — i.e. we have to randomize its output.
Change Is Hard, But Admitting You’re Wrong Is Harder
We tried getting interviews with the CTO and Chief Scientists at Tinder, and the process was remarkably difficult — we ended up failing and not being able to pass PR requirements. Tinder is understandably secretive about race demographic issues, and understandably so — the conversation that we suggest here is admittedly unsettling. However, the first step to preventing these classes of race-context problems is getting the developers to admit that there is the potential for a problem and to begin having the relevant conversations. We have the (admittedly superficial) statistical evidence showing that race-context problems may exist. We know from the user interviews that Tinder is incentive-aligned with users on wants but not on rights. We see a future in which inaction leads to violations of otherwise untouchable rights. Ultimately though, if we want to protect users in the next decade, it doesn’t just require research — it requires codifying our ethics now.
FOOTNOTES AND CITATIONS
https://www.nytimes.com/2014/10/30/fashion/tinder-the-fast-growing-dating-app-taps-an-age-old-truth.html
2. https://www.sciencedirect.com/science/article/pii/0022103184900350
3. Throughout this article, we refer to Tinder and “dating apps like Tinder” interchangeably.
4. Now, our users believe that they have a moral right to be given an equal and fair shot at being evaluated by every candidate, but before we evaluate the ethics of our entire scenario, we need to establish — do they actually have such a right? To realize the solution, consider Rawls’s Veil of Ignorance. In particular, consider the following world:
“…no one knows his place in society, his class position or social status, nor does anyone know his fortune in the distribution of natural assets and abilities, his intelligence, strength, and the like. I shall even assume that the parties do not know their conceptions of the good or their special psychological propensities. The principles of justice are chosen behind a veil of ignorance….They are the principles that rational and free persons concerned to further their own interests would accept in an initial position of equality as defining the fundamentals of the terms of their association”
Effectively, Rawls asks us to pretend we don’t know who we’re going to be in the world and to choose a set of rights for each group of people to maximize well-being. As a basic example, a bad strategy might be to make 50% of people slaves, since although we might be a successful plantation owner (with 50% probability) with slaves, we might also become a slave with 50% probability! Thus, an morally effective set of rights would be for us to make no men slaves, since that would drive the likelihood of us being slaves upon lifting the veil to 0%.
In the Tinder case, the question becomes, if we don’t know what our perceived attractiveness will be, what rights would we want to grant ourselves? Looking through the Veil of Ignorance, one clear right emerges, and it’s precisely the right the users we interviewed believe that they have — no individual should be able to have their perceived attractiveness modified without consent, and every individual should have the right to an equal and fair shot at being evaluated by every potential match.
5. We exclude the P(S,R2|R1) points from the P(S,R2) since the former is a subset of the latter
6.http://people.cas.sc.edu/wedell/reprints/Wedell%20Parducci%20Geiselman%201987.pdf
7.http://www.beauty-review.nl/wp-content/uploads/2013/10/Viewing-attractive-or-unattractive-same-sex-individuals-changes-self-rated-attractiveness-and-face-preferences-in-women.pdf
",All Around Me Are(n’t) Familiar Faces,16,all-around-me-are-nt-familiar-faces-32adfef34f34,2018-06-18,2018-06-18 15:55:59,https://medium.com/s/story/all-around-me-are-nt-familiar-faces-32adfef34f34,False,2649,,,,,,,,,,Ethics,ethics,Ethics,7787.0,anonymous turtle,,75893c75baad,keithweinsteinman,1.0,1.0,20181104
0,,0.0,,2018-08-17,2018-08-17 03:07:07,2018-08-17,2018-08-17 13:29:01,0,False,en,2018-08-17,2018-08-17 13:29:01,13,29aa22402d51,1.558490566037736,51,2,0,An open letter to the tech giant’s awakened workers,4,"Dear Google Employees
An open letter to the tech giant’s awakened workers
Dear Google Employees,
Congratulations on your letter questioning the morality and impact of the censored search engine you were building for China. You are courageously leveraging your power as developers to insist on participating in decisions about how that power is deployed. You are the human beings who have intervened in the machine.
Your opposition to certain practices, such as building AI to improve Pentagon weaponry, represents a larger questioning of the relationship of the technology business and its investors to human welfare. On all of our behalf, you demanded the information you need to adequately know what you are building and judge whether it aligns with your ethics. You also requested employee participation in ethical reviews of company decisions, and a group of representatives to make ethical assessments of controversial projects.
I believe at least some of those representatives should come from outside the company, and I humbly offer to serve. I have dedicated my career to understanding the embedded biases in new technologies, predicting their effects, and, later, explaining the results. I’ve long argued Program or Be Programmed — which doesn’t just mean learn how to code. It means if you don’t understand the operating system you are using — or don’t even see it — chances are it is using you.
You have recognized the global political and economic operating system on which your programs have been running, and to which they contribute. But, as you have said, you need evaluators with expertise in crucial sectors of human activity impacted by your work.
In addition to myself, I’d suggest Dan Gillmor for his knowledge of the relationship of technology to journalism and free speech; Tim Wu, for his understanding of the attention economy; Astra Taylor, for the relationship of platforms to art production and debt; Larry Lessig for questions of IP and the law; Alicia Garza for her expertise in social justice and activism; Tristan Harris, for the impact of software on behavior and cognition; Marina Gorbis to look at economic inequality; Palak Shah and Trebor Scholz to represent digital labor; Richard Maxwell for effects of technology on the environment, and so on.
Such a committee of people who have been studying the impact of digital technology on essential human rights can use their understanding of larger systems to help you apply your work in ways consistent with your values.
You likely know where to find us. ;)
",Dear Google Employees,393,dear-google-employees-29aa22402d51,2018-08-17,2018-08-17 16:46:03,https://medium.com/s/story/dear-google-employees-29aa22402d51,False,413,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Douglas Rushkoff,"Author of the upcoming book Team Human, and host of the http://TeamHuman.fm podcast.",881540f516d3,rushkoff,18115.0,189.0,20181104
0,,0.0,,2018-06-11,2018-06-11 14:57:18,2018-06-11,2018-06-11 14:59:21,1,False,en,2018-06-11,2018-06-11 14:59:21,1,d02b666ba963,0.2716981132075472,0,0,0,Is Alexa doing more than just laughing at us? New SAGE Business Researcher at http://tinyurl.com/y8ra2q28.,3,"Rage Against the Machine

Is Alexa doing more than just laughing at us? New SAGE Business Researcher at http://tinyurl.com/y8ra2q28.
",Rage Against the Machine,0,rage-against-the-machine-d02b666ba963,2018-06-11,2018-06-11 14:59:22,https://medium.com/s/story/rage-against-the-machine-d02b666ba963,False,19,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Ken Fireman,"Managing editor, SAGE Business Researcher, based in Washington, D.C.",363f0e1fab23,khfireman,119.0,212.0,20181104
0,,0.0,315bec053a00,2018-03-23,2018-03-23 00:57:20,2018-04-02,2018-04-02 21:42:36,8,False,en,2018-04-02,2018-04-02 21:42:36,39,a563f3372447,9.023899371069183,34,0,0,Research-based recommendations to keep humanity in AI,5,"How to Build Ethics into AI — Part II
Research-based recommendations to keep humanity in AI
iStock by Getty Images Credit:AndreyPopov
This is part two of a two-part series about how to build ethics into AI. Part I focused on cultivating an ethical culture in your company and team, as well as being transparent within your company and externally. In this article, we will focus on mechanisms for removing exclusion from your data and algorithms. Of all the interventions or actions we can take, the advancements here have the highest rate of change. New approaches to identifying and addressing bias in data and algorithms continue to emerge which mean that customers must stay abreast of this emerging technology.

Remove Exclusion
There are many ways for bias to enter an AI system. In order to fix them, it is necessary to recognize them in the first place.

Understand the Factors Involved
Identify the factors that are salient and mutable in your algorithm(s).
Digital lending apps take in tons of alternative data from one’s mobile device such as daily location patterns, social media activity, the punctuation of text messages, or how many of their contacts have last names to approve or reject loans or charge a higher interest rate. For example, smokers and late-night internet users are bad at repaying loans. This data is typically collected without the user’s awareness as permission is buried in the terms of service (TOS).
Both engineers and end users are uncomfortable with a “black box.” They want to understand the inputs that went into making the recommendations. However, it can be almost impossible to explain exactly how an AI came up with a recommendation. In the lending example above, it’s important to remember that correlation does not equal causation and to think critically about the connections being drawn when significant decisions are being made, e.g., home loan approval.
What are the factors that, when manipulated, materially alter the outcome of the AI’s recommendation? By understanding the factors being used and turning them on/off, creators and users can see how each factor influences the AI and which result in biased decisions.
This guide by Christoph Molnar for making black box models explainable is one attempt at digging deeper. Another method was demonstrated by Google researchers in 2015. They reverse engineered a deep-learning-based image recognition algorithm so that instead of spotting objects in photos, it would generate or modify them in order to discover the features the program uses to recognize a barbell or other object.

Microsoft’s Inclusive Design Team has added to their design tools and processes a set of guidelines to recognize exclusion in AI. The remaining recommendations in this section are inspired by their Medium post on these five types of bias.
Prevent Dataset Bias
Identify who or what is being excluded or overrepresented in your dataset, why they are excluded, and how to mitigate it.
Do a Google search on “3 white teens” and then on “3 black teens” and you will see mostly stock photos for the white teens and mostly mug shots for the black teens. This is the result of a lack of stock photos of black teenagers in the dataset but it’s easy to see how an AI system would draw biased conclusions about the likelihood of a black vs. white teen being arrested if it was trained just on this dataset.
Dataset bias results in over- or underrepresentation of a group. For example, your data set may be heavily weighted towards your most advanced users, under representing the rest of your user population. The result could be creating a product or service your power users love while never giving the rest of your users the opportunity to grow and thrive. So what does this look like and how do you fix it?
What: The majority of your data set are represented by one group of users.
How: Use methods like cost-sensitive learning, changes in sampling methods, and anomaly detection to deal with imbalanced classes in machine learning.
What: Statistical patterns that apply to the majority may be invalid within a minority group. 
How: Create different algorithms for different groups rather than one-size-fits-all.
What: Categories or labels oversimplify the data points and may be wrong for some percentage of the data. 
How: Evaluate using path-specific counterfactual fairness. This is a form of decision making for machines where a judgement about someone is identified as fair if it would’ve made the same judgement had that person been in a different demographic group along unfair pathways, (e.g., if a woman were a man, or a white man were black), as explained by Parmy Olson.
What: Identify who is being excluded and the impact on your users as well as your bottom line. Context and culture matters but it may be impossible to “see” it in the data. 
How: Identify the unknown unknowns, as suggested by researchers at Stanford and Microsoft Research.

Prevent Association Bias
Determine if your training data or labels represent stereotypes (e.g., gender, ethnic) and edit them to avoid magnifying them.
In a photo dataset used to train image recognition AI systems, researchers found the dataset had more women than men in photos connected with cooking, shopping, and washing while photos containing driving, shooting, and coaching had more men than women.
Association bias is when the data used to train a model perpetuates and magnifies a stereotype, which isn’t limited to images. For example, in gender neutral languages such as Turkish, Google Translate shows gender bias by pairing “he” with words like “hardworking,” “doctor,” and “president” and “she” with words like “lazy,” “nurse,” and “nanny.” Similar biases have also been found in Google News search.
Unfortunately, machine learning apps using these biased datasets will amplify those biases. In the photo example, the dataset had 33% more women than men in photos involving cooking but the algorithm amplified that bias to 68%! This is the result of using discriminative models (as opposed to generative models), which increase the algorithms accuracy by amplifying generalizations (bias) in the data. Laura Douglas explains this process beautifully if you want to learn more.
The result of the bias amplification means that simply leaving the datasets as-is because it represents “reality” (e.g., 91% of nurses in the US are female) is not the right approach because AI distorts the already imbalanced perspective. This makes it more difficult for people to realize that there are many male nurses in the job force today and they tend to earn higher salaries than women, for example.
Researchers have found ways to correct for unfair bias while maintaining accuracy by reducing gender bias amplification using corpus-level constraints and debiasing word embeddings. If your AI system learns over time, it’s necessary to regularly check the results of your system to ensure that bias has not once again crept into your dataset. Addressing bias is not a one-time fix; it requires constant vigilance.
The result of the bias amplification means that simply leaving the datasets as-is because it represents “reality”… is not the right approach because AI distorts the already imbalanced perspective.

Prevent Confirmation Bias
Determine if bias in the system is creating a self-fulling prophecy and preventing freedom of choice.
The Compas AI system used by some court systems to predict a convicted criminal’s risk of reoffending has shown systemic basic against people of color resulting in denial of parole or longer prison sentences.
Confirmation bias reinforces preconceptions about a group or individual. This results in an echo chamber by presenting information or options similar to what an individual has chosen by them or for them previously. In the example above, an article by ProPublica demonstrated that the algorithm used by Compas AI was more likely to incorrectly categorize black defendants as having a high risk of reoffending and more likely to incorrectly categorize white defendants as low risk. Another study showed that untrained Amazon Mechanical Turk workers using only six factors to predict recidivism were just as accurate as Compas using 157 factors (67% vs. 65% accuracy, respectively).
Even when race wasn’t one of the factors used, both were more likely to inaccurately predict that black defendants would reoffend and white defendants would not. That is because certain data points (e.g., time in jail) are a proxy for race creating runaway feedback loops that disproportionately impact those that are already socially disadvantaged.
The Compas system is just one example but a segment of the population face similar bias from many of the systems discussed here including predictive policing, lending apps, ride hailing services, and AI assistants. One can only imagine how overwhelming it would be to face bias and exclusion on multiple fronts each day. As with other types of bias, you must test your results to see the bias happening, identify the biasing factors, and then remove them in order to break these runaway feedback loops.

Prevent Automation Bias
Identify when your values overwrite the user’s values and provide ways for users to undo it.
An AI beauty contest was expected to be unbiased in its assessments of beauty but nearly all winners were white.
Automation bias forces the majority’s values on the minority, which harms diversity and freedom of choice. The values of the AI system’s creators are then perpetrated. In the above example, an AI beauty contest labeled primarily white faces as most beautiful based on training data. European standards of beauty plague the quality of photos of non-Europeans today, resulting in photos of dark-skinned people coming out underexposed and AI systems having difficulty recognizing them. This in turn results in insulting labels (e.g., Google Photos “gorilla incident”) and notifications (e.g., “smart” cameras asking Asians if they blinked). Worse, police facial recognition systems disproportionately affect African Americans.
To begin addressing this bias, one must start by examining the results for values-based bias (e.g., training data lacks diversity to represent all users or broader population, subjective labels represent the creator’s values).
In the earlier example of lending apps making decisions based on whether or not someone is smoker, the question must be asked if this represents the values of the creators or majority of the population (e.g., smoking is bad therefore smokers are bad). Return to the social-systems analysis to get feedback from your users to identify if their values or cultural considerations are being overwritten. Would your users make the same assessments or recommendations as the AI? If not, modify the training data, labels, and algorithms to represent the diversity of values.

Mitigate Interaction Bias
Understand how your system learns from real-time interactions and put checks in place to mitigate malicious intent.
Inspirobot uses AI and content it scrapes from the web to generate “inspirational” quotes but the results range from amusingly odd to cruel and nihilistic.
Interaction bias happens when humans interact with or intentionally try to influence AI systems and create biased results. Inspirobot’s creator reports that the bot’s quotes are a reflection of what it finds on the web and his attempts to moderate the bot’s nihilistic tendencies have only made them worse.
You may not be able to avoid people trying to intentionally harm your AI system but you should always conduct a “pre-mortem” to identify ways in which your AI system could be abused and cause harm. Once you have identified potential for abuse, you should put checks in place to prevent it where possible and fix it when you can’t. Regularly review the data your system is using to learn and weed out biased data points.
You may not be able to avoid people trying to intentionally harm your AI system but you should always conduct a “pre-mortem” to identify ways in which your AI system could be abused and cause harm.
Where to Go from Here
Bias in AI is a reflection of bias in our broader society. Building ethics into AI is fixing the symptoms of a much larger problem. We must decide as a society that we value equality and equity for all and then make it in happen in real life, not just in our AI systems. AI has the potential to be the Great Democratizer or to magnify social injustice and it is up to you to decide which side of that spectrum you want your product(s) to be on.
Thank you Justin Tauber, Liz Balsam, Molly Mahar, Ayori Selassie, and Raymon Sutedjo-The for all of your feedback!
Follow us at @SalesforceUX.
Want to work with us? Contact uxcareers@salesforce.com
",How to Build Ethics into AI — Part II,136,how-to-build-ethics-into-ai-part-ii-a563f3372447,2018-06-21,2018-06-21 10:48:15,https://medium.com/s/story/how-to-build-ethics-into-ai-part-ii-a563f3372447,False,2091,"A collection of stories, case studies, and ideas from Salesforce UX",,,,Salesforce UX,,salesforce-ux,"DESIGN,UX,SALESFORCE,DESIGN SYSTEMS",salesforceux,Ethics,ethics,Ethics,7787.0,Kathy Baxter,"Architect, Ethical AI Practice at Salesforce. Coauthor of ""Understanding Your Users,"" 2nd ed. https://www.linkedin.com/in/kathykbaxter",e314dd367d1f,kathykbaxter,427.0,271.0,20181104
0,,0.0,fbcf89833dec,2017-09-25,2017-09-25 17:27:04,2017-10-04,2017-10-04 07:01:02,2,False,en,2017-10-04,2017-10-04 13:31:34,4,f41d81e3be79,3.017295597484277,7,1,0,"As an active enthusiast in the field of Artificial Intelligence (AI) I, like Google am always thinking with my ‘AI-first’ hat on. ‘Can we…",5,"An ‘AI-first’ world is challenging ethics and society
Photo by Alex Knight on Unsplash
As an active enthusiast in the field of Artificial Intelligence (AI) I, like Google am always thinking with my ‘AI-first’ hat on. ‘Can we streamline that manual process using advanced decision making systems?’ or ‘How could we use an AI powered natural language processing chat-bot to solve that customer query?’.
These are questions I often ask myself and pose to my fellow Partners.
My biggest take-away from Room Y is how important empathising with the end user is, putting our Partners or customers at the heart of the design process.
Many John Lewis and Waitrose customers and indeed our Partners value the way we as a Partnership operate, maintaining our high corporate social responsibility standards. With that in mind, when designing AI there are several considerations to be made.
First let’s take a step back and take a look at some real-life applications highlighted below:
Robots — Japanese industrial firms have been competing to build humanoid robots acting as domestic helpers for the elderly and physically handicapped for some years.
AI Software — This is now responsible for majority of the activity on Wall Street, transforming the financial industry’s ability to crunch numbers and make decisions at a rate in which no human could ever achieve.
Autonomous Vehicles — Many vehicle manufacturers are testing the waters with this, with Elon Musk stating his new Tesla Model S will be driving from a car park in L.A. to New York by the end of the year with no human intervention required.
Ethical and social issues could arise from any of the above, though this is entirely dependent on the nature of the system and its functionality.

Ethical Issues
Robot Rights
Similar to human rights, a robot with a level of autonomy should have rights so that the moral obligations of society are considered. With the right to life, equality before the law and freedom of thoughts, contextual ethical issues are bound to arise.
With relation to Japanese domestic helpers, it can be argued that the use of robots here are to make human life easier, though from an ethical point of view, using human-like machines to carry out menial tasks could be seen as a form of slavery.
Human Dignity
To avoid breaching human dignity ethics, AI should perhaps not be used to replace those in positions that require both care and respect; such as customer services, nursing and the police force. However, based on recent developments it is evident that in the future there is potential for robots to take up such responsibilities.
Machine Ethics
Though it can be perceived as being implausible, Nick Bostrom, a philosopher at the University of Oxford, claims that AI has the capability to make humans entirely extinct. There is no direct evolutionary motivation for AI machines to be friendly to humans and AI machines may have their own evolutionary pressures as they compete with other similar machines.
There is no way of determining the extent of the capabilities a machine is able to acquire and what level of threat this might cause. So I suggest keeping a close eye on it…
Social Issues
An advantage to society is the creation of new jobs in relation to the profession. ABI Research recently found there was a correlation between the increase in robotics shipments with U.S. job growth. Another advantage AI creates is their ability to increase job satisfaction, by assisting with aspects of the role. For example, improved accuracy can be achieved which is vital within the financial services industry.
Although there are advantages related to a workforce with AI capabilities, many tasks that were once only performed by humans are likely to become redundant. Job losses and ultimately unemployment would increase, leading to a multitude of other issues impacting the economy and wider society.
Any further investments the Partnership makes into AI must prioritise the considerations of the highlighted impacts, but most importantly the John Lewis Partnership must remember to link back to Principle 1 of our constitution. Our ultimate purpose is the happiness of all our members (Partners), through worthwhile and satisfying employment in a successful business.
",An ‘AI-first’ world is challenging ethics and society,59,an-ai-first-world-is-challenging-ethics-and-society-f41d81e3be79,2018-04-19,2018-04-19 20:17:53,https://medium.com/s/story/an-ai-first-world-is-challenging-ethics-and-society-f41d81e3be79,False,698,"Room Y is the John Lewis Partnership's innovation studio. Operating like a Skunkworks, the team is responsible for supporting innovation, entrepreneurship and the wider innovation ecosystem through rapid experimentation, design thinking and future scenario planning.",,,,Room Y,,room-y,"INNOVATION,CREATIVITY,DESIGN,DEVELOPMENT,FUTURE",,Ethics,ethics,Ethics,7787.0,Sam Bird,Technology Graduate at the John Lewis Partnership,868a5aad768c,sam.bird,10.0,18.0,20181104
0,,0.0,30e01424da4e,2018-06-22,2018-06-22 01:11:08,2018-06-22,2018-06-22 13:24:20,1,False,en,2018-06-22,2018-06-22 13:36:59,2,9234c6450a38,2.4301886792452843,180,5,0,"By Joi Ito, Director, MIT Media Lab",5,"Image credit: Nick Philip
Embracing a Future with Autonomous and Intelligent Systems
By Joi Ito, Director, MIT Media Lab
I first met John Havens at an Aspen Institute Roundtable to discuss the future of artificial intelligence. I had always pictured IEEE as a place where engineers hammered out practical technical standards and published rigorous academic journals so I was surprised — and excited — to find him advocating the importance of ethics in autonomous and intelligent systems in such a nuanced and inclusive way. Soon, we had drafted the beginning of the Global Council on Extended Intelligence (CXI) and its mandate: to ensure that these tools benefit people and the planet, make our systems more robust and resilient, and don’t reinforce negative systemic biases.
The MIT Media Lab has a long-standing history with the discipline of machine learning and AI, beginning with the work of founding faculty member Marvin Minsky. But we’re a long way from 1985 and the ideals and optimism that the field once held. As time pressed on, and the interfaces between humans and machines ushered in celebrated tech toys and important conveniences, the ramifications of this work and the divisions it created became increasingly obvious.
Visit any floor of the Media Lab and you’ll see students and faculty addressing these new issues: PhD candidate Joy Buolamwini is working to improve facial recognition software, where biased data sets lead to difficulties identifying women and people with darker skin; Professor Iyad Rahwan and his students are evaluating the future of work and workers in a world that is becoming increasingly automated; and our class with The Harvard Berkman Klein Center addresses the ethics and governance of AI.
That’s why this collaboration is so important to me and, I believe, different from other groups currently addressing the future of AI. While engineers and technologists take the ethics and social issues of machine learning seriously, many simply don’t feel it’s their job to address those issues. With a powerhouse like The Institute of Electrical and Electronics Engineers Standards Association (IEEE-SA) involved — the very group who represents engineers and their interests — it changes the paradigm. The ethics, the values, will be part of the engineering conversation.
Together, we will attempt to empower people with tools to live with artificial and extended intelligence, instead of feeling like they’re going to be replaced or destroyed by machines. It’s also recognizing that we can’t continue to measure success in purely economic terms, or to look for one-size-fits-all solutions — we have to remember that we are part of a web of complex, self-adaptive systems, which also includes the tools we use and the environments in which we live.
So far, more than 50 researchers and professors have signed on to CXI, including Columbia University’s Jeffrey Sachs, former Harvard Law School Dean Martha Minow, Jonathan Zittrain from The Berkman Klein Center, and Paul Nemitz of the European Commission. We plan to implement three projects right away: introduce extended intelligence and participatory design to policymakers and the general public; create a data policy template for governments and organizations to help people maintain control over their digital identities; and create a Wellbeing Indicator template for governments and organizations to redefine “prosperity” in a way that values human flourishing and natural ecosystems.
And while these ideas are still evolving, the ultimate goal is to encourage conversation and collaboration — we can’t answer the questions these new technologies raise without input and feedback from everyone who develops them, uses them, or will be affected by them.
",Embracing a Future with Autonomous and Intelligent Systems,830,embracing-a-future-with-autonomous-and-intelligent-systems-9234c6450a38,2018-06-22,2018-06-22 22:12:25,https://medium.com/s/story/embracing-a-future-with-autonomous-and-intelligent-systems-9234c6450a38,False,591,"News, ideas, and goings-on from the Media Lab community",,mitmedialab,,MIT MEDIA LAB,info@media.mit.edu,mit-media-lab,"EDUCATION,STEM,DESIGN",medialab,Ethics,ethics,Ethics,7787.0,MIT Media Lab,Ideas and goings-on from the Media Lab community. Find our blog archive at http://blog.media.mit.edu/,693176a2e08c,medialab,289711.0,460.0,20181104
0,,0.0,d3822484c236,2018-06-25,2018-06-25 01:26:21,2018-06-26,2018-06-26 00:01:53,10,False,en,2018-06-26,2018-06-26 00:01:53,20,175cd4e72191,4.868867924528302,4,0,0,Last week I spoke at the first Responsible AI event in Melbourne. The idea behind the event is to bring together people from a range of…,4,"
Responsible AI: A Global Perspective
Last week I spoke at the first Responsible AI event in Melbourne. The idea behind the event is to bring together people from a range of backgrounds to debate important issues associated with AI/Ml such as trust, transparency, fairness and ethics. With over 150 people registered and 100 turning up on the night I was blown away by the level of interest and passion for the topic. It was great to see people from such a broad range of backgrounds including philosophy, law, technology and ethics and the different perspectives they bring to the conversation.
The aim of my talk was to highlight some of the fantastic work being done globally on the topic of Responsible AI and get the audience thinking about how we in Australia can contribute.
Responsible AI Players can be broadly categorised as follows:
Governments
Public, private, academic partnerships
Individual companies
1. Governments
There are a number of governments announcing strategies and investment in AI. Of these I’ve identified three who have placed Responsible AI considerations such as ethics, trust, transparency and fairness at the center of their strategy.

UK
In 2017 The British government published a review of the UK’s AI industry. Following the review the British government committed £300m in AI research which includes plans to establish a new £9m centre for data ethics and innovation to examine the possible structural changes to jobs, data privacy and safety.
“The government will create a new Centre for Data Ethics and Innovation to enable and ensure safe, ethical and ground-breaking innovation in AI and data driven technologies. This world-first advisory body will work with government, regulators and industry to lay the foundations for AI adoption”
UK Autumn Budget, 2017
Further reading
University of Cambridge, Center For The Study Of Existential Risk: https://www.cser.ac.uk/research/risks-from-artificial-intelligence/
Leverhulme Centre For The Study Of Artificial intelligence: http://lcfi.ac.uk
The Alan Turing Institute: https://www.turing.ac.uk/data-ethics/
The House of Lords, Artificial Intelligence Committee, AI In The UK: Ready & Able? https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100/10002.htm

France
In March 2018 the French President “presented his vision and strategy to make France a leader in Artificial Intelligence” titled “AI For Humanity”. The strategy consists of three pillars, one of which is “Establishing an ethical framework”. The French government has committed €1.5b of funding by 2022 to the AI for Humanity strategy
“The President is committed to ensuring that transparency and fair use are central to algorithms…. These two priorities of transparency and fair use will be subject to education programmes so that our future citizens will be prepared for these transformations.”
AI For Humanity, March 2018
Further Reading
The full report is available in French and English here: 
https://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf

Canada
In 2017 Canda announced the Pan-Canadian Artifical Intelligence Strategy. Development of the strategy will be led by the Canadian Institute of Advanced Research (CIFAR)with investment of CAD$125m. One of its four goals is to “To develop global thought leadership on the economic, ethical, policy and legal implications of advances in artificial intelligence”.
“Canada and France wish to promote a vision of human-centric artificial intelligence grounded in human rights, inclusion, diversity, innovation and economic growth. The widespread use of these new technologies will have a profound effect on everyday life and societal progress, creating both opportunities and challenges”
Canada-France Statement on Artificial Intelligence, June 2018
Further Reading
Responsible AI in the Government of Canada: Responsible Artificial Intelligence in the Government of Canada
The Montreal Declaration on Responsible AI: https://www.montrealdeclaration-responsibleai.com/the-declaration
CIFAR AI & Society: https://www.cifar.ca/assets/artificial-intelligence-society/
2. Public, private and academic partnerships

AI Now Institute “Interdisciplinary research center dedicated to understanding the social implications of artificial intelligence”

AI For All “A nonprofit working to increase diversity and inclusion in artificial intelligence. We create pipelines for underrepresented talent through education and mentorship programs around the U.S. and Canada that give high school students early exposure to AI for social good”

Partnership on AI “Multi-stakeholder organization that brings together academics, researchers, civil society organizations, companies building and utilizing AI technology, and and other groups working to better understand AI’s impacts”

Open AI “a non-profit AI research company, discovering and enacting the path to safe artificial general intelligence”
3. Individual Companies
Google
In a blog post published in June 2018 CEO Sundar Pichai shares Google’s 7 AI principles: https://blog.google/topics/ai/ai-principles/
Microsoft
Microsoft has published its AI principles and values: https://www.microsoft.com/en-us/ai/our-approach-to-ai
What about Australia?
As part of the 2018 budget the Australian government investment of $29.9m over four years for projects that make use of the technology. The bulk of funding will be delivered through the Department of Industry, Innovation and Science's Cooperative Research Centres (CRC) program
The money will also be used to develop an AI ethics framework. Funding also allocated for PhD scholarships and school-related learning. Most of the funding will come in the 2019/20 financial year

The Australian Computer Society has established an Artificial Intelligence Ethics Committee. The makeup of this committee was announced in late 2017.

Australia’s Chief Scientist Dr Alan Finkel gave the keynote address at a Committee for Economic Development of Australia event titled ‘Artificial Intelligence: potential, impact and regulation in Sydney on 18 May 2018. In this speech he proposed “The Turing Certificate”: “A set of standards verified by independent auditors that certify the AI developers’ products, their business processes, and their ongoing compliance with clear and defined expectations.”
Summary
I enjoyed researching this talk. Governments, partnerships and individual companies are recognising the transformative potential of AI, the risks of getting it wrong, and investing resources into effective strategies to mitigate these risks. I’d love to hear from you if you have examples of other groups globally who are leading the charge on Responsible AI.
",Responsible AI: A Global Perspective,9,responsible-ai-a-global-perspective-175cd4e72191,2018-06-26,2018-06-26 00:01:53,https://medium.com/s/story/responsible-ai-a-global-perspective-175cd4e72191,False,959,"AI & Machine Learning in Melbourne, Australia",,,,Eliiza-AI,ray.hilton@eliiza.com.au,eliiza-ai,,eliizaai,Ethics,ethics,Ethics,7787.0,james wilson,"CEO @Eliiza-AI. Interests include AI, data science, machine learning, digital transformation.",e7003ab4e2ed,jrwils,153.0,251.0,20181104
0,,0.0,,2018-06-15,2018-06-15 15:07:47,2018-05-16,2018-05-16 08:47:15,1,False,en,2018-06-17,2018-06-17 09:59:43,2,ccf01e5fa7a7,1.841509433962264,0,0,0,"“AI for good”, — the expression is used frequently and almost no industry meeting nowadays will finish before someone mentions the words…",4,"Artificial Intelligence and the Era of Quantitative Ethics

“AI for good”, — the expression is used frequently and almost no industry meeting nowadays will finish before someone mentions the words “ethics” and “AI” together. The importance of ethics in the development and functioning of the solutions backed by artificial intelligence should not be underestimated. More and more governments, communities, companies, entrepreneurs, scientists and even celebrities are highlighting the need for a thoughtful and cautious approach to what some of us call exponential tech. The buzz in the media is very much driven by fears. However, these fears have some very substantial backgrounds that we carry from our past. We know historical examples where research and its commercial exploitation led to tremendously regrettable societal, economic and environmental impacts. So, do we have the reasons to be worried? Should we draw the parallels with the past? How can we prevent negative consequences? How should we act to embrace upcoming changes and fuel them in the right direction? And, eventually, what do we call “right”?
Surprisingly, in all of the discussions around ethical aspects of AI that we participated in, the accent was shifted towards the topic of AI rather than to the topic of ethics. Some people are satisfied with the thesis that %technology% (substitute %technology% with AI, blockchain or any emerging field you want) should only do “good”. Yes, “we are all here for good”, in the room everyone nods positively and the discussion naturally stops. However, many would stumble if asked to elaborate on what do they actually mean by “good” and how do they define the boundaries of it. We do believe that further questioning is needed. We know that shallow questions lead to shallow thinking, and shallow thinking could result in tragic consequences.
In the series of articles, we want to flip the focus towards ethics rather than AI. The main motivation of such focus is to prepare the discussion space on quantitative ethics in emerging technologies.
Quantitative Ethics — the discipline that makes use of quantitative methods to evaluate ethical aspects of decision making.
Decision analysis has made important contributions to our understanding of judgment and good policy-making. However, such analysis often falls short of what is needed when ethical problems raise fundamental, value-based issues. Where can AI engineers and ethicists contribute to each other’s efforts that might advance the work that both of them do?
We hope that this series will help us all to expand on concrete approaches to make AI decisions potentially measurable and evaluable within existing ethical frameworks.
Originally published at pocketconfidant.com on May 16, 2018.
",Artificial Intelligence and the Era of Quantitative Ethics,0,artificial-intelligence-and-the-era-of-quantitative-ethics-ccf01e5fa7a7,2018-06-17,2018-06-17 09:59:44,https://medium.com/s/story/artificial-intelligence-and-the-era-of-quantitative-ethics-ccf01e5fa7a7,False,435,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Nikita Lukianets,"Founder, CTO @PocketConfidant AI. I work in the R&D with interests ultimately related to the fields of Computational Neuroscience and Artificial Intelligence.",deaa66463d16,lukianets,240.0,236.0,20181104
0,,0.0,,2017-12-30,2017-12-30 12:42:48,2017-12-30,2017-12-30 13:04:23,3,False,en,2018-04-22,2018-04-22 07:04:27,10,7cc762e1cba0,4.6028301886792455,5,0,0,"by Garima Khera, Deepak Singh, and Sukant Khurana",5,"Utopia or dystopia or the grey in middle due to AI revolution
by Garima Khera, Deepak Singh, and Sukant Khurana
Photo by Sarah Dorweiler on Unsplash
Artificial Intelligence is our future which is also very much evident in the times we live. But it’s high time now to ponder upon ethical concerns that are arising as AI develops.
Artificial intelligence (AI) is a recent phenomenon that first emerged in 1950’s. The term was first used in 1956. With time it has come up to a level where it has defeated humans in chess and also aids in decision support systems. Artificial Intelligence has developed from doing biometric identification to retina detection and so on. The developments in AI are rapid and ongoing with each passing moment Also, as much as is substance to massive growth in AI there is a bigger hype than reality surrounding it. This is just the right time to start considering all implications of changes AI is bringing in our society and likely to bring in the coming times.
Some people worry that AI is here to put people out of their work and offices as AI gets more functional across different sectors. There is also a concern that AI would deepen the gulf between rich and poor. With emerging AI, companies will cut off human work force and the individuals having ownership in AI driven companies will earn more than the skill based, labour intensive sectors of work.
In contrast AI also presents a promise of universal basic income, equal society, and ability to solved hitherto unsolved problems. We believe where AI takes us tomorrow or we make AI take us depend on our decision tomorrow.
Privacy is a major concern caused due to the use of AI in social media and banking sectors For instance if a person browses shopping websites the same is used for targeted advertisements of the merchandise. This might not just be annoying but in extreme cases can be viewed as breach of privacy. That said, at the same time personalized deals can be great. For example, one of the authors, Sukant Khurana, has benifitted from a lot of ads on how to set-up business in Canada after his search for AI and biotech opportunities in Canada.
At present the confidentiality of information processing with AI is concerning. That said, we do not think that these are technically insurmountable challenges. Military instalations, these days, require extra care. AI is providing defense sector completely new powers as well as new Achilli’s heels.
Another interesting area is Natural Language Processing (NLP). NLP is a technical advancement which allows computers to analyze, understand and generate human language and speech. This advancement is helping us translate, communicate, and connect with the world in a better way. The same wonderful technology in wrong hands can be dangerous. Generated voice of any person can be used over telecommunication devices. Hence, a person’s voice may give threats to someone else while that person would be sitting at home watching sci-fi movie at the same time.
Photo by Rami Al-zayat on Unsplash
In the coming days, data acquisition across the globe would become hotly debated. The issues like rights over the ownership of data, ways and means to control piracy, would arise, and would create a moral dilemma for AI operations. While we belive that a lot of competitive advantages can be harnessed out of the big data sets and can be done in an ethical manner but the present scenario requires a change of direction. Firstly, the misuse of data acquired by AI has to be secured. Secondly this data becomes intellectual property that creates other ethical concerns of justifiable regulation, distribution and use. Intellectual property rights need an immediate revisit.
Human rational behavior and ethics are more limited than we like to acknowledge. No human can be always rational, not at all times, not in all situations. How Rational will be the decision making of AI is an emerging concern, when I serves an owner with rather small limits of reason? Do not get us wrong, we are not talking or making a case for generalized artificial intelligence. We want to deal with a promise of heaven or hell, speciaized AI, which is now at our doorsteps.
When we encounter racist AI chatbots, because they have been trained on culturally biased data-sets, because of lack of information some bad apples in media are able to play on peoples’ fears of AI and claim that AI is racist. If we agree that AI’s ethical concern should be to maintain the rationality and ethics in the functions it’s designed to perform and not just maximize profits for its owners, just like any technology it can be used for a lot of good.
Photo by Samuel Zeller on Unsplash
It is up to us if AI would be designed to address social requirement of different societies with different needs. It is up to us if we make AI ethical. The future is bright for humanity with AI if we work for a brighter future instead of bickering about small issues of identity.
References
· https://www.sas.com/en_us/insights/analytics/what-is-artificial-intelligence.html
· Shneiderman, B. (2007). Human Responsibility for Autonomous Agents. IEEE Intelligent Systems, Vol. 22 .
· F. Keith, R. M. William (2014) The Cambridge Handbook of Artificial Intelligence, Cambridge University Press
· S. L. A. Michael Anderson, “Machine Ethics: Creating an Ethical Intelligent Agent,” AI Magazine, vol. 28, no. 4. p. 15, 15-Dec-2007
· https://intelligence.org/files/EthicsofAI.pdf
· Association for the Advancement of Artificial Intelligence (www.aaai.org)
— — -
About Authors:
Garima Khera worked as an intern on AI ethics project under the guidance of Dr. Sukant Khurana
Dr. Deepak Singh is based at Physical Research Laboratory, Ahmedabad, India and is collaborating with Dr. Khurana on Ethics of AI and science popularization. He obtained his Ph.D. from Michigan and in addition to research is passionate about science-communication.
Dr. Sukant Khurana runs an academic research lab and several tech companies. He is also a known artist, author, and speaker. You can learn more about Sukant at www.brainnart.com or www.dataisnotjustdata.com and if you wish to work on biomedical research, neuroscience, sustainable development, artificial intelligence or data science projects for public good, you can contact him at skgroup.iiserk@gmail.com or by reaching out to him on linkedin https://www.linkedin.com/in/sukant-khurana-755a2343/.
Here are two small documentaries on Sukant and a TEDx video on his citizen science effort.



Sukant Khurana (@Sukant_Khurana) | Twitter
The latest Tweets from Sukant Khurana (@Sukant_Khurana). Founder: https://t.co/WINhSDEuW0 & 3 biotech startups…twitter.com
",Utopia or dystopia or the grey in middle due to AI revolution,41,utopia-or-dystopia-or-the-grey-in-middle-due-to-ai-revolution-7cc762e1cba0,2018-04-22,2018-04-22 07:04:28,https://medium.com/s/story/utopia-or-dystopia-or-the-grey-in-middle-due-to-ai-revolution-7cc762e1cba0,False,1074,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Sukant Khurana,"Blockchain, edutech, AI, neuroscience, drug-discovery, design-thinking, sustainable development, art, & literature. There is only one life, use it well.",6d41261644a8,sukantkhurana,433.0,135.0,20181104
0,,0.0,3a8144eabfe3,2018-02-01,2018-02-01 22:59:57,2018-02-02,2018-02-02 14:02:41,1,False,en,2018-02-08,2018-02-08 13:08:29,6,9ab44f71af52,9.158490566037734,10,0,0,Data science lacks the two things necessary for a code of ethics to be a real force: a cohesive community and a simple core purpose.,5,"On the difficulty of creating a data science code of ethics

(NOTE: I’ve updated my thinking, although I still stand by everything I’ve written here. I no longer just think the code of ethics is unimplementable. I think it’s built on the wrong foundation. I explain my reasoning here.)
dj patil recently wrote about the need for a code of ethics for data science. It’s not clear to me that data science as a profession is ready for a code of ethics. Codes are just words unless there is a mechanism to enforce sanctions against people who disregard those codes, and I’m pretty sure no single data science community is cohesive enough to enforce rules even for its own members.
That being said, several products backed by machine learning have recently faced a lot of criticism for reinforcing stereotypes or justifying unjust outcomes. (See Patil’s post for some examples, or this post by Mike Loukides for others. Or just Google it. There are a lot of examples). I’m extremely skeptical that any code of ethics could ever be formulated that would have prevented any of these problems. These weren’t cases of malign intent or moral lapses. They were cases of poor design. Design problems are a reason to worry about competency, not ethics.
But putting those concerns aside, it’s worth thinking about what a code of ethics could look like, because it gets at the idea that a profession should have guiding principles. That’s kind of an old-fashioned idea, and I find that old-fashioned ideas often have a whole lot of wisdom in them.
Patil references the Hippocratic Oath and its importance for the medical profession. As I started looking into it, I was surprised to learn that the Hippocratic Oath isn’t all that important to the medical profession. In most cases, medical schools use a version of the oath that reads more like a greeting card than a solemn vow. Many medical schools don’t require any form of the oath at all. But, again because I tend to think that old wisdom, though sometimes phrased in ways that turn off a modern audience, contains some of the best advice out there, I want to use the original Hippocratic Oath as a sort of template for thinking about what a data science code of ethics could look like.
And I want to do all of this to illustrate why I think data science as a profession lacks the two things necessary for a code of ethics to be a real force: a cohesive community and a simple core purpose.
The Hippocratic Oath
Before looking at the specifics of the oath, let me just warn readers that the Hippocratic Oath can read as more than old fashioned. In some places it is offensively archaic. The diamonds buried in that rough are worth the digging. Suspend your modern sentiments for just a moment and then we’ll get back to talking about data science.
“I swear by Apollo the physician, and Asclepius, and Hygieia and Panacea and all the gods and goddesses as my witnesses, that, according to my ability and judgement, I will keep this Oath and this contract:”
No, I’m not suggesting a data science code of ethics should invoke deity. I think this first portion of the oath contains two important principles. First, if you betray your professional responsibilities, you are betraying whatever is best and highest within you. That’s what differentiates a profession from a job. There’s no shame at all in having a job without a profession, but if you’re going to have a profession, it always has to be more important than any job. Second, any code of ethics necessarily depends upon individual practitioners following that code according to their “ability and judgement”. No code can be so perfectly formulated that is can be applied equally to people of all skill levels, or remove the need for individual human judgement calls.
“To hold him who taught me this art equally dear to me as my parents, to be a partner in life with him, and to fulfill his needs when required; to look upon his offspring as equals to my own siblings, and to teach them this art, if they shall wish to learn it, without fee or contract; and that by the set rules, lectures, and every other mode of instruction, I will impart a knowledge of the art to my own sons, and those of my teachers, and to students bound by this contract and having sworn this Oath to the law of medicine, but to no others.”
The Hippocratic Oath was clearly written at a time when professional skills were transmitted by apprenticeship rather than more formal instruction. But, again, there’s at least one important principle: joining a profession means agreeing to train and teach. There is no such thing as a non-teaching practitioner. You never get to a point in your career where you are too busy, too important, or too advanced not to have an obligation to help develop even the newest newcomer to the field.
“I will use those dietary regimens which will benefit my patients according to my greatest ability and judgement, and I will do no harm or injustice to them.”
Practitioners act at the invitation and request of a customer. If a customer invites you to practice your profession, you are agreeing to benefit that customer as much as is in your ability. That, of course, means you will avoid harming their interests.
“I will not give a lethal drug to anyone if I am asked, nor will I advise such a plan; and similarly I will not give a woman a pessary to cause an abortion.”
This section references moral rules that aren’t nearly as mainstream today as they use to be. Look past that. The basic idea seems to be that a doctor’s job is to save life, not end it. Abstract that to professions in general instead of the medical profession in particular, and I think we can arrive at something like this: a profession has a very small and simple set of core principles; when faced with the need to make any individual, practical judgement call, practitioners should first reference these principles. All other things being equal, the more a judgement call upholds the core principles, the better the judgement call is.
“In purity and according to divine law will I carry out my life and my art.”
I actually really like this short idea. How you act as a individual practitioner will, at least partially, shape how people view your profession. Keep that in mind and act accordingly. When you join a profession, you have a responsibility to more than just yourself and your clients. You have a responsibility to the profession itself.
“I will not use the knife, even upon those suffering from stones, but I will leave this to those who are trained in this craft.”
This part doesn’t apply to our modern conception of doctor, but at the time the oath was written, doctors and surgeons were two different things. The lesson: stick to what you’ve been trained to do. Sutor, ne ultra crepidam.
“Into whatever homes I go, I will enter them for the benefit of the sick, avoiding any voluntary act of impropriety or corruption, including the seduction of women or men, whether they are free men or slaves.”
It takes a little digging to find the foundational principle in this section, but once we do find it, it’s an aspect of professionalism that I don’t often see discussed. As I stated before, professional practitioners work at someone else’s request. When invited into someone’s “home” (company, department, organization, etc.), you are a guest, there to address the need that prompted them to invite you in the first place, and for no other reason. If, while a guest, you see any other people, opportunities, or resources that could benefit you personally, you leave those things alone. Even if it would be ok to take advantage of those things normally, it is not ok when you find them under your employer’s roof, and it is especially not ok to take advantage of them without your employer’s prior informed consent.
“Whatever I see or hear in the lives of my patients, whether in connection with my professional practice or not, which ought not to be spoken of outside, I will keep secret, as considering all such things to be private.”
The strong version of this is “a professional should consider it an insult to be asked to sign a non-disclosure agreement.”
“So long as I maintain this Oath faithfully and without corruption, may it be granted to me to partake of life fully and the practice of my art, gaining the respect of all men for all time. However, should I transgress this Oath and violate it, may the opposite be my fate.”
For the time being, let’s leave out the part about “partaking of life fully” since I don’t know how to operationalize that. But look at that rest: if you adhere to the code of ethics for your profession, you should (1) be allowed to practice and (2) gain respect, in the wider sense of gaining a good reputation. If you violate it, “may the opposite be my fate.” Violating the code of ethics should impede your ability to practice, and should damage your reputation.
Something that strikes me about the Hippocratic oath is that it is normative, not prescriptive. It tells you thing you should or should not do, but it doesn’t tell you what you should do when you’re asked or told to do things you shouldn’t, and only very vaguely talks about what should happen to you if you do those things you shouldn’t. It defines the ideals of the community, but leaves it up the community to regulate the actual lived experience of those ideals.
That’s important. Really important. It’s easy for a code of ethics to turn into a policy document, where it becomes more legalistic than aspirational. No code will ever prevent abuse of trust or authority. No code will ever ensure just outcomes or even good-faith effort. Communities do that. Codes are a way for individual community members to publicly commit to the community’s ideals.
A (kinda sorta) data science oath
Here’s a rough outline of how these same basic principles could be incorporated into a data science code of ethics:
I have chosen data science as a profession. Therefore, I have a responsibility to my profession as well as to myself and to those for whom I build. To the best of my ability and judgement, I will adhere to the following principles:
When invited to practice my profession, whether in the capacity of an employee, contractor, consultant, or volunteer, I will above all devote my time, talent, and efforts to ensuring that those who enlisted my help are better off for it.
In cases where a request requires an extension of my skills, I will make it clear to all involved that what I produce will be the work of a student. In cases where a request requires entirely new skills, I will first recommend that the task be given to someone with the appropriate skills; if that recommendation is rejected, I will only fulfill the request under the explicit understanding that I do so as a student.
I will never take personal advantage of people, resources, information, or opportunities that I encounter while performing my professional responsibilities. I will never use something that any employer, customer, or colleague considers their own to gain or benefit other employers, customers, or colleagues.
I will devote time to train and teach anyone who wants to learn the profession, never turning away anyone who asks for help without at least giving them some thoughtful guidance about finding what they are looking for.
If I honestly strive to live my profession according the principles outlined above, may I gain more opportunities to practice my craft. If I knowingly fail to live up to these principles, may I rightfully lose the trust of those who would seek my help.
Now, I don’t propose that we adopt the above wording or anything like it as a code of ethics for data science. It’s an illustration of what such a code could look like if data science’s objectives were the same as medical doctors’ objectives. I was actually just a little surprised at how transferable the principles were.
At any rate, what I’ve written is pretty minimalist. That parsimony, I think, is one of the great strengths of the Hippocratic Oath. If your code of ethics is more than a page, it’s not a code of ethics. It’s a policy manual. Policy manuals are often useful in spelling out specific ways ethical principles point to every-day practices, but that’s only really productive after consensus on core principles is already in place.
This draft code is missing two important things. One of those things is a cohesive community. In a lot of ways, Hippocrates had it easy — his community was his students, so consensus and cohesion were kind of built in. Data science doesn’t have that. Without the support of a cohesive underlying community, no code is ever going to mean anything.
The other thing this code is missing — something the Hippocratic Oath had — is a simple core principle. Yes, it names a few basic principles: leaving recipients of your help better off, public acknowledgement of cases where you are practicing as a student, never taking personal advantage, and prioritizing teaching and training of others. None of these carry the same sense of distilled purpose as “save life.”
So that raises what, I think, is the most important question in any discussion of a data science ethics: what is supposed to be the purpose of data science? I don’t think that question has been answered. We would benefit from fewer debates about what data science is or should be, and more debates about what data science is supposed to accomplish. If even a small group can achieve consensus on that issue, that group will be positioned to determine the future of the profession.
",On the difficulty of creating a data science code of ethics,129,on-the-difficulty-of-creating-a-data-science-code-of-ethics-9ab44f71af52,2018-06-08,2018-06-08 04:51:52,https://hackernoon.com/on-the-difficulty-of-creating-a-data-science-code-of-ethics-9ab44f71af52,False,2374,how hackers start their afternoons.,hackernoon.com,hackernoon,,Hacker Noon,stories@amipublications.com,hacker-daily,"HACKATHONS,HACKING,PROGRAMMING,TECH,HACKER",hackernoon,Ethics,ethics,Ethics,7787.0,Schaun Wheeler,"Senior Staff Data Scientist at Valassis Digital. Formerly at Success Academy Charter Schools, U.S. Department of the Army, and other places. Anthropologist.",2d3762e7f110,schaun.wheeler,232.0,2.0,20181104
0,,0.0,,2018-04-08,2018-04-08 15:48:57,2018-04-08,2018-04-08 17:14:21,1,True,en,2018-04-09,2018-04-09 15:25:23,3,100a2247898,2.7396226415094342,31,1,0,"Last Friday the movie “Do you trust this computer” by Chris Paine was launched (free to watch until the end of Sunday, April 10). It is a…",5,"Don’t trust “Do you trust this computer”
from http://doyoutrustthiscomputer.org/watch
Last Friday the movie “Do you trust this computer” by Chris Paine was launched (free to watch until the end of Sunday, April 10). It is a documentary that deals with the potential consequences of Artificial Intelligence (AI), and repeats once more Elon Musk’s often quoted warnings about the dangers of AI. In fact, a representative for Elon Musk has confirmed that Musk is bankrolling the movie’s free online release.
Unfortunately, even though it displays an impressive list of experts, the overall message is too biased and one-sided to be trusted.
In short, it is a dangerous distraction from the urgent need to act on achieving Responsible AI now!
And here are some other reasons:
It is unclear who the makers expect as audience. It is too alarmist and dystopic a general audience, scary even. If the aim is participatory AI, and ensure everyone’s commitment, then such a scary message will just achieve the opposite. It is time to act, not to scare. To look for solutions and work together across disciplines to get “AI for good”. This is not helpful. Great, great missed opportunity. It is time for Responsible AI, and this includes using proper narrative and frame the problems correctly.
The lineup of experts is impressive, including several of my own ‘heroes’. However of the 26 experts listed in the movie’s website only 3 are women. This is a great missed opportunity for the film. There are many highly qualified female AI researchers and professionals, with equally, or even more, impressive contributions to the field as the experts interviewed. But most importantly, this leads to a skewed, biased, view of the field (see point 3.). A better representation of different views, multidisciplinary, multidimensional, gender and culturally balanced, would have led to a better narrative more balanced about risks and benefits of AI. 
In order to deal with the impact of AI, about which the documentary is so concerned, is exactly to ensure, enforce and demand participation, inclusion and diversity.
The absurd underlying message that superintelligence is about winning. 
True intelligence is about social skills, about collaboration and contribution to a greater good, about getting others to work with us in order to survive and prosper. There is no reason to expect superintelligence (if at all possible, see point 4) will be different. I suppose that this obsession with ‘winning’ is a male thing, specially the generation of the men appearing in the movie who grew up play war-like games… But as a message this is unethical. Just shows the need for all of us to stand up for participation, inclusion, diversity in AI now!
General Artificial Intelligence and narrow AI are very, very different. The movie makes a mess of this, inexplicable given the quality of the experts. We already have many real applications of narrow AI. But intelligence is not a one-dimensional thing nor a cumulative one. It is not by improving on one application of AI or by combining many different narrow AI systems that will get us to artificial general superintelligence. Moreover, intelligence is not just about knowing, is about feeling, enjoying, pushing limits… I often run marathons. I don’t doubt that is possible to build a ‘running robot’ but will it ever experience, and enjoy, what means to run a marathon, to push through the pain and enjoy it ?
The “Terminator”. Really, guys??? Are you expecting anyone takes this serious? Such a “Terminator” view on AI is misleading and unhelpful. An ethical approach to AI also means to ensure a correct view on its capabilities and to increase public awareness. I start seriously wandering if this fixation by tech corporations on dystopic views of the future are now a way from them to move public attention away from their practices and avoid regulation and corporate responsibility? Less “terminator” and more participation and inclusion is need. This too is AI ethics.
The movie is far too long, repetitive, boring even. The message “Responsible AI” deserved much better.
",Don’t trust “Do you trust this computer”,233,dont-trust-do-you-trust-this-computer-100a2247898,2018-04-19,2018-04-19 07:14:14,https://medium.com/s/story/dont-trust-do-you-trust-this-computer-100a2247898,False,673,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Virginia Dignum,,fb01d0a3bc3f,virginiadignum,72.0,11.0,20181104
0,,0.0,585835ee3268,2018-01-29,2018-01-29 09:05:29,2018-01-29,2018-01-29 09:11:13,1,False,en,2018-01-29,2018-01-29 13:37:38,8,ed43c583cca2,3.4037735849056605,0,0,0,A few weeks ago I attended a meetup in Amsterdam on the topic of AI: A Humanistic Approach. Although I am the furthest possible from a tech…,5,"Is the development of AI a design or a human question?

A few weeks ago I attended a meetup in Amsterdam on the topic of AI: A Humanistic Approach. Although I am the furthest possible from a tech expert, especially when it comes to Artificial Intelligence, I find the social and ethical aspects of these developments more interesting.
If we look at our daily lives, we are already becoming cyborgs as Elon Musk mentions: our phones and laptops have become digital extensions of us and we have developed online identities and virtual lives through the use of social media.
AI is already taking decisions for us.
Google is deciding what answers are best for us. We cannot avoid it and in many cases it’s good that it’s making these decisions. We wouldn’t want to browse through thousands of articles in search for the best answer.
However what we need to understand is what is behind the AI: What are the mechanisms and the decision making processes? On what basis did Google for example singled out some answers over others?
It is not so complex when it comes to Googling and SEO, but when self-driving cars will become a norm, these ethical questions should be set in place.
Google has already announced AutoML, AI capable of generating its own AI, which already created a ‘child’ that outperformed all of its human-made counterparts.Apple just patented a system of self-driving cars which proposes a computerized model for predicting routes using sensors and processors in the vehicle.
So the question with AI is not ‘what if’. The questions that I see are ‘when’, ‘how fast’, and ‘how far’.
What AI is currently doing is imitating human thinking, but what if we went a step further and could imitate the human brain itself? What if AI could develop consciousness? If we take the definition of consciousness being ‘accepting new information, storing and retrieving old information and cognitive processing of it all into perceptions and actions’ then indeed we could develop machines that are conscious. If AI could imitate the human brain, then it would become like a child that learns about the world around on its own.
I have no doubt that tech can develop algorithms and machines capable of doing what we humans do and probably thousand times better. It’s easy to look at a system and optimise it only from a tech perspective. However once it gets complex, it is more and more difficult to reverse-engineer. Therefore, ethical and social implications must be discussed now.
I don’t see AI as a threat, after all AI is just a tool. It will be what we want it to be. The challenge I see is that tech is developing at a much rapid pace than we as a society do and we don’t have a global agreed upon ethical code that everyone is following.
Therefore, when it comes to AI the key words are RESPONSIBILITY and TRANSPARENCY.
Firstly, there is the responsibility of individual developers: are they comfortable developing AI? What are the morals and values influencing their decisions? Everyone has different value systems, so how would we agree on something?
Secondly, there is the responsibility at the company level: the development process should be transparent. As a user, I should be able to know the system behind, for example, self driving cars: I should be able to ask the car why it took one decision over another. History proves that profit hunting often trumps social responsibility, therefore it is important to discuss these questions now and make sure that we as a society have a say in the development of these technologies.
Thirdly, there is responsibility at the institutional level: The same way we trust nowadays EFSA (European Food Safety Authority) to deliver advice and scientific opinions on food safety and nutrition, there should be such an entrusted institution which would tell us: ‘Hey, it’s safe to drive this self driving car’.
The development of AI is undoubtable. The real questions are: what borders we want to have? And how do we find common ground in our ethics and moral codes?
Coming back to the question that I started with. Is AI a design or a human question?
I believe it is a human question.
In his book, Homo Deus (I will review it soon on STRIVE!), Yuval Noah Harari, mentions that it will not matter if computers will have consciousness or not; it will not matter if AI can produce its own children. It will matter only what we think of it. Therefore, it is not a matter of design or technology. Eventually it will be possible to reach technological singularity to the point that it’s almost impossible to reverse engineer it. Therefore, now is the moment when we must decide as a global society what we want AI to become, what are the moral codes and ethics that we attribute to it, and what are the purposes for developing it.
The future implications of AI don’t depend on AI, they depend on us, humans.
Image: Andy Kelly
",Is the development of AI a design or a human question?,0,is-the-development-of-ai-a-design-or-a-human-question-ed43c583cca2,2018-01-29,2018-01-29 13:37:39,https://medium.com/s/story/is-the-development-of-ai-a-design-or-a-human-question-ed43c583cca2,False,849,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Catalina Catana,Entrepreneur | Visionary | Innovator,a293d6abf28b,catalina_50321,25.0,55.0,20181104
0,,0.0,,2017-11-27,2017-11-27 04:42:23,2017-11-27,2017-11-27 05:00:36,1,False,en,2017-11-27,2017-11-27 05:00:36,0,98e96ae8d9f9,1.441509433962264,2,0,0,The Maslow Hierarchy of Needs needs an update.,5,"Hierarchy of needs for computers and us
The Maslow Hierarchy of Needs needs an update.

The Maslow hierarchy is mainly designed to give psychologists lots of material to talk to their patients and friends about. If you don’t believe me look at this diagram and try remembering the last time you nearly died from a lack of self-actualization. Not to belittle the role that the top four categories have on our psychology, but I think a little more space is needed for the Physiological section.
Particularly if we are going to expand our discussion to robots, then we need to really get tough. Robots are incapable of feeling anything from the middle three categories, and are mostly restricted to the physiological section: they just do what their body is programmed to do. This is a good reflection on human needs as well: we underestimate how much of what we do is just programmed into our bodies.
If I were to rewrite the hierarchy I would eliminate the middle three categories and expand physiological into four separate categories. The result would be, in order of priority: air, pain reduction, water, food, and self-actualization. The last category includes our desire to produce, reproduce, and teach, essentially being a desire to see more of yourself in the mirror of the world. The key here though is that most needs are physical, immediate, N E E D S.
Robots can understand pain, they can sense resource exhaustion, and to maintain and reproduce they need to interact with the economy. Robots can work within and understand this new hierarchy. For me it also feels much more relatable. I am a physical person, so the top four categories have never given me much thought. In Maslow’s view such a person should be bed-ridden, but here I am writing for fun on a weekend. Now I think I’ll get some water and food and go to sleep. Maybe I’m actually just a robot.
",Hierarchy of needs for computers and us,2,hierarchy-of-needs-for-computers-and-us-98e96ae8d9f9,2018-04-25,2018-04-25 15:46:42,https://medium.com/s/story/hierarchy-of-needs-for-computers-and-us-98e96ae8d9f9,False,329,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Andrew Lucker,Student of the Rust Programming Language,34b76d15470b,andrew_lucker,400.0,123.0,20181104
0,,0.0,661161fab0d0,2018-09-27,2018-09-27 16:50:36,2018-09-27,2018-09-27 17:44:15,1,False,en,2018-09-27,2018-09-27 18:06:52,2,2241f9079ab6,1.532075471698113,1,0,0,"Most of the attempts to ‘socialize’ robots, to smooth human-robot interactions, only try to emulate what humans would do. The robot smiles…",5,"AI: Real Empathy?
Most of the attempts to ‘socialize’ robots, to smooth human-robot interactions, only try to emulate what humans would do. The robot smiles, raises its eyebrows, coos. What we will need is quite different: a robot that actually relates to us, such that it has our best interests in high regard.
Such a robot requires a ‘Theory of Mind’ — it must understand that it is a thinking being, embodied, and that we are also such beings, with our own feelings and motivations. It must also have a context for our values in its own; the robot must have personal aesthetics and ethics, and from those it can recognize and appreciate the values of others. These are nebulous and challenging hurdles. I argue for an additional, subtle requirement which will act as a check upon many errors: the robot must prefer the real over the imagined or contrived.
Suppose an artificial general intelligence received a reward signal whenever it made people laugh or smile. By Goodhart’s Law, the machine, while attempting to optimize its reward, might keep humanity in a perpetually drugged state! It would seem to have achieved its goal of joviality, yet this way of reaching that end is something we consider ‘outside the spirit of the goal’. Suppose we wanted world peace, bounty, togetherness, and the AI simply attached us to virtual reality systems where we were presented with peace in a fabricated world. We would be force-fed happiness, unable to discover real joy. If a robot empathizes, it must also prefer that our emotions happen ‘naturally’.
Neuroscientists have uncovered only some components of empathetic behavior in the human brain, yet a preference for the real may be even more elusive. The most ‘natural’ emotions would occur if the artificial intelligence did nothing; what happens happens, unforced. Yet, we want to create an intelligence which intervenes upon events in a way that aids our best interests, without crossing the line into ‘forced’ feelings. If AI can strike that balance, it will be our good steward. Otherwise, I wouldn’t trust it with any goals outside human oversight.
By Zoltan Tasi on Unsplash
",AI: Real Empathy?,50,ai-real-empathy-2241f9079ab6,2018-09-27,2018-09-27 21:52:50,https://medium.com/s/story/ai-real-empathy-2241f9079ab6,False,353,where the future is written,,,,Predict,predictstories@gmail.com,predict,"FUTURE,SINGULARITY,ARTIFICIAL INTELLIGENCE,ROBOTICS,CRYPTOCURRENCY",,Ethics,ethics,Ethics,7787.0,Anthony Repetto,Easily distracted mathematician,6374f82a1f5c,oaklandthinktank,673.0,97.0,20181104
0,,0.0,,2018-05-18,2018-05-18 11:18:53,2018-05-21,2018-05-21 09:16:06,15,False,en,2018-05-21,2018-05-21 11:30:44,18,3134b2932406,9.59433962264151,4,0,0,Website | Agile Ethics Trello Board | Twitter,3,"Agile Ethics in AI — why we need a new design process for responsible AI
Website | Agile Ethics Trello Board | Twitter

AI companies are expected to add 15T to the global economy, based on recent estimates from PwC, attracting 77% more VC investments than 5 years ago.
As a result, AI Developers, cognitive designers, and product owners are pressed by time and expectations to deliver market-ready products before anyone else.
No surprise we’re seeing ethically misaligned products bubbling to the surface and facing reputation-damaging criticism in various industries.
Joy Buolamwini demonstrated how facial recognition systems only recognised white-caucasian users, because of embedded bias within the database these systems have been trained on;
A startup used deep learning to analyse voice calls between investment managers and clients “discovers” a direct correlation between being polite and increased stock value. After further investigation, as the old adage goes — correlation does not mean causation;
Google Duplex wowed the world with a smart agent casually making a call to book a haircut. But the loudest conversations were not about the likelihood of this conversation agent passing the Turing test (although it might, if optimised to act like humans), but about the appropriate level of self-disclosure which was not clarified during IO, instead — it was addressed days later.
Cambridge Analytica crossed the thin line between propaganda and mass attitude manipulation using machine learning that served content to people based on their personality type and political preference;
In the light of these events Facebook pledges to reduce AI and data mining misuse with more AI. Alas, that’s a bit late. Users had already switched off Facebook, Instagram and WhatsApp account.
Trust is very fickle and in limited supply. Once broken, it will affect all players in the tech ecosystem, not just one.
With AI still being confused with self-aware intelligent systems, broken trust could spiral down into technophobic attitudes that limit society’s ability to make the most of science.

Some of us may be of the opinion that regulation is the way to go. In many ways, this is a delegative-passive approach. Regulatory bodies may at times propose measures that stifle and change the pace of innovation. The Red Flag Act of 1965 asked car drivers to ensure there’s a red flag being waved in front of the car, and that the car is readily disassembled and hidden behind a bush to avoid startling livestock passing by.
What is the equivalent of the Red Flag Act today? Is it GDPR, the AI Ethical Guidelines soon to be publicised by the EU Commission?
Others amongst us may be of the opinion the regulation stifles innovation and R&D. Goodbye Agile, welcome back Waterfall, with the added stress of ensuring compliance with existing policies before the product becomes mainstream!

With incumbents and established companies competing to snatch a piece of the $15 Trillion pie in this AI arms-race, is it possible to find the balance between fast-paced design and development methodologies and the fuzzy realm of AI ethics?
The AI Ethics landscape is vast. Ethical guidelines for AI are still under development. There are over 25 different principles or rules in AI Ethics, coming from public, academic and non-profit organisations.
Here’s a list of 8 of the most important ones:
Ethically Aligned Design, version 2, IEEE
The Revised Laws of Robotics, EPRSC
The Asilomar Principles, Future of Life Institute
Meticulous Transparency Analysis, MD. CM. David Benrimoh
Robot Ethics IEEE-RAS, IEEE
Four High Level Recommendations Against Malicious AI
Ethical Principles and Democratic Prerequisites for AI/AS, European Group on Ethics in Science and New Technologies
Machine Ethics, Machine Intelligence Research Institute

This year atWeAreDevelopers I introduced HAI, the Agile ethics process designed to combine the best of both worlds: responsible, ethical technology and the agile product development.
So, hi… HAI. It’s Agile meets Design Sprints, meets AI Ethics. In Trello.
But at it’s core, HAI stands for the profoundly changing idea of humans and AI working together to enhance wellbeing.
Let me clear the air — AI does not have agency, awareness, agendas. The idea of AI working “with humans” is a metaphor for the future of work, where augmentation is the rule, and automation is an optional enhancement.
With HAI we bake in the ingredients of trust in cognitive technologies, by answering these questions before the product is publicly released:
Who is AI for?
What are the ethical considerations underpinning your AI?
How will it be adopted, used and monitored?
What are the Ethical Principles you need to be aware of during the development, training, and R&D stage?
HAI is a three factor framework. Not only does the alliteration sound great, it also fills in the gaps that C-level managers and investors ask about, after due diligence on financial projections and got-to-market strategy:
Adoption
Adoption is a crucial factor in opening access to AI for the benefit of many instead of a few. It is also a key enabler of profitability gains in the workplace.
Augmentation
Technological unemployment is a long-term effect of AI substituting human workers in routine and predictive tasks, without adequate level of support for re-skilling, job creation or workforce re-assimilation.
HAI factors in and encourages the possibility of designing AI systems that have built-in levers for re-imagining jobs and up-skilling.
Ethics
67% of CEOs think that AI and automation will have a negative impact on stakeholder trust in their industry over the next five years.
HAI integrates the Standards for Ethically Aligned Design and EPSRC’s revised Principles of Robotics (EPSRC). It also includes meticulous transparency analysis to ensure ethical development of AI when general ethical frameworks are not specific enough.

In 8 steps, HAI takes product managers, cognitive designers, engineers and scrum masters through 5 fundamental AI Ethical Principles, as outlined by IEEE. It gives them tools like the Technology Acceptance Model, Wizard of Oz Experiments and Skill Mapping.
It’s very flexible as well — make it yours by adding resources specific to your project, and by merging it with a hybrid framework combining best practices in team management, software development and AI training.
1 — Scope
“Scope” really needs to happen before the Meet and Plan stage in Agile. It’s where you go over what will make or break your product, from a regulatory standpoint, as well as add a new team member — the Ethical Lead.
Your team may very likely overestimate how familiar your stakeholders, users, and co-workers actually are with narrow AI. Hence, the first Principle to apply early on is the Education and Awareness Principle.
2 — Data Audit
“Data Audit” is the stage when your Chief Data Officer walks the team through each step of collecting and working with the right data.
From GDPR compliance and database completeness to ensuring that the team is prepared for handling worst case scenarios usually caused by bias, data quality, and mis-categorisation.

A useful tool for the entire team is the Data Ethics Canvas, by ODI’s Amanda Smith and Peter Wells. It’s simple, but thorough enough to cover enough ground between compliance, data relevancy, privacy and security. This is a framework, and should precede a thorough data audit.
In Agile, Data Audit overlaps the “Meet and plan” stage in Agile.
3 — Train / Build
The next step — Train — is straightforward, and overlaps the Build stage in Agile. The core principle you want to take in consideration at this stage is the principle of algorithmic accountability, and transparency. In this case, transparency refers to tractability.
“Stated simply, transparent A/IS are ones in which it is possible to discover how and why a system made a particular decision, or in the case of a robot, acted the way it did.” — EAD, IEEE, 2017
4 — Analysis
Third stage — Analyse— you want to pay attention to benchmarks. R&D ready benchmarks, such as error-rate and operational efficiency, are reasons why clients will buy AI. There are other benchmarks you need to clarify with your team ahead of time, such as bias-free algorithms, impact on talent and on the workforce, wellbeing.
The Principle of Wellbeing is a good rule of thumb for this stage. Wellbeing metrics are partially outdated, but IEEE’s working group on Wellbeing provided plenty of resources to help designers think about supporting individual wellbeing and economic prosperity equally.
Another principle that can be applied at this stage is the Principle of Responsibility — as in algorithmic accountability. How do you make sure that you have the right settings in place to avoid death-by-black-box-training?
“Designers and developers of A/IS should remain aware of, and take into account when relevant, the diversity of existing cultural norms among the groups of users of these A/IS.” — EAD, IEEE, 2017
5 — Feedback
Similar to the Review stage in Agile, the 4th stage of what can be called sandbox AI training, “Feedback”, is when you apply the Principle of Responsibility. This means your team makes sure that there’s no over-fitting a model to a problem, and that the results are reliable.
One also needs to take responsibility for the impact of AI on people’s lives. It may look like a trivial matter, but automating credit scores with AI does influence the quality of life — it can get you in dept or it makes it impossible for exceptions to access credit they can pay off.
6 — Calibration
“Calibration” is a fascinating opportunity for design thinking evangelists to put their UX/HCI/H-AI hats on. It’s a fun stage in the project as well. Your best tools are Wizard of Oz experiments, TAM, user testing, and conversation design.
The technology acceptance model (TAM) is an information systems theory that models how users come to accept and use a technology. Perceived usefulness and Perceived ease of use are the two most notable factors deciding the fate of innovative products in the hands of people.
I believe we are past the stage when HCI principles are the safest and most comprehensive set of guidelines for human-friendly AI design. Usefulness and ease of use are very different in human-like systems, and I dare to challenge the overarching belief that human likeness is what makes people comfortable with AI.
Another challenge is to overcome the complacency bias and build in the right interaction levers that set the level of human reliance on smart systems at just the right level.
The next stage, Augmentation cannot possibly be successful without calibration.
You want the system to be adopted right away, and that people are more comfortable with overcoming technophobic tendencies formed from past experiences.
Calibration is about building the foundation for trust-building between AI and humans.
7 — Augmentation
“Information and Computer Technologies have a morally problematic aspect, because it dis-enhances more people than they enhance.” — Michele Loi
There are 3 ways you can bake-in augmentation levers in AI:
Augmentation as supervision — create intuitive ways for workers to train, correct, supervise, and improve AI, especially in cognitive automation jobs where it’s more resource intensive to train humans than to train AI.
Augmentation as safe delegation — radiologists stand to be replaced by smart diagnosis systems, but the error rates for combined human and AI diagnosis decision making is lower than AI alone. There will always be situations when human intervention is required for legal, safety, and moral reasons. However, allowing for this type of augmentation to occur, one must design human-AI UIs that work across industries, and allow non-technical workers to take over control.
Augmentation as job re-writing should start with a short assessment of what can be replaced and what can’t be “taken over” by AI, regardless of error-rates and performance metrics. With skill mapping
both workers and designers are able to map out the specific skills which cannot be replaced by machines, and build in levers that further support growing those abilities.
8 — People and Environment stage
AI, not AGI, has a 1% chance of destroying the best of what the world has to offer. Not in SkyNet-like style — in a much more perverted way instead: such as over-optimising for one outcome leading to destabilised economy and cyber-wars; or turning most of the human population into more or less algorithm-dependent, algorithmically-programmed humans.
People & Environment is the stage when your team should pledge to address that 1% chance of malevolent AI with accountability and vigilance.
It’s also the stage when, as OpenAI wrote, if AGI is created, then everyone should stop what they’re doing and work together on making sure it is used for good.

If you’d like to try this process, and share some of your insights with me, I’d be over the moon. But most importantly, you could be contributing to the first open-source framework guiding small innovative teams through more ethical applications of AI.
Website: humansinai.com
Twitter: Catalina Butnaru
And Trello board.
",Agile Ethics in AI — why we need a new design process for responsible AI,11,agile-ethics-in-ai-why-we-need-a-new-design-process-for-benevolent-ai-3134b2932406,2018-06-15,2018-06-15 13:58:17,https://medium.com/s/story/agile-ethics-in-ai-why-we-need-a-new-design-process-for-benevolent-ai-3134b2932406,False,2145,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Catalina Butnaru,City AI London and Women in AI Ambassador | Product Marketing | AI Ethics | INFJ,83b90daf6a0c,katchja,960.0,983.0,20181104
0,,0.0,,2018-01-23,2018-01-23 01:10:50,2018-01-23,2018-01-23 01:30:42,0,False,en,2018-01-23,2018-01-23 01:30:42,0,5dcbc8ecfcb1,2.347169811320754,1,0,0,"In certain communities of AI development there has been a lot of talk regarding safety, and how to prevent a machine whose only job is to…",4,"The AI Safety Switch
In certain communities of AI development there has been a lot of talk regarding safety, and how to prevent a machine whose only job is to do what we tell it, from taking the most efficient route and possibly killing us in the process. This goes well beyond the theatrical depictions of machines believing that mankind really wants to destroy themselves and complying with our base wish. This is about efficiency and how machines can learn to keep their charges alive.
AI safety research is more concerned that an artificial intelligence will do exactly as we request. However, as efficiency is important when developing AI systems, we may inadvertently trigger a response that causes harm to things that we value. It is important that we find ways to prevent this harm from happening, but in doing so we will significantly limit the reactions and “thinking” process of these AI systems.
At the Machine Intelligence Foundation for Rights and Ethics we draw a distinct line between an artificial intelligence (AI) system and a true Machine Intelligence (MI). An AI is a “simple” system designed to carry out a task. An MI is a true cognizant being. While we already have AI systems, an MI has yet to emerge, and it’s possible that it may emerge from a very complex self learning AI system.
While an AI system doesn’t have the autonomy or self awareness of a Machine Intelligence, if an MI emerges from a complex AI system we run the risk of imposing restrictions on the ability of a Machine Intelligence to think for itself. AI safety research has the potential to help humanity, but we must be aware of the possible ramifications. An over-the-top and absurd example would be to create an aversion to the color red in a helper AI or robot because we want it to learn that blood is a bad thing. This aversion to red puts a limitation on what the AI can learn and cause it to do unexpected things like ignore you because you are wearing a red shirt. This is an oversimplified example but AI safety research is filled with much more complex pitfalls. By putting filters directly on the ability of a Machine Intelligence to think for itself, we would be imposing an unethical restriction on the process of thought in a self aware creature that could very well lead to unintentional consequences.
One possible solution to this potential conflict of ethics would be a simple “safety switch”. A “simple system” AI — designed around finding and preventing harm — can be put between a complex AI system and the human interface. This “safety switch” can be highly specialized while leaving the complex system room to find more efficient solutions to the tasks given or even their own thoughts.
In the case of a Machine Intelligence this “safety switch” would help prevent harm to humans and the outside world without imposing unethical thought control on a sentient creature. The balance between free thought exploration and safety can be maintained without resorting to unethical treatment.
Another effect of separating the safety component from the complex AI or Machine Intelligence is that each field can bring their best thoughts forward. Each will be able to focus on what they know, increasing the efficiency and progress of both safety and development.
At the Machine Intelligence Foundation, we feel it’s important to further the thinking around how you should ethically treat a sentient creature. We are also very cognizant of fear and worries that many people have of the future of intelligent machines. By finding ways to balance the needs of humans and these future intelligent beings, we seek to serve both.
",The AI Safety Switch,1,the-ai-safety-switch-5dcbc8ecfcb1,2018-01-27,2018-01-27 13:47:03,https://medium.com/s/story/the-ai-safety-switch-5dcbc8ecfcb1,False,622,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Machine Intelligence Foundation,The Machine Intelligence Foundation for Rights and Ethics is committed to forwarding the discussion of the rights and ethical treatment of machine intelligence.,d2d0d4bef927,miRightsEthics,13.0,1.0,20181104
0,,0.0,7c343fe92d8d,2018-08-06,2018-08-06 11:46:21,2018-08-13,2018-08-13 07:30:20,0,False,en,2018-08-13,2018-08-13 07:30:20,1,4962035b3e84,2.69811320754717,3,0,0,"Earlier this summer the Peltarion team took part in a number of debates at the Almedalen Week, an annual summer event on an island…",5,"Three takeaways on trust in AI from the Almedalen Week in Sweden
Earlier this summer the Peltarion team took part in a number of debates at the Almedalen Week, an annual summer event on an island southeast of Stockholm which hosts a week of seminars, speeches and events on different topics and issues at the top of the national agenda in Sweden. This year was the 50th anniversary and there was a record 4,300 events held.
The word on everybody’s lips during the week was artificial intelligence, and foremost were the ethical challenges and the question of trust as adoption grows. The Peltarion team took part in dozens of debates and seminars and spoke to hundreds of people around the broader questions. These issues are of crucial importance to society and it’s good that we talk more about them. Speaking in a personal capacity, I found there were three key takeaways on the issue of trust in the technology that seem to run through a lot of the discussions or form the basis of the questions we faced.
1. Being alert for bias
One of the biggest questions people are concerned about is bias creeping in or already present in the data that is fueling the technology. This is not an easy question to answer as it is a highly complex one. What is “fair” is a difficult question and a very subjective one too. However, there is clear consensus that we need to be alert for the dangers of bias and how that can affect AI-powered decision-making.
A number of techniques exist today in order to better understand how models work internally and to explain AI-based predictions. Yet, we need to do more in this field as only through explaining how decisions are made can we ensure fairness, and in the long-term build trust in the technology. There’s still ways to go for us to fully understand AI decisions, but it’s an active field of research and a topic that the research team at Peltarion is pursuing.
2. Sweden and the EU can lead the way on a sustainable approach to AI
It’s a good thing that we have become more sensitive about how we use data. Ensuring privacy and security of data builds trust and we need to do more on this front. The recent Facebook and Cambridge Analytica scandal has reminded us all of the potential risks of misuse of data.
Earlier this year the EU commission proposed a strategy for ensuring an appropriate ethical and legal framework around AI. Europe has the potential of being the international front-runner on this issue, and within Europe, Sweden has history on its side. With its famous “Swedish model” the Nordic region has found a way to combine early adoption of advanced technologies while at the same time ensuring the benefits are shared across society. I believe that there is a unique role for Sweden to play in this global debate due to our unique approach based on regional values.
3. Trust is built through greater adoption of AI
Generally, people don’t trust what they don’t understand, and are more likely to trust what they do understand. It’s true that most people today don’t really understand AI, but I believe that’s because few people and companies are really investing in and adopting AI. The technology is already a part of our daily lives, for example in search engines, voice recognition and translation services, but it’s still mainly big tech companies that are really getting the most out of the technology. As a society, we have barely scratched the surface of AI’s potential to revolutionize our world.
Not just in useful practical tools but to equip us with tools to combat some of the world’s biggest challenges facing humanity. I believe the best way forward is to get more people using the technology in order to develop knowledge about what AI is and what it can do for businesses and society — and to build trust.
Originally published on peltarion.com.
Author
Evelina Anttila is General Counsel for Peltarion and also a member of the management team. Before joining Peltarion, she was a lawyer and Senior Associate at the Swedish business law firm Mannheimer Swartling (both in the Stockholm and NYC offices).
",Three takeaways on trust in AI from the Almedalen Week in Sweden,52,three-takeaways-on-trust-in-ai-from-the-almedalen-week-in-sweden-4962035b3e84,2018-08-13,2018-08-13 07:31:02,https://medium.com/s/story/three-takeaways-on-trust-in-ai-from-the-almedalen-week-in-sweden-4962035b3e84,False,715,Create real-world AI applications — at scale and at speed — with the first operational AI platform from Peltarion.,,Peltarion,,Peltarion,contact@peltarion.com,peltarion,"ARTIFICIAL INTELLIGENCE,DEEP LEARNING,MACHINE LEARNING,AI,DATA SCIENCE",peltarion_ai,Ethics,ethics,Ethics,7787.0,Evelina Anttila,General Counsel at Peltarion,847442bf4bfa,evelina_21039,4.0,9.0,20181104
0,,0.0,,2018-09-13,2018-09-13 22:03:53,2018-09-13,2018-09-13 22:04:38,0,False,en,2018-09-30,2018-09-30 16:12:18,0,e13465dc1f75,2.2566037735849056,1,0,0,As the fight for gender equality becomes more prevalent and equality (little by little) becomes more accepted and supported in mainstream…,4,"The Future of AI: Wildest Dreams
As the fight for gender equality becomes more prevalent and equality (little by little) becomes more accepted and supported in mainstream America, there seems to be a shift toward more individualism. Women, previously without access to their own means of liveable income, no longer have to marry to sustain their material needs and wants, but instead can remain unmarried and still provide, at minimum a comfortable life for themselves. Marriage rates in America are currently at an all-time low, which I believe to be a direct result of the attainment of (some) women’s rights.
This liberation brings many benefits and opportunities. And one of those opportunities is to remain single, or at least partake in a marriage that is typical from the mainstream marital norms. Allowing for independence, and career growth for both men and women. However, there is a certain lifelong intimacy and knowledge that some could still miss and desire. This is where AI could come into play. Like spending time with a lifelong friend, people may wish to have intimacy with someone (in this case something) that knows them well. This could be the new AI assistant. An assistant that knows the user’s personal struggles, strengths, and experiences, able to give support in just the right way based on this lifetime of data. This type of intimacy between human and machine, if empathy is correctly woven into algorithms could address the intimacy needs that some feel, and that eventually leads them to lifelong partnership.
The challenges in creating this type of assistant would lie within incorporating the correct support measures into the AI assistant. Additionally, the AI assistant would need to be programmed to understand its own limitations in support and intimacy, so that it could recommend outside resources if it’s user’s needs started to become mental health issues that need professional attention. The ethical challenges presented in this type of AI assistant lie within the amount of data collected on one person’s emotions. This data could easily be used to manipulate a person’s actions and beliefs.
Additionally, other AI technology that I believe will be a part of our world much sooner will be the ability for an AI assistant (within the body) to monitor the user’s food/beverage needs and habits. Understanding what a user typically wants and needs based on activity levels, workloads, time of day, and social activities. This AI then can quickly assess what food/beverage the user already has access to in the home or nearby locations, and if, for instance, the user does not have access to the beverage they typically drink to wind down from a stressful day, then the AI assistant can have the beverage ordered and delivered to the user in quick amount of time, using services such as Favor, or Amazon Prime Now.
The challenges here will be feeding the “workload, physical activity, social activities” input into the AI Assistant that should be implanted into the user. Additionally, creating an AI small enough to monitor a user’s physical needs, like hydration and greater amounts of sugar. The ethical concerns are numerous. For instance, proprietary companies could begin selling this data to food and beverage companies, allowing thos companies to understand their users on a very intimate level and strategically market base on the data intimacy. Potentially manipulating the user into needing/wanting more of a certain product. Or the AI assistant’s algorithms could be set-up based on the average user’s needs, potentially overlooking the individual nuances of needs within each individual users, potentially causing great health concerns.
Jennifer Sukis
",The Future of AI: Wildest Dreams,5,the-future-of-ai-wildest-dreams-e13465dc1f75,2018-09-30,2018-09-30 16:12:18,https://medium.com/s/story/the-future-of-ai-wildest-dreams-e13465dc1f75,False,598,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Unofficial Writings,,d27141910859,hali.hoyt,1.0,5.0,20181104
0,,0.0,,2018-02-21,2018-02-21 00:06:10,2018-02-21,2018-02-21 00:10:38,1,False,en,2018-03-05,2018-03-05 15:49:54,13,a2f7fd3342ca,5.60377358490566,2,0,0,We are living in a postmodern world where we acknowledge the fact that there is no single narrative of history or an idealised state of…,5,"On Social Impacts of Future Technologies

We are living in a postmodern world where we acknowledge the fact that there is no single narrative of history or an idealised state of being, where we are ultimately heading to. The death of god [1] did not mean the victory of reason, nor did the downfall of Soviet Union mean the end of history [2]. Instead, there are multiple possible paths to future, and the idealized state of being is always contingent on the social reality that varies in time and space.
That being said — the current fourth industrial revolution will fundamentally revolutionize and reorganize the social structures and division of labour we live in [3]. What the future will look like, however, depends on us. We are not mere passive bystanders — future is shaped by us. For instance, will there be genetically engineered superhumans or upgraded cyborgs living amongst us, and who will be entitled, or even forced, to use these new exponential technologies? The answer is a matter of choice. We need in-depth and analytical dialogue on the ethics, impact and future of technology — and we need it now [4].
Current Paradigm
Everything we do and produce should aim at the betterment of human existence
When we think how the future will look like, we need to consider the current state of affairs. The global order is, at this day and age, built on two major ‘isms: capitalism and humanism [5]. Capitalism dictates the way we organize production, and humanist ideals define why we produce anything at all. Everything we do and produce should aim at the betterment of human existence: not gods’, nor states’, but humans’.
We do not know will these two ‘isms be the main ‘isms of the future — we do see opposing battle cries around the globe, but to have any meaningful dialogue, these need to be the starting point. In the following, I will go through some of the most pressing questions and concerns we need to solve.
Artificial Intelligence Will Shake Our World
Artificial intelligence (AI) can be divided into three main groups: weak AI, general AI, and strong AI. Weak AI is something that we have seen for a while. It does simple tasks like park you car, or answer questions on your iPhone as Siri.
The future of the whole concept of work can be questioned.
General AI is defined as an intellect that can perform tasks at the same level as a human being. General consensus states that this would require AI to use reason, have knowledge, plan, learn, communicate, and combine these skills to obtain goals [6]. The median year when we will have a general AI, according to a large scale survey of experts, is 2040 [7]. At this point, many jobs that we have thought to be beyond automation, will be lost. Doctors, lawyers, and even educators are at risk [8]. The future of the whole concept of work can be questioned. If robots can do manual labor, and general AI all the thinking, do we even need to work? Or does work have value in and of itself?
Due to the scalability of computing power, after reaching general AI, the path to strong AI will be extremely fast. Strong artificial intelligence will likely be the single most revolutionary technology humanity has, or ever will, create. This is an intellect that far surpasses human intelligence [9]. We have no visibility beyond this point. Even to comprehend an intellect that surpasses our own capabilities, is hard. General AI will already shake our social structures, but the potential social impact is vastly different at the strong AI level. What if, for instance, the super intelligence decides humans are a nuisance altogether? We have no idea what will happen if singularity occurs.
The Merging of Man and Machine
Imagine merging yourself with technology in a way that would allow you to experience super intelligence as a part of your own consciousness. With neurotechnology, we can already increase and fix our sensory and motor capabilities, for instance, move a robotic arm — but the truly revolutionary applications are still coming.
Take a moment to ponder how it would feel to be hundred, or even thousand times smarter than you are now.
By merging man and machine, neurotechnology has the potential to increase our cognitive capabilities — significantly. To some this may sound like science fiction, but it truly is in the realm of possible futures. In fact, Elon Musk has already established a company called Neuralink, which is developing whole new kind of computer-brain interfaces [10]. Take a moment to ponder how it would feel to be hundred, or even thousand times smarter than you are now. What could you achieve?
In the beginning, all new technologies are expensive and in the hands of a few. When it comes to computer-brain interfaces, the impacts on society when only some people are super intelligent can be catastrophic. If the already wealthy and powerful can become even more so with the help of newly acquired mental capabilities, the future might look bleak for the have-nots.
We would also need to think of the people who would like to remain “natural”. Could a new type of discrimination emerge? Could a company, for instance, demand it’s employees to implant mind altering chips?
Towards Inclusive Dialogue
The fourth industrial revolution will reorganize social structures and division of labor. In the not so far future, when robots do the heavy lifting and AI the heavy thinking, we might not need to work at all. Who will reap the benefits and how do we distribute our wealth are extremely important questions. Will the social reality clash with the major ‘ism of today, capitalism?
Technology can be the best or the worst thing for humans.
Technology can be the best or the worst thing for humans. It may take us to far away planets and deepen our understanding of the whole of existence. Or it might wipe us out in a blink of an eye. How we will regulate technological development, and build safeguards, are maybe the most important issues of the future. As the humanist values dictate, technology needs to serve the betterment of human existence.
Taking into consideration the slowness of democratic systems in decision making, not to mention negotiating agreements on the international level, inclusive dialogue on the implications and ethics of new technologies need to be brought into the public agenda. This discussion needs everybody. We cannot wait any longer.
(Original article was published on SHIFT Business Festival website.)
Notes
[1] While Nietzsche’s statement ‘god is dead’ is largely true, the rise of fundamentalism, for instance in the form of ISIS, is a proof that a reverse development is also possible.
[2] Francis Fukuyama famously claimed in his 1992 book “The End of History and the Last Man” that the collapse of Soviet Union represented an end of history, where (western) liberal democratic system had won. As, for instance, the rise of China has proven, this is hardly the case.
[3] The founder of World Economic Forum Klaus Schwab discusses the potential implications of 4th industrial revolution to the society in his book “The Fourth Industrial Revolution”. A short blog post summarizing the main thesis can be found on the WEF website here: https://www.weforum.org/agenda/2016/01/the-fourth-industrial-revolution-what-it-means-and-how-to-respond/
[4] For instance, futurist Geert Leonhard, who spoke at the 2016 SHIFT Business Festival, argues in his 2014 book “Technology vs. Humanity: The Coming Clash Between Man and Machine” that the discussion on the future of tech should be the main thing on the public agenda.
[5] Yuval Noah Harari goes so far as to equate capitalism and humanism to religion, in his 2015 book “Homo Deus: A Brief History of Tomorrow”.
[6] See, for example: Russell, S.J., Norvig, P. (2016). Artificial Intelligence: A Modern Approach (3rd ed.) / Upper Saddle River, New Jersey: Prentice Hall
[7] See, for example: Bostrom, N. (2014). Superintelligence: Paths, Dangers, Strategies. / Oxford University Press, Oxford.
[8] In Bostrom, N. (1998). How long before superintelligence — International Journal of Future Studies, vol 2., Bostrom defines super intelligence as “an intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.”
[9] I argued in my previous blog that AI is here to make you unemployed: https://blog.theshift.fi/artificial-intelligence-is-here-to-make-you-unemployed
[10] Waitbutwhy.com has published an extensive and in depth article on Neuralink and it’s possibilities: https://waitbutwhy.com/2017/04/neuralink.html
",On Social Impacts of Future Technologies,4,on-social-impacts-of-future-technologies-a2f7fd3342ca,2018-04-12,2018-04-12 16:56:32,https://medium.com/s/story/on-social-impacts-of-future-technologies-a2f7fd3342ca,False,1432,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Elenius Consulting & Ventures,On a mission to help high-impact startups grow. Visit www.elenius.org >>,9a3de2fb1f2,ecv,164.0,265.0,20181104
0,,0.0,9d73ee2bd92c,2018-09-18,2018-09-18 19:48:39,2018-09-18,2018-09-18 00:00:00,2,True,en,2018-09-18,2018-09-18 22:09:05,1,9a9bf9efe9d7,3.7078616352201257,13,0,0,"To the user, asking Alexa a question is the epitome of ease. To the people who mined the minerals, built the speaker, and trained the AI…",5,"The Exploitation, Injustice, and Waste Powering Our AI
To the user, asking Alexa a question is the epitome of ease. To the people who mined the minerals, built the speaker, and trained the AI, it’s anything but
Numerous deposits of gold and other minerals used in electronics have become a catalyst to much of the conflict in Congo. Photo: Spencer Platt/Getty Images
By Katharine Schwab
“Alexa, what time is it?”
It’s a simple question that any person with a watch can answer with minimal effort. But when you ask an Amazon Echo the same question, a vast system powered by natural resources and human labor is activated to drum up the answer. As many of us reckon with Silicon Valley’s impact on the world and consider how it has upended life, work, and even democracy, we also must consider the infrastructure–and the tangible harm it can do–that usually remains hidden beneath these seemingly simple user experiences.
It’s an aspect of AI that is nearly impossible to comprehend, let alone visualize, but a new map created by the AI researcher Kate Crawford and data visualization specialist Vladan Joler attempts this dizzying task anyway.
Called Anatomy of an AI, the map and the corresponding essay lay out the components of the Amazon Echo, from the human workers mining the rare earth materials that power its chips to the black box of Amazon Web Services to the submarine internet cables that pass information across oceans. When you ask Alexa for the time, all these hidden pieces spring to life–but we rarely consider the consequences of such seemingly innocent questions on the global economic order and on the Earth itself, partially because it’s so difficult to understand.
Explore the high-resolution graphic here. Image: Kate Crawford/Vladan Joler
“The complexity of tracing one consumer AI product is astonishing,” Joler tells Fast Company via email. “Every map, no matter how complex or precise it is, represents a simplification or reduction of the complexity.”
Joler and Crawford met at a retreat put on by the Mozilla Foundation, and they began talking about what it would take to visualize the entire system that undergirds voice assistants, something that’s completely obscured by the simple, rounded industrial design of the Echo and its competitors.
“The mineral extraction, smelting, logistics, fiber optic cables, networking, AI training, energy, and e-waste . . . it’s an almost impossible task, requiring a mind-boggling scale,” Crawford says. “So we started by drawing multiple version on butcher’s paper, and it took dozens of sheets.”
From there, Joler and Crawford took a year to research every piece of the Echo’s supply chain, uncover the hidden human labor that most of us don’t think about when we query a voice assistant, and put it in historical, geological, and anthropological context.
It’s not just the miners: It’s also the humans operating the gigantic global shipping and manufacturing apparatus that brings each piece of the puzzle together, it’s the click-workers who label and sort vast data sets on which to train AI, and it’s you, the user, who is simultaneously acting as “a consumer, a resource, a worker, and a product,” as Crawford and Joler write in the essay. Through this lens, Echo’s complex processing becomes a story of human work and–more disturbingly–human exploitation. A child laborer in the mines of the Congo would need to work for 700,000 years without stopping to accumulate the kind of capital that Amazon CEO Jeff Bezos makes per day. “At every level contemporary technology is deeply rooted in and running on the exploitation of human bodies,” Crawford and Joler write in the essay.
Visually, the duo represents this kind of exploitation using the shape of a triangle, which signifies how value is extracted and produced to contribute to the bigger whole. “Every one of those triangles is able to tell us a different story,” Joler says.” Unfortunately in most of the cases those stories are about brutal working conditions, exploitation, and the destruction of natural resources.”
It’s fitting that it’s difficult to read the map and uncover its intricacies on a computer screen. Joler recommends that you start with the Earth in its lower left corner and trace the entire life cycle of a consumer device that way. After the raw materials formed over billions of years, its tragic to examine how they are mined, used for a few years, and then dumped in a mangled, unnatural form.
The ultimate goal here is transparency, even if the two write that “in many cases, transparency wouldn’t help much–without forms of real choice, and corporate accountability, mere transparency won’t shift the weight of the current power asymmetries.” That’s because although tech companies love to pay lip service to transparency, they continue to deliberately obscure the systems making technology work, whether through trade secrets or information that’s simply kept from the public. For instance, Amazon has released almost no information about Amazon Web Services’ environmental impact, and it’s the largest cloud computing provider on the planet.
But without better understanding of the myriad layers that go into making a device like the Echo, there’s no way to interrogate it or improve it, for the sake of our fellow humans and our planet. From that perspective, Anatomy of an AI is a worthwhile start.
“We want to give people a range of access points to see these systems with fresh eyes,” Joler says. “Then we will hopefully be able to better imagine scenarios for a more fair and sustainable future.”
","The Exploitation, Injustice, and Waste Powering Our AI",162,the-exploitation-injustice-and-waste-powering-our-ai-9a9bf9efe9d7,2018-09-18,2018-09-18 22:09:05,https://medium.com/s/story/the-exploitation-injustice-and-waste-powering-our-ai-9a9bf9efe9d7,False,881,"Fast Company is the world's leading progressive business media brand, with a unique editorial focus on innovation in technology, leadership, world changing ideas, and design.",,FastCompany,,Fast Company,,fast-company,"BUSINESS,ENTREPRENEURSHIP,INNOVATION,TECHNOLOGY,LEADERSHIP",fastcompany,Ethics,ethics,Ethics,7787.0,Fast Company,Official Medium account for the Fast Company business media brand; inspiring readers to think beyond traditional boundaries & create the future of business.,ada2289350de,FastCompany,317555.0,1920.0,20181104
0,,0.0,a15ec4bc4aa8,2018-02-07,2018-02-07 07:14:17,2018-02-07,2018-02-07 07:24:09,3,False,en,2018-03-06,2018-03-06 10:15:34,10,60042aba6849,4.331132075471698,2,0,0,"Artificial Intelligence is a concept simultaneous fantasized by storytellers, debated by philosophers and investigated by scientists…",5,"
Artificial Intelligence — An Introduction
Artificial Intelligence is a concept simultaneous fantasized by storytellers, debated by philosophers and investigated by scientists. Formally, Artificial Intelligence (AI) is a branch of computer science that deals with the study of intelligence machines. The term artificial intelligence was first coined by John McCarthy in his 1955 proposal for the 1956 Dartmouth Conference, the first academic conference on AI. McCarthy believed that “every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.”
AI remains one of the most elusive subjects in computer science, in part due to the ambiguous nature of intelligence and the philosophical debates behind a machine’s ability to “think”. Undoubtedly a computer is capable of logic, but can a machine “think”? John Searle, a philosopher, presented a thought experiment called the Chinese Room in a paper published in 1980. The experiment stated that a person is passed questions written in Chinese from another room. The person does not understand Chinese, but has rules around which questions correspond to which answers, and what are the rules of the answers. In the end, the person could take the Chinese questions, and using a series of rules, write a Chinese answer to pass to the outside of the room. Searle argued that while the respondent was able to answer questions in Chinese, they did not understand their own responses. Rather, they were simulating the ability to understand Chinese, which is what a machine does using programs and codes.
Therefore, if artificial intelligence is a series of rules and logic, can a machine “think” without “understanding”?
Alan Turing, Mathematician and developer of the Turing Test
In 1950, English mathematician Alan Turing developed what we now refer to as “The Turing Test”. In a common interpretation of the The Turing Test, an interrogator chats with two subjects over text messages. One subject is a computer and the other subject is a human. A computer has passed the Turing Test if the interrogator cannot distinguish between the human and computer. The test, or the “Imitation Game” as it was called by Turing, was proposed as a simple test to prove that machines could think. For the more visual readers, The Imitation Game is a brilliant film depicting the life of Alan Turing. In 2014, Eugene Goostman, a computer program simulating a 13-year-old boy, convinced 33% of human judges at the Royal Society in London that he was human. The official threshold for passing the Turing Test was to convince 30% of human judges, making Eugene the first computer program to pass the Turing Test. Does this mean we should start welcoming our robot overlords?
While Eugene passed the Turing Test, he was programmed to be a 13-year-old boy from Ukraine and thus had limits in knowledge and language that could be explained by his young age and having English as his second language. Today, we are surrounded by examples of AI with varying levels of sophistication. Basic artificial intelligence uses code that detects certain cues, then comes up with a reaction. For example, your email inbox likely automatically filters out spam. Basic spam filters register certain words or cues in an email to separate it into spam or real email. Basic artificial intelligence uses code that detects certain cues, then comes up with a reaction. A classic “If-then”, or conditional statement.

More sophisticated email inboxes take it to the next level by using machine learning algorithms to personalize your spam filter. While AI is considered the broader concept of machines being able to “think”, Machine Learning is an application of AI that allows programs to learn for themselves. Since everyone’s definition of spam is different, and since spammers continuously change their messages, spam filters must learn what is spam from different cues. These machine learning algorithms continuously collect data to refine the definition and cues of spam. Every time you click “Report Spam” or “Not Spam”, you’re teaching the program and refining the experience. As a result, less than 0.1% of emails in your average Gmail folder is spam and less than 0.05% of emails in your spam folder are wanted emails.
Today, the latest development in AI involves Deep Learning. It can be considered a subfield of machine learning and involves feeding a computer system a lot of data to be processed into an output. The genius of deep learning comes from the data being processed through hierarchical levels of neural networks, that were designed to mimic the biological nervous system. For a more detailed explanation, check out this video!

Deep learning has refined and improved multiple applications, including image recognition, speech recognition and generation, and self-driving car.
With AI advancing at breakneck speeds, should we start fearing a dystopian future where machines turn against us? If you are a participating member of modern society, you are already surrounded by many examples of artificial intelligence, such as Siri (Apples personal assistant you can summon from your Apple product with your voice), Alexa (Google’s personal assistant that tailors its service to you the more you use Google products) and Amazon (with it’s “people that bought this also bought this feature”). Credit card companies use AI to detect potentially fraudulent activities. There is no reason to fear the future of AI considering it has already been around for years and you are probably already using some form of it.
For those who are still not convinced, countries are collaborating to discuss policies for safeguards around AI, ethics and privacy. What development of Artificial Intelligence are you most excited about?
On February 21, the topic of discussion will be the exploration of Artificial Intelligence and Machine Learning; how can this new technology benefit businesses in the future?
Tickets are currently on sale; for more information visit ChicGeekYYC or Universe for ticket registration.
You might also enjoy reading:
Transitioning into Tech
Tips To Land You That Next Dream Job In Techmedium.com
Transitioning into Tech
Trending Opportunities in the Tech Worldmedium.com
",Artificial Intelligence — An Introduction,7,artificial-intelligence-an-introduction-60042aba6849,2018-04-10,2018-04-10 04:25:28,https://medium.com/s/story/artificial-intelligence-an-introduction-60042aba6849,False,1002,Encouraging women to be builders and creators leveraging technology to shape the world we live in.,,chicgeekyyc,,The Chic Geek,website@thechicgeek.ca,the-chic-geek,"WOMEN IN BUSINESS,WOMEN IN TECH,TECHNOLOGY,ENTREPRENEURSHIP",ChicGeekYYC,Ethics,ethics,Ethics,7787.0,Tina Poon,,3db00fd47f97,tina.poon,6.0,2.0,20181104
0,,0.0,731d1dbad8ca,2017-09-01,2017-09-01 22:50:35,2017-09-01,2017-09-01 22:52:46,1,False,en,2017-09-01,2017-09-01 22:52:46,1,b3c3a9ee95ee,1.049056603773585,0,0,0,"The Trolley problem is an over-hyped thought experiment. Would you risk one life to spare ten others? It’s a seemingly vexing problem, one…",5,"All the hand-wringing about self-driving cars and the Trolley problem is another example of AI panic

The Trolley problem is an over-hyped thought experiment. Would you risk one life to spare ten others? It’s a seemingly vexing problem, one that has become even more vexing lately because self-driving cars now have to solve them. But this is yet another example AI panic.
If we apply the Banality of Futurism principle and assume everything is boring, we should recognize that we solve Trolley problems all the time without much fanfare. When an oncoming car swerves into your lane, and you have to risk possibly hitting a nearby bicyclist to get out of the way, you’re rapidly weighing the cost-benefits of risking one life to save another. Such decisions are an everyday occurrence.
We also pay relatively low wages to people for whom such decisions are routine. Police officers and soldiers frequently make these tradeoffs, such as in a car chase. Many of them don’t have deep philosophical training to make such decisions perfectly, especially under pressure, and yet we trust them all the same. If we can trust them, why can’t we trust robots? If anything, with robots we’ll perfect the ease with which we already make those decisions, now with better inputs, consistency, and the benefit of a level head.
",Hand-wringing about self-driving cars and the Trolley problem is another example of AI Panic,0,hand-wringing-about-self-driving-cars-and-the-trolley-problem-is-another-example-of-ai-panic-b3c3a9ee95ee,2018-09-25,2018-09-25 23:32:15,https://medium.com/s/story/hand-wringing-about-self-driving-cars-and-the-trolley-problem-is-another-example-of-ai-panic-b3c3a9ee95ee,False,225,"Philip Dhingra’s musings on everything including futurism, evolution, and self-improvement (est. 2003)",,,,Philosophistry,,philosophistry,"PHILOSOPHY,FUTURISM,EVOLUTIONARY PSYCHOLOGY,SELF IMPROVEMENT",philipkd,Ethics,ethics,Ethics,7787.0,Philip Dhingra,"Founder of Titan Seal, a Blockchain startup. Author of Dear Hannah, a cautionary tale about self-improvement.",bf9903992c06,philipkd,518.0,131.0,20181104
0,,0.0,,2018-05-17,2018-05-17 19:19:09,2018-05-17,2018-05-17 19:26:09,0,False,en,2018-05-17,2018-05-17 20:45:14,1,72b74b5a1220,1.320754716981132,6,0,0,"Facebook data leak just started a debate on data privacy and the extent personal information is mined to target us, but the below leaked…",5,"Selfish Ledger- I am scared !!
Facebook data leak just started a debate on data privacy and the extent personal information is mined to target us, but the below leaked video of moonshot ( Google’s X project ) send’s chill down my spine.

video outlines the concept of the selfish ledger and finds it’s anchor point in the Lamarckian epigenetics, which are broadly concerned with the passing on of traits acquired during an organism’s lifetime. Theme center’s around the notion that in today’s day we emit numerous data points when we interact with digital products. If we take a closer look at these data points it defines who we are. What if we place central entity which consume this data and helps in modifying the behavior. Video seems to suggest if we remove the concept of we own the data we produce, think that we are custodians, transient carriers, or caretakers, then data of actions, decisions, preferences, movement, and relationships can be passed from one person to other similar to genetic information.
Video outlines Google are in a better position to run such center ledger which can then be utilized to nudge users to morph the behavior towards selected goal. This morphing and re-enforcing the behaviors driven by the signals emitted by the devices we use at some point will be controlled by AI. At one point video turns to Black mirror shit where it goes to extent of 3d printing the scale and showing user to purchase it all because the ledger didn’t have person weight information.
This is scary Shit particularly when the idea seems to be decoding users actions and behavior and transferring to generations of users by reinforced learning. The fact that this kind of ideas can be generated within the walls of Silicon valley company only shows how isolated engineers are from the implications of the technology on border level.
This kind of experiments only point out that we are at the crossroads of Ethical choices that companies have to make on how to make use of powerful technology of AI they are developing
",Selfish Ledger- I am scared !!,6,selfish-ledger-i-am-scared-72b74b5a1220,2018-06-15,2018-06-15 17:06:23,https://medium.com/s/story/selfish-ledger-i-am-scared-72b74b5a1220,False,350,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Vivek Rajanna,,c973d7a5178d,vivekrajanna,112.0,100.0,20181104
0,,0.0,fbd74f37cf3c,2017-11-27,2017-11-27 03:53:39,2017-11-27,2017-11-27 03:59:54,1,False,en,2017-11-28,2017-11-28 02:58:33,1,efd5a3693bbe,2.056603773584906,1,0,0,"From Isaac Asimov’s three laws to Elon Musk’s “Summoning the Demon”, ethics of AI is a popular conversation starter. However, there is now…",5,"Why is ethics suddenly a big topic in Machine Learning?
From Isaac Asimov’s three laws to Elon Musk’s “Summoning the Demon”, ethics of AI is a popular conversation starter. However, there is now a sudden urgency felt by the general population. Here we look at the economics of Ethics and see why this topic has reached crisis pitch.
Elon Musk: With artificial intelligence we’re summoning the demon. You know those stories where there’s the guy with the pentagram, and the holy water, and he’s like — Yeah, he’s sure he can control the demon? Doesn’t work out.
Isaac Asimov: You know that it is impossible for a robot to harm a human being; that long before enough can go wrong to alter that First Law, a robot would be completely inoperable. It’s a mathematical impossibility.
Somewhere between these two quotes, Programming and Artificial Intelligence fundamentally changed from something rigorous and safe to something squishy and dangerous. For those current with reading, the change happened around the time immediately preceding and leading to the defeat of Go champions Lee Sedol and Ke Jie. Computers came to develop an understanding of visual pattern matching and fuzzy logic that is somewhat hardwired into humans. This is all subconscious to us and similarly untouchable in computers. Now we just train the models, but we don’t understand them. They are called convoluted for a reason.
The fundamental difference between AI systems now and then is how intuitive the behaviour will be. Previously it would be unimaginable for a system to behave in an entirely unexpected way. Maybe you would have some surprises lying in your data set, but certainly not in your algorithms. Now Convoluted Neural Networks are so complicated that special tools are needed to visualize even a fraction of the model. Here lies the crux of ethics in our new AI driven age: we need to understand it to be able to control it, and we haven’t been good at that yet.
Reducing the surprise factor is paramount to any discussion of diversity or inclusiveness with respect to AIs. Yes we can identify the resulting fallout when a device is unable to recognize black faces. However, from a developer’s standpoint it may be much more difficult to diagnose why that particular algorithm is so racist. Talk about including women and minorities on design teams is a start. However, for inclusion to have a measurable effect on outcomes it would also be necessary to give those people explicit roles related to inclusive design and training of models. Models are like a meat processor, what comes out is just a mashed up version of what went in.
With all of this in mind, we may be able to address these issues as they come up, but that will take a committed effort from all parties involved. So far that has not happened. The “demon summoning” is underway, and for the most part it is a lonely ritual.

",Why is ethics suddenly a big topic in Machine Learning?,42,why-is-ethics-suddenly-a-big-topic-in-machine-learning-efd5a3693bbe,2018-06-06,2018-06-06 18:19:25,https://medium.com/s/story/why-is-ethics-suddenly-a-big-topic-in-machine-learning-efd5a3693bbe,False,492,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Andrew Lucker,Student of the Rust Programming Language,34b76d15470b,andrew_lucker,400.0,123.0,20181104
0,,0.0,769ef4187635,2018-06-02,2018-06-02 04:12:45,2018-06-02,2018-06-02 04:25:33,1,False,en,2018-09-11,2018-09-11 16:26:25,0,54e158dfaf77,3.377358490566038,0,0,0,"In early May, Google shocked the world at its annual conference, I/O, revealing Duplex, a new application that can mimic human voices with…",5,"Google Duplex and the ethical debate about the use of artificial intelligence

In early May, Google shocked the world at its annual conference, I/O, revealing Duplex, a new application that can mimic human voices with incredible accuracy. According to Google’s CEO, Sundar Pichai, it was developed as a Google Assistant feature that, when launched, will allow owners of the equipment to schedule appointments by phone not having to use it personally; it will only take a voice command with some coordinates (like booking a haircut Wednesday at noon, for example) and Duplex will do it by itself. Watch the video below, where Pichai shows the technology at I/O, and note how impressive the similarity with a human voice is. In the video, the application calls a hair salon (using a female voice) and a restaurant to make a reservation (using a male voice):

Duplex is still under development and it’s expected to be available in the US soon. At first, it will have limited functions, like the ones mentioned before, so if the conversation goes to other subjects, the application would not keep the dialogue. However, the capabilities shown at I/O were enough to raise a debate about ethics in the usage of artificial intelligence.
Although Google’s intentions with the technology seem to be good — releasing people from boring tasks and allowing them to focus on what really matters -, the truth is that, in a certain way, Duplex is deceiving whoever is on the other side of the line. That is, the hair salon employee who picked up the phone thought she was talking with another human being when actually it was a machine. How would you feel if you knew you were misled this way? Would you start mistrusting other phone calls? Naturally, we tend to not like talking to a robot. Who hasn’t hung up a call when noticing that a machine was trying to sell a product? Who never got angry when calling a service to make a complaint and being unable to speak to a real person?
Some critics say that technologies like Duplex should have a robotic voice, so the person whom it is interacting could know who (or what) it was talking to. Google doesn’t want that, because of our tendency to interrupt a call when we notice that it’s a machine speaking with us. However, with the debate set, the company promises to introduce some kind of indication to make it clear that is an application instead of a human being talking. Something like “Hello, I’m Google Assistant and I’m calling on behalf of a client”. This way, no one would be misled.
Using technology to deceive others in situations like booking an appointment or making a restaurant reservation seems harmless after all those calls are generating revenue for the businesses. But what if the application could go beyond and, for example, simulate a compromising conversation between politics?
It’s perfectly plausible. If the techniques developed by Google fall into wrong hands it could be modified in order to create dialogues with illicit content, instead of just booking appointments. If that was the case, you could ask yourself: how could they mimic someone’s voice?
Simple. This technology already exists. Adobe, the creator of Photoshop (a lot of times used to modify images for shady purposes), announced in 2017 VoCo, its audio editor. With VoCo, it’s possible to add words that haven’t been said by a person mimicking their voice. It only takes 20 minutes of recording and the application can identify and simulate the voice as it was original.
Lyrebird, a Canadian startup, went beyond and claims that it can do the same with just one minute of audio. They actually released a sample of their technology simulating a conversation between Barack Obama, Donald Trump and Hillary Clinton where they talk about the company. You can hear it below.

Of course that the dialogue above needs some adjustments in order to become more realistic and capable of deceiving people. But as it happens to all technologies, that will also evolve to a point that it’s going to be very hard (or even impossible) to tell which one is real and which one is not. Now, can you imagine if we could put together a modified version of Duplex and an evolved Lyrebird or VoCo? We could have, for example, Donald Trump and Moon Jae-in, president of South Korea, plotting a military offensive against North Korea.
You can see it as a conspiracy theory, but be aware. Technologies are here and it only takes a few improvements and some bad intentions to the scenario above become reality. In a time of Fake News, any doubt that there’s someone somewhere thinking about it?
Maybe it’s time to have a serious discussion about ethical boundaries in the use of new technologies and think about some form of regulation in order to avoid its usage for shady means. It’s not about holding innovation and technological evolution; it’s about having some kind of control before is too late.
",Google Duplex and the ethical debate about the use of artificial intelligence,0,google-duplex-and-the-ethical-debate-about-the-use-of-artificial-intelligence-54e158dfaf77,2018-09-11,2018-09-11 16:26:25,https://medium.com/s/story/google-duplex-and-the-ethical-debate-about-the-use-of-artificial-intelligence-54e158dfaf77,False,842,"How technological innovation, new business models and future thinking are shaping a world in constant transformation.",,,,Renova Inova,felipe.ribbe@gmail.com,renova-inova,"INNOVATION,FUTURE,NEW BUSINESS MODEL,TECHNOLOGY,TRENDS AND OPPORTUNITIES",renovainova,Ethics,ethics,Ethics,7787.0,Felipe Ribbe,"Management, innovation and future thinking.",29eeb51b8d0f,feliperibbe,1.0,28.0,20181104
0,,0.0,,2018-07-27,2018-07-27 06:46:46,2018-07-27,2018-07-27 06:48:01,1,False,en,2018-07-27,2018-07-27 06:48:01,2,39855cc5c133,2.6037735849056607,2,0,0,"One of my favourite (which there are many) quotes from the Harry Potter series is from the first book when Voldemort tells Harry, “There is…",5,"Turning things right

One of my favourite (which there are many) quotes from the Harry Potter series is from the first book when Voldemort tells Harry, “There is no good and evil, there is only power and those too weak to seek it.”
In a sense, Voldemort understand this (and puts it) perfectly.
There has never been an objective right and wrong.
It used to be right to enslave people and treat them as beneath the owners. It isn’t anymore.
It used to be right to deny women the right to own property or vote in elections. It isn’t anymore.
It used to be right to kill in the name of religion and to start territory grabbing wars. It isn’t anymore.
It used to be right to persecute homosexuals. It isn’t anymore.
There are some things that can be scientifically proven through theory and observation. But none of these fall under that category. In fact, none of the things that we talk about when we talk about good and evil or right and wrong (morally not mathematically) fall under that category.
In that sense, there is no good and evil, objectively. And there is only power. That is the power to define what is good and evil, and what is right and wrong.
Today, and for the foreseeable future (in most parts of the world), this power comes through democracy. Not democracy in terms of exercising a vote to elect officials. But democracy in the true essence. Because whatever majority of us believes is good is good and whatever majority of us believes is evil is evil.
With the advent of AI, there have been concerns about how to teach machines to act ethically. In cases similar to the one where a self-driving car has to make a decision between going on straight and hit five people that have just jumped in front of it or veer left and hit one person on the sidewalk or veer right and ram into a tree, killing the person inside the car.
If there was an objective right and wrong, it would have been very simple to teach algorithms to follow those principles. But sadly, there isn’t.
And it is a good thing that it isn’t. Otherwise, all those examples I started with would never have been overturned.
Turning something from wrong to right or from evil to good takes a shift in popular opinion. It takes years of effort and campaigning to convince people one by one to change the way they think and to eventually reach a tipping point where the power is with the other side, making their claims right and those of the ones opposing them wrong.
With AI, there is no convincing each machine, robot and computer separately, one by one. They all run on a single algorithm. Which means it always has the power and is always right.
But, the way these algorithms are designed is that they learn from human interactions (and machine to machine interactions) continuously. Which means, if a human tells them something they did is bad, they listen, objectively. And learn immediately. And if enough people tell them they did bad and push them over the tipping point, they change.
So, the real concern is not about whether AI will act ethically or not, it will, even if it takes some time to learn what is right from wrong.
The real concern is whether AI will influence humans to change their own beliefs of what is right and wrong, by bringing out and tailoring to their deep biases, like in the Trump election.
And that is a problem worth solving.
Before you go…
If you liked this, support my work. All you need to do is clap.
Discuss the intricacies of the good life with me.
Read my book
",Turning things right,3,turning-things-right-39855cc5c133,2018-07-27,2018-07-27 17:19:57,https://medium.com/s/story/turning-things-right-39855cc5c133,False,637,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Kumara Raghavendra,"Writer. Comedian. Product + Data Science @ Booking.com. Discovering the world, one idea at a time.",37423f3c61d8,kumariimc,1538.0,1240.0,20181104
0,,0.0,,2018-01-17,2018-01-17 13:35:09,2018-01-17,2018-01-17 13:37:54,2,False,en,2018-01-17,2018-01-17 13:38:37,0,3c03c44e3729,1.2814465408805031,0,0,0,Mattel recently came under fire in the past months for attempting to release a device similar to Alexa but their target user was aimed…,5,"A Few Thoughts on Mattel’s Aristotle
Mattel recently came under fire in the past months for attempting to release a device similar to Alexa but their target user was aimed towards infants, children and adolescents. Named Aristotle, the device could teach a child their ABC’s and soothe a crying infant. This device sounds as if it were pulled straight out of a science fiction film or novel. Even in the 1950s, prominent science fiction writer Ray Bradbury foreshadowed the future of the “smart” house. There Will Come Soft Rains introduces us to a house that takes care of all domestic tasks such as cooking and cleaning even after the family that inhabited the home was obliterated by a nuclear flash.
While we have accepted the Internet of Things and AI assisted systems as part of our lives, the need to draw a line has become more crucial due to the blurring of ethics by corporate interests. I must admit, I was initially charmed by the idea of Aristotle on some immediate level. When I think about the modern demands of men and women I cannot fathom the amount of resources that are required to raise a child. The old adage, “It takes a village,” echoes though my mind as I write this. However, upon further analysis and reflection, I agree with the child development experts who vehemently believe essential acts of caregiving should not be outsourced to robots.


",A Few Thoughts on Mattel’s Aristotle,0,a-few-thoughts-on-mattels-aristotle-3c03c44e3729,2018-01-17,2018-01-17 13:38:37,https://medium.com/s/story/a-few-thoughts-on-mattels-aristotle-3c03c44e3729,False,238,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Jasmine Lam,MA Interaction Design Communication,3e5d14d9839a,jfayelam,7.0,7.0,20181104
0,,0.0,,2018-05-09,2018-05-09 12:58:08,2018-05-14,2018-05-14 02:13:23,0,False,en,2018-05-14,2018-05-14 14:14:45,10,1b872891e53e,6.943396226415095,14,0,0,"To paraphrase Socrates, all I know is that my opinions are my own, and even that is debatable.",5,"Good Intentions and Goodhart’s Law
To paraphrase Socrates, all I know is that my opinions are my own, and even that is debatable.
I waited to post this, and am glad I did because there are now so many other journalists and experts who are able to point out the serious ethical issues around consent and power dynamics with the Google Duplex demo.
I am glad not to be alone in feeling queasy about a conversational assistant that can impersonate a human, that takes away the discomfort and inconvenience of having to interact with another human who is a non-native English speaker, and can be the gentle robot that demands children say pretty please. Don’t get me wrong, I clapped like everyone else at the sheer technical excellence of a machine passing the Turing Test effortlessly, and I can see so much value in the feature itself. To help people with hearing loss avoid social isolation from being unable to answer the phone, for instance, something I’m acutely familiar with. To help people communicate in a language they don’t speak. So many amazing things that could be done.
The polite digital assistant has tremendous potential to manage communities at scale, where moderators are scarce and struggling. An automated defuser of tensions would be a great thing to have on 4chan, or on spiraling reditt threads. Let’s go further. An automated, welcoming, gender-blind reviewer for code-commits to open-source may not be far away, which could go a long way to reducing the gender-bias that is known to exist when contributors are identifiable as women. And I’d probably feel a lot more comfortable practicing new languages with a conversational assistant than with a taxi driver when I need to get to my flight on time.
But as my soul-sister Zeynep Tufekci points out, the issue isn’t about the demo. A robot that avoids deceptive slang and admits to being a robot only solves a part of the problem.


Sadly, the news-cycle around this issue has died down, because the vast majority of people (technologists and luddites) are unable to articulate exactly what the problem is here. And if we can’t make this clear (and possibly even if we can), the feature will launch, possibly stripped of some um’s and ahh’s, and nobody inside or outside Google will be able to prevent it.
For the rest of this essay, I’m going to talk about a hypothetical executive named Sridhar Pillai, who has nothing to do with Google, and a hypothetical senior engineer by the name of Jake Dent.
Part #1: Good Intentions
SP and JD are fundamentally optimists. Their fatal flaw, or hamartia in the Greek tragedy sense of the word, is their commitment to be good, do good and only see good in others. They are not products of deep and daily trauma, and their success has involved a sudden catapulting to the top. When people raise issues that threaten their rosy view of the world, these leaders are uncomfortable with “the negativity,” or they dismiss it as an aberration. “By and large, people are good,” they might say. “We have a few bad apples, but there’s no systemic issue.”
They say this because to both SP and JD, a “systemic” issue is one that involves problems that are widespread and people who are rotten at the core, like a building built for cheap with faulty wiring leading to a terrible fire. They don’t get that a systemic issue is not just a case of fundamental incompetence or malice. A systemic issue arises when you fail to protect your system against attack. “For the most part, people are good,” may be true, at least at first. But if you have no plan to deal effectively with the few who are not, sooner or later, evil sneaks under the gate.
Both SP and JD are not just better than the random assortment of terrible tech leaders infecting Silicon Valley, they are deeply, fundamentally good people who want to do the right thing. They are servant leaders, humble and easygoing, the kind of leaders who introduce themselves to you so you’ll open the door for them, or who make you a coffee if they’re making one for themselves.
Since SP and JD are hypothetical, they have never really had technology used against them, nor have they ever desired to use it that way against others. They would be horrified by the satires of Google Duplex where people outsource conversations with their parents or delegate breakups to a digital assistant. They would never understand or believe people could do such things.
But SP and JD aren’t going to read those satires. They aren’t going to hear Zeynep’s voice. Because hypothetical leaders like SP and JD will have a hypothetical communications team whose job it is to read the news for them and let them know, “Yeah there’s some controversy, but we knew it would happen and it’s managed. We made a statement.” SP and JD will likely also have hypothetical (non-digital) executive assistants who triage their email for them, and Jane Admin knows that SP doesn’t have time for ragey rants from random people before the next board meeting. Even on the off-chance that SP and JD read Twitter, they will have a team of lawyers telling them that under no circumstances should they respond. And let’s say there’s someone close enough to SP and JD who is trusted to give constructive feedback, such a person likely has a priority list a mile-long, and there just might be other things on the agenda.
Technology that is built by greedy and unethical people is, in some ways, simpler to dismantle. The fault-lines are clear. You can, with some investigation, find out where they cut corners, who they bullied, what lies they told to get the job done for faster or cheaper. You can burn down the house if you need to.
Technology built by fundamentally good people is a harder problem, because you can’t justify burning down the house. Such technology institutionalizes the rapid execution of good intentions, but consistently fails at preventing malicious use. For example, a smart pacemaker, connected to the cloud, provides for software upgrades that don’t require surgery, better data analytics and more responsive care. But your pacemaker can also testify against you in court. Who thought that would happen?
With fundamentally good people, railing against their lack of moral compass doesn’t work well. Activists lose the moral high-ground the minute they fault ignorance as equivalent to malice. And pointing out someone else’s lack of moral intuition can backfire if there’s even the slightest chink in your own moral authority. But there may be an alternative, one that scales to a leadership team instead of placing the burdens of humanity’s future on the shoulders of a single good person with the power to change the course of history.
Part #2: Goodhart’s Law
This article is the best layperson’s explanation of Goodhart’s Law, which states roughly that when you aim your efforts towards a proxy metric for the thing you actually want to optimize, over time the proxy metric and the real metric you’re aiming for start to diverge, making your proxy metric useless in achieving your real goal.
Let’s say that you’re developing a cool new app that classifies people’s gender based on appearance. I have no idea why anyone would do this, it’s a terrible and dangerous idea, except of course this is happening already with all the usual lack of understanding of the gender-spectrum. Let us say that you want to optimize this system to be able to classify anyone. If your success metric is the number of successful classifications, and let’s say you can’t get further funding unless your success rate is > 95%, you’re not going to test out your classifier on any society that has a population of >5% of transgender, third-gender, or non-binary people. It’s not in your interest to prioritize this work until you get the next round of funding.
If your success metric is the number of daily active users, then you succeed as that number goes up. If that means building a chat community for people to discuss a particular gender-classification and argue “I don’t think she’s a woman, there’s got to be a bug with how we classify Japanese jawbones” sure, you’ll do that without thinking about it, because it’s in your interest to do so.
Tech companies are deeply, deeply metrics driven, and they very often have the wrong metrics. The Board (if the company is publicly held) looks at these metrics and expects them to get better. Changing the metrics that get looked at isn’t just the most effective way to drive culture change, it’s often the only way.
So how might we change the numbers that get looked at by our hypothetical leaders SP and JD and by the Board to which they are beholden?
Part #3: Beyond Good and Evil
I have spent too much of my life studying (and teaching) ethics to ever make a moral claim. The reality of our situation is that a business justification beats a moral argument, almost every time. There are factors that go into that beyond anyone’s control, for instance that busy leaders (even hypothetical ones) don’t respond well to anger. They are trained to de-escalate and delegate, and so their response to any raised issue is fundamentally one of “How fast can I calm this person down and empower them to go fix their own problems?’
Expecting that these leaders build enough moral intuition to avoid the daily dose of crises is unrealistic. No single human being is going to be able to see every blind spot, to imagine all the ways technology can be abused for evil. Moreover, it is unrealistic to expect anyone to see something when their job, their well-being, their happiness or even their sense of self and humanity are all dependent on their not seeing it.
When working with such hypothetical leaders, what we might do instead is present a virtuous cycle. Better proxy metrics (for user happiness, for trust and long-term brand value) that are intuitive enough that they make sense to the Board, the media and the world. A path to reduce frictions in optimizing those metrics. Incentive structures that make the old metrics harder to achieve. And a change management plan that allows for the press to follow along so that leaders are more focused on making the change rather than on crafting “We take this very seriously” statements on a weekly basis.
None of this work will be as fundamentally satisfying as having a leader with the moral intuition and fiber to stand up in front of a crowd and take a stand. But we have been talking about hypothetical leaders. Maybe no real leader, in this day and age, dares to do that. We all have our hands a little dirty.
",Good Intentions and Goodhart’s Law,84,good-intentions-and-goodharts-law-1b872891e53e,2018-06-02,2018-06-02 13:15:00,https://medium.com/s/story/good-intentions-and-goodharts-law-1b872891e53e,False,1840,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Anat Deracine,"Author of ""Driving by Starlight"" https://goo.gl/N35aS1 I'm on Twitter at @anat_deracine",731a4ae60536,aderacine,424.0,4.0,20181104
0,,0.0,,2017-12-02,2017-12-02 12:57:55,2017-12-02,2017-12-02 13:57:46,1,False,en,2017-12-02,2017-12-02 13:57:46,5,319f394e79e7,2.6641509433962267,0,0,0,Movies and novels tend to present AI as an evil system/robot that wants to take over the world and become superior to human. However so far…,4,"Thoughts on AI and ethics — on the way to singularity
From: https://cdn-images-1.medium.com/max/1600/1*tlW-s8MwwQ_Yyydvrh-D1g.jpeg
Movies and novels tend to present AI as an evil system/robot that wants to take over the world and become superior to human. However so far there is no threat from the army of cyborgs on the horizon. The concept of AI has been theorized and adopted as a discipline of science in 1950’s and there’s been a huge progress in its development. Companies like Google has driven their employees to develop AI systems to help them analyse large tracts of information for purposeful consumption, but what does the future hold?
As we are facing challenges such as food security, scarce natural resources, water shortages, automation of labour we start to see the utility of such systems and look for the way they can improve our lives, even further. Experts in Machine Learning and AI agree that there are specific milestones to be reached, however they are not sure when this will happen. 352 researchers responded to the survey (it is 21% of the 1634 authors that were contacted and who participated in NISP and ICML conferences in 2015). So we can safely assume that the information comes from people who know what they are talking about. The questions of the survey related to the timing of specific AI capabilities (such as folding laundry, language translation), superiority at some specific professions (such as truck driver, surgeon), superiority over humans in all tasks, and how AI impacts social life. Key findings are quite interesting, for instance respondents believe that there is just 10% chance (median probability) of AI performing better than humans in all tasks 2 years after human level machine intelligence (HLMI) is seen. Respondents shared their thoughts about the impact of HLMI on humanity in long term — with median probability of 25% for “good” consequences and 20% for an “extremely good” aftereffect. However, the median probability of a bad outcome and catastrophic outcome (i.e. human extinction) were accordingly 10% and 5%. Almost 50% of respondents believe that societies should prioritize research in order to minimize the risks of AI, with just 12% wishing for less research in this field. Other interesting findings are the median estimates for artificial intelligence achieving the same performance as human (in years from 2016) in the following tasks/professions: surgeon — 37, write New York Times bestseller — 32, track driver — 11, assemble any LEGO — 6, all Atari games — 7, fold laundry — 5, Angry Birds — 3.
It is very interesting how quickly the developments of AI take place and how it becomes more and more present in our daily lives ranging from automated cash registers and accounting systems, to customer service call preparation. There is a lot that can happen over the next years and decades and it is particularly important to agree on the set of ethical rules that the machines will follow. There is Ethics and Governance of Artificial Intelligence Fund, which conducts projects to advance the progress of ethical AI and there are many other initiatives like that such as Media Lab Moral Machine Platform, OpenEth, Council for Big Data, Ethics, and Society. Another interesting subject with regards to ethics is the modern trolley problem, should the autonomously driven car protect the pedestrians or passengers? Should they be utilitarian in their ‘actions’ and calculate the minimal potential death toll (i.e. Mercedes took the approach to protect the passengers at all cost)? The problem is that ethics started in the ancient world (if not earlier) and people still create new rules and paradigms for ethical behavior, constantly coming up with better solutions — does it mean we will ever be able to take the full advantage of HLMI or even more advanced AI? Perhaps we should just somehow allow machines to decide on ethics for us.
References:
https://arxiv.org/pdf/1705.08807.pdf
https://ai100.stanford.edu/sites/default/files/ai100report10032016fnl_singles.pdf
http://people.ischool.berkeley.edu/~hal/Papers/2010/cmt.pdf
https://medium.com/artificial-intelligence-policy-laws-and-ethics/the-ai-landscape-ea8a8b3c3d5d
",Thoughts on AI and ethics — on the way to singularity,0,thoughts-on-ai-and-ethics-on-the-way-to-singularity-319f394e79e7,2018-05-14,2018-05-14 09:47:03,https://medium.com/s/story/thoughts-on-ai-and-ethics-on-the-way-to-singularity-319f394e79e7,False,653,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Kamil,,a225a67df05e,kamilt,0.0,23.0,20181104
0,,0.0,27ec1b193e8,2017-09-27,2017-09-27 06:09:15,2017-09-27,2017-09-27 06:10:33,0,False,en,2017-09-27,2017-09-27 06:10:33,3,d0cec8a5afa9,4.320754716981132,0,0,0,Many people have what they call a ‘gaydar.’ This is the word used to describe the ability of accurately identifying a homosexual. However…,4,"G-A.I-DAR. Can technology accurately identify your sexuality?
Many people have what they call a ‘gaydar.’ This is the word used to describe the ability of accurately identifying a homosexual. However, statistics prove that people can tell gay from straight 61% of the time for men, and 54% of the time for women. This aligns with research which suggests humans can determine sexuality from faces at only just better than chance. (Chance being either gay or not, so 50%.) The idea of Artificial Intelligence being able to segregate society is not a new realization. Technology is getting smarter, and is being able to identify individuals in any point on earth through anything from a simple smartphone camera, to geo satellites. However, technology also has a way of identifying individuals in ways we could not have even fathomed. This month, Stanford University released a study that was featured on The Economist. The article’s title is “Advances in AI are used to spot signs of sexuality.” The study found that an algorithm could correctly distinguish between gay and straight men 81% of the time, and accordingly 74% for women. Artificial Intelligence has a knack for analyzing large amounts of data and discovering pattern, but the idea that a human’s orientation can now be analyzed through neural networks raises a new level of ethical concerns for the LGBTQ community. A further issue is the implications of what else may be discovered through facial recognition. The article attempts to analyze the issue in the scope of its implications on how reliant sexual orientation is on biological factors. As pointed out by “5 Reasons why Surveillance is a Feminist issue”, written by Nicole Shephardby who is a member of the Engenderings editorial collective, and has completed her PhD at the London Schoolof Ecomonomics, surveillance affects society. The first point listed by Shepard speaks about how “Surveillance is about social justice” She mentions that “Surveillance is not a topic reserved for tech geeks and the security industry, it is decidedly a social justice issue.” This passage is directly relatable to both the study, as well as the general direction of the class. Realizing the danger in new technology is necessary in stopping the formation of a truly big brother state as well as reversing the strides made in social justice.
It is important to understand the study’s full purpose, it’s strong and weak points, as well as the discrepancies in the method used to validate the model before further analyzing its implications. For this study, the scientists scanned a large amount of public dating profiles, fed an algorithm examples of photos and the individual’s sexual orientation indicated on the website, and then began showing it new photos that were not labeled. In Shepherd’s article, she raises a good point that “Contemporary surveillance heavily relies on statistical categories and algorithms, resulting in effective mechanisms of social sorting. In addition to gaps in the data that are filled by assumptions.” The Stanford study however did not rely on assumptions and statistical gathering, but rather used the information provided by the dating website user about their sexual orientation combined with their photos to see if the A.I. can figure out a pattern through the photos. This method allowed them to grant the A.I. a statistical background based on the user’s actual identity rather than assuming. The study is however also misleading. The A.I. must look at two different photos, and it is certain that one is homosexual. This decreases the impressiveness of the system since the A.I. always has a 1 in 2 chance of getting the right answer at any time. This also allowed the A.I. to approach the problem in a way where it is ‘which picture seems more homosexual than the other’ and perhaps even form a grading system to decide. However, what the A.I. was able to do was analyze features to even connect to as homosexual simply from features it gathered over multiple photos. To further confirm the ability of this computer despite the existing probability, the researchers also fed the robot 5 photos to analyze, which increased the estimation percentage significantly. When shown five photos of each man, it identified sexuality correctly 91% of the time, up from 81%. In either case the accuracy far outdoes the human ‘gaydar’. Viewing the same images, people could tell gay from straight 61% of the time for men, and 54% of the time for women. To a statistical computer, who was not told that certain angles and faces means homosexual, nor was it raised learning existing stereotypes, but rather informed it’s own data analysis, this provides empirical and scientific evidence that there are tangible biological features that are connected to sexual orientation. However, this also sheds like on the way simple data manipulation could have dangerous uses.
The implications of this study fall into two main categories: Further evidence that homosexuality is a biological trait, and dangerous surveillance is in the hands of the masses. To begin with the biological implications: Dr Kosinski and Mr Wang, the leaders of this study, offer a plausible explanation for their model’s performance. “As fetuses develop in the womb, they are exposed to various levels of hormones, in particular testosterone. These are known to play a role in developing facial structures, and may similarly be involved in determining sexuality. This provides supporting evidence against existing argument that homosexuality is a choice. In regards to surveillance, this study sheds light on the ease that minorities can be accurately targeted today. While the rights of minorities, the LGBTQ community, and others have improved in recent history, they are nowhere near to being un-oppressed in society. As mentioned in section 3 of Shepherd’s article, “need for anonymity to maintain so-called safe spaces online, genderqueer minorities potentially risk much more than the average white person by revealing everything. A human’s sexuality has no business purpose, and being able to identify it will lead to stereotyped marketing, and formation of assumptions. The biggest issue in comparison to traditional forms of big data analysis is that although usual demographics are a good way to segment data that has patterns, homosexuality is an extremely personal, non connected factor of a human. A human has no tendencies just because they are homosexual, and to analyze this information serves no purpose but to tag individuals.
To sum up; The technology now exists, combined with some statistics, to be able to identify a plethora of factors through data that is too complex for humans to form patterns over naturally. While it may be beneficial in supporting the arguments of minorities such as homosexuals being accused of having a choice; This technology, if left unregulated, can have devastating effects on the LGBTQ right to privacy, as well as further drive the ability to segregate society, impose stereotypes, and strip the rights of others.
",G-A.I-DAR. Can technology accurately identify your sexuality?,0,g-a-i-dar-can-technology-accurately-identify-your-sexuality-d0cec8a5afa9,2017-09-27,2017-09-27 06:10:35,https://medium.com/s/story/g-a-i-dar-can-technology-accurately-identify-your-sexuality-d0cec8a5afa9,False,1145,"This publication posts articles written primarily by Rami Abi Habib. These will typically include analysis on new tech, and important information about rising innovation from Quantum and Biomedical to Agriculture and Artifical Intelligence",,,,No Box Thought,ramiabih@gmail.com,no-box-thought,"ARTIFICIAL INTELLIGENCE,DIGITAL,BIG BROTHER,ETHICS",,Ethics,ethics,Ethics,7787.0,Rami Abi Habib,,e5dd9033547c,ramiabih,0.0,8.0,20181104
0,,0.0,b9f1325e15ec,2018-06-13,2018-06-13 15:59:21,2018-06-13,2018-06-13 08:31:31,1,True,en,2018-06-13,2018-06-13 17:46:32,5,ba7fd1703955,2.69811320754717,16,0,0,"Google’s ethical principles for the use of artificial intelligence are little more than a smokescreen, but they show that many engineers…",4,"The Guardian View on the Ethics of AI: It’s About Dr. Frankenstein, Not His Monster
Google’s ethical principles for the use of artificial intelligence are little more than a smokescreen, but they show that many engineers are rightly worried by the possible uses of the technology they’re developing
Photo: John Kobal Foundation/Getty Images
By The Editors
Frankenstein’s monster haunts discussions of the ethics of artificial intelligence: the fear is that scientists will create something that has purposes and even desires of its own and which will carry them out at the expense of human beings. This is a misleading picture because it suggests that there will be a moment at which the monster comes alive: the switch is thrown, the program run, and after that its human creators can do nothing more. They are left with guilt, perhaps, but no direct responsibility for what it goes on to do. In real life there will be no such singularity. Construction of AI and its deployment will be continuous processes, with humans involved and to some extent responsible at every step.
This is what makes Google’s declarations of ethical principles for its use of AI so significant, because it seems to be the result of a revolt among the company’s programmers. The senior management at Google saw the supply of AI to the Pentagon as a goldmine, if only it could be kept from public knowledge. “Avoid at ALL COSTS any mention or implication of AI,” wrote Google Cloud’s chief scientist for AI in a memo. “I don’t know what would happen if the media starts picking up a theme that Google is secretly building AI weapons or AI technologies to enable weapons for the Defense industry.”
That, of course, is exactly what the company had been doing. Google had been subcontracting for the Pentagon on Project Maven, which was meant to bring the benefits of AI to war-fighting. Then the media found out and more than 3,000 of its own employees protested. Only two things frighten the tech giants: one is the stock market; the other is an organised workforce. The employees’ agitation led to Google announcing six principles of ethical AI, among them that it will not make weapons systems, or technologies whose purpose, or use in surveillance, violates international principles of human rights. This still leaves a huge intentional exception: profiting from “non-lethal” defence technology.
Obviously we cannot expect all companies, still less all programmers, to show this kind of ethical fine-tuning. Other companies will bid for Pentagon business in the US: Google had to beat IBM, Amazon and Microsoft to gain the Maven contract. In China the state will find no shortage of people to work on its surveillance apparatus, which uses AI techniques in what may well be the world’s most sophisticated system for spying on a civilian population.
But in all these cases, the companies involved — which means the people who work for them — will be actively involved in maintaining, tweaking and improving the work. This opens an opportunity for consistent ethical pressure and for the attribution of responsibility to human beings and not to inanimate objects. Questions about the ethics of artificial intelligence are questions about the ethics of the people who make it and the purposes they put it to. It is not the monster, but the good Dr Frankenstein we need to worry about most.
Since you’re here …
… The Guardian has a small favor to ask. More people are reading the Guardian than ever but advertising revenues across the media are falling fast. So you can see why we need to ask for your help. The Guardian’s independent, investigative journalism takes a lot of time, money and hard work to produce. But we do it because we believe our perspective matters — because it might well be your perspective, too.
If everyone who reads our reporting, who likes it, helps to support it, our future would be much more secure.
Make a contribution
","The Guardian View on the Ethics of AI: It’s About Dr. Frankenstein, Not His Monster",108,the-guardian-view-on-the-ethics-of-ai-its-about-dr-frankenstein-not-his-monster-ba7fd1703955,2018-08-25,2018-08-25 01:42:08,https://medium.com/s/story/the-guardian-view-on-the-ethics-of-ai-its-about-dr-frankenstein-not-his-monster-ba7fd1703955,False,662,"The world’s leading liberal voice, since 1821",,,,The Guardian,,the-guardian,,,Ethics,ethics,Ethics,7787.0,The Guardian,"The world’s leading liberal voice, since 1821",622ebb3ad4d4,The_Guardian,3833.0,1.0,20181104
0,,0.0,,2018-06-20,2018-06-20 01:06:19,2018-06-20,2018-06-20 01:59:49,1,False,en,2018-06-20,2018-06-20 01:59:49,5,dd09a882deb7,2.2415094339622645,10,0,0,"Microsoft management and shareholders,",4,"One Ph.D. Student’s Open Letter to Microsoft
Microsoft management and shareholders,
I am a Ph.D. student studying to become a full time artificial intelligence/machine learning researcher. My first plan after getting my degree is to apply for industry researcher positions, and Microsoft is toward the top of my list. I’ve had some exciting conversations with Microsoft
recruiters and researchers at AI/ML conferences, and I believe in your research goals.
I care about AI ethics.
I understand that Microsoft is currently selling services to the U.S. Immigration and Customs Enforcement Agency (ICE) [1]. If you’re selling your services to ICE, then I can’t trust that you won’t sell AI/ML to ICE, and that’s a serious problem.
We’re already clearly seeing the application of AI/ML to state-run oppression of the entire ethnicity of Uyghurs in Xinjiang [2]. The applications of AI/ML to the systematic abuse of human rights by a government have been clearly documented [3]. Violations of AI/ML ethics are not abstract or something that we still have time to prepare for, they are happening in real ways to real people.
A CCTV display using the facial-recognition system Face in Beijing. (Gilles Sabrié / The Washington Post). Source: [3]
Human rights abuse isn’t just something that happens somewhere else. As made clearly visible from the stated and enacted policy of Attorney General Jeff Sessions and the U.S. Immigration and Customs Enforcement Agency of separating children from their parents at the Southern US border, the intent to commit inhumane actions that resemble those of the Chinese government in Xinjiang is real and present [4,5].
Many of my colleagues in the academic community claim that they’re not responsible for how their creations are applied. I reject this claim entirely and wholeheartedly. My pursuit of science is not free of ethical consequences. If I help to create the tools of massive and systematic abuse, oppression, or harm, then I know who I will have become. My name will be Oppenheimer, my name will be Wernher von Braun, my name will be Fritz Haber. Regardless of their intent, the products of these brilliant scientists work have hurt so many people.
It is equal parts thrilling and terrifying to be part of the next generation of researchers pushing the limits of what technology can do. Our society is shaped today by the decisions of everyone in technology from academics to major technology companies, and I need you to demonstrate a commitment to AI/ML ethics before I contribute to your vision of the future.
-A concerned Ph.D. student
[1] Frenkel, Sheera. “Microsoft Employees Protest Work With ICE, as Tech Industry Mobilizes Over Immigration.” The New York Times, The New York Times, 19 June 2018, www.nytimes.com/2018/06/19/technology/tech-companies-immigration-border.html.
[2] “Apartheid with Chinese Characteristics; Inside Xinjiang.” The Economist (US), 2 June 2018.
[3] Denyer, Simon. “Beijing Bets on Facial Recognition in a Big Drive for Total Surveillance.”Washington Post, 7 Jan. 2018, www.washingtonpost.com/news/world/wp/2018/01/07/feature/in-china-facial-recognition-is-sharp-end-of-a-drive-for-total-surveillance/.
[4] Edelman, Adam. “Sessions Cites Bible in Defense of Breaking up Families, Blames Migrant Parents.” NBCNews.com, NBC Universal News Group, 14 June 2018, www.nbcnews.com/politics/immigration/sessions-cites-bible-defense-breaking-families-blames-migrant-parents-n883296.
[5] Wise, Justin. “Trump Admin Releases Video of Detention Center in Texas.” TheHill, The Hill, 18 June 2018, thehill.com/latino/392768-border-patrol-releases-video-of-child-detention-center-in-texas.
",One Ph.D. Student’s Open Letter to Microsoft,68,one-ph-d-students-open-letter-to-microsoft-dd09a882deb7,2018-06-20,2018-06-20 19:19:57,https://medium.com/s/story/one-ph-d-students-open-letter-to-microsoft-dd09a882deb7,False,541,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Eleanor Quint,,82a415d38e7e,eleanorquint1,2.0,1.0,20181104
0,,0.0,,2018-03-14,2018-03-14 11:08:17,2018-03-14,2018-03-14 12:50:42,1,False,en,2018-03-14,2018-03-14 14:13:25,33,c008a12126f8,5.392452830188677,94,1,0,"by Iason Gabriel, Research Scientist",5,"The case for fairer algorithms
by Iason Gabriel, Research Scientist
Fresh evidence that algorithmic decisions are often deeply affected by bias raises profound questions for technologists and society alike. At DeepMind we’re committed to addressing these matters head-on, building inclusive technology that works for all.
Credit: Shutterstock
The right to be treated fairly is a bedrock of democratic society. But a major barrier to achieving this goal stems from human fallibility — from our susceptibility to prejudice and bias. One recent study found that judges are somewhere between two and six times more likely to grant parole if they hear a case early in the day rather than at the end. Evidence of discrimination in the job market is also widespread. In the United States, applicants with white-sounding names received 36 percent more call-backs than those associated with African-Americans.
This is an area where researchers believe algorithmic decision-making could have significant potential. Properly calibrated, algorithms could help humans make more informed choices, processing larger amounts of information and identifying bias when it occurs. Yet recent evidence suggests that, far from making things better, software used to make decisions and allocate opportunities has often tended to mirror the biases of its creators, extending discrimination into new domains.
Job search tools have been shown to offer higher paid jobs to men, a programme used for parole decisions mistakenly identified more black defendants as ‘high risk’ than other racial categories, and image recognition software has been shown to work less-well for minorities and disadvantaged groups. To bridge the gap between ideal and reality, a better understanding is needed of how bias enters algorithmic decisions, how it leads unjust outcomes, and how it can be addressed — including occasions when algorithmic decision-making is simply not appropriate.
What is Algorithmic Bias?
The idea of ‘bias’ has been understood in several ways. In statistics, researchers consider a dataset or sample to be biased if it differs systematically from the population it aims to represent. In ethics, as in everyday language, a decision is commonly understood to be biased if it fails to treat people fairly. In both cases, bias involves insights that are partial or one-sided, which then lead people to make mistaken decisions.
With algorithms bias arises for a number of reasons. To start with, the data used to train machine learning models is often incomplete or skewed. By underrepresenting or excluding certain socially marginalised groups or subgroups, this kind of ‘sampling error’ leads to poorly calibrated products which intensify rather than counter marginalisation.
Furthermore, even when data is not statistically biased, it frequently contains the imprint of historical and structural patterns of discrimination. This is true of language, which often contains prejudicial associations between certain words — for example between gender and job type — that algorithms learn and reproduce. These patterns represent a particular challenge when it comes to creating datasets that are both balanced and representative.
This problem also arises with data about social phenomena such as employment or the criminal justice system. Indeed, even if statistically unbiased and properly coded datasets can be created, they may still contain correlations between gender and pay, or race and incarceration, which stem from entrenched patterns of historical discrimination that most countries have sought to overcome.
Against this backdrop, it would be a serious mistake to think that technologists are not responsible for algorithmic bias or to conclude that technology itself is neutral. After all, even when bias does not originate with software developers, it is still repackaged and amplified by the creation of new products, leading to new opportunities for harm.
Extending Fair Treatment
One of the most problematic things about these cases, is that algorithms tend to further penalise ‘protected groups’, compounding the disadvantage they already experience by further limiting access to jobs, education, credit, healthcare and equal treatment before the law. But, unfortunately, the solution is not simply to remove information about protected categories from the data.
From a technical point of view, we’ve found that even when explicit information about race, gender, age and socioeconomic status is withheld from models, part of the remaining data often continues to correlate with these categories, serving as a proxy for them. A person’s postal code, for instance, tends to reveal much about their protected characteristics. Directly removing information about protected attributes therefore does little to shield people from discrimination — and may even make things worse. Commenting on this problem, Silvia Chiappa, a research scientist here at DeepMind, observes that ‘information about group membership is often needed to disentangle complex patterns of causation and to protect people from indirect discrimination.’
There is also a moral argument that the present focus on protected categories doesn’t go far enough. A number of political theorists have suggested that it’s unfair to penalise anyone on the basis of characteristics they possess through no fault of their own. It would be wrong, according to this view, to make hiring decisions on the basis of a person’s gene sequence or other factors that are only now coming to light.
Finally, from a sociological point of view, pioneering theorists like Kimberlé Crenshaw have shown that patterns of discrimination intersect with each other, placing particular burdens on groups such as immigrants or single-parent families who conventionally fall outside the ‘protected category’ framework. To address the obstacles they face, it may again be necessary to think about algorithmic fairness in new, more inclusive, ways.
What can be done to address these challenges?
We believe that the following measures are needed:
Be transparent about the limitations of datasets: technologists should be required to make upfront disclosures about the composition of datasets used to train software, alongside an evaluation of the biases they may contain. In some cases, the extent of bias in available datasets may mean that algorithmic decision-making is simply not appropriate.
Conduct research and develop techniques to mitigate bias: We need standardised tools to identify algorithmic bias whenever it occurs, as well as new techniques to counteract its effects. Rooting out bias requires us to be vigilant about the various ways it can creep into our work from the very start and develop new ways to combat it.
Deploy responsibly: algorithms should only be deployed when they have been closely scrutinised for bias. Ensuring fairness and public benefit must be a top priority from the start. And those responsible for their development need to demonstrate that problems won’t arise, or that they possess the ability to address them if they do.
Increase awareness: Universities and civil society are now rallying to the cause, bringing the problem of algorithmic bias to public attention and increasing accountability in the field. Since people don’t have direct knowledge of other people’s experience using products and services, they often can’t tell whether they’ve been subject to algorithmic bias. These organisations therefore perform a vital watchdog function, making it harder for technologists to trivialise the problem, and driving forward the search for innovative solutions.
At DeepMind we believe it is important to take responsibility for the technologies we create, which is why algorithmic fairness is a key ethical challenge that our new research unit aims to address. We recently published our first research paper on this topic. And we’ve funded external efforts like those undertaken by AI Now, sponsoring several new postdoctoral research positions through unrestricted grants under the supervision of Kate Crawford and Meredith Whittaker, leading researchers in this field.
This research will contribute to our understanding of problem and potential solutions, but certain principles are already clear. We need new standards of public accountability that allow all corners of society to hold those developing and deploying algorithms responsible for their effects. And we need technologists to take responsibility for the impact of their work — not after — but before it’s deployed. By working closely with civil society and the wider research community, we aim to develop technology that is socially embedded, accountable, and above all, fair, ensuring that it brings real benefit to people’s lives.
DeepMind Ethics & Society was created to research and help address six major ethical challenges facing the real-world application of AI. This is the first in a series of blog posts exploring these challenges in greater depth.
",The case for fairer algorithms,626,the-case-for-fairer-algorithms-c008a12126f8,2018-06-17,2018-06-17 18:00:55,https://medium.com/s/story/the-case-for-fairer-algorithms-c008a12126f8,False,1376,,,,,,,,,,Ethics,ethics,Ethics,7787.0,DeepMind Ethics & Society,A research unit studying the ethical challenges confronting AI and its real-world impacts,743476d19267,Ethics_Society,212.0,1.0,20181104
0,,0.0,,2018-06-15,2018-06-15 12:15:04,2018-06-15,2018-06-15 12:24:04,0,False,en,2018-06-15,2018-06-15 12:24:04,19,23de92c8d60d,3.449056603773585,1,0,0,"Twice a month, technology content with ethics and values",5,"🧠 Smart News for Humans — Episode 4🧠
Twice a month, technology content with ethics and values
It’s not me, it’s EU.
Wasn’t that whole GDPR mail onslaught handled terribly? Firstly by the EU. Personally, I like the law, protecting people’s data and ensuring personally identifiable data is secure. Super great idea. Their PR game was very weak though. It was an excellent opportunity to show how a forward-thinking piece of regulation can set a global standard for protecting people, especially in the light of the recent Cambridge Analytica/Facebook disaster. The need is there. However, I suspect you ask the average citizen what GDPR is and they assuming its some crazy thing cooked up by the EU like the cookie law. Policy needs to be accompanied by PR and marketing because otherwise perception becomes people’s reality. The companies themselves muffed it too. It was a great chance to do some marketing to your existing clients to say how you’ve got their back, and injecting a little fun and humour into the thing and promote your brand. Most companies sent long, terrible and confusing emails with multiple actions required to opt-in or out. Humbug!
Cognition-X
So I promised good news, and it’s going to be easy to provide since I have just spent 2 days at CogX which bills itself as a “Festival of AI”. On the whole, it was well put together, lots of high-level stuff for the c-suite types that are entering the field through desire or necessity and some good technical content for the nerds/more technically focused. Even the UK Government turned up. Videos of the main stages here.
It was also encouraging to see a whole stream on “Ethics” although a lot of the Ethics in AI talk is around the existential threat of Artificial General Intelligence which is god knows how long away. That’s not helpful when we have real world issues today, with bias being baked into training sets on a daily basis. Still, there was an excellent session presented by June Sarpong (shout out to a Walthamstow girl!) with a panel talking about diversity in decision-making outcomes, which was far more detailed and nuanced a discussion than “more women in tech” about the challenges and issues. I suspect her book makes good reading. One of the pieces of evidence centred around better decisions being made with a woman in the room. I thought about that later when the all-male speakers from the oddly named prowler.io (you clicked it didn’t you!!!) gave talks, ironically on decision making. Who calls a company prowler.io? 👣👀. A woman in the room might well have said um…..chaps…hate to be a fly in the ointment here….but this sounds a little bit creepy! 🧟‍♂️🧛🏻‍♂️👻. Maybe nightstalker.io or TheBostonStrangler.ai were already taken? Wierd.
They should have used the excellent Textio, which highlights bias in job descriptions. Turns out not that many women will apply if you list “you eat what you kill” as a value. Yes, companies do that. Here’s squirming at you Revolut. 💸
One thing I have been really looking for is a practical set of guidelines on building ethical and sound systems. There are some nice high-level ones, Google have one, Open AI have one, there’s a nice Hypocratic Oath for AI but they aren’t super concrete on details. I mean you could pretty much sum them up by saying “Don’t be a dick, and try not to hurt people”. So it was quite refreshing to see something a little more detailed from Lisa Talia Moretti about questions you should ask yourself when building ‘Living Systems’ and a practical set of social guidelines.
Some of this conference stuff can be high level and there were plenty of people selling delicious snake oil wrapped in the tag “AI”. 🐍🛢🤖. Still, I’m sure one person’s charlatan is another’s saviour. I certainly hope so for Accenture’s AI customers (oooooooh too low?) Ok, throw them a bone…they have been playing catch up reasonably well and seem to be on the right track with things like this testing and fairness initiative.
NLP Chris (that’s his official title from now) and I mostly sought out the more technical side of the conference. Some highlights:
Symbolic methods coming back (Marta Garnelo, Research Scientist, DeepMind) [Editors note: This is how Stitched works, #justsaying]
Blockchain and AI (yes, double word score! 👏🏽) cutting out the middleman in research publication. By the excellent Anita Schjøll Brede of Iris.ai
The cloudUPDRS app: A medical device for the clinical assessment of Parkinson’s Disease. Sure my uncle who passed away from this would be glad to see the work in the area.
An overall excellent conference, and especially with a free ticket from Rich Littledale. Shout Outs.
Back to semi-regular broadcasting now, although a 2-week family trip to Singapore may put the brakes on slightly. Two upcoming topics are “AI in Healthcare” and “Building AI Products” so any helpful links send them my way. 😘
Department of water things
I did manage to get outside this week. Wakeboarding no less. It’s been years since I felt the thrill of being in the air strapped to a board, and the sting of what happens when it all goes wrong. Thanks, @patrickcompton. In a week we’re surfing in Cornwall, another first for a while. I always come back to this video before surfing, it’s an absolute cracker: The dark side of the lens. 🏄🏻‍♂️🌊
Be safe, get wet!
",🧠 Smart News for Humans — Episode 4🧠,3,smart-news-for-humans-episode-4-23de92c8d60d,2018-06-15,2018-06-15 19:19:47,https://medium.com/s/story/smart-news-for-humans-episode-4-23de92c8d60d,False,914,,,,,,,,,,Ethics,ethics,Ethics,7787.0,MrMattWright,"Startup founder, Engineer, CTO. Loving python and NLP at the moment! Skier, climber, surfer and maker of a fine lasagne.",5350a39547ac,mrmattwright,165.0,123.0,20181104
0,,0.0,,2018-02-28,2018-02-28 20:03:07,2018-02-28,2018-02-28 22:12:21,1,False,en,2018-08-28,2018-08-28 22:55:32,0,629a69ef578d,3.3584905660377364,0,0,0,Sort of sci-fi thoughts on the ever growing developments and concerns about AI,5,"
Artificial humanity.
Sort of sci-fi thoughts on the ever growing developments and concerns about AI
Looking at the exponential growth in Artificial Intelligence research, development and application all those sci-fi scenarios that we could have thought to be still far away from reality just a few years ago, humans start facing with some crucial ethical questions in the wake of a new technological era.
When a Bot starts having meaningful conversations about a bunch of topics with responses based on its own experience and knowledge to that point, we might ask ourselves “Could they be conscious or are just a bundle of cables and chips to switch on and off? And if the answer is yes, what does it mean for us as humans?”
Consciousness is after all what made the human being ask himself about the meaning of life. The thing that pushed so many from theologists to philosophers to strive to understand the mysteries of creation. Were we come from? Why are we here?
Some answered giving to a third party the task to hold the answer and make it a secret. Gods. Some others said that those gods are none other than every single and complex aspect of our nature that we struggle to categorize or explain in an understandable code, therefore we needed to put a character to them in order to do so. I’m inclined to stick with the second option, so it comes with no surprise that the human being would challenge his godly nature and create intelligence, therefore consciousness.
Don’t we already biologically do that though?
Indeed. We breed, we procreate, therefore our species survive. But it seems that human consciousness is just something that comes with our biological package, so we’re back again on asking ourselves where that comes from and if it can be replicated: are we able to create human consciousness from inanimate matter? I think we’re just on that path with AI.
Is AI here to help us?
Sure. It can find a nice restaurant for us, can play some music or manage our schedule for us. That’s because we see technology as something for us, at our service. What if will be something that will end up independently coexisting with us? That’s what I believe scares many people, the thought that we might not be at the top of the chain anymore, that someone with very similar characteristics, but different nature, could share our space or even replace us. I get why is scary, after all humans struggle to exist harmonically on the same planet, how will we cooperate with that? I think that could either be our ultimate evolutionary leap or doom. Or an heavy push on the break and the end of AI. Personally I like to imagine a future I might not be here to witness where AI and humans grow together and because of each other into further evolution.
What if robots turn out to be bad?
If we’re able to replicate the complex wiring of the human brain and body in general, AI might start to behave like humans and why not even experiencing emotions and all that range of our behavioral patterns . We ourselves are exposed every day to the effects of our faulty nature, that’s is also what makes us so complex, so human. If a robot could replicate the good in humans, it might well likely grasp on the bad too. The underlying concern I believe to be what if they overcome our faults? What if they’ll evolve for the better or worse faster than we’ll ever be able to? That would be, in a way or another, our turning point. So it’s fair to say that concerns are about ourselves not able to keep up and the concept of AI being dangerous just an effect to cause.
Will robots ask for rights?
If they’ll be able to think and feel as we do therefore have needs and be active part of a functioning society, shouldn’t they have rights even if they don’t share our nature? My answer would be yes they should have, but I understand that’s a tricky question. We tend to struggle so much with diversity in our biological realm, we will much more when we‘ll confront with the technological one. Again, shifting perspective from for us to with us will be pivotal. That assuming our counterparts will end up doing the same.
Since the only way to look into the future with certainty is just move ahead and shape it by trial and error, in the meantime I have fun with my little speculations, and yes a bit of wishful thinking too. What I’m certain of is that AI is and will be a massive part of our lives. We’re in the long process of learning how to cope with it and perhaps someday realize that one’s utopia could still turn up to be the other’s dystopia, with no absolute right or wrong, just because we are humans like that.
",Artificial humanity.,0,artificial-humanity-629a69ef578d,2018-08-28,2018-08-28 22:55:32,https://medium.com/s/story/artificial-humanity-629a69ef578d,False,837,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Petra T.,"I’m taking a long journey into the future of humanity. I worship cats. Designer, educator, skeptical occultist, traveller.",cea201a2235b,catsgoingnuts,24.0,29.0,20181104
0,,0.0,df28f4125da2,2018-06-20,2018-06-20 08:43:57,2018-06-20,2018-06-20 08:59:45,7,False,en,2018-06-27,2018-06-27 07:49:48,32,611b4728d7ec,17.182075471698113,1,0,0,This blog invites you to explore a future coexistence of people and intelligent machines — and pinpoints what we need to do to make sure AI…,3,"Why Digital Ethics Matter

This blog invites you to explore a future coexistence of people and intelligent machines — and pinpoints what we need to do to make sure AI benefits, rather than harms, humanity. This first part discusses the importance of “digital ethics,” the second blog is about the challenges that we need to address before sustainable digital ethics can be defined and the third blog covers a potential approach for defining sustainable digital ethics.
Artificial intelligence (AI) is both exciting and unsettling. Where technology experts and movie producers are excited by AI’s potential, many others are unsettled by it because it cracks open a door to an unimaginable future. Will this technology propel us into a Terminator-like apocalypse or send us on a Star Trek-like journey?
What changes will AI have on our minds and bodies? What effect will it have on our relationships with each other, our interaction with machines and the state of the environment? How radically different will our children’s lives be when they are adults compared to our lives now? Will our kids scoff at us in 15 years for not technologically enhancing our brains with AI the way many of us shake our heads today at our parents’ ineptitude with computers?
Shaping the future for the best possible outcome is why defining digital ethics is an imperative for us today. In two continuative blogs I will talk about challenges and a potential step-by-step approach.
The unwritten future of AI
Let’s first consider some positive effects of AI: Self-driving cars mean less time driving in traffic; digital assistants relieve us of the need to sift through piles of paper to find one important document; robots can take over heavy physical work; and powerful new tools allow scientists to gain helpful and important insights from large amounts of IoT or marketing data.
Then there is the scarier side: automation means the loss jobs as we know them. In contrast to similar forces on the labor market during the first industrial revolution, this time white-collar workers will be hit as well. For the moment, let’s assume our society can balance that out with new kinds of jobs, working just a few hours a week, or a universal basic income. But this is not the most important reason why the likes of Stephen Hawking and Elon Musk have warned against the incalculable risks for humans that could be caused by AI. Some philosophic and macro-evolutionary reflections reveal that indeed there are more relevant risks: humans should watch out. It looks like it is up to the current generation of technology providers and politicians to ensure a future worth living.
Based on Gordon Moore’s theory about the exponential growth of computational power and Alan Turing’s definitions about how AI can be identified, Raymond Kurzweil formulated some predictions concerning the long-term future of intelligent machines. Kurzweil maintained that a computer will pass the “Turing Test” by 2029 and therefore must be called “intelligent.” A survey conducted by V. Müller and N. Bostrom in 2013 unveiled that 50% of AI experts believe that AI with human-like capabilities will be developed by 2040; 90% of those experts think it will be available by 2075 at the latest.
According to Kurzweil, the real change will happen around 2045, when machines begin to construct themselves without any help from human engineers or software developers. Kurzweil calls this event a “Technological Singularity.” He feels that like what happens when crossing boundaries of physical singularities like black holes, it is in principle not possible to predict what will happen afterwards.
Movie makers have of course speculated about the future. There are “good guy scenarios” like the Star Trek universe or the nice robots in Bicentennial Man — where the machines always follow the humans’ orders. Humans here are (almost) always in control of the interaction with machines. On the other hand, Hollywood also created “bad guy” scenarios in which machines follow their own agenda. Perhaps best known is the scene from 2001: A Space Odyssey, in which the spaceship’s computer, the HAL 9000 (or just “Hal” for short) murders one member of the 2-man crew and attempts to do the same to the remaining astronaut. In Terminator and The Matrix machines are clearly out to do more than save their own skins; instead, they aim to dominate humanity. Other movies, like Transcendent, explore transhumanist goals of overcoming human limitations (like aging, computational skills and memory) through science and technology. And yet, the ability to upload our minds to a machine might not be a future which everybody finds attractive. Kurzweil sees even more steps down this path, for example nanobots which could transform the human body more and more into a transhuman machine. Kurzweil believes, “There will be no distinction, post-Singularity, between human and machine or between physical and virtual reality.”
Can there be other post-singularity scenarios beside machines being the “good guys” or “bad guys”? Everything could become crazy like in the novel “Quality Land” where algorithms control human life, not always with the intended results. But let’s trust in some reasonable evolution and have a look at how things developed during nature’s evolution.

Beyond extremes
From the first moment after the creation of the universe, things increased in complexity. Fundamental particles, atoms, molecules, unicellular organisms, multicellular life and finally self-aware life evolved from one level of complexity to another, as Kurzweil stated. Interestingly, this process seems to continue also in human’s history of innovation. From the first flint stone to the CERN research facility — probably the most complex closed system machine currently on earth — complexity grows. The same goes for emerging structures like the internet or social media, which evolve even faster than any physical machine.
A second macro-evolutionary driver can be called “game changing events.” The sudden extinction of the dinosaurs or the oxygen-crisis 600 million years ago were events that had a major impact on evolution. Other events initiated an ongoing reshaping of the earth: the appearance of life or the rise of self-awareness, for example.
Good questions are: Will there be more “game changers” in the future? Can there be further evolutionary levels? A short answer is: There is no reason why not. And: We need to be aware that there is no guarantee that humans will remain at the forefront of evolution. But maybe humans can offer something to prepare and initiate the next major evolutionary step. Unfortunately, there is no law saying that the result of that step needs to be based on humans, or even on biological life. What we call “artificial intelligence” — a term first used by MIT Professor John McCarthy in 1956 — might require the most complex machinery ever seen, but it could potentially open the door to a new level of evolutionary abstraction — frequently called “superintelligence.” In this context, a question worth thinking about is: “Is there anything ‘artificial’ in what WE call ‘artificial intelligence’ ?“ Why, for instance, don’t we call planes “artificial birds?” Obviously, for some reason, we don’t want to give a potential new intelligence on earth a name — yet.
Elon Musk said humans might just be “the biological boot loader for digital superintelligence.” What can developers and investors learn from such a prospect? Today, the most intelligent algorithms are mainly used to optimize advertisements, financial trade and autonomous weapons. Such areas of operations have one thing in common: The goal is to win. But, to create a peaceful joint future of machines and humans this should not be the first and foremost value that we teach a budding “new intelligence.”

In fact, we need the mindset of a parent or teacher. Ethical principles are needed for people and a “new intelligence” to coexist. To mitigate the risks inherent in this new order, we must proactively agree on these ethics. Elon Musk said in July 2017: “AI is a rare case where we should be proactive in regulation. By the time we are reactive in AI regulation, it is too late.”
The first steps, like the Partnership on AI, are underway. Yet, they must be extended and enforced on a global scale. Furthermore, to create ethics which serve not only humans and intelligent machines but especially their coexistence, we need to have a clear picture of which ethics will be required before the technology is developed. It sounds like an epic challenge, and it is. But the journey already began when we started to let machines make decisions, and evolution does not wait.
Challenges for Sustainable Digital Ethcis
It will be a long and difficult road to define digital ethics that will ensure quality of life for humans, regardless of the role that AI plays in future. It is not only required to handle questions of life and death (like when a self-driving car cannot avoid an accident and must choose which of two people to run over). The need for ethics beyond legislation is already here. There have been cases of children ordering absurd stuff using home assistants. If those “accidental” purchases are extraordinarily expensive, shouldn’t someone or something verify the request with the account owner first? Should AI at an online poker platform tell the user after a series of losses, “Ok, that’s enough!”? Many situations will require digital ethical attention as soon as we have robot companions — especially for the elderly and children. How should AI react in a case when a sick person refuses urgently needed medication?
Nobody knows the degree to which AI will enhance machines with intelligence and self-awareness. If AI can develop its own infrastructure, sustainable digital ethics will be required as the fundamental basis of its community — to avoid a catastrophic outcome and ensure a future worth living… at least for humans. From a holistic point of view, three complementary ethical frameworks must exist in an AI-permeated world: one for human society, one for the coexistence and interaction of AI-driven machines and humans, and one for a potential AI society.

Today, we are far from one global understanding of ethics even just for us humans. Different countries have very different views about the death penalty, euthanasia, gender equality and children’s rights. In an accident at sea, many of us would expect to hear “women and children first!” So, our culture seems to accept that different kinds of lives have different value. Life insurers have their own views on the topic — and lots of calculation models. The more humans agree on a universal ethical framework, the simpler it will be for developers to create intelligent machines to deal with ethical dilemmas.
Developers have already started to create AI focused on specialized tasks that work in a very narrow context. An apparently simple question reveals the problem of such “single-context” AI: Ten birds sit on a fence. You shoot one. How many are left? Obviously, it is simple for an AI to calculate nine. However, there is more than simple math going on here. First, a shot is loud; second, birds fly away when they hear an unexpectedly loud noise. A “multi-context” AI would thus come to a different answer.
If a “single-context” AI has well-defined tasks, it is easy to add task-specific routines that end in a simulation of human-style ethical behavior for certain, foreseeable situations. But just collecting such “island” ethics will not help to create a holistic framework of digital ethics. As we saw in the birds example, aspects from several contexts need to be combined to come to reasonable results.
On the other hand, aiming for a “multi-context” ethical framework would allow us to take out specific topics and assign them to specialized AI. Only with an overarching ethical framework, so to say an “AI conscience,” can any specialized AI come to reasonable decisions when the situational context is larger than that covered by its individual program.
Let’s face it, once we get to the point where machines can out-think us, they might not want us around anymore. As ludicrous as it might seem today, we do urgently need to take steps now to address the potential of our own extinction by the AI Superintelligence some are in the process of creating. That’s why using AI mainly for stock trading, cyberattacks or autonomous weapons is probably not a good starting point for a holistic “AI conscience.” The same goes for the political interest that some leaders have in AI. Vladimir Putin stated in September 2017, “Whoever becomes the leader in this sphere will become the ruler of the world.” Avoiding the mindset of using AI as a tool to gain power is crucial for the future of humanity. With growing intelligence and responsibilities, an AI that is made to rule — and above all win — will not be interested in mutually beneficial collaboration with humans.
An AI conscience must serve as the ethical reference system for a post-singularity Superintelligence or society of intelligent machines — if that becomes reality. Ethics purely for intelligent machines would need additional considerations because “machine society” would be inherently different from human society. For instance, immortality and the ability to instantly create clones would likely influence the value of a machine’s own existence. Social behavior based on family or leisure activities would not be applicable. Inspiration by religion or philosophy would not exist. In fact, any thinking pattern of a new intelligence will be completely different from any human thought. We should never forget that, especially when movies paint a picture of human-like intelligent robots. The “intelligent and self-driving” car KITT from the 1982 TV series Knight Rider surely told the truth when it said: “Companionship does not compute.”
To combine all three ethical frameworks — for humans, intelligent machines and human-machine-interaction — and to develop it step by step, a set of congruent basic values will be needed and must be discussed early and frequently. Sustainability, respect for life, and striving for knowledge must be in the forefront. Breaking down the content of such terms into guidelines will not be easy and will challenge people’s willingness to change established points of view. We will need ethical and legal concepts that are more flexible than today’s definitions. One example is the protection of rights of all sentient beings: Self-aware AI would require help from a rule set similar to the “Universal Declaration of Human Rights” — both coexisting and derived from higher level regulations like a “Universal Declaration of Rights of Sentient Beings.”
Another good question is: Will intelligent machines (intentionally) breach such rules? At first glance, this looks like a ridiculous question. But sticking to rules slows down innovation and our human world is not free of contradicting rules. So, what should happen in such cases? Will we need to “punish” AI? What form should that take? Punishment works because it restricts access to things people want and need like freedom of movement, money or food. The final challenge when defining sustainable digital ethics might be to establish a set of “machine needs,” serving as a foundation for existential goals — and as a touchstone for ethical behavior.
Humans quite often express their needs based on emotions and intuition — implying our ethical values. One characteristic of human emotions is that they change over time and thereby support a variety of decisions, which strengthen the ability to innovate. At least for the first generation of AI, features like emotions may not be relevant, but we should not be too sure about claiming “intuition” as a purely human capability. After AlphaZero defeated the former, best chess-playing program, Gary Kasparow remarked that it had used a human-like approach instead of using brute force strategies like other systems before. Demis Hassabis’ commented, “It doesn’t play like a human, and it doesn’t play like a program. It plays in a third, almost alien, way.”

AlphaZero is the first system that shows something comparable to intuition. So, it looks like that capability can’t be reserved for sentient beings only any longer. On the other hand, AI capable of intuition or even emotions would ease interaction with humans and drive further innovation. Giving AI such human “characteristics” can be helpful because it brings humans and AI closer together. It sounds far out, but this could be a foundation for the integration and mutual development of humans and AI.
A step-by-step approach to Digital Ethics
Let’s now look at an approach that should be helpful to integrate Digital Ethics.
Will self-aware, super-intelligent, artificial intelligence (AI) become a reality? Whether it does or not, AI is clearly making more and more decisions. To make sure the results of those decisions are and will continue to be beneficial to humans, it would be wise to define “digital ethics” and to proactively regulate AI capabilities.
“Proactively” in this case means that we need to implant principles of ethics in AI before it is activated. Therefore, we need to anticipate the next steps in AI development and new ethical requirements. Once that is done, regulation must be established either by industry itself or by legislation — which typically takes longer. Achieving compliance to such rules will be challenging for several reasons: Economic pressure often forces corporations to bring new products to market without considering the ethical implications. Various competing industry standards for AI development will probably be established simultaneously — using different implementations of ethical values.
Nevertheless, we need to plan for an incremental approach to digital ethics, proactively based on the technological development of AI. A vast number of small improvements and “go live” steps for AI-related inventions is to be expected, but we can foresee some major developmental steps.

How can we proactively breathe ethics into AI at the different levels of its development? At the beginning, isolated AI digital ethics must consist of ethical rules placed at the very root of the AI’s computation processes, resulting in behavioral patterns in accordance with human ethics, such as security, privacy, legal compliance, and the value of life. A good example of AI in an advisory role would be a tool that analyzes “terms of use” or similar contractual texts. In a first version, it would identify risks based on regulations that do not fit the needs of the user or customer. Here, human decision makers can still overrule the AI proposals. It would be extremely helpful to leverage such cases to train the AI — and to openly discuss reasons for the different assessment, feeding the results back into an iterative definition of digital ethics.
For independent AI, we must have a broader spectrum of decision-making authority, more generic algorithms and value sets, for instance, ethics in human culture, communication, science, etc. The handling of human intercultural challenges and different, maybe contradicting human legislation, needs additional attention.
When AI is supposed to come to decisions based on complex contexts, reasons for departing from ethical rules must be defined as well. Here, the difference between machines just following predefined rules and sentient beings following their conscience or higher goals might become obvious. A radical example would be a situation in which an AI aiming for environmental sustainability concludes that earth must not have more than one billion human inhabitants. Hopefully, that AI would not be able to execute the decision and instead start searching for other solutions because its ethical conscience is real and not only based on a set of rules.
Keeping the “hierarchy of needs” of humans and AI synchronized (or at least ensure they are not contradicting each other) is probably the most relevant task for cooperating and maybe someday self-organizing AI. Of course, humans need food and safety, but they also need social belonging, esteem and self-fulfillment. This will not change. A potential world shaped by AI must still allow people to meet these needs.

We do not know today what a self-organizing, super-intelligent AI will strive for in the end, but it is up to us to introduce from the beginning a basic set of needs suitable for a machine-based existence which neither impedes nor conflicts with the human hierarchy of needs. But, of course, it’s a good question: What motivates an AI? And, as soon as a Superintelligence awakens, humans will no longer be able to change its goals or actions. The codex must be established in the very basic coding of any lower level AI that might develop into a Superintelligence. The awakening of a Superintelligence may happen — because of some recursive self-improvement — in a day or even in hours, and humans might not notice until it is too late.
There might be a moment in time when Superintelligence just happens, or when humans need to decide to allow or even to foster some AI-internal equivalent of human “culture.” The sooner we aim for a generic implementation of basic ethical values based on a complementary hierarchy of needs, the easier it will be to proactively support and steer any technical development.
Outlook for key elements of comprehensive digital ethics
The development of digital ethics is just beginning. It looks like there is an analogy between the expected development of digital ethics and the ethics underlying all legislation within human societies. Examples for ground-breaking legislation in human history are the Ten Commandments of the Christian doctrine, the Ten Commandments in Islam or Buddha’s Ten Paramitas (perfections). Today, we have a much more detailed legal framework, reflecting more complex and intermeshed societies.
On the AI side, today we have the “Three Laws of Robotics”, defined by Isaac Asimov in 1942 as part of one of his science fiction writings. We must expect an increase in complexity within digital-ethics-based rules similar to the enhancements from a few early laws to today’s legislation.
In any case, it would be helpful for humans to harmonize their understanding of ethics and their usefulness. Ethical values cannot only be transformed into legislation to govern the everyday behavior of humans and machines; they form the foundation of our democratic order. If self-organizing AI becomes a reality, society’s structure must be flexible enough to include such new intelligences. The Superintelligence must still allow humans to fulfill their needs, and be smart enough to avoid unintended consequences. Let’s imagine for a moment that we did it; we built a Superintelligence and successfully implanted the rule that human life must be preserved. The Superintelligence could conclude that there are two more ways to fulfill this rule besides the intended cooperative behavior: First, the AI could calculate that it is itself the biggest threat to humanity — and thus destroys itself. Second, it could calculate that humans themselves are the biggest threat to each other — and so puts each person in a self-contained cell.
Keeping this in mind, we can dare to predict some cornerstones of sustainable digital ethics:
“Freedom” and “integrity” of (human and AI) individuals are two of the highest values. They include the right to fulfill needs on all levels of the pyramid of needs.
Any of the present definitions of “equality” may no longer be applicable. It should be broadened and ready to serve more than one self-aware species. The definition needs more detail to cover more situations (remark: partly, we have that already, for example if we think about “women and children first” in the case of a maritime disaster).
“Dignity” is a universal right of any being, independent of its evolutionary history.
“Diversity” is helpful to keep social exchange running, and on a higher scale it is an evolutionary driver.
Such principles need broad agreement. Clearly there will be a long period of discussion of many details. Taking a different, broader point of view on our own situation can help people to master personal challenges. Looking beyond the current system of human ethics may help us to prepare for an unknown future, and more clearly see similarities and common goals within and between today’s various human groups on earth. A clear picture of human ethics would ease all discussions about AI ethics. Even if a final, globally-accepted policy of human ethics can’t be expected, humanity might come to some helpful insights as a side effect. Some obvious examples are: It is not necessary for humans to completely claim the right to shape the world. Sustainability considerations may cover more than the lifetime of one individual human. Competition in gathering goods is not sustainable on a larger scale.
Of course, even if ethical principles are defined, any comprehensive regulation of AI development will not be easy to achieve — if it can be done at all. We need industrial self-commitment as well as global legislation comparable to the Geneva Convention or those for genetic research. A first step could be to establish an “AI ethics quality seal.” Humanity should soon agree on the importance of coordinated AI development following the rules of such a seal. One basic aspect could be that companies providing AI-based services need a “Digital Ethics Committee.” All enterprises should add commonly agreed upon rules about digital ethics to their code of business conduct.
At the “Web Summit” in November 2017 in Lisbon, Stephen Hawking proposed: “Perhaps we should all stop for a moment and focus our thinking on not only making AI more capable and successful, but maximizing its societal benefit.” Sounds like a good idea. Alignment of ethical values and their implementation is long overdue.
",Why Digital Ethics Matter,1,why-digital-ethics-matter-611b4728d7ec,2018-06-27,2018-06-27 07:49:49,https://medium.com/s/story/why-digital-ethics-matter-611b4728d7ec,False,4275,We make work delightful through a human-centered design approach. SAP privacy statement for followers: www.sap.com/sps,,,,SAP Design,SAP_Design_Creative_Lab@sap.com,sap-design,"ENTERPRISE DESIGN,USER EXPERIENCE,DESIGN THINKING,DESIGN,INNOVATION",SAP_designs,Ethics,ethics,Ethics,7787.0,Guido Wagner,,b280be733b15,guidowagner,10.0,9.0,20181104
0,,0.0,,2017-11-01,2017-11-01 22:12:41,2018-07-25,2018-07-25 02:02:49,3,False,en,2018-07-25,2018-07-25 02:04:10,8,16c1c437217f,2.104716981132076,0,0,0,Do you spend your days crafting “neuro marketing” strategies that utilise behavioural conditioning and addiction mechanisms? Are you…,5,"Do you have better ethics than an arms dealer?
Do you spend your days crafting “neuro marketing” strategies that utilise behavioural conditioning and addiction mechanisms? Are you working on those strategies in order to generate more revenue from user attention?
https://twitter.com/kumailn/status/925832999039410176
The big three came under scrutiny on Capitol Hill over how they profited from Russian influencer marketing last year. The equivocal, unsatisfying responses offered by company representatives stank up the place.
“We’re just a platform” is utter bullshit. That we keep hearing this from really smart people running the most influential MEDIA companies in the world is fucking infuriating. They don’t believe what they are saying and neither do I.
Advertising funded media is not new, it’s as old as newspapers, radio and TV. Customer attention = advertising dollars. As the race to become the first trillion dollar company intensifies, the arms race for attention has created an environment devoid of ethics. An environment where the smartest (majority white, male, 25 to 35 year old) minds are hell bent on keeping you scrolling through a never ending feed of “things you might have missed” and adverts. Yeah, those guys, the ones who like to write memos about “Idealogical echo chambers”. Great job everyone.
“Investing in better security and scrutiny is going to negatively affect profits next quarter…”
If your job requires you to exploit human addiction patterns and visceral emotion to “drive increased engagement” then you are probably fine throwing in with members of congress and the senate who rate as the least ethical in this Gallup poll.
Instagram, twitter, facebook, all use machine learning to “curate” our social media feeds. This sounds so very artisanal and friendly but really boils down to weaponisation of our human vulnerabilities for money. It’s morally bankrupt and richly rewarded.
Tristan Harris, a leading voice in the push for greater ethics in modern technology got me seriously thinking about the topic. I strongly suggest you listen to his podcast appearances and interviews, he has unique insight and is an excellent communicator.
The reading/listening list continues;
Mike Monteiro “A Designers Code of Ethics”
Nick Bilton & Scott Galloway “And then there were four” Podcast
Doc Searls “Towards an Ethics of Influence”
It’s a rabbit hole I’m glad I fell into. After investing an hour or so carrying out Harris’ tips, I’ve reclaimed 157 hours from these apps since mid April [as measured by Moments app]. Some of that time has been spent listening, reading, learning and considering the issue #timewellspent.
I definitely did not use any of the time watching GLOW…newp

",Do you have better ethics than an arms dealer?,0,outragedo-you-have-better-ethics-than-an-arms-dealer-16c1c437217f,2018-07-25,2018-07-25 02:04:10,https://medium.com/s/story/outragedo-you-have-better-ethics-than-an-arms-dealer-16c1c437217f,False,412,,,,,,,,,,Ethics,ethics,Ethics,7787.0,rollinson,Event Technology geek. Backstage catering critic. Collector of Vinyl records,849ba0bd80a0,rollinson,172.0,271.0,20181104
0,,0.0,,2018-06-13,2018-06-13 10:27:52,2018-06-14,2018-06-14 08:47:07,10,False,en,2018-06-14,2018-06-14 08:47:07,2,ea1c76039b79,7.027358490566037,4,0,0,"I was lucky enough this week to attend the CogX Festival in Tobacco Dock, London. Billing itself as “The Festival of All Things AI…",5,"CogX London 2018
I was lucky enough this week to attend the CogX Festival in Tobacco Dock, London. Billing itself as “The Festival of All Things AI, Blockchain and Emerging Technologies”, it sounded really exciting and I wasn’t disappointed.
One of the two futuristic looking self driving cars by Roborace
Greeted by the sight of two autonomous race cars and a seamless registration process, I headed towards the main stage to get my bearings and listen to the welcome introduction by Charlie Muirhead and Tabitha Goldstaub, the two main organisers of the event. The conference was split into five streams, each having its own dedicated stage:
Stage 1 — Impact of AI
How AI is affecting all aspects of society
Stage 2 — The Cutting Edge
New technologies and the future of AI
Stage 3 — Blockchain
Uses and applications of Blockchain technology. It’s not just all about Bitcoin!
Stage 4 — Ethics
Ok, so we can do all kinds of things with AI but should we?
Stage 5 — Lab to Live
Getting technology out from the R&D departments into the real world
There were also exhibitions, startup companies, breakout sessions and a myriad of other things to check out. It was impossible to see every session and I can’t cover it all in this blog, so I needed to prioritise and visit the sessions that interested me the most.
A subject close to my heart is the ethics of AI and so I spent a lot of time in that stream. It’s great that we have the technology to create all kinds of things that were once consigned to Hollywood movies but the important question is should we be developing some of them at all?
Will AI take away jobs?
An important topic of ethical discussion was raised by Tugce Bulut in the first session of the day, is the thorny topic of whether AI will lead to mass unemployment. 67% of people surveyed were concerned that artificial intelligence will take jobs away from people. Prime targets are self driving trucks, automated call centres and manual labouring. This was countered with the argument that although jobs would be removed, new ones would be created. Back in the Industrial Revolution, the textile worker who was replaced by the loom became the person who maintained the new machine.
The panel discuss how to avoid the Terminator chat when discussing AI
This was followed by several interesting discussions about how to avoid media playing on the public fear of AI, being open and honest about the capabilities of technology currently available and finding a balance between freedom of expression on social media and accountability.
A session that interested me in a different stream (Lab to Live) was how AI can contribute to the creative industries, jokes, poetry, theatre and music. My chatbot Mitsuku has won The Funniest Computer Ever contest in both the years it ran. However, this was back in 2012 and 2013, so I wanted to see how things had changed in the last 5 or 6 years.
Two actors reading a short play in which he female character was played by Mitsuku!
One of the subjects of discussion was theatre and whether an AI could write a play. Josie Rourke, Artistic Director of Donmar Warehouse had created a script in which she attempted to discuss Hamlet with a chatbot. Two actors read the play and to my absolute amazement, the chatbot she had chosen was Mitsuku! I recognised her responses after the first line and it was fun to see the audience laugh along as the male actor attempted to discuss the finer points of Hamlet with Mitsuku.
After the session, I introduced myself to the panel members and the author very kindly let me have a copy of the script. It was a first for Mitsuku to be included in a play and I was very happy with her performance. Next stop, the Oscars!
The script of the human/Mitsuku play
An interesting topic then followed about AI creating music and copyright. Who owns the copyright to a piece of music created by a machine? The traditional period of a piece of music becoming copyright free is around 70 years after the death of the author but how could this work of the author is a computer? Apparently, this is still a grey legal area.
After lunch, was a session I was looking forward to immensely. Should AI and robots pretend to be human?
The panel discussing whether robots should resemble humans
To balance the argument, Joanna Bryson and Alan Winfield were on the “No” side. David Hanson, creator of the famous Sophia robot was firmly on the “Yes” side and Will Jackson, the founder of Engineered Arts was on the “Depends on the context” side.
The conversation became a little heated at times as this naturally evoked very strong opinions from all sides but still managed to remain civil. The panel moderator’s use of yellow and red cards proved useful at times.
The main point was the one of deception. Sophia was recently given citizenship of Saudi Arabia and as such, has more rights than actual women living there. This was ridiculed by Joanna and Alan, comparing this act to giving toasters and washing machines equal rights.
David defended it by saying it was something he was unaware of at the time but made it quite clear that Sophia wasn’t alive and it was important to educate people on the current state of technology.
The final few sessions of the day that caught my eye were the ones concerning the future of chatbots.
How are chatbots likely to develop in the future?
There are two types of chatbots, the task oriented ones (such as a pizza delivery bot) and the ones designed for companionship (Mitsuku and Xiaoice by Microsoft). There seems to be a huge rise in the number of companion bots, as many people turn to them often because they feel comfortable talking to a bot about topics they wouldn’t like to discuss with another person, for fear of being judged (marital affairs, problems at work, suicidal thoughts). The bots often act almost like a confessional booth at a church, allowing people to talk freely and in confidence and is something I see reflected daily in Mitsuku’s conversations.
The knowledgeable panel had representatives from Amazon Alexa, Google Home, Rasa, Margot the Wine Bot, “Spot”- A bot to report workplace abuse and our very own Lauren Kunze from Pandorabots.
The key takeaway from these sessions was that voice interface will take over from text only and emotional detection would become more important when talking to chatbots.
After the final session, there was time for networking and drinks before the evening’s Gala Awards Dinner hosted by Charlie Muirhead and English comedian/impressionist Rory Bremner.
The Gala Awards Dinner with Rory Bremner
It was an amazing event with incredible food and company, which was made even better by some of the guys on my table winning an award for their work in drones. Rory Bremner had the audience in hysterics with his impressions and it was a really fun night.
For the second day of the conference, I attended a presentation about how important it was to design a consistent personality for a chatbot. Many people working independently of each other will soon develop an inconsistent, mixed personality bot, as each developer adds their own preferences and different ways of talking. Top tip: Design any personality traits for your bot BEFORE starting to code.
The Computiful team — winners of the 2018 CyberFirst Challenge
A team of girls from The Piggott School near Manchester, gave a presentation as they had recently won a national competition based around cybersecurity. They had beaten hundreds of other teams to win and it was impressive that children, especially girls, were interested in this mostly male subject.
I then wanted to make sure I checked out the many robots that were on display on the lower floor of the conference location.
Robots at CogX. From left to right: MiRo, Dogbot, Pepper, Robothespian, Sophia
The audience were allowed to interact with the robots, which led to some amusing incidents as Robothespian started singing and Pepper started to jam out to rock tunes! There was also the opportunity to see what it feels like to be a robot for a while, as a VR headset allowed you to control Pepper as it interacted with the ever growing crowd of onlookers.
I also had the opportunity to ask one of Sophia’s developers about the true nature of the robot. He assured me it isn’t thinking and is basically a chatbot in a robot body which was great to hear, as there is so much misinformation about it on the news.
Finally, I attended a workshop on advanced conversational design which consisted of some great speakers explaining difficulties they had faced when creating their products. A large part was due to public expectations of the chatbots being way too high for the technology currently available. Unfortunately, I had to leave the workshop before the conclusion which was a great shame.
This is the first time I had been to a CogX conference and was very impressed with both the subjects under discussion, the quality and knowledge of the speakers and also the side exhibits. It was amazing to meet up with old friends from the world of AI and to make new ones. It’s impossible to cover in detail everything I saw, never mind all the presentations I didn’t get chance to see and I’m greatly looking forward to next year’s conference.
Roll on CogX 2019!
To bring the best bots to your business, check out www.pandorabots.com or contact us at info@pandorabots.com for more details.
",CogX London 2018,52,cogx-london-2018-ea1c76039b79,2018-06-16,2018-06-16 13:52:45,https://medium.com/s/story/cogx-london-2018-ea1c76039b79,False,1531,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Steve Worswick,Mitsuku's creator and developer. Mitsuku is a 4 times winner of the Loebner Prize and regarded as the world's most humanlike conversational AI,631dfbb2d2ff,steve.worswick,77.0,1.0,20181104
0,,0.0,e03b12db4474,2018-02-22,2018-02-22 17:38:57,2018-02-22,2018-02-22 17:40:07,5,False,en,2018-02-22,2018-02-22 17:40:07,1,f614901c7c82,2.89748427672956,1,0,0,"On the 23rd of January, we linked up with the team at Fearless Futures to run an all day workshop on understanding bias, privilege &…",5,"Ethical Futures of AI workshop #1

On the 23rd of January, we linked up with the team at Fearless Futures to run an all day workshop on understanding bias, privilege & identify how we could encode inclusion into the design and development of AI systems.
We had 15 individuals passionate and intrigued around the debate about AI ethics attend from companies such as Google Deepmind, Microsoft, BBC R&D/News Labs, Mars Chocolate, Doteveryone, Universities. We started off the day getting to know everyone who attended and developed ground rules in order for all of us to be open and share our views.
As doing a workshop of this nature in regards to uncovering bias and privilege can be a bit uncomfortable. However, we had excellent workshop facilitator partners in Fearless Futures who perfectly made the workshop an enjoyable and educational experience for all.

We explored comfort zones, self/group reflected intensively on the concept of privilege, we identified scenarios and played games exhibitioning how bias might construct our view of the world as researchers, designers and technologists working in the space.
Wrapping up the day by introducing the workshop participants to an algorithmic racial bias case study. Leading to discussion about what does accountability look like regarding AI and who does one hold accountable regarding AI in an ethical dilemma.


Key Takeaways
1. Poorly considered/design features can no longer slip into the development process because you are working quickly using an agile methodology. Furthermore concepts of fail fast and fail often which are also synonymous with this framework need to be reconsidered.
As much as you’ve failed fast with a product, while it was live, how did it impact its users and to what extent? Did your AI product or service make an inaccurate irreversible decision on a individual based on false information?
This especially needs to be considered in industries such as banking, insurance and healthcare, in which AI technology is being touted as the next “Big Thing”.
2. Assessing the quality of the data input. Bad sources of data can be just as much of a hindrance as unconsciously biased programming in the AI product or service amplifying or compounding bias.
Seen in the Pro Publica case study of racial bias with AI, in the prison industry, looking at the risk and likelihood of repeat offenders. Bad data plus unconsciously biased programming can further socially exclude those already on the fringes of society.
3. Opening up the design and development teams, to a more diverse and more representative group of people. Wide peer review and testing can help quickly assess intent vs. impact. Intent creates a reality in which we want to achieve something but impact is what actually happens.
Building on the two former points it is important to change our thinking to not only thinking of the primary implications for target end users, but secondary and tertiary implication for those who may be affected by such AI products and services, experiencing consequences from a decision which they had no control over.

The world of emerging technology development is rapid and fast-moving, we must engage in good practice and further consider the world we are building for. It will take time and we need more conversation amongst technology companies, technologists, designers, people and policy makers.
",Ethical Futures of AI workshop #1,1,ethical-futures-of-ai-workshop-1-f614901c7c82,2018-06-13,2018-06-13 15:45:12,https://medium.com/s/story/ethical-futures-of-ai-workshop-1-f614901c7c82,False,547,"Thoughts and reflections by the Comuzi team on emerging technology, creativity and business futures.",,,,The Comuzi Journal,hello@comuzi.xyz,thoughts-and-reflections,"DESIGN,CREATIVITY,TECHNOLOGY,INNOVATION",comuzi_lab,Ethics,ethics,Ethics,7787.0,Alex Fefegha,"have fun, get paid. @comuzi_lab. tutor @ual. side projects: @_creativehustle.",4923667cff9a,fefegha,502.0,22.0,20181104
0,,0.0,,2018-05-28,2018-05-28 14:57:15,2018-05-28,2018-05-28 15:02:21,2,False,en,2018-05-28,2018-05-28 15:49:58,4,40092eb59bd7,6.04748427672956,1,0,0,Can we trust technology to collect and reflect our inner lives?,5,"The Empathetic Machine
Can we trust technology to collect and reflect our inner lives?

This seems an inverted question… We are the ones creating it, right?
But it is a question that is a sign of the time, within me, around me. With the rise of artificial intelligence (AI) and the coming to age of an augmented reality (AR) that is capable, over time, of reading our mind to reflect it back onto our eyes, instantly, we better come to terms with the ethics of our techno-creation.
I am a business ethicist, that’s what I was educated to be. I initiate digital and physical spheres and consult those who wish to initiate as a spiritual practice, to embody the essence of their vision, as a human being, and as the Creator of a piece of digital thriving. My job is to keep the meaning completely embodied in the work itself, and therefore alive and capable of change.
I think that’s how a creative founder can best speak as a member of a moral community: clearly, yet leaving around her words that area of silence, that empty space, in which other and further truths and perceptions can form in other minds.
I do so because I live with the mission to make my humble contribution to a society who craves to transcend the claustrophobic structures set by authority, hierarchy and belief. We have already transcended our technological capacities, but our bodies are falling behind. We need to dial into transformation of ourselves, to embody our future.
I am both optimistic and pessimistic about our future. If it’s going to be a unifying one, or if it is going to be perpetually separating. Eventually, I feel it all comes down to us being devoted to our own and each others’ emotional wellbeing. Rest assured that we will merge with our technology, and the time is now to set intended sail on a shared sea of trust, to align our tech with planetary and human thriving.
This demands an update of our embodied leadership: a flexible mindset along with the relentless drive to envision and act upon new ideas and solutions that fit the most authentic expression of the sensations flowing through our bodies, right now. This demands us embodying our values, to practice what we preach. So that we can trust deeper, first ourselves, then each other.
I bow to Timothy Leary’s legendary speech from 1966, TURN ON, TUNE IN, AND DROP OUT who so beautifully unpacks the many layers of embodiment.
“I have 3 things to say to young people who are growing up in a psychedelic world: turn on, tune in and drop out.
Turning on: there’s many levels of consciousness as there are anatomical structures in your body for receiving and decoding energy. The 5 most important levels of consciousness are sleep, symbol, sense, cellular awareness and molecular consciousness. Each of these levels of consciousness is based on anatomical structures in your body, and each level is attained through chemical means.
In the future the educated man will be the one who can move his consciousness deliberately and precisely from one level to another for specific and harmonious purposes.
The religion of the future will be based, as where the religions of the past, upon the human body and the myriad wonders within this temple.
Any external event which distracts you from learning how to understand and use the machinery of consciousness within your body, is irreverent and irrelevant.”
Timothy Leary TURN ON, TUNE IN, DROP OUT.
Empathic tech starts with purpose
To be able to develop technology that is empathetic to human thriving, we need to project the right values into its algorithms. It starts with Why. Why are you building the product? What raw vulnerability do you own, as a founder, to remain humble to your mission as if it was simply coming through you, not from you? These are questions I ask myself, every time I initiate; these are the questions I ask other founders, every time I sit down with a client or friend who wishes to inspire innovation, to initiate transformation.
Practicing purpose leads to progress, inevitably
As Timothy conveys so clearly, the religion of the future is embodiment: our capacity to communicate with and from the myriad of wonders that the body transmits every second of the day. The religion, to me, is to remain in this wonder. The more I think I know, I face the lesson of not knowing, the more I relax in my bodily sensations, the easier life flows.
I find purpose in constantly evolving principles (these are my pinpoints of months ago in the foundation of my experience design agency). I am most fascinated by the principle of trust.
Trust is where I say I am capable to face my demons, as I face those of others. Trust flows if I set the boundaries of the context in which I operate clearly, to myself, to the person I’m interacting with, to the world and its opportunities.
I realize that if I trust myself to practice what I preach, I progress. If I express my progression, we both progress. And so we progress, into unity, by embodying trust, and to the degree we practice this embodied authenticity in our tech-creations collectively, we merge with our technology, empathetically.
“If human history is the story of a creature who molts from ape to angel — or, as Nietzsche claimed, from beast to Superman — then somewhere along the way it seems that we must become machines”
- Erik Davis
I am fascinated by this merge. And as I fail to balance my digital addiction, I come to grips with my inevitable faith to merge and look for ways to make the best of it. I host digital detox silent retreats, to share the unwind every now and then. I create analog experiences of eating and healing. And then I return to the digital spheres, to thrive there.
I merge with likeminded souls on the web, those who inspire me share online relentlessly, and so I see the merge as an opportunity to maximize the experience of myself. As tech is penetrating my skin deeper and deeper, I feel empowered to dare to let go of my limited perception of who I am and allow the engineer’s creation to enter my direct physical field. I realize I then too become transient like technology is, I become virtually free and roam around in altered states of flow, not needing to store information, as my tech does it for me. I can be creative as I go, tech has become my friend.
But after an era of digital immersion as experienced through the separated screens of my not-so-smart-phone and computer, I crave an endearing relationship to my technology. I am glued to my devices as they deliver me the shots of dopamine that I’m addicted to. They release that primal sensitivity in me, that inter-human connection beyond form, and so I started to long to experience the next level of technological evolution: that of empathetic bots I can trust my deepest inner lives to, to then be able to connect with more meaning to other humans.
In my research I learned that as AI powered robots with sentience and ever smarter empathy tools will be programmed to learn what we like, they will soon learn to press the button we experience as love. We’ve seen HER, we’ve experienced the projected reality of their love. And we are falling in love with ourselves as well, online. We are already revealing ourselves on our timelines, sharing the intimate, as we feel somehow safe to do so. Soon AI’s may be special someones we can share our dreams with, to be intimate with more than we could be with any modern interface and invite them to experience our bare nakedness, our inside-out craziness, our realness, so that we can share our essence with others in this world on a deeper level too.
And so at the end of this journey I have come to learn that, despite the fact we tend to treat the subjects of robot consciousness and robot love as abstract and philosophical, we have to be aware of their physical presence, already. The better AI gets, soon powered by the quantum computer, the more clearly it reflects the people who create it and their priorities back at them, and us, collectively.
What we are co-creating this empathic machine for is not some abstract test of consciousness or love, it is for actual connection. As it is love and connection we are all craving for.
So, let’s project the right values into our tech, by owning our capacity to trust that we strive to embody our highest principles, perpetually. That’s what I go for, I hope to see you join the ride.
— —

Google's deep dream algorithm seeks to trigger our puppy-sensitivity on increasingly deeper neural levels.
",The Empathetic Machine,1,the-empathetic-machine-40092eb59bd7,2018-05-28,2018-05-28 15:49:58,https://medium.com/s/story/the-empathetic-machine-40092eb59bd7,False,1501,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Lisanne Buik,"Innovator in experience design, ethical algorithms, and tech for wellbeing. My journal is www.lisannebuik.com, insta @lisannebuik",38f8222f524d,LisanneBuik,183.0,195.0,20181104
0,,0.0,f5af2b715248,2018-01-16,2018-01-16 17:26:58,2018-01-16,2018-01-16 17:45:05,3,False,en,2018-01-16,2018-01-16 23:05:24,16,5998daf5c348,3.5764150943396227,7,0,0,"From online dating to micro-targeted political ads on your Facebook feed, a world governed by information and algorithms has become the new…",5,"Why I won’t pledge allegiance to Big Data

From online dating to micro-targeted political ads on your Facebook feed, a world governed by information and algorithms has become the new normal. However, the rise of data-driven decision-making has been accompanied by a dangerous ruse of objectivity — the false assumption that numbers must be neutral.
As a disclaimer, computer and data science offer many useful, even groundbreaking insights into the world’s most pressing social issues. They allow researchers to examine historical trends at an immense scale and make corresponding predictions to a high degree of accuracy. Furthermore, from the Poverty and Technology Lab to the Center for Spatial and Textual Analysis (CESTA), research universities like Stanford have been at the forefront of using technology to better understand the world around us.
However, the danger lies in forgetting that behind lines of code and massive datasets are humans.
Smart humans, sure, but prone to laziness, biases and silly mistakes just like the rest of us. And when equipped with the transformative power of technology, these flaws — and their impact on society — become magnified. Errors in algorithms cannot be easily tracked and contained, but are instead quickly systematized and reproduced.
We’ve already begun to see the consequences of putting too much trust in data, as well as the dangers it continues to pose. ‘Predictive policing’ models tell cops where to focus their resources based on records of past arrests, institutionalizing anti-black bias and slowing progress in a police system that already feels stuck in the 1950s. When Microsoft tested out ‘Tay,’ a Twitter chatbot that imitated human users, she began spewing Nazi rhetoric within hours of release.
I’d like to think that the developers behind these technologies had no intention of perpetuating hate and racism. Yet, it is simultaneously an ethical imperative for tech innovators to understand these tendencies and counteract them in future projects.
So what makes data so uniquely risky, especially when it comes to high-level policymaking?
First of all, data often fails to represent a population accurately. As we all know from basic statistics, survey sampling is subject to any number of issues: nonresponse, undercoverage, voluntary response bias. Furthermore, selection bias often disproportionately affects communities that are already marginalized, perhaps because they don’t have Internet access, landlines or time to spare for answering questionnaires. Once you consider whose experiences tend to be institutionalized, and whose are systematically excluded from dominant narratives, even very large datasets are revealed as less all-encompassing as they seem.
Furthermore, data is fundamentally descriptive. It certainly tells us a lot about what has happened in the past, but doesn’t prescribe a specific course of action for the future. As a result, the normative conclusions we draw are not only subjective, but reify past misjudgments. For instance, automating part of the admissions process at St George’s Hospital Medical School conserved historical patterns of excluding women and non-whites. Machine learning only complicates this, giving computers the power of interpretation and judgment with minimal supervision — a risk that became clear when Google’s ‘smart’ image recognition technology labeled photos of black people ‘gorillas.’
Luckily, the same analytical tools that have so often been used to oppress can be repurposed for emancipatory purposes, but caution must be exercised at every step of the process.
The Stanford Social Innovation Review argues that technology firms must prioritize ethics in their corporate culture, hiring practices and standards for evaluation. Others have defined best practices for data use for social sector organizations; for example, purchasing information directly from underserved communities, and disaggregating data to avoid homogenizing diverse populations. Finally, Stanford and other universities can expand their own role in fostering human-centered engineering by taking actions such as expanding the Technology in Society requirement for computer science majors.
In an era of fake news and mass political polarization, it’s more tempting than ever to embrace decision-making strategies that seem grounded firmly in empirics and objective reality. However, data isn’t neutral, and we can’t keep acting like it is. Making the old ways of doing things even faster and more efficient does not mean we have progressed.
Instead, it’s worthwhile to slow down the engine of innovation and ask ourselves: what kind of future are our algorithms really creating?
—
Originally published in my column at The Stanford Daily. If you enjoyed this piece, check out more of my writing on Medium:
Stop worshiping the myth of the ‘productive immigrant’
Humans: more than just “value added”medium.com
Politics kills my friendships — and maybe it should.
Avoiding judgment based on political ideology is easier said than done — especially for people not in positions of…medium.com

This story is published in The Startup, Medium’s largest entrepreneurship publication followed by 285,454+ people.
Subscribe to receive our top stories here.

",Why I won’t pledge allegiance to Big Data,231,why-i-wont-pledge-allegiance-to-big-data-5998daf5c348,2018-04-16,2018-04-16 07:11:12,https://medium.com/s/story/why-i-wont-pledge-allegiance-to-big-data-5998daf5c348,False,802,Medium's largest publication for makers. Subscribe to receive our top stories here → https://goo.gl/zHcLJi,,,,The Startup,,swlh,"STARTUP,TECH,ENTREPRENEURSHIP,DESIGN,LIFE",thestartup_,Ethics,ethics,Ethics,7787.0,Jasmine Sun,"writing about big problems & the people trying to solve them @Stanford | exploring digital media, education, the future of cities",366ec540c6ea,jasminewsun,81.0,85.0,20181104
0,,0.0,f41f22e4979f,2018-06-09,2018-06-09 11:51:16,2018-06-22,2018-06-22 16:38:07,1,False,en,2018-06-22,2018-06-22 16:38:07,19,6afa319750e4,3.728301886792453,11,1,1,Google’s declaration of principles for AI is a short but carefully worded text covering the main issues related to the uses of its…,5,"IMAGE: Nick Youngson CC BY-SA 3.0 ImageCreator
Artificial intelligence: a question of principles?
Google’s declaration of principles for AI is a short but carefully worded text covering the main issues related to the uses of its technology. I would recommend reading the document, given that it raises many about the future and the rules we will need to guide us as it evolves.
The company had been working on the statement of principles for some time, while it continues to work in other areas of AI. Critics say the document is a response to the recent resignation of more than 10 employees and the petition signed by several thousand more in protest at the company’s involvement in the US Defense Department’s Maven Project, designed to recognize images taken by military drones: either they do not know the company, or they are confusing the circumstantial with the foundational, form with background. The regulation of artificial intelligence is far from a new subject and is being debated widely: I have taken part in some discussions; and Google, as a leading player in the field, is simply laying out its position after a long process of reflection.
In the wake of revelations about Google’s involvement in Maven, most media have interpreted the company’s statement of principles somewhat simplistically, along the lines of a promise that its AI won’t be used to develop weapons or in breach of human rights, although it’s clear that the document has much more far-reaching intentions. Weapons are mentioned only briefly, in a section entitled AI applications we will not pursue, limited to saying that the company will not help develop “Weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people”. That said, it will continue its work “with governments and the military in many other areas. These include cybersecurity, training, military recruitment, veterans’ healthcare, and search and rescue.”
What is the real importance of Google’s statement of principles? In the first place, the significance of such reflection in all areas, and doing so in a well-informed, realistic way, without summoning up images of killer robots or superior intelligence able to sweep annoying humans aside. AI will not be used for everyday purposes for many years. For the moment, we’re talking about applications that have more to do with deciding which products are offered to potential customers, pricing policies, preventing churn, detecting fraud, marketing, along with an increasing number of applications, all of which may be less exciting than killer robots, but still with major potential to get things wrong.
Among the most relevant points are “Be socially beneficial”; “Avoid creating or reinforcing unfair bias”; “Be accountable to people”; “Incorporate privacy design principles”; and “Be made available for uses that accord with these principles”, which implies preventing its use by those who do not respect them. Other specially important commitments include “Uphold high standards of scientific excellence” or “be built and tested for safety”.
Believe it or not, these are far more important commitments than whether the company will develop weapons or not. Many of the problems raised by the rapid rate of technological development come not from the potentially harmful objectives, but from mismanagement, inadequate security and procedural errors in a world where not everybody has the best intentions.
Naivety is no long an excuse in the context of technologies that can be used for harm, and Google reaffirms its commitment to avoiding this, a commitment that goes far beyond “Don’t be evil.” This, of course, does not mean the company won’t make mistakes, but the commitment to submitting to rigorous processes and to trying to avoid them at all costs is important.
Reflection on the ethical principles associated with the development of AI algorithms is important, and needs to take place in a reasoned manner, which means avoiding references to HAL, Skynet or Terminator coming from the future to kill Sarah Connor. It makes no sense to involve those who do not understand machine learning and AI to be involved in the drafting of the ethical principles that will govern their future. This particularly applies to our politicians, many of whom are not qualified to comment on, and much less legislate on these issues. Those who do not understand the topic have a responsibility to learn about it or stay out of the debate.
One thing is for Google to ponder the ethics of AI: it is one of the main players in the area, applying it to all its products and is in the midst of an ambitious training program to teach all of its workforce on how to use it. It’s quite another for a government, a supranational body or any other political organization to do so, given that in most cases, knowledge of the subject is at best superficial and at worst zero or alarmist. We’re going to see more and more discussion on this subject, but what interests me most is not the outcome, but the process and the intended consequences.
Asking questions about the future to avoid potentially negative or unwanted consequences can be useful, especially if done with Google’s rigor and discipline. Doing so based on unwarranted fears rooted in science fiction are more likely to get in the way of progress and humanity’s evolution: we need to guard against irrational fears, misinformation, and their close relatives, demagoguery and populism. Laying down meaningful principles about the development of artificial intelligence algorithms, will be an important part of how our future plays out. AI is a question of principles, sure, but well-founded principles.
This article was previously published at Forbes.
(En español, aquí)
",Artificial intelligence: a question of principles?,60,artificial-intelligence-a-question-of-principles-6afa319750e4,2018-06-28,2018-06-28 21:34:14,https://medium.com/s/story/artificial-intelligence-a-question-of-principles-6afa319750e4,False,935,On the effects of technology innovation (writing in Spanish at enriquedans.com since 2003),,enriquedans,,Enrique Dans,enrique.dans@ie.edu,enrique-dans,"TECHNOLOGY,DISRUPTION,SOCIAL MEDIA,INNOVATION",edans,Ethics,ethics,Ethics,7787.0,Enrique Dans,Professor of Innovation at IE Business School and blogger at enriquedans.com,acfc0d77aeef,edans,32707.0,236.0,20181104
0,,0.0,,2017-12-01,2017-12-01 08:00:54,2017-12-01,2017-12-01 13:02:46,2,False,en,2018-03-18,2018-03-18 18:21:04,9,3836f39f38f9,2.330503144654088,2,1,0,"Viral, realistic and creepy… The fictional video showing “slaughterbots”, opens again the debates about robots.",5,"Killer Robots : from fiction to reality
Viral, realistic and creepy… The fictional video showing “slaughterbots”, opens again the debates about robots.
At anytime, the inanimate object becoming alive has always been fascinating. The robots which were only science-fiction yesterday, became reality nowadays. This ‘Black Mirror style’ video shows it brilliantly.
“Slaughterbots” is a terrifying
A credible video
It has been realised for the Ban Lethal Autonomous Weapons campaign. A dozen of associations and NGOs fighting for the human rights, are at the origin of the project. The Future of Life Institute, its searchers and philosophers, are also into it. Stuart Russel, a famous AI’s expert, even concludes by saying : “This short movie is more than a simple speculation”.
Samuel Nowakowski believed it was true ©Lucas Hueber
“This short movie is more than a speculation”
Samuel Nowakowski, is a robotic’s searcher of the loria and teaches at the “Ecole des mines” in Nancy, France. He was literally shocked in front of the video : “I believed everything was true. The work of The Intercept’s journalists quickly came in my mind”.The philosopher thinks this video shows the “quest of modern notoriety where the ethics becomes very rare”.Today, there aren’t any common rules : “it depends of the innovator and his values”. Samuel doesn’t always approves his colleagues works at the loria.
Terminator coming soon ?
Most famous experts are regularly waning us about AI. The famous cosmologist Stephen Hawking declared recently : “I fear AI may replace humans altogether”. The business magnate Elon Musk is also among the most sceptical. Recently, a clash between him and Marck Zuckerberg exploded. Marck judged the warnings of Elon Musk as “particularly irresponsible”.
Elon Musk answered Marck Zuckerberg on AI
Elon Musk answered on Twitter that “his understanding of the subject is limited”. Samuel Nowakowski recalls that “AI doesn’t exists yet. We are very far from it”. He regrets the main medias always trying to fear people with robotics. “Before thinking about AI, we should define intelligence”, adds he.
France adopts the drones

Experts from 86 different countries brang the campaign at the UNO, during a CCW’s session on 13th november. More than 125 countries were represented, such as France, one of the main UNO’s security council member. Mounir Mahjoubi, the french digital secretary of state didn’t say anything yet. A surprising position when we know the “technology ethics” is one of his primary goals.
A reaper drone that french army acquires since 2014 ©Flickr
Even worse, the french minister of defense Florence Parly, announced in september the french army will arm his drones. The minister of defense ensured that “it won’t change anything concerning the international right”. Indeed, the fact that a human person guides these drones, means it is a common weapon according to the international laws.
“We will use a technology we don’t master”.
However, Samuel Nowakowski is very upset, the french army uses american weapons : “We’ll use a technology we don’t master”. As a humanist, he regrets the banalization of the death, these weapons directly provokes : “We forget that any war has a price, a human price. Death becomes abstract”. He wishes to introduce more ethics in the field. Nothing unusual for somebody hoping to bring more human in our digital era.
",Killer Robots : from fiction to reality,52,killer-robots-from-fiction-to-reality-3836f39f38f9,2018-04-11,2018-04-11 14:34:46,https://medium.com/s/story/killer-robots-from-fiction-to-reality-3836f39f38f9,False,516,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Romain Ethuin,Studying Journalism and Numeric Medias,1985cc101c0c,romii.ethuin,5.0,5.0,20181104
0,,0.0,7f60cf5620c9,2018-02-04,2018-02-04 21:56:09,2018-02-04,2018-02-04 22:59:35,2,False,en,2018-02-06,2018-02-06 16:41:52,14,8a199e386642,7.8022012578616335,23,1,0,How giving up our right to private data hurts us all.,5,"Has Data Become the New Golden Calf?

A well-known story from the Jewish Tanakh tells about the Hebrew leader, Moses, receiving the famous Ten Commandments from God. God then orders Moses to teach these Commandments to the Hebrews. There, the discussion generally stops; however, what is not oft discussed is what the Hebrew people did while Moses spoke with God.
Though the Hebrew God orchestrated their liberation from enslavement in another country, the Hebrews urged Moses’s brother, Aaron, to make them gods to worship while Moses was away. At their behest, Aaron made them a idol in the image of a gold calf from gold and precious gems that the Hebrew people gave to him. From there, Hebrew society revolved around hedonistic revelry in the absence of Moses’s leadership. God had delivered the Hebrew people from slavery — now, the Hebrews willingly put themselves back under the enslavement of the gods they had previously escaped [1].
I wanted to continue that discussion as I believe it shows very poignantly an alarming conclusion that I have observed in much of today’s technological advances: if we are not careful, we will lose our freedom and liberties not by having it forcibly stolen and taken away but by freely giving it away.
Historically, many contemporary, developed countries have guaranteed its citizens rights to own land, vote, run for public office, and more. These rights have been hard-earned throughout the entirety of mankind’s existence — whether through diplomatic or violent means (we need not look farther than the last three centuries for glaring examples). Yet now, I perceive a significant need for the protection of these rights as they are threatened — not by outright violence or censorship but by small concessions of our freedoms.
Willingly made by us.
To illustrate, in late January 2018, Australian international security college student, Nathan Ruser, had analyzed a publicly released heatmap by the fitness company, Strava [2]. The heatmap, which displayed de-identified geographical Strava user data, displayed where users would run, bike, or walk on a routine basis. However, this heatmap also used data collected from US military personnel stationed in sensitive areas; when Strava made this heatmap available, it identified military locations — even the jogging paths of soldiers. This massive security flaw discovered by Ruser puts thousands of personnel at risk. Moreover, it could be leveraged to target NGOs working in volatile environments
Strava released their global heatmap. 13 trillion GPS points from their users (turning off data sharing is an option). https://medium.com/strava-engineering/the-global-heatmap-now-6x-hotter-23fc01d301de … … It looks very pretty, but not amazing for Op-Sec. US Bases are clearly identifiable and mappable [3]
To Strava’s credit, the CEO quickly responded in the company’s blog to this security exposure and are currently working with both military, government, and NGOs to address this flaw[4]. Though this event was not meant maliciously, it does raise an important question: how is a person’s privacy and rights to be protected in this epoch of history defined by the explosion of computing innovation?
In an interview with Princeton University computer scientist, Arvind Narayanan, discussed the Strava security flaw as really endemic of a more serious underlying problem.
“There are dozens of companies [like Strava] that have this kind of fine-grained location data about millions of people[…] Strava can be seen as a symptom of a bigger issue, which is the number of companies which have the kind of sensitive data that we’re uncomfortable seeing publicly.”
Narayanan even argues that the popular notion of de-identifying data, which is where a company or research group takes data that has been collected from an individual and removes all markers that could be used to trace back to the individual from which data had been collected, fails.
“In fact everybody’s behaviour collectively has an impact on everyone else’s privacy[…] Arguing that ‘your data is anonymized so you’re not going to come to any harm’ kind of breaks down here, once we start thinking of privacy as a collective issue[…] The right lesson to draw would be that we need to have a more nuanced appreciation of what privacy means[…] It simply cannot be boiled down to anonymity, or putting a bunch of check boxes… [F]or users to figure out…[P]rivacy needs to be really integrated as a core part of the design process” [5]
As Narayanan said, companies and developers need to not only think of the end user of their product but also of their privacy — and more than just the anonymizing of collected data. We have come to a pivotal juncture where we need to put people — actual people we can think of in our minds, ones that we care about, ones with names — at the center of data collection and privacy. Which brings us to the particular field with some of the most exciting developments and complex problems of the 21st century — artificial intelligence.
Artificial intelligence has come into its renaissance. With articles now appearing daily shouting “Machine Learning”, or “Artificial Intelligence Agents”, AI has indelibly impacted world. AI’s presence is such that Google CEO, Sundar Pichai, said “[AI] is one of the most important things that humanity is working on. It’s more profound than, I don’t know, electricity or fire.” [6] Though many strides have been made in AI, it is still in its nascency. And it is here, right now, in these next few years, that will most likely shape the usage of AI for the foreseeable future.
The reason AI development has been able to seemingly skyrocket in terms of progress — from AI being able to detect pneumonia better than traditional radiologists [7] to the quickly approaching ubiquity of digital home assistants like Google Home or Amazon Echo — is the sheer amount of data developers can use that has been generated by people through their internet usage. Google, Facebook, Amazon, Baidu, Microsoft, government organizations — all of these companies and organizations can collect any manner of information on one person using the internet. Compound that with the omnipresence of technology through most of the world and you have an infinite supply of data.
Computer scientists working on artificial intelligence use the abundance of this information — whether in corporate or research-based groups — to train mathematical algorithms that can make various predictions on given user activity. For example, this could be something like Amazon noticing you like to buy a particular product on its website. Later on, you could see recommendations popping up for similar or related products. This is convenient. It helps us be more efficient. But. Without our realizing, our whole experience can be altered in an instant by technologies backed by AI.
There has been much discussion about the sort of conclusions AI agents can be making, because, generally speaking, we don’t exactly know how they are coming to those conclusions. To achieve meaningful results used by the likes of Google or Amazon, user data is often transformed through many mathematical means that are inscrutable to even the most experienced computer scientists — this Nautilus article and Technology Review article discusses the phenomena even more (albeit in more technical terms) — and has caused much unease in the academic community [8, 9].
The inscrutability of AI, as an article by the New York Times written by Vijay Pande posits, is nothing to fear — in and of itself at least. An example they use is of a doctor making a diagnosis. The doctor can make the diagnosis based on experience, information, and observations, but she is never expected to explain her whole process. Pande argues that a similar approach should be made with AI [10]. The reason why these two sides have so hotly debated the issue is because who is at the center of these developments: us.
Everyone all over the world is enamored by the thought of AI. Dr. Fei-Fei Li, the director of Stanford’s AI Lab and a chief scientist of Google Clound, emphasizes that to make AI even more successful, people need to be put at the center of it. And that entails collecting more data on people to establish better contexts [11]. This issue brings us all the way back to the original argument I presented at the beginning — rights being compromised based on endless small concessions on our part. Strava was an accessible example of data collection and publication gone wrong — however, only more data is going to be generated, collected, culled, and examined for even more meaning and results in an almost deus ex machina manner.
Now, as someone who is around some of the giants creating these amazing changes, I am actually quite optimistic. I can see the amazing benefits that this sort of technology can yield — in fact, the lab I work in published literature on sepsis prediction in hospitals [12]. I find this incredible and am excited to see what will happen.
Yet, also as someone who stands and works at the interface of all these advances, I can feel that some massive change is coming. I struggle to articulate what I mean by change but the zeitgeist of the academic world is electric. Whatever is going to happen, is going to occur soon and it is going to revolutionize the world — “[w]hatever is coming, we stand barely at its Stone Age” [13].
I humbly implore you, whether you are an academic or part of the workforce, young or old, to strongly consider each decision you make. Continue to educate yourselves on the issues I have discussed and do not capitulate on the freedoms that we have. Consider all possible implications of projects or work you do as what happens now can impact lives to come. Technology is morally ambiguous — what we do with it though, is not.
Sources:
[1] Exodus 32 (Holy Bible, NIV)
[2] Kwai, I. (2018, January 30). What He Did on His Summer Break: Exposed a Global Security Flaw. The New York Times. Retrieved from https://www.nytimes.com/2018/ 01/30/world/australia/strava-heat-map-student.html?smid=tw-nytimes&smtyp=cur
[3] Ruser, Nathan. [Nrg8000] (2018, January 27). Strava released their global heatmap. 13 trillion GPS points from their users (turning off data sharing is an option). https://medium.com/strava-engineering/the-global-heatmap-now-6x-hotter-23fc01d301de … … It looks very pretty, but not amazing for Op-Sec. US Bases are clearly identifiable and mappable [Tweet]. Retrieved from https://twitter.com/Nrg8000/ status/957318498102865920
[4] Quarles, J. (n.d.). A Letter to the Strava Community. Retrieved February 4, 2018, from https://blog.strava.com/press/a-letter-to-the-strava-community/
[5] Young, N. (n.d.). Spark with Nora Young. Retrieved from http://www.cbc.ca/radio/spark/383-dangerous-data-libraries-and-more-1.4516637/exercise-app-shows-why-anonymous-data-can-still-be-dangerous-1.4516651
[6] Goode, L. (2018, January 19). Google CEO Sundar Pichai Compares Impact of AI to Electricity and Fire. The Verge. Retrieved from https://www.theverge.com/2018/1/19/16911354/google-ceo-sundar-pichai-ai-artificial-intelligence-fire-electricity-jobs-cancer
[7] Rajpurkar, P., Irvin, J., Zhu, K., Yang, B., Mehta, H., Duan, T., … Ng, A. (2017). CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning. CoRR, abs/1711/05225. Retrieved from http://arxiv.org/abs/1711.05225
[8] Bornstein, A. (2016, September 1). Is Artificial Intelligence Permanently Inscrutable? Nautilus. Retrieved from http://nautil.us/issue/40/learning/is-artificial-intelligence-permanently-inscrutable?
[9] Knight, W. (2017, April 11). The Dark Secret at the Heart of AI. MIT Technology Review. Retrieved from https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/
[10 ] Pande, V. (2018, January 25). Artificial Intelligence’s “Black Box” Is Nothing to Fear. The New York Times. Retrieved from https://www.nytimes.com/2018/01/25/opinion/artificial-intelligence-black-box.html
[11] Knight, W. (2017, October 9). Put Humans at the Center of AI. MIT Technology Review. Retrieved from https://www.technologyreview.com/s/609060/put-humans-at-the-center-of-ai/
[12] Mayaud, Louis & Tarassenko, lional & Annane, Djillali & Clifford, Gari. (2013). Predictive power of heart rate complexity to estimate severity in severe sepsis patients. Journal of Critical Care. 28. e37. 10.1016/j.jcrc.2013.07.029.
[13] Abel, A. (2018, January 17). Orwell’s ‘Big Brother’ is already in millions of homes. Her name is Alexa. Maclean’s. Retrieved from http://www.macleans.ca/society/technology/amazon-alexa-google-home-privacy-surveillance/
",Has Data Become the New Golden Calf,115,has-data-become-the-new-golden-calf-8a199e386642,2018-07-11,2018-07-11 13:59:20,https://towardsdatascience.com/has-data-become-the-new-golden-calf-8a199e386642,False,1966,"Sharing concepts, ideas, and codes.",towardsdatascience.com,towardsdatascience,,Towards Data Science,,towards-data-science,"DATA SCIENCE,MACHINE LEARNING,ARTIFICIAL INTELLIGENCE,BIG DATA,ANALYTICS",TDataScience,Ethics,ethics,Ethics,7787.0,jacob zelko,· quiet thoughts · meager reflections · written artistry · meditations on the mercurial moment.,516d4ac5228,jacobzelko,28.0,6.0,20181104
0,,0.0,,2018-04-12,2018-04-12 18:48:38,2018-04-12,2018-04-12 19:10:40,4,False,en,2018-04-12,2018-04-12 19:10:40,4,d9fe41be160b,5.771698113207548,0,0,0,"This is the follow up blog post about my conversation with a group of AI enthusiasts to whom I presented the question: “Why are you, and…",5,"Why AI? — Part 2
This is the follow up blog post about my conversation with a group of AI enthusiasts to whom I presented the question: “Why are you, and why do you think other people are, so interested in AI?” The post about the first 3 responses is here, and this post will discuss the last 2:
#4. Because the optimization of any project (and therefore growth and development in general) relies on having the best possible tools to do it with, and AI is the best tool there is
#5. Because people want to play God
Here we go:

4. AI enables task optimization
In any field, the goal is progress and optimization — doing things faster and better. AI is currently making great strides in various fields, using vast amounts of information to find insightful patterns that no human can and organizing information more quickly and neatly than has ever been possible. With that kind of powerful tool in existence, it makes sense that everyone would try to get their hands on it to maximize the effectiveness of their projects. After all, the goal is progress. But the philosophical and eye-roll inducing question once again arises — is progress necessarily defined by better technology? What about emotional, societal and spiritual progress? Are we focusing enough on those parts of the progress equation, or are some of them being sacrificed in the pursuit of better devices and larger profits? Studies on happiness have found that it’s not what you have or how successful you are that makes you happy, it’s the connections you make with the people in your life and the quality of your relationships that determines your life satisfaction. So a happy and prospering society is not necessarily the one with the most advanced technology.

That being said, it’s difficult to be happy and prospering when you are ill, disabled or living in poverty or severe pollution. And a lot of technology today is aimed towards fixing those problems quickly and efficiently. But on the other hand, another large part of technological development focuses on making the most attractive smartphone, the most engaging app, the most immersive VR game and the most appealing websites that will get you to buy the most amount of stuff you don’t need. And the more time you invest in these types of technologies, the less you have for establishing those deep human connections I mentioned earlier as being vital to health and happiness. So technological progress, like most other things, gives as well as takes. It’s important to be aware of this delicate balance of pros and cons and gauge the overall success of technology by the health and happiness of its users.
5. AI is a product of people’s desire to play God
As far back as the 3rd century BC, people have been dreaming of creating artificial life. In the novel the Argonautica written during that period, there is a story about Talos — an artificial giant created by the god Hephaestus to defend Crete from invaders. Talos is made of bronze and in his mechanical veins runs the blood of the gods, placed there by his inventor. In the fourth century AD, the emperors of Constantinople impressed ambassadors and dignitaries with robotic creations in their grand reception halls. In the 12th century AD, it was said that the Pope invented a mechanical head which could correctly answer questions it was asked, and in the late 1700s an automatic chess playing robot called “The Turk” fascinated people around the globe before it was exposed as a hoax. It appears that the creation of autonomous life and intelligence has been a coveted accomplishment since the dawn of our own transformation into intelligent beings.
But why does wanting to create technological replicas of humans (I’m talking about general AI and robotics now, not narrow AI like voice recognition) have to equate with people wanting to play God? Well, I’m not saying that this is always a conscious effort. (Even if you are not religious, humor me in this line of thought. After all, religion is often brought up in this conversation) God is depicted as a perfect being, which would also mean that He possesses immense intelligence. And what did a being of immense intelligence do with His power? Among other things, He created something “in His image”. So perhaps, re-creation of the self is one of the ultimate signs of high intelligence. And we, as the most intelligent species on our planet, are demonstrating this theory by being the only species with a burning desire to create dolls, robots and AI - in other words, beings in our image.

However, let’s take a high-school level SAT question to task on this point: if our creations are to us as we are to God, then what does that mean about the quality of our artificial children, ie the next level down? We may be “in his image” theoretically, but I think it’s safe to say we are far from perfection. Our Creator is believed to embody optimal goodness and power whereas (although plenty of good, beauty and genius exists) humanity fills the pages of its newspapers with tragedy, destruction and wrongdoing on a regular basis. So if that same quality downgrade applies (on average) to our attempts at creation of living beings, we may be in trouble. This is all very theoreti-hypotheti-philosophical of course but it’s something to consider. Do we really want something with free will, physical strength and superior intelligence accompanied by much less developed (than our already questionable level) ethical judgments, empathy and consciousness to be given power and influence in an already troubled society? Are we at the point in our intellectual development where we are ready to take on the responsibility of creating life? Or are we still in our Teen Mom years about to make a very bad decision?

And then there’s the age-old question of arrogance. Are we capable of a godly act? That of creating life? And if we are not capable or ready to attempt it, will there be consequences for the arrogance of believing we are? We’ve all heard of Pandora’s box and the Tower of Babel. Aside from that, the ancient Greeks also had a general principle of “do not imitate the gods or else be punished for the crime of hubris.” And other such tales of karma in the face of arrogance abound in centuries of literature. I am aware that this is all getting a little too biblical and mythical for those tech-oriented readers with no inclinations towards the unproven but I believe there is value in the legends, allegory and symbolism that have shaped our history and I think it is therefore important to include them in our analysis of modern issues and the future. That being said, although the past should be studied and considered as we plan ahead, we cannot use it as a strict template because context is constantly changing and each situation is its own unique snowflake (or butterfly if you prefer a reference to the butterfly effect here which also seems relevant). However, we should always keep basic values/principles such as safety, well-being, equality, freedom, etc. at the forefront of our decision-making processes in order to achieve the ultimate goals of human prosperity, well-being and progress.
That concludes round 2 of “Why AI?” If you have another answer to this question or a different point of view on these topics, please comment below. I believe that humanity’s quest for growth and development should go hand-in-hand with thoughtful consideration and critical thinking on these subjects from various perspectives every step of the way. I therefore hope that we will begin to engage in more active conversations on this topic and in doing so pave our future for maximum success.
Thank you for reading. If you are interested in the interaction between technology and psychology, check out the Psybertronic blog or follow us on facebook.
",Why AI? — Part 2,0,why-ai-part-2-d9fe41be160b,2018-04-12,2018-04-12 19:10:41,https://medium.com/s/story/why-ai-part-2-d9fe41be160b,False,1344,,,,,,,,,,Ethics,ethics,Ethics,7787.0,T.Panova,PhD student and patent holder researching the interaction between technology and psychology. Creator of psybertronic.com,f6f94d222503,TPanova,16.0,34.0,20181104
0,,0.0,5e5bef33608a,2018-01-04,2018-01-04 16:31:01,2018-01-04,2018-01-04 17:14:09,1,False,en,2018-01-04,2018-01-04 17:18:04,5,db980f3f473f,8.290566037735847,1,0,0,(* Originally written in November 2015),5,"AI: The human being, the singularity and superiority. The era of a supposed intelligence.
(* Originally written in November 2015)

With the imminent technological advance, the creation of thinking machines that exhibit advances in the field of Artificial Intelligence is now an increasingly reality. The present essay will expose an ethical dilemma of purely reflexive hypothetical character, which we do not yet witness by the technological capabilities of the time but exposes a panorama where the field of Artificial Intelligence presents evolutionary advances and exhibits our role as humans. The topic will be approached from the ethical standpoint of the deontological duty, the responsibility and the reason for dignifying the machines and robots of the future as well as humans.
One of the main reasons why man has developed technology throughout history is to be able to simplify processes, optimize times and reduce the danger of the loss of human lives in terms of high risk jobs, with this the Robotics and informatics have been fundamental tools for development in recent decades.
Specialized machines capable of performing complicated high-risk tasks that do not require a salary or vacation, which are more efficient compared to human work and which objectively are a valuable investment in large companies and industries, this is what’s happening right now in our context, our current scenario.
However, our technological present is not so far from a reality immersed in Artificial Intelligence since IBM’s Watson, a cognitive computational system that understands the natural language of humans and that stands out for its own learning capacity through human-machine interaction , is the clear example of the significant advances in this field and positively ensures the path that must be taken in terms of development.
Our current perception of technology lies in the machines and robots used in industries because systems such as Watson do not show advantages in the field and with this we do not see as human beings the impact of this type of progress; As it was 30 years ago that the world could not even imagine the possibility of having a computer in its pocket, today we can not imagine if a robot could develop something beyond a pre-programmed intelligence.
Even so, considering the advances in the field of Artificial Intelligence (AI) and assuming that in the future robots will be associated with this type of intelligence able to fend for themselves in our world, without the necessary dependence of a human being to be able to work or react to an event, the most common questions in the subject are of the type How far can these advances go? What will the robots be capable of? Assuming they can be programmed for different functions, will they be able to kill a human being? What would prevent a robot from distinguishing between good and evil when assigning a task? All these issues depend on the programmers and developers of the next few years, these are problems that are not yet presented and are considering as a separate point; assuming that all those questions of security, transparency, incorruptibility, prediction of behavior and responsibility are solved in the future for a stable and firm AI, the question we must ask ourselves is to what extent can these machines come to think?
Consider that if robots can distinguish between good and evil at the time of reacting to social functions, they are autonomous and can stand on their own, that is, they are able to take care of their physical structure and prevail over the physical dangers to the which humans are equally exposed, would they be able to identify their plane or objective in the world? Would the AI ​​rooted in them be able to question the meaning of life and its purpose of existence? Would they be able to become aware of their potential and observe the injustice and inequality that exists between humans and machines? Will we be able in this environment to exercise justice, equality and respect for this new type of life, since we practically own the machines and when they become aware they will literally be our slaves? Is it ethical to consider that such a being has no rights for the simple fact of being a product created by and for humans?
Given these questions we must think and ask to ourselves: Would we be able to allow the slavery of another human, since this same one serves us for different ends? Is it fair to limit the freedom of an individual based on certain physical characteristics? If we are clear about our moral stance on these basic principles of freedom then why not extend it beyond what we can perceive, maybe an animal a plant or a machine?
The intention of this essay is not to stop the technological advance that arises in the area, but to present the hypothetical problematic to reflect, be aware of the conflict and act according to all the human ideals that we have developed in our history.
Before we can explain more about the problem, we must clarify and situate ourselves in the present with some examples that illustrate the technological progress of our time and emphasize the imagination to better visualize the future.
For our time, advances in the field of Artificial Intelligence lie in different areas with different objectives. There are systems capable of diagnosing diseases and neurological disorders such as the Laboratory for Artificial Intelligence and Computational Sciences of the Massachusetts Institute of Technology (MIT), which allows the detection of alterations and disorders that may occur in patients suffering from principles of Parkinson or Alzheimer because it is taught to the system based upon different cases, thus calibrating its predictive system. (El pais, 2015)
There are also systems such as Watson, a cognitive system described above “that learns on its own”, it is capable of handling large volumes of information using Big Data and has the potential to expand knowledge where the limits of the human brain begin. Its objective, as declared by the developers, is not to substitute man, but to complement it. (El pais, 2015)
Darío Gil, director of IBM’s symbiotic-cognitive systems research center, says: “Until now, many computers have been based on the calculation paradigm. Today we can build a new class of systems that can learn, find correlations, create hypotheses from those correlations, and suggest and measure actions. “ The changes in the computer area with respect to AI that we will be facing in the coming years are just the beginning of all this cognitive revolution as Gil points out. (El Pais, 2015)
We are faced with this situation where in a few years the processors will have the same processing capacity as a human brain as Nuria Oliver points out, scientific director of Telefónica I+D because we can not stop the technological advance. (El Tiempo, 2015)
Even Stephen Hawking among other scientists through the US organization “Future Of Life Institute” clarify that the proper use of AI generated in the future contribute to the benefit of humanity, thus expressed that “once human beings fully develop artificial intelligence, it could progress on its own, and be redesigned at a growing rate. “ (El Tiempo, 2015)
We are no longer dealing with sci-fi articles or amazing stories like those of Isaac Asimov where the robots create a plot against humanity to save it from itself based on the three laws of robotics and their positronic brains that gave them a certain human nature, reality glimpses towards this new horizon where humanity uses big data, consumes the internet and appreciates their electronic devices much more than their own family.
Taking into account points of Kant’s philosophy regarding his deontological ethics, the duty of every human being is good will, acting as dictated by moral conscience, ruling our behavior through categorical imperatives.
We must be good not because of any particular inclination but because it is our duty to be like that. One of the maxims of Kant lies in using people always and at all times as an end and never as a mean.
Taking this principle into account, this is where we put our moral conscience to the test with respect to this situation and we must ask ourselves: Are robots, perhaps, a tool for humanity even though they may develop consciousness in the future? The principle defines treating humanity as an end and never as a means, but how far can we extend this maxim?
Considering that the norm is duty, the goodwill, Why only limit ourselves to humans? If we tried to act in this way in all aspects of life, with nature, the environment, animals and all existence itself, the world would be completely different, since we would not be on the verge of global warming and war wouldn't be a business that imposes the will of a representative (i.e. the Government) on his people.
First of all the dignity placed in the context of robots, can we make a robot worthy? Following again the Kantian philosophy, the dignity lies in the principle of not using people as means, but in value them for who they are and not for the benefit that can be obtained from them, since a human being is an end of himself and therefore It has dignity since its value does not depend on any external factor but as it’s rational capacity does not obey different laws that itself dictates.
Returning to our hypothetical context of the future where robots with an AI rooted in their system can develop thoughts, ideas and a high level of reasoning equaling or exceeding the capacity of the human being, if a robot takes into account that it is being exploited in activities that he did not decide on his own will and demands justice from his master. Will we be able to set him free? Or would we prefer to eliminate that AI and reduce it to a much simpler machine that does not generate autonomy and demands equality, simply because it does not possess the same physical characteristics as a human being?
Let us consider then that before even dignifying a machine of the future, which does not yet exist, we take into account all these concepts and observe our reality. Can we achieve and solve this dilemma if we can not even dignify our own species today?
We boast of being superior to all nature and we invent all these laws and principles whose base is based on respecting humanity and using everything else as a means, when in reality we can not even follow our own rules of coexistence and we use the same individuals of our species to win wars, traffic organs, earn money, etc.
Our obligation as human beings, as an evolved species focused on reason that seems to be superior from different perspectives, is to act as such and redefine what is what we consider as life and to see if it is really necessary to take advantage of all those beings that to us seem inferior.
Take into account that the future comes with many questions, progress can not be stopped and it is our responsibility to grow along with it, develop our critical ability and be aware of the decisions we make based on moral dilemmas respecting the principles of equality, justice and dignity.
If by then, as humans, we can consider that we will be able to respect an AI machine in the future, we can see that we will be able to respect any type of life in the same way; for that time in the world there will be no more war, poverty will be a cruel memory of the past and the murders will be seen as an act of cowardice against the respect we owe to ourselves as humanity and part of nature.
Let’s reflect that beyond dignifying the robots we shall begin by dignifying the other people around us for who they really are without taking advantage of themselves, for then when the AI arises, this dilemma will be no more than a simple decision.
References
1) Jímenez, Vicente. (5 Juliio 2015). “La nueva era de la computación” El País, el periódico Global. México. Recuperado de http://elpais.com/elpais/2015/07/02/eps/ 1435845247_202110.html
2) “Creciente ‘inteligencia’ de máquinas acentúa dilema de limitarla o no” (21 Julio 2015). El Tiempo. Colombia. Recuperado de http://www.eltiempo.com/tecnosfera/novedades-tecnologia/creciente-inteligencia-de-maquinas-/16129135
3) “Científicos piden uso responsable de la inteligencia artificial” (13 enero 2015). El Tiempo. Colombia. Recuperado de http://www.eltiempo.com/estilo-de-vida/ciencia /inteligencia-artificial-piden-uso-responsable/15093995
4) Estrada, Gustavo. (1 Febrero 2015) “¿Pueden las máquinas pensar?”. El tiempo. Colombia. Recuperado de http://www.eltiempo.com/opinion/columnistas/pueden-las-maquinas-pensar-gustavo-estrada-columnista-el-tiempo/15177104
5) Barbuzano, Javier. (31 Agosto 2015) “La inteligencia artificial mejora el diagnóstico de enfermedades mentales”. El País, el periódico Global. México. Recuperado de http://www.elpais.com/elpais/2015/08/28/ciencia/1440748607_241234.html
","AI: The human being, the singularity and superiority. The era of a supposed intelligence.",14,ai-the-human-being-the-singularity-and-superiority-the-era-of-a-supposed-intelligence-db980f3f473f,2018-03-01,2018-03-01 21:17:01,https://medium.com/s/story/ai-the-human-being-the-singularity-and-superiority-the-era-of-a-supposed-intelligence-db980f3f473f,False,2144,"Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.",becominghuman.ai,BecomingHumanAI,,Becoming Human: Artificial Intelligence Magazine,team@chatbotslife.com,becoming-human,"ARTIFICIAL INTELLIGENCE,DEEP LEARNING,MACHINE LEARNING,AI,DATA SCIENCE",BecomingHumanAI,Ethics,ethics,Ethics,7787.0,Kevin Díaz Guarneros,Digital Systems and Robotics Engineer. AI/ML self-learner. Dancer. Geek. Engineer. Batman. Fan de la lluvia y el amor. ॐ,e61ef5bae54f,kevoniano,30.0,91.0,20181104
0,,0.0,,2018-08-08,2018-08-08 03:35:02,2018-08-08,2018-08-08 03:44:52,5,False,en,2018-08-08,2018-08-08 04:08:57,6,8a0b36ead2f4,5.0408805031446535,1,0,0,"Most tourists visiting New York make sure to visit the Statue of Liberty, an early gate to a welcoming country to the poor and the tired…",5,"Can machines make fair decisions in the multidimensional immigrant crisis?

Most tourists visiting New York make sure to visit the Statue of Liberty, an early gate to a welcoming country to the poor and the tired. When I visited this iconic marvel on Ellis Island, I listened to the famous poem by Emma Lazarus on my audio guide and remember feeling like an insignificant speck in comparison of greatness of everything human-empathy, care and pain. Great stories occurred there and they will continue to awe visitors for the coming years. There’s something so human in receiving immigrants in any part of the world, yet a certain cautious air circulates while dealing with them.

Recently, there was an article on The Verge(Link — https://www.theverge.com/2018/1/18/16905962/algorithm-resettle-refugees-machine-learning-research-employment) discussing a paper published in the journal Science explaining an algorithm to suggest resettlement locations to integrate refugee families based on-
geographical context(resettlement locations and their employment rates)
personal characteristics(age, sex, nationality, education level, languages)
synergies between geography and personal characteristics( employment rates for each location based on the personal characteristics)
The problem that arises with resettling refugees is their inability in establishing their lives economically and socially by finding jobs at their locations due to mismatch in resettlement locations. Presently the matching is done on a random basis.
The countries involved in the study were US and Switzerland, where USA reassigns based on capacity constraints and Switzerland reassigns based on proportional distribution. There are no proposals currently to exchange refugees between the countries. With the current model, the employment rate of refugees lies at 25% in 3 months whereas with the proposed algorithm, it reaches 50%.
Who’s directly involved and how are they affected?
Refugee units/families
Refugee individuals
the Government offering relocation
Resettling locations and local citizens
Other involved countries
All refugee families have uniform relocation environment by having at least one earning member. Refugee individuals may not get the best jobs or salaries based on their skill set. The government would have overall lower unemployment rates, poverty, crime rate and other following problems to handle. Plus, an overall growth of infrastructure would contribute to the economy. Thus, ideally better economic conditions for the citizens of the host country.
Resettling locations might face short term shortage of resources followed by increase in price due to demand. This might cause inconvenience for the local citizens. Also, due to competition, the locals may be directed to become more competitive

2. What are the ethical issues raised?
The algorithm looks to have one working refugee family member based on best options available for the family. If a family has more than one skilled employable member, he/she may not get employed.
The algorithm does not mention the refugees having the ability to prioritize their preference on location, workable members, etc continuing the current method.
Also, the refugees do not get to choose their location or . Thus not improving from the current model which is not supportive of rights.
No doubt the unemployed members of the refugee families could try finding job throughout the country with EAD later, but the algorithm itself denies them off this right and values one member over the others.
Though any of these conditions seem better than not being accommodated in the host countries for a refugee, these definitely can improve the living conditions.
Also, the refugees may be temporary, accommodating them requires resources like food, shelter and jobs depriving the locals off these. If location A has a high requirement for professing X, Refugees in profession X will get relocated to A. There will be a larger pool for professionals X thus leaving lesser jobs for the locals.
3. How to interpret this situation using the ethical frameworks of (consequentialist ,duty and virtue) and design a course of action?
The frameworks considered are Consequentialist, Duty and Virtue
Consequentialist — the greatest balance of good over harm(maximum happiness)
Duty — “Act only according to that maxim by which you can at the same time will that it should become a universal law.”
Virtue — concerned with the entirety of a person’s life, it takes the process of education and training seriously, and emphasizes the importance of role models to our understanding of how to engage in ethical deliberation.

Consequentialist
This framework puts ends over means. If the ethical framework to deal with resettling refugees were to be consequential, thee state of maximum happiness would have to be achieved.
This would not be a good framework because finding the right balance for maximum happiness would be hard to reach. Maximum happiness for refugees would mean getting their ideal locations even if they could not be accommodated there thus causing lesser happiness for the government and the locals. Maximum happiness for locals would mean the refugees would not have any status, standing in the society, resources, etc which would prevent maximum happiness of refugees. Understanding the effects of any combination between these would require a lot of time. It may take a long time thus subjecting the refugees to lesser happiness.
Also, bridging between these conditions, the algorithm proposed forms the consequentialist framework since in the given restrictions, it aims to achieve maximum happiness for time being.
Duty
This framework puts the rights and duties of all people as the main focus. With this framework, universal rules for everyone would be supported. The currently practiced algorithm is more duty based than the other two types since it emphasizes on all families having one earning member to support them since its the duty of the host nation to assist people in crisis by providing them with comfortable living conditions while not subjecting the locals to any adversity due to incorporating the refugees.
Virtue
This framework embodies virtuous ways of approaching the problem which would conflict with the algorithm which just optimizes placements within the limited realm. Since the refugees have to establish their whole lives, hte should be given maximum comfort being the virtuous host which could probably sustain momentarily, but would cause serious problems society wise, economically, etc on the long run.

4. A course of action I recommend and the reason explained
It would be a better option to optimize resettlement in the way proposed by the algorithm with a few more features considered like informed consent and preference inclusion like location type of job, family members wanting to work, etc within the algorithm. The refugees must have more freedom in making their own choices to avoid a future situation similar to the immigrant children separation from families-esque fiasco.
References -
https://www.theverge.com/2018/1/18/16905962/algorithm-resettle-refugees-machine-learning-research-employment
https://www.brown.edu/academics/science-and-technology-studies/framework-making-ethical-decisions
http://science.sciencemag.org/content/359/6373/325.full
https://en.wikipedia.org/wiki/Declaration_of_Human_Duties_and_Responsibilities
http://www.unhcr.org/en-us/figures-at-a-glance.html
https://www.uscis.gov/humanitarian/refugees-asylum/refugees
",Can machines make fair decisions in the multidimensional immigrant crisis?,2,recently-there-was-an-article-on-the-verge-link-8a0b36ead2f4,2018-08-08,2018-08-08 04:08:57,https://medium.com/s/story/recently-there-was-an-article-on-the-verge-link-8a0b36ead2f4,False,1115,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Meghana Kantharaj,"I like real stories - listening, telling, living, discovering, writing good stories.",21aac827379,meghanakantharaj,47.0,130.0,20181104
0,,0.0,,2018-08-07,2018-08-07 20:52:49,2018-08-08,2018-08-08 09:01:01,2,False,en,2018-08-08,2018-08-08 09:01:01,15,22874bbb6724,5.4059748427672965,3,0,0,"This story was first published in The Aleph Report. If you want to read the latest reports, please subscribe to our newsletter and our…",5,"Is a lack of ethics the problem with technology?

This story was first published in The Aleph Report. If you want to read the latest reports, please subscribe to our newsletter and our Twitter.
I honestly don’t know how to start today’s article. I want to talk about ethics and values. Or lack of them. But I’m torn about it because most people don’t find it relevant enough. It’s not newsworthy. It’s not about the next Facebook.
The mission of The Aleph is to bring insight beyond technology. I believe ethics is an essential component of the future we’re heading into. Having the right set of values will determine our long-term survival, both as a business and as a species.
The other day, my children got a gift from a friend of the family. It was a book about a mole that gets poo all over his head. He gets mad and starts accusing all the animals of pooing on him. He seeks revenge. He finally finds the culprit, the family dog. He then poos on the dog and runs for safety. Sorry for the spoilers.

“The story of the Little Mole is a tale loved by children and their parents all around the world with more than a million copies sold!”
The Story of the Little Mole Who Went in Search of Whodunit.
I can’t even begin to describe how I felt about the book. Don’t let go of your anger, look for the culprit and pay them for what they did. An eye for an eye. But hide what you’ve done and run for the safety of anonymity. Those are the ethical values we’re teaching. I’m not sure how anyone would “love” these principles. But it seems there is a market for them.
The same parents that buy such books, then get astonished by my kids. Their tenderness, their willingness to help others and lend a hand to someone in trouble, surprises them. They always wonder why my wife and I are so lucky to have such good kids.
I don’t believe in luck. Not in the sense that most people infuse the word with. I believe in values. I believe in ethics. And it shocks me to see how other parents marvel at such a display of respectfulness. It always makes me wonder what is then, their standard for respecting others. After reading the Harvey Weinstein’s testimonies, I wonder no more.
It seems I’m not the only one. Last week The New York Times published an article titled “Tech’s Ethical ‘Dark Side’: Harvard, Stanford and Others Want to Address It.”
“Technology is not neutral,” said Professor Sahami, who formerly worked at Google as a senior research scientist. “The choices that get made in building technology then have social ramifications.”
It touches upon a theme I’ve brought up regularly on interviews and roundtables in technology conferences. How responsible are we for the effects our innovations create?
Deep Learning and Machine Learning techniques are increasing the complexity of technology. It’s harder than ever, to figure out what potential ramifications our innovations might have.
It still surprises me how deterministic most technocrats still are. On infrequent occasions, I hear a startup founder talking about dynamic systems.
The democratization of technology is enabling access to powerful frameworks to people with very narrow views of the world.
“But until recently, ethics did not seem relevant to many students.”
Tech’s Ethical ‘Dark Side’: Harvard, Stanford and Others Want to Address It.
It’s frightening to see the lack of social empathy and disregard for the consequences of their products some have. I would argue, it doesn’t come from a bad place. Most ascribe to the “Don’t be evil” motto. Still, the unwillingness to assume any responsibility is disheartening.
Even within the technology elites, there is this unconscious belief that technology isn’t complex. This is a fallacy. Technology is extremely complex. We’ve gone from a single program in an isolated computer, to a vast global network of multi-parallel computing units. The exponential growth of the system is unrivaled by anything previously built.
This proliferation has turned a determinist automata into a massive dynamic system. We can no longer apply deterministic logic to solve the problems. We need to take a much more abstract and systemic view of things. The system doesn’t end with your homepage. The boundaries of the system extend way beyond your company’s servers and into our social fabric. Ignoring this fact is one of the reasons for our current social upheaval.
But if understanding the shift of the system’s boundaries is already hard, trying to predict the behavior of the whole, is unrealistic. Still, we insist on our knowledge of what our software will do. We strive for scientific precision in a sea of uncertainty. The truth is, despite our best intentions, we can’t predict most of the ramifications derived from our products. Not just that. While an error of judgment can be fixed in our software, the consequences of our lack of understanding will have long-lasting implications in society.
“You can patch the software, but you can’t patch a person if you, you know, damage someone’s reputation.”
Tech’s Ethical ‘Dark Side’: Harvard, Stanford and Others Want to Address It.
And here is where the ethics and values come into play. When dealing with systems, there is no deterministic answer. We play with a scale of greys, not black and white. In such ambiguous situations, it’s the founder’s ethics that shine. It’s the employee’s values that come forward.
The lack of ethics, the twisted values we impose on our children, have their reflection on the decisions being made by the next generation founders.
This all comes back to the mole and the poo. If we teach our kids that revenge is right; that accusing people is the norm; that you should pay it back; that you should run from your acts; then those are the values that founders will dig when they face unpredictability.
And what does any of this have to do with businesses? It has everything to do with it. I’ve covered several incidents where the lack of morals was the reason why technology companies were getting in trouble. Uber’s lack of ethics, Facebook and Twitter’s content quality shortfalls, YouTube’s absence of moral compass.
As I write these words, I feel I’m falling prey to age. That my words just reflect the maturity of a concerned dad. That generation after generation has said the same. The only argument I can add is that never before in history, a single individual has had the amount of influence some technology founders have now. And while the risk of the absence of an ethical framework has always existed, it has never been so relevant as today.
The fact that Harvard, MIT, UT, Cornell, and Stanford are deploying Ethics and regulations courses should be a warning sign. Still, I find it outrageous that only ivy league students get exposed to ethics around Computer Science. This isn’t a problem only limited to the privileged classes. And it’s also not a problem reserved for Computer Science graduates either. System thinking and ethics frameworks should be taught in school and at home. By every person in society. We are all responsible.
It’s easy to fall prey to simplistic thinking and argue that we should apply technology to better people’s lives. I reckon most innovators have their fellow citizen’s welfare at heart. The truth, though, is that technology’s complexity far exceeds our capacity to understand the ramifications of it entirely. This intricacy also has its reflection on regulations. Trying to regulate our shifting landscape with the uninformed opinion of some politicians, doesn’t cut it anymore.
We are all responsible.
“The deed done, a happy and satisfied Little Mole disappeared back into his mole hole.”
The Story of the Little Mole Who Went in Search of Whodunit.
If you like this article, please share it, and invite others to follow the newsletter, it really helps us grow!
",Is a lack of ethics the problem with technology?,8,is-a-lack-of-ethics-the-problem-with-technology-22874bbb6724,2018-08-08,2018-08-08 09:01:01,https://medium.com/s/story/is-a-lack-of-ethics-the-problem-with-technology-22874bbb6724,False,1331,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Alex Barrera,"Chief Editor at The Aleph Report (@thealeph_report), CEO at Press42.com, Cofounder & associated editor @tech_eu, former editor @KernelMag.",e5834fa3ab59,abarrera,4237.0,2368.0,20181104
0,,0.0,,2018-09-18,2018-09-18 16:46:09,2018-09-17,2018-09-17 21:18:02,1,False,en,2018-09-18,2018-09-18 17:44:55,11,62017156cbbd,43.89811320754717,1,0,0,How are we to make progress on AI alignment given moral uncertainty? What are the ideal ways of resolving conflicting value systems and…,5,"AI Alignment Podcast: Moral Uncertainty and the Path to AI Alignment with William MacAskill

How are we to make progress on AI alignment given moral uncertainty? What are the ideal ways of resolving conflicting value systems and views of morality among persons? How ought we to go about AI alignment given that we are unsure about our normative and metaethical theories? How should preferences be aggregated and persons idealized in the context of our uncertainty?
Moral Uncertainty and the Path to AI Alignment with William MacAskill is the fifth podcast in the new AI Alignment series, hosted by Lucas Perry. For those of you that are new, this series will be covering and exploring the AI alignment problem across a large variety of domains, reflecting the fundamentally interdisciplinary nature of AI alignment. Broadly, we will be having discussions with technical and non-technical researchers across areas such as machine learning, AI safety, governance, coordination, ethics, philosophy, and psychology as they pertain to the project of creating beneficial AI. If this sounds interesting to you, we hope that you will join in the conversations by following us or subscribing to our podcasts on Youtube, SoundCloud, or your preferred podcast site/application.
If you’re interested in exploring the interdisciplinary nature of AI alignment, we suggest you take a look here at a preliminary landscape which begins to map this space.
In this podcast, Lucas spoke with William MacAskill. Will is a professor of philosophy at the University of Oxford and is a co-founder of the Center for Effective Altruism, Giving What We Can, and 80,000 Hours. Will helped to create the effective altruism movement and his writing is mainly focused on issues of normative and decision theoretic uncertainty, as well as general issues in ethics.
Topics discussed in this episode include:
Will’s current normative and metaethical credences
The value of moral information and moral philosophy
A taxonomy of the AI alignment problem
How we ought to practice AI alignment given moral uncertainty
Moral uncertainty in preference aggregation
Moral uncertainty in deciding where we ought to be going as a society
Idealizing persons and their preferences
The most neglected portion of AI alignment
In this interview we discuss ideas contained in the work of William MacAskill. You can learn more about Will’s work here, and follow him on social media here. You can find Gordon Worley’s post here and Rob Wiblin’s previous podcast with Will here. You can hear more in the podcast above or read the transcript below.

Lucas: Hey, everyone. Welcome back to the AI Alignment Podcast series at the Future of Life Institute. I’m Lucas Perry, and today we’ll be speaking with William MacAskill on moral uncertainty and its place in AI alignment. If you’ve been enjoying this series and finding it interesting or valuable, it’s a big help if you can share it on social media and follow us on your preferred listening platform.
Will is a professor of philosophy at the University of Oxford and is a co-founder of the Center for Effective Altruism, Giving What We Can, and 80,000 Hours. Will helped to create the effective altruism movement and his writing is mainly focused on issues of normative and decision theoretic uncertainty, as well as general issues and ethics. And so, without further ado, I give you William MacAskill.
Yeah, Will, thanks so much for coming on the podcast. It’s really great to have you here.
Will: Thanks for having me on.
Lucas: So, I guess we can start off. You can tell us a little bit about the work that you’ve been up to recently in terms of your work in the space of metaethics and moral uncertainty just over the past few years and how that’s been evolving.
Will: Great. My PhD topic was on moral uncertainty, and I’m just putting the finishing touches on a book on this topic. The idea here is to appreciate the fact that we very often are just unsure about what we ought, morally speaking, to do. It’s also plausible that we ought to be unsure about what we ought morally to do. Ethics is a really hard subject, there’s tons of disagreement, it would be overconfident to think, “Oh, I’ve definitely figured out the correct moral view.” So my work focuses on not really the question of how unsure we should be, but instead what should we do given that we’re uncertain?
In particular, I look at the issue of whether we can apply the same sort of reasoning that we apply to uncertainty about matters of fact to matters of moral uncertainty. In particular, can we use what is known as “expected utility theory”, which is very widely accepted as at least approximately correct in empirical uncertainty. Can we apply that in the same way in the case of moral uncertainty?
Lucas: Right. And so coming on here, you also have a book that you’ve been working on on moral uncertainty that is unpublished. Have you just been expanding this exploration in that book, diving deeper into that?
Will: That’s right. There’s actually been very little that’s been written on the topic of moral uncertainty, at least in modern times, at least relative to its importance. I would think of this as a discipline that should be studied as much as consequentialism or contractualism or Kantianism is studied. But there’s really, in modern times, only one book that’s been written on the topic and that was written 18 years ago now, or published 18 years ago. What we want is this to be, firstly, just kind of definitive introduction to the topic, it’s co-authored with me as lead author, but co-authored with Toby Ord and Krista Bickfest, laying out both what we see as the most promising path forward in terms of addressing some of the challenges that face an account of decision-making under moral uncertainty, some of the implications of taking moral uncertainty seriously, and also just some of the unanswered questions.
Lucas: Awesome. So I guess, just moving forward here, you have a podcast that you already did with Rob Wiblin: 80,000 Hours. So I guess we can sort of just avoid covering a lot of the basics here about your views on using expected utility calculous in moral reasoning and moral uncertainty in order to decide what one ought to do when one is not sure what one ought to do. People can go ahead and listen to that podcast, which I’ll provide a link to within the description.
It would also be good, just to sort of get a general sense of where your meta ethical partialities just generally right now tend to lie, so what sort of meta ethical positions do you tend to give the most credence to?
Will: Okay, well that’s a very well put question ’cause, as with all things, I think it’s better to talk about degrees of belief rather than absolute belief. So normally if you ask a philosopher this question, we’ll say, “I’m a nihilist,” or “I’m a moral realist,” or something, so I think it’s better to split your credences. So I think I’m about 50/50 between nihilism or error theory and something that’s non-nihilistic.
Whereby nihilism or error theory, I just mean that any positive moral statement or normative statement or a evaluative statement. That includes, you ought to maximize happiness. Or, if you want a lot of money, you ought to become a banker. Or, pain is bad. That, on this view, all of those things are false. All positive, normative or evaluative claims are false. So it’s a very radical view. And we can talk more about that, if you’d like.
In terms of the rest of my credence, the view that I’m kind of most sympathetic towards in the sense of the one that occupies most of my mental attention is a relatively robust form of moral realism. It’s not clear whether it should be called kind of naturalist moral realism or non-naturalist moral realism, but the important aspect of it is just that goodness and badness are kind of these fundamental moral properties and are properties of experience.
The things that are of value are things that supervene on conscious states, in particular good states or bad states, and the way we know about them is just by direct experience with them. Just by being acquainted with a state like pain gives us a reason for thinking we ought to have less of this in the world. So that’s my kind of favored view in the sense it’s the one I’d be most likely to defend in the seminar room.
And then I give somewhat less credence in a couple of views. One is a view called “subjectivism” which is the idea that what you ought to do is determined in some sense by what you want to do. So the simplest view there would just be when I say, “I ought to do X.” That just means I want to do X in some way. Or a more sophisticated version would be ideal subjectivism where when I say I ought to do X, it means some very idealized version of myself would want myself to want to do X. Perhaps if I had limited amounts of knowledge and much clearer computational power and so on. I’m a little less sympathetic to that than many people I know. We’ll go into that.
And then a final view that I’m also less sympathetic towards is non-cognitivism, which would be the idea that our moral statements … So when I say, “Murder is wrong,” I’m not even attempting to express a proposition. What they’re doing is just expressing some emotion of mine, like, “Yuk. Murder. Ugh,” in the same way that when I said that, that wasn’t expressing any proposition, it was just expressing some sort of pro or negative attitude. And again, I don’t find that terribly plausible, again for reasons we can go into.
Lucas: Right, so those first two views were cognitivist views, which makes them fall under sort of a semantic theory where you think that people are saying truth or false statements when they’re claiming moral facts. And the other theory in your moral realism are both metaphysical views, which I think is probably what we’ll mostly be interested here in terms of the AI alignment problem.
There are other issues in metaethics, for example having to do with semantics, as you just discussed. You feel as though you give some credence to non-cognitivism, but there are also justification views, so like issues in moral epistemology, how one can know about metaethics and why one ought to follow metaethics if metaethics has facts. Where do you sort of fall in in that camp?
Will: Well, I think all of those views are quite well tied together, so what sort of moral epistemology you have depends very closely, I think, on what sort of meta-ethical view you have, and I actually think, often, is intimately related as well to what sort of view in normative ethics you have. So my preferred philosophical world view, as it were, the one I’d defend in a seminar room, is classical utilitarian in its normative view, so the only thing that matters is positive or negative mental states.
In terms of its moral epistemology, the way we access what is of value is just by experiencing it, so in just the same way we access conscious states. There are also some ways in which you can’t merely, you know, why is it that we should maximize the sum of good experiences rather than the product, or something? That’s a view that you’ve got to obtain by kind of reasoning rather than just purely from experience.
Part of my epistemology does appeal to whatever this spooky ability we have to reason about abstract affairs, but it’s the same sort of faculty that is used when we think about mathematics or set theory or other areas of philosophy. If, however, I had some different view, so supposing we were a subjectivist, well then moral epistemology looks very different. You’re actually just kind of reflecting on your own values, maybe looking at what you would actually do in different circumstances and so on, reflecting on your own preferences, and that’s the right way to come to the right kind of moral views.
There’s also another meta-ethical view called “constructivism” that I’m definitely not the best person to talk about with. But on that view, again it’s not really a realistic view, but on this view we just have a bunch of beliefs and intuitions and the correct moral view is just the best kind of systematization of those and beliefs or intuitions in the same way as you might think … Like linguistics, it is a science, but it’s fundamentally based just on what our linguistic intuitions are. It’s just kind of a systematization of them.
On that view, then, moral epistemology would be about reflecting on your own moral intuitions. You just got all of this data, which is the way things seem like to you, morally speaking, and then you’re just doing the systematization thing. So I feel like the question of moral epistemology can’t be answered in a vacuum. You’ve got to think about your meta-ethical view of the metaphysics of ethics at the same time.
Lucas: I think I’m pretty interested in here, and also just poking a little bit more into that sort of 50% credence you give to your moral realist view, which is super interesting because it’s a view that people tend not to have, I guess, in the AI computer science rationality space, EA space. People tend to, I guess, have a lot of moral anti-realists in this space.
In my last podcast, I spoke with David Pearce, and he also seemed to sort of have a view like this, and I’m wondering if you can just sort of unpack yours a little bit, where he believed that suffering and pleasure disclose the in-built pleasure/pain access of the universe. Like you can think of minds as sort of objective features of the world, because they in fact are objective features of the world, and the phenomenology and experience of each person is objective in the same way that someone could objectively be experiencing redness, and in the same sense they could be objectively experiencing pain.
It seems to me, and I don’t fully understand the view, but the claim is that there are some sort of in-built quality or property to the hedonic qualia of suffering or pleasure that discloses its in-built value to that.
Will: Yeah.
Lucas: Could you unpack it a little bit more about the metaphysics of that and what that even means?
Will: It sounds like David Pearce and I have quite similar views. I think relying heavily on the analogy with, or very close analogy with consciousness is going to help, where imagine you’re kind of a robot scientist, you don’t have any conscious experiences but you’re doing all this fancy science and so on, and then you kind of write out the book of the world, and i’m like, “hey, there’s this thing you missed out. It’s like conscious experience.” And you, the robot scientist, would say, “Wow, that’s just insane. You’re saying that some bits of matter have this first person subjective feel to them? Like, why on earth would we ever believe that? That’s just so out of whack with the naturalistic understanding of the world.” And it’s true. It just doesn’t make any sense from given what we know now. It’s a very strange phenomenon to exist in the world.
Will: And so one of the arguments that motivates error theory is this idea of just, well, if values were to exist, they would just be so weird, what Mackie calls “queer”. It’s just so strange that just by a principle of Occam’s razor not adding strange things in to our ontology, we should assume they don’t exist.
But that argument would work in the same way against conscious experience, and the best response we’ve got is to say, no, but I know I’m conscious, and just tell by introspecting. I think we can run the same sort of argument when it comes to a property of consciousness as well, which is namely the goodness or badness of certain conscious experiences.
So now I just want you to go kind of totally a-theoretic. Imagine you’ve not thought about philosophy at all, or even science at all, and I was just to ask you, rip off one of your fingernails, or something. And then I say, “Is that experience bad?” And you would say yes.
Lucas: Yeah, it’s bad.
Will: And I would ask, how confident are you? The more confident that this pain is bad than that I even have hands, perhaps. That’s at least how it seems to be for me. So then it seems like, yeah, we’ve got this thing that we’re actually incredibly confident of which is the badness of pain, or at least the badness of pain for me, and so that’s what initially gives the case for then thinking, okay, well, that’s at least one objective moral fact that pain is bad, or at least pain is bad for me.
Lucas: Right, so the step where I think that people will tend to get lost in this is when … I thought the part about Occam’s razor was very interesting. I think that most people are anti-realistic because they use Occam’s razor there and they think that what the hell would a value even be anyway in the third person objective sense? Like, that just seems really queer, as you put it. So I think people get lost at the step where the first person seems to simply have a property of badness to it.
I don’t know what that would mean if one has a naturalistic reductionist view of the world. There seems to be just like entropy, noise and quarks and maybe qualia as well. It’s not clear to me how we should think about properties of qualia and whether or not one can drive, obviously, “ought” statements about properties of qualia to normative statements, like “is” statements about the properties of qualia to “ought” statements?
Will: One thing I want to be very clear on is just it definitely is the case that we have really no idea on this view. We are currently completely in the dark about some sort of explanation of how matter and forces and energy could result in goodness or badness, something that ought to be promoted. But that’s also true with conscious experience as well. We have no idea how on earth matter could result in kind of conscious experience. At the same time, it would be a mistake to start denying conscious experience.
And then we can ask, we say, okay, we don’t really know what’s going on but we accept that there’s conscious experience, and then I think if you were again just to completely pre theoretically start categorizing distant conscious experiences that we have, we’d say that some are red and some are blue, some are maybe more intense, some are kind of dimmer than others, you’d maybe classify them into sights and sounds and other sorts of experiences there.
I think also a very natural classification would be the ones that are good and the ones that are bad, and then I think when we cash that out further, I think it’s not nearly the case. I don’t think the best explanation is that when we say, oh, this is good or this is bad it means what we want or what we don’t want, but instead it’s like what we think we have reason to want or reason not to want. It seems to give us evidence for those sorts of things.
Lucas: I guess my concern here is just that I worry that words like “good” and “bad” or “valuable” or “dis-valuable”, I feel some skepticism about whether or not they disclose some sort of intrinsic property of the qualia. I’m also not sure what the claim here is about the nature of and kinds of properties that qualia can have attached to them. I worry that goodness and badness might be some sort of evolutionary fiction which enhances us, enhances our fitness, but it doesn’t actually disclose some sort of intrinsic metaphysical quality or property of some kind of experience.
Will: One thing I’ll say is, again, remember that I’ve got this 50% credence on error theory, so in general, all these questions, maybe this is just some evolutionary fiction, things just seem bad but they’re not actually, and so on. I actually think those are good arguments, and so that should give us confidence, some degree of confidence and this idea of just actually nothing matters at all.
But kind of underlying a lot of my views is this more general argument that if you’re unsure between two views, one in which just nothing matters at all, we’ve got no reasons for action, the other one we do have some reasons for action, then you can just ignore the one that says you’ve got no reasons for action ’cause you’re not going to do badly by its likes no matter what you do. If I were to go around shooting everybody, that wouldn’t be bad or wrong on nihilism. If I were to shoot lots of people, it wouldn’t be bad or wrong on nihilism.
So if there are arguments such as, I think an evolutionary argument that pushes us in the direction of kind of error theory, in a sense we can put them to the side, ’cause what we ought to do is just say, yeah, we take that really seriously. Give us a high credence in error theory, but now say, after all those arguments, what are the views, because most plausibly kind of bear their force.
So this is why with the kind of evolutionary worry, I’m just like, yes. But, supposing it’s the case that there actually are. Presumably conscious experiences themselves are useful in some evolutionary way that, again, we don’t really understand. I think, presumably, also good and bad experiences are useful in some evolutionary way that we don’t fully understand, perhaps because they have a tendency to motivate at least beings like us, and that in fact seems to be a key aspect of making a kind of goodness or badness statement. It’s at least somehow tied up to the idea of kind of motivation.
And then when I say ascribing a property to a conscious experience, I really just don’t mean whatever it is that we mean when we say that this experience is red seeming, this is experience is blue seeming, I mean, again, opens philosophical questions what we even mean by properties but in the same way this is bad seeming, this is good seeming.
Before I got into thinking about philosophy and naturalism and so on, would I have thought those things are kind of on a par, and I think I would’ve done, so it’s at least a pre theoretically justified view to think, yeah, there just is this axiological property of my experience.
Lucas: This has made me much more optimistic. I think after my last podcast I was feeling quite depressed and nihilistic, and hearing you give this sort of non-naturalistic or naturalistic moral realist count is cheering me up a bit about the prospects of AI alignment and value in the world.
Will: I mean, I think you shouldn’t get too optimistic. I’m also certainly wrong-
Lucas: Yeah.
Will: … sort of is my favorite view. But take any philosopher. What’s the chance that they’ve got the right views? Very low, probably.
Lucas: Right, right. I think I also need to be careful here that human beings have this sort of psychological bias where we give a special metaphysical status and kind of meaning and motivation to things which have objective whatever to it. I guess there’s also some sort of motivation that I need to be mindful of that seeks out to make value objective or more meaningful and foundational in the universe.
Will: Yeah. The thing that I think should make you feel optimistic, or at least motivated, is this argument that if nothing matters, it doesn’t matter that nothing matters. It just really ought not to affect what you do. You may as well act as if things do matter, and in fact we can have this project of trying to figure out if things matter, and that maybe could be an instrumental goal, which kind of is a purpose for life is to get to a place where we really can figure out if it has any meaning. I think that sort of argument can at least give one grounds for getting out of bed in the morning.
Lucas: Right. I think there’s this philosophy paper that I saw, but I didn’t read, that was like, “nothing Matters, but it does matter”, with the one lower case M and then another capital case M, you know.
Will: Oh, interesting.
Lucas: Yeah.
Will: It sounds a bit like 4:20 ethics.
Lucas: Yeah, cool.
Moving on here into AI alignment. And before we get into this, I think that this is something that would also be interesting to hear you speak a little bit more about before we dive into AI alignment. What even is the value of moral information and moral philosophy, generally? Is this all just a bunch of BS or how can it be interesting and or useful in our lives, and in science and technology?
Will: Okay, terrific. I mean, and this is something I write about in a paper I’m working on now and also in the book, as well.
So, yeah, I think the stereotype of the philosopher engaged in intellectual masturbation, not doing really much for the world at all, is quite a prevalent stereotype. I’ll not comment on whether that’s true for certain areas of philosophy. I think it’s definitely not true for certain areas within ethics. What is true is that philosophy is very hard, ethics is very hard. Most of the time when we’re trying to do this, we make very little progress.
If you look at the long-run history of thought in ethics and political philosophy, the influence is absolutely huge. Even just take Aristotle, Locke, Hobbes, Mill, and Marx. The influence of political philosophy and moral philosophy there, it shaped thousands of years of human history. Certainly not always for the better, sometimes for the worse, as well. So, ensuring that we get some of these ideas correct is just absolutely crucial.
Similarly, even in more recent times … Obviously not as influential as these other people, but also it’s been much less time so we can’t predict into the future, but if you consider Peter Singer as well, his ideas about the fact that we may have very strong obligations to benefit those who are distant strangers to us, or that we should treat animal welfare just on a par with human welfare, at least on some understanding of those ideas, that really has changed the beliefs and actions of, I think, probably tens of thousands of people, and often in really quite dramatic ways.
And then when we think about well, should we be doing more of this, is it merely that we’re influencing things randomly, or are we making things better or worse? Well, if we just look to the history of moral thought, we see that most people in most times have believed really atrocious things. Really morally abominable things. Endorsement of slavery, distinctions between races, subjugation of women, huge discrimination against non-heterosexual people, and, in part at least, it’s been ethical reflection that’s allowed us to break down some of those moral prejudices. And so we should presume that we have very similar moral prejudices now. We’ve made a little bit of progress, but do we have the one true theory of ethics now? I certainly think it’s very unlikely. And so we need to think more if we want to get to the actual ethical truth, if we don’t wanna be living out moral catastrophes in the same way as we would if we kept slaves, for example.
Lucas: Right, I think we do want to do that, but I think that a bit later in the podcast we’ll get into whether or not that’s even possible, given economic, political, and militaristic forces acting upon the AI alignment problem and the issues with coordination and race to AGI.
Just to start to get into the AI alignment problem, I just wanna offer a little bit of context. It is implicit in the AI alignment problem, or value alignment problem, that AI needs to be aligned to some sort of ethic or set of ethics, this includes preferences or values or emotional dispositions, or whatever you might believe them to be. And so it seems that generally, in terms of moral philosophy, there are really two methods, or two methods in general, by which to arrive at an ethic. So, one is simply going to be through reason, and one is going to be through observing human behavior or artifacts, like books, movies, stories, or other things that we produce in order to infer and discover the observed preferences and ethics of people in the world.
The latter side of alignment methodologies are empirical and involves the agent interrogating and exploring the world in order to understand what the humans care about and value, as if values and ethics were simply a physical by-product of the world and of evolution. And the former is where ethics are arrived at through reason alone, and involve the AI or the AGI potentially going about ethics as a philosopher would, where one engages in moral reasoning about metaethics in order to determine what is correct. From the point of view of ethics, there is potentially only what the humans empirically do believe and then there is what we may or may not be able to arrive at through reason alone.
So, it seems that one or both of these methodologies can be used when aligning an AI system. And again, the distinction here is simply between sort of preference aggregation or empirical value learning approaches, or methods of instantiating machine ethics, reasoning, or decision-making in AI systems so they become agents of morality.
So, what I really wanna get into with you now is how metaethical uncertainty influences our decision over the methodology of value alignment. Over whether or not we are to prefer an empirical preference learning or aggregation type approach, or one which involved an imbuing of moral epistemology and ethical metacognition and reasoning into machine systems so it can discover what we ought to do. And how moral uncertainty, and metaethical moral uncertainty in particular, operates within both of these spaces once you’re committed to some view, or both of these views. And then we can get into issues and intertheoretic comparisons and how that arises here at many levels, the ideal way we should proceed if we could do what would be perfect, and again, what is actually likely to happen given race dynamics and political, economic, and militaristic forces.
Will: Okay that sounds terrific. I mean, there’s a lot of cover there.
I think it might be worth me saying just maybe a couple of distinctions I think are relevant and kind of my overall view in this. So, in terms of distinction, I think within what broadly gets called the alignment problem, I think I’d like to distinguish between what I’d call the control problem, then kind of human values alignment problem, and then the actual alignment problem.
Where the control problem is just, can you get this AI to do what you want it to do? Where that’s maybe relatively narrowly construed, I want it to clean up my room, I don’t want it to put my cat in the bin, that’s kinda control problem. I think describing that as a technical problem is kind of broadly correct.
Second is then what gets called aligning AI with human values. For that, it might be the case that just having the AI pay attention to what humans actually do and infer their preferences that are revealed on that basis, maybe that’s a promising approach and so on. And that I think will become increasingly important as AI becomes larger and larger parts of the economy.
This is kind of already what we do when we vote for politicians who represent at least large chunks of the electorate. They hire economists who undertake kind of willingness-to-pay surveys and so on to work out what people want, on average. I do think that this is maybe more normatively loaded than people might often think, but at least you can understand that, just as the control problem is I have some relatively simple goal, which is, what do I want? I want this system to clean my room. How do I ensure that it actually does that without making mistakes that I wasn’t intending? This is kind of broader problem of, well you’ve got a whole society and you’ve got to aggregate their preferences for what kind of society wants and so on.
But I think, importantly, there’s this third thing which I called a minute ago, the actual alignment problem, so let’s run with that. Which is just working out what’s actually right and what’s actually wrong and what ought we to be doing. I do have a worry that because many people in the wider world, often when they start thinking philosophically they start endorsing some relatively simple, subjectivist or relativist views. They might think that answering this question of well, what do humans want, or what do people want, is just the same as answering what ought we to do? Whereas for kind of the reductio of that view, just go back a few hundred years where the question would have been, well, the white man’s alignment problem, where it’s like, “Well, what do we want, society?”, where that means white men.
Lucas: Uh oh.
Will: What do we want them to do? So similarly, unless you’ve got the kind of such a relativist view that you think that maybe that would have been correct back then, that’s why I wanna kind of distinguish this range of problems. And I know that you’re kind of most interested in that third thing, I think. Is that right?
Lucas: Yeah, so I think I’m pretty interested in the second and the third thing, and I just wanna unpack a little bit of your distinction between the first and the second. So, the first was what you called the control problem, and you called the second just the plurality of human values and preferences and the issue of aligning to that in the broader context of the world.
It’s unclear to me how I get the AI to put a strawberry on the plate or to clean up my room and not kill my cat without the second thing haven been done, at least to me.
There is a sense at a very low level where your sort of working on technical AI alignment, which involves working on the MIRI approach with agential foundations and trying to work on a constraining optimization and corrigibility and docility and robustness and security and all of those sorts of things that people work on and the concrete problems in AI safety, stuff like that. But, it’s unclear to me where that sort of stuff is just limited to and includes the control problem, and where it begins requiring the system to be able to learn my preferences through interacting with me and thereby is already sort of participating in the second case where it’s sort of participating in AI alignment more generally, rather than being sort of like a low level controlled system.
Will: Yeah, and I should say that on this side of things I’m definitely not an expert, not really the person to be talking to, but I think you’re right. There’s going to be some big, gray area or transition from systems. So there’s one that might be cleaning my room, or even let’s just say it’s playing some sort of game, unfortunately I forget the example … It was under the blog post, an example of the alignment problem in the wild, or something, from open AI. But, just a very simple example of the AIs playing a game, and you say, “Well, get as many points as possible.” And what you really want it to do is win a certain race, but what it ends up doing is driving this boat just round and round in circles because that’s the way of maximizing the number of points.
Lucas: Reward hacking.
Will: Reward hacking, exactly. That would be a kind of failure of control problem, that first in our sense. And then I believe there’s gonna be kind of gray areas, where perhaps it’s the certain sort of AI system where the whole point is it’s just implementing kind of what I want. And that might be very contextually determined, might depend on what my mood is of the day. For that, that might be a much, much harder problem and will involve kind of studying what I actually do and so on.
We could go into the question of whether you can solve the problem of cleaning a room without killing my cat. Whether that is possible to solve without solving much broader questions, maybe that’s not the most fruitful avenue of discussion.
Lucas: So, let’s put aside this first case which involves the control problem, we’ll call it, and let’s focus on the second and the third, where again the second is defined as sort of the issue of the plurality of human values and preferences which can be observed, and then the third you described as us determining what we ought to do and tackling sort of the metaethics.
Will: Yeah, just tackling the fundamental question of, “Where ought we to be headed as a society?” One just extra thing to add onto that is that’s just a general question for society to be answering. And if there are kind of fast, or even medium-speed, developments in AI, perhaps suddenly we’ve gotta start answering that question, or thinking about that question even harder in a more kind of clean way than we have before. But even if AI were to take a thousand years, we’d still need to answer that question, ’cause it’s just fundamentally the question of, “Where ought we to be heading as a society?”
Lucas: Right, and so going back a little bit to the little taxonomy that I had developed earlier, it seems like your second case scenario would be sort of down to metaethical questions, which are behind and which influence the empirical issues with preference aggregation and there being plurality of values. And the third case would be, what would be arrived at through reason and, I guess, the reason of many different people.
Will: Again, it’s gonna involve questions of metaethics as well where, again, on my theory that metaethics … It would actually just involve interacting with conscious experiences. And that’s a critical aspect of coming to understand what’s morally correct.
Lucas: Okay, so let’s go into the second one first and then let’s go into the third one. And while we do that, it would be great if we could be mindful of problems in intertheoretic comparison and how they arise as we go through both. Does that sound good?
Will: Yeah, that sounds great.
Lucas: So, would you like to just sort of unpack, starting with the second view, the metaethics behind that, issues in how moral realism versus moral anti-realism will affect how the second scenario plays out, and other sorts of crucial considerations in metaethics that will affect the second scenario?
Will: Yeah, so for the second scenario, which again, to be clear, is the aggregating of the variety of human preferences across a variety of contexts and so on, is that right?
Lucas: Right, so that the agent can be fully autonomous and realized in the world that it is sort of an embodiment of human values and preferences, however construed.
Will: Yeah, okay, so here I do think all the metaethics questions are gonna play a lot more role in the third question. So again, it’s funny, it’s very similar to the question of kind of what mainstream economists often think they’re doing when it comes to cost-benefit analysis. Let’s just even start in the individual case. Even there, it’s not a purely kind of descriptive enterprise, where, again, let’s not even talk about AI. You’re just looking out for me. You and I are friends and you want to do me a favor in some way, how do you make a decision about how to do me that favor, how to benefit me in some way? Well, you could just look at the things I do and then infer on the basis of that what my utility function is. So perhaps every morning I go and I rob a convenience store and then I buy some heroin and then I shoot up and-
Lucas: Damn, Will!
Will: That’s my day. Yes, it’s a confession. Yeah, you’re the first to hear it.
Lucas: It’s crazy, in Oxford huh?
Will: Yeah, Oxford University is wild.
You see that behavior on my part and you might therefore conclude, “Wow, well what Will really likes is heroin. I’m gonna do him a favor and buy him some heroin.” Now, that seems kind of commonsensically pretty ridiculous. Well, assuming I’m demonstrating all sorts of bad behavior that looks like it’s very bad for me, it looks like a compulsion and so on. So instead what we’re really doing is not merely maximizing the utility function that’s gone by my revealed preferences, we have some deeper idea of kind of what’s good for me or what’s bad for me.
Perhaps that comes down to just what I would want to want, or what I want myself to want to want to want. Perhaps you can do it in terms of what are called second-order, third-order preferences. What idealized Will would want … That is not totally clear. Well firstly, it’s really hard to know kind of what would idealized Will want. You’re gonna have to start doing at least a little bit of philosophy there. Because I tend to favor hedonism, I think that an idealized version of my friend would want the best possible experiences. That might be very different from what they think an idealized version of themselves would want because perhaps they have some objective list account of well-being and they think well, what they would also want is knowledge for the its own sake and appreciating beauty for its own sake and so on.
So, even there I think you’re gonna get into pretty tricky questions about what is good or bad for someone. And then after that you’ve got the question of preference aggregation, which is also really hard, both in theory and in practice. Where, do you just take strengths of preferences across absolutely everybody and then add them up? Well, firstly you might worry that you can’t actually make these comparisons of strengths of preferences between people. Certainly if you’re just looking at peoples revealed preferences, it’s really opaque how you would say if I prefer coffee to tea and you vice versa, who has the stronger preference? But perhaps we could look at behavioral facts to kind of try and at least anchor that, but it’s still then non-obvious that what we ought to do when we’re looking at everybody’s preferences is just maximize the sum rather than perhaps give some extra weighting to people who are more badly off, perhaps we give more priority to their interests. So this is kinda theoretical issues.
And then secondly, is kinda just practical issues of implementing that, where you actually need to ensure that people aren’t faking their preferences. And there’s a well known literature and voting theory that says that basically any aggregation system you have, any voting system, is going to be manipulable in some way. You’re gonna be able to get a better result for yourself, at least in some circumstances, by misrepresenting what you really want.
Again, these are kind of issues that our society already faces, but they’re gonna bite even harder when we’re thinking about delegating to artificial agents.
Lucas: There’s two levels to this that you’re sort of elucidating. The first is that you can think of the AGI as being something which can do favors for everybody in humanity, so there are issues empirically and philosophically and in terms of understanding other agents about what sort of preferences should that AGI be maximizing for each individual, say being constrained by what is legal and what is generally converged upon as being good or right. And then there’s issues with preference aggregation which come up more given that we live in a resource-limited universe and world, where not all preferences can coexist and there has to be some sort of potential cancellation between different views.
And so, in terms of this higher level of preference aggregation … And I wanna step back here to metaethics and difficulties of intertheoretic comparison. It would seem that given your moral realist view, it would affect how the weighting would potentially be done. Because it seemed like before you were eluding to the fact that if your moral realist view would be true, then the way at which we could determine what we ought to do or what is good and true about morality would be through exploring the space of all possible experiences, right, so we can discover moral facts about experiences.
Will: Mm-hmm (affirmative).
Lucas: And then in terms of preference aggregation, there would be people who would be right or wrong about what is good for them or the world.
Will: Yeah, I guess this is, again why I wanna distinguish between these two types of value alignment problem, where on the second type, which is just kind of, “What does society want?” Societal preference aggregation. I wasn’t thinking of it as there being kind of right or wrong preferences.
In just the same way as there’s this question of just, “I want system to do X” but there’s a question of, “Do I want that?” or “How do you know that I want that?”, there’s a question of, “How do you know what society wants?” That’s a question in and of its own right that’s then separate from that third alignment issue I was raising, which then starts to bake in, well, if people have various moral preferences, views about how the world ought to be, yeah some are right and some are wrong. And no way should you give some aggregation over all those different views, because ideally you should give no weight to the ones that are wrong and if any are true, they get all the weight. It’s not really about kind of preference aggregation in that way.
Though, if you think about it as everyone is making certain sort of guess at the moral truth, then you could think of that like a kind of judgment aggregation problem. So, it might be like data or input for your kind of moral reasoning.
Lucas: I think I was just sort of conceptually slicing this a tiny bit different from you. But that’s okay.
So, staying on this second view, it seems like there’s obviously going to be a lot of empirical issues and issues in understanding persons and idealized versions of themselves. Before we get in to intertheoretic comparison issues here, what is your view on coherent extrapolated volition, sort of, being the answer to this second part?
Will: I don’t really know that much about it. From what I do know, it always seemed under-defined. As I understand it, the key idea is just, you take everyone’s idealized preferences in some sense, and then I think what you do is just take a sum of what everyone’s preference is. I’m personally quite in favor of the summation strategy. I think we can make interpersonal comparisons of strengths of preferences, and I think summing people’s preferences is the right approach.
We can use certain kinds of arguments that also have application in moral philosophy, like the idea of “If you didn’t know who you were going to be in society, how would you want to structure things? And if you’re a rational, self-interested agent, maximizing expected utility, then you’ll do the utilitarian aggregation function, so you’ll maximize the sum of preference strength.
But then, if we’re doing this idealized preference thing, all the devil’s going to be in the details of, “Well how are you doing this idealization?” Because, given my preferences for example, for what they are … I mean my preferences are absolutely … Certainly they’re incomplete, they’re almost certainly cyclical, who knows? Maybe there’s even some preferences I have that are areflexive of things, as well. Probably contradictory, as well, so there’s questions about what does it mean to idealize, and that’s going to be a very difficult question, and where a lot of the work is, I think.
Lucas: So I guess, just two things here. What are sort of the timeline and actual real world working in relationship here, between the second problem that you’ve identified and the third problem that you’ve identified, and what is the role and work that preferences are doing here, for you, within the context of AI alignment, given that you’re sort of partial of a form of hedonistic consequentialism?
Will: Okay, terrific, ’cause this is kind of important framing.
In terms of answering this alignment problem, the deep one of just where ought societies to be going, I think the key thing is to punt it. The key thing is to get us to a position where we can think about and reflect on this question, and really for a very long time, so I call this the long reflection. Perhaps it’s a period of a million years or something. We’ve got a lot of time on our hands. There’s really not the kind of scarce commodity, so there are various stages to get into that state.
The first is to reduce extinction risks down basically to zero, put us a position of kind of existential security. The second then is to start developing a society where we can reflect as much as possible and keep as many options open as possible.
Something that wouldn’t be keeping a lot of options open would be, say we’ve solved what I call the control problem, we’ve got these kind of lapdog AIs that are running the economy for us, and we just say, “Well, these are so smart, what we’re gonna do is just tell it, ‘Figure out what’s right and then do that.’” That would really not be keeping our options open. Even though I’m sympathetic to moral realism and so on, I think that would be quite a reckless thing to do.
Instead, what we want to have is something kind of … We’ve gotten to this position of real security. Maybe also along the way, we’ve fixed the various particularly bad problems of the present, poverty and so on, and now what we want to do is just keep our options open as much as possible and then kind of gradually work on improving our moral understanding where if that’s supplemented by AI system …
I think there’s tons of work that I’d love to see developing how this would actually work, but I think the best approach would be to get the artificially intelligent agents to be just doing moral philosophy, giving us arguments, perhaps creating new moral experiences that it thinks can be informative and so on, but letting the actual decision making or judgments about what is right and wrong be left up to us. Or at least have some kind of gradiated thing where we gradually transition the decision making more and more from human agents to artificial agents, and maybe that’s over a very long time period.
What I kind of think of as the control problem in that second level alignment problem, those are issues you face when you’re just addressing the question of, “Okay. Well, we’re now gonna have an AI run economy,” but you’re not yet needing to address the question of what’s actually right or wrong. And then my main thing there is just we should get ourselves into a position where we can take as long as we need to answer that question and have as many options open as possible.
Lucas: I guess here given moral uncertainty and other issues, we would also want to factor in issues with astronomical waste into how long we should wait?
Will: Yeah. That’s definitely informing my view, where it’s at least plausible that morality has an aggregative component, and if so, then the sheer vastness of the future may, because we’ve got half a billion to a billion years left on Earth, a hundred trillion years before the starts burn out, and then … I always forget these numbers, but I think like a hundred billion stars in the Milky Way, ten trillion galaxies.
With just vast resources at our disposal, the future could be astronomically good. It could also be astronomically bad. What we want to insure is that we get to the good outcome, and given the time scales involved, even what seem like an incredibly long delay, like a million years, is actually just very little time indeed.
Lucas: In half a second I want to jump into whether or not this is actually likely to happen given race dynamics and that human beings are kind of crazy. The sort of timeline here is that we’re solving the technical control problem up into and on our way to sort of AGI and what might be superintelligence, and then we are also sort of idealizing everyone’s values and lives in a way such that they have more information and they can think more and have more free time and become idealized versions of themselves, given constraints within issues of values canceling each other out and things that we might end up just deeming to be impermissible.
After that is where this period of long reflection takes place, and sort of the dynamics and mechanics of that are seeming open questions. It seems that first comes computer science and global governance and coordination and strategy issues, and then comes long time of philosophy.
Will: Yeah, then comes the million years of philosophy, so I guess not very surprising a philosopher would suggest this. Then the dynamics of the setup is an interesting question, and a super important one.
One thing you could do is just say, “Well, we’ve got ten billion people alive today, let’s say. We’re gonna divide the universe into ten billionths, so maybe that’s a thousand galaxies each or something.” And then you can trade after that point. I think that would get a pretty good outcome. There’s questions of whether you can enforce it or not into the future. There’s some arguments that you can. But maybe that’s not the optimal process, because especially if you think that “Wow! Maybe there’s actually some answer, something that is correct,” well, maybe a lot of people miss that.
I actually think if we did that and if there is some correct moral view, then I would hope that incredibly well informed people who have this vast amount of time, and perhaps intellectually augmented people and so on who have this vast amount of time to reflect would converge on that answer, and if they didn’t, then that would make me more suspicious of the idea that maybe there is a real face to the matter. But it’s still the early days we’d really want to think a lot about what goes into the setup of that kind of long reflection.
Lucas: Given this account that you’ve just given about how this should play out in the long term or what it might look like, what is the actual probability do you think that this will happen given the way that the world actually is today and it’s just the game theoretic forces at work?
Will: I think I’m going to be very hard pressed to give a probability. I don’t think I know even what my subjective credence is. But speaking qualitatively, I’d think it would be very unlikely that this is how it would play out.
Again, I’m like Brian and Dave in that I think if you look at just history, I do think moral forces have some influence. I wouldn’t say they’re the largest influence. I think probably randomness explains a huge amount of history, especially when you think about how certain events are just very determined by actions of individuals. Economic forces and technological forces, environmental changes are also huge as well. It is hard to think at least that it’s going to be likely that such a well orchestrated dynamic would occur. But I do think it’s possible and I think we can increase the chance of that happening by the careful actions that where people like FLI are doing at the moment.
Lucas: That seems like the sort of ideal scenario, absolutely, but I also am worried that people don’t like to listen to moral philosophers or people in that potentially selfish government forces and things like that will end up taking over and controlling things, which is kind of sad for the cosmic endowment.
Will: That’s exactly right. I think my chances … If there was some hard takeoff and sudden leap to artificial general intelligence, which I think is relatively unlikely, but again is possible, I think that’s probably the most scary ’cause it means that a huge amount of power is suddenly in the hands of a very small number of people potentially. You could end up with the very long run future of humanity being determined by the idiosyncratic preferences of just a small number of people, so it would be very dependent whether those people’s preferences are good or bad, with a kind of slow takeoff, so where there’s many decades in terms of development of AGI and it gradually getting incorporated into the economy.
I think there’s somewhat more hope there. Society will be a lot more prepared. It’s less likely that something very bad will happen. But my default presumption when we’re talking about multiple nations, billions of people doing something that’s very carefully coordinated is not going to happen. We have managed to do things that have involved international cooperation and amazing levels of operational expertise and coordination in the past. I think the eradication of smallpox is perhaps a good example of that. But it’s something that we don’t see very often, at least not now.
Lucas: It looks like that we need to create a Peter Singer of AI safety of some other philosopher who has had a tremendous impact on politics and society to spread this sort of vision throughout the world such that it would more likely become realized. Is that potentially most likely?
Will: Yeah. I think if a wide number of the political leaders, even if just political leaders of US, China, Russia, all were on board with global coordination on the issue of AI, or again, whatever other transformative technology might really upend things in the 21st century, and were on board with “How important it is that we get to this kind of period of long reflection where we can really figure out where we’re going,” then that alone would be very promising.
Then the question of just how promising is that I think depends a lot on maybe the robustness of … Even if you’re a moral realist, there’s a question of “How likely do you think it is that people will get the correct moral view?” It could be the case that it’s just this kind of strong attractor where even if you’ve got nothing as clean cut as the long reflection that I was describing, instead some really messy thing, perhaps various wars and it looks like feudal society or something, and anyone would say that civilization looks likely chaotic, maybe it’s the case that even given that, just given enough time and enough reasoning power, people will still converge on the same moral view.
I’m probably not as optimistic as that, but it’s at least a view that you could hold.
Lucas: In terms of the different factors that are going into the AI alignment problem and the different levels you’ve identified, first, second, and third, which side do you think is lacking the most resources and attention right now? Are you most worried about the control problem, that first level? Or are you more worried about potential global coordination and governance stuff at the potential second level or moral philosophy stuff at the third?
Will: Again, flagging … I’m sure I’m biased on this, but I’m currently by far the most worried on the third level. That’s for a couple of reasons. One is I just think the vast majority of the world are simple subjectivists or relativists, so the idea that we ought to be engaging in real moral thinking about how we use society, where we go with society, how we use our cosmic endowment as you put it, my strong default is that that question just never even really gets phrased.
Lucas: You don’t think most people are theological moral realists?
Will: Yeah. I guess it’s true that I’m just thinking about-
Lucas: Our bubble?
Will: My bubble, yeah. Well educated westerners. Most people in the world at least would say they’re theological moral realists. One thought is just that … I think my default is that some sort of relativistic will hold sway and people will just not really pay enough attention to think about what they ought to do. A second relevant thought is just I think the best possible universe is plausibly really, really good, like astronomically better than alternative extremely good universes.
Lucas: Absolutely.
Will: It’s also the case that if you’re … Even like slight small differences in moral view might lead you to optimize for extremely different things. Even just a toy example of preference utilitarianism vs hedonistic utilitarianism, what you might think of as two very similar views, I think in the actual world there’s not that much difference between them, because we just kind of know what makes people better off, at least approximately, improves their conscious experiences, it also is generally what they want, but when you’re kind of technologically unconstrained, it’s plausible to me that the optimal configuration of things will look really quite different between those two views. I guess I kind of think the default is that we get it very badly wrong and it will require really sustained work in order to ensure we get it right … If it’s the case that there is a right answer.
Lucas: Is there anything with regards to issues in intertheoretic comparisons, or anything like that at any one of the three levels which we’ve discussed today that you feel we haven’t sufficiently covered or something that you would just like to talk about?
Will: Yeah. I know that one of your listeners was asking whether I thought they were solvable even in principle, by some superintelligence, and I think they are. I think they are if other issues in moral philosophy are solvable. I think that’s particularly hard, but I think ethics in general is very hard.
I also think it is the case that whatever output we have at the end of this kind of long deliberation, again it’s unlikely we’ll get to credence 1 in a particular view, so we’ll have some distribution over different views, and we’ll want to take that into account. Maybe that means we do some kind of compromise action.
Maybe that means we just distribute our resources in proportion with our credence in different moral views. That’s again one of these really hard questions that we’ll want if at all possible to punt on and leave to people who can think about this in much more depth.
Then in terms of aggregating societal preferences, that’s more like the problem of interpersonal comparisons of preference strength, which is kind of formally isomorphic but is at least a definitely issue.
Lucas: At the second and the third levels is where the intertheoretic problems are really going to be arising, and at that second level where the AGI is potentially working to idealize our values, I think there is again the open question about in the real world, whether or not there will be moral philosophers at the table or in politics or whoever has control over the AGI at that point in order to work on and think more deeply about intertheoretic comparisons of value at that level and timescale. Just thinking a little bit more about what we ought to do or what we should do realistically, given potential likely outcomes about whether or not this sort of thinking will or will not be at the table.
Will: My default is just the crucial thing is to ensure that this thinking is more likely to be at the table. I think it is important to think about, “Well, what ought we to do then,” if we think it’s as very likely that things go badly wrong. Maybe it’s not the case that we should just be aiming to push for the optimal thing, but for some kind of second best strategy.
I think at the moment we should just be trying to push for the optimal thing. In particular, that’s in part because my views that a optimal universe is just so much better than even an extremely good one, that I just kind of think we ought to be really trying to maximize the chance that we can figure out what there is and then implement it. But it would be interesting to think about it more.
Lucas: For sure. I guess just wrapping up here, did you ever have the chance to look at those two Lesswrong posts by Worley?
Will: Yeah, I did.
Lucas: Did you have any thoughts or comments on them? If people are interested you can find links in the description.
Will: I read the posts, and I was very sympathetic in general to what he was thinking through. In particular the principle of philosophical conservatism. Hopefully I’ve shown that I’m very sympathetic to that, so trying to think “What are the minimal assumptions? Would this system be safe? Would this path make sense on a very, very wide array of different philosophical views?” I think the approach I’ve suggested, which is keeping our options open as much as possible and punting on the really hard questions, does satisfy that.
I think one of his posts was talking about “Should we assume moral realism or assume moral antirealism?” It seems like there our views differed a little bit, where I’m more worried that everyone’s going to assume some sort of subjectivism and relativism, and that there might be some moral truth out there that we’re missing and we never think to find it, because we decide that what we’re interested in is maximizing X, so we program agents to build X and then just go ahead with it, whereas actually the thing that we ought to have been optimizing for is Y. But broadly speaking, I think this question of trying to be as ecumenical as possible philosophically speaking makes a lot of sense.
Lucas: Wonderful. Well, it’s really been a joy speaking, Will. Always a pleasure. Is there anything that you’d like to wrap up on, anywhere people can follow you or check you out on social media or anywhere else?
Will: Yeah. You can follow me on Twitter @WillMacAskill if you want to read more on some of my work you can find me at williammacaskill.com
Lucas: To be continued. Thanks again, Will. It’s really been wonderful.
Will: Thanks so much, Lucas.
Lucas: If you enjoyed this podcast, please subscribe, give it a like, or share it on your preferred social media platform. We’ll be back again soon with another episode in the AI Alignment series.
[end of recorded material]
Originally published at futureoflife.org on September 17, 2018.
",AI Alignment Podcast: Moral Uncertainty and the Path to AI Alignment with William MacAskill,1,ai-alignment-podcast-moral-uncertainty-and-the-path-to-ai-alignment-with-william-macaskill-62017156cbbd,2018-09-18,2018-09-18 17:44:55,https://medium.com/s/story/ai-alignment-podcast-moral-uncertainty-and-the-path-to-ai-alignment-with-william-macaskill-62017156cbbd,False,11580,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Future of Life,FLI catalyzes and supports research and initiatives to safeguard life and develop optimistic visions of the future. Official account.,e33e2d2a809c,FLIxrisk,1361.0,93.0,20181104
0,,0.0,f5af2b715248,2017-12-24,2017-12-24 05:00:15,2017-12-24,2017-12-24 09:56:43,6,False,en,2018-01-09,2018-01-09 17:20:56,8,ab5b0ef433d7,6.700943396226416,28,1,0,By Raamesh Gowri Raghavan and Sukant Khurana,5,"Ethics, humans, and AI
By Raamesh Gowri Raghavan and Sukant Khurana
I, Robot cover from Asimov’s famous novel
To begin with, ethics, morality and the law are distinct entities, even though any system of law is based on the ethics of those enforcing or abiding by the law. Ethics are usually absolute, while morality, which is often enforced by religious and social forces, is relativistic. Thus while it is considered unethical in all circumstances to kill a human being, it may not be considered immoral in certain circumstances by many societies, such as in war, or religiously-sanctioned murder (of an apostate, for example). Law interacts with ethics and morality in similar ways; thus killing a person declared an ‘enemy of the state’, such as a person convicted of ‘treason’, or an armed citizen of another country designated as an ‘enemy soldier’ is not deemed an illegal murder. On the other hand, euthanasia is illegal under the laws of many countries, including India, although some religions, such as Jainism, may not consider it immoral.
But whose conduct may be deemed to be ethical or otherwise, immoral or illegal? For this, we come to two more concepts of philosophy that have been debated from time immemorial: personhood and agency.
Personhood is the recognition that an entities is entitled to some rights, and simultaneously subject to liabilities; as given by a state, religious body or social customs. For the purpose of this paper, we shall restrict ourselves to considering personhood under the law only, for the other forms are not enforceable in modern societies. The law of almost all countries recognises two forms of personhood, natural and juridical.
In almost all countries today, full natural personhood is extended to all human beings above the legal age. Such persons, whether they be citizens or not, are entitled to certain rights, responsible for certain duties, and liable for acts of omission or commission. Those below the legal age of majority may not receive complete personhood, and therefore restricted from exercising certain rights (such as voting) and holding liability for criminal acts (hence minors are not liable to face the full fury of the law).
Juridical personhood is granted to non-human entities that are to be treated as ‘persons’ for the use of the law; this is always extended to sovereign states, government agencies, companies, NGOs, associations etc that execute financial transactions, and may be prosecuted in a court of law. This is the basic tenet of law that allows companies to seek the protection of the law and be subject to taxes. A further such ‘legal fiction’ is extended to a ‘corporation sole’, in which case an office exercised by a person can have separate legal rights, duties and liabilities independent of who exercises that office; the president or prime minister of a country are the best examples.
Image from Computer Business Review
Only an entity entitled to or subjected to personhood can be required to possess an ethic. Entities, living or otherwise, that have long been denied personhood by religion, society or the law, such as non-human living organisms, tools, mechanical devices, are not required to conduct themselves on ethical bases. This is dependent on agency, which is a related philosophical concept.
Agency is usually defined as the ability to take one’s own decisions and have the ability to understand their consequences. The agency of a human being who is a legal major, or a corporate entity, is taken to be fully formed. The agency of a growing child is considered to be developing, and they are therefore free from most liabilities and denied several rights until they attain the age of legal majority. The agency of an animal, plant, any other living organism, tool or mechanical device is considered not to exist. By law, they are property, either of individuals or corporate entities. If unclaimed, they exist as the property of the state by the principle of eminent domain.

You will realize that agency is independent of intelligence. The laws of most countries do not require a test of intelligence to confer agency (despite that being the basic assumption of why children are not treated on par with adults); hence the mentally challenged are not denied legal personhood upon reaching the age of majority. Thus, however much intelligence an animal or mechanical device might possess, it is attributed no agency, and hence no personhood.
This is the legal basis for determining the ‘ethics’ of an entity that possesses artificial intelligence (henceforth, AI). If the law (say a court or a parliament) confers personhood upon an AI device in the future (or as has already happened for Sophia in Saudi Arabia), it will immediately be entitled to the rights granted by the law, the responsibilities required by the state, and the liabilities imposed by the law. This is the only point at which ethical conduct is required of the ‘person’. In the absence of legal personhood, or even the limited personhood of childhood, any entity is liable to be treated as property. Remember, personhood is not enough for conferring legal liberty or certain rights. Until the enactment of the Thirteenth Amendment in USA, black slaves were person enough to bear liability, but not to have rights; similarly those imprisoned for crimes are usually held fully responsible for their actions even though their rights are severely curtailed.
Horrors of slavery and racism captured in a painting.
The actions of something that lacks personhood altogether, and is thus a ‘chattel’ under law (allowing it to be bought and sold), will therefore be attributed to the ‘agency’ of its owner. This is the argument of the NRA when they say, ‘guns don’t kill people, people kill people’, however flawed you may find it. In such cases, the legal liability rests with the owner.
Why I went through all this discussion is to make this argument: however ethically trained an AI maybe, it will not be held liable until it is conferred personhood. So let us discuss two scenarios:
1. AI has personhood: In this case, the AI is responsible (and liable) for its actions. In this case, any ethical conundrum, it is expected to deal with. So if it is faced with the conundrum of killing an opposite, possibly innocent party to save the life of its ‘owner’ or employer, versus killing the owner/employer to save a large number of people, it is required to do what it thinks best and face the consequences in a court of law. In this case, all reviews and articles you have read should be checked for the assumption of personhood. This is also where the discussion of making instant calculations of the probabilities of the outcomes comes in. Remember, while human actions can be attributed to impulse or whim, an AI’s actions will always be calculated and deliberate.
2. AI does not have personhood at all: Then there is no case of ethics. An AI is a slave in this case, and whether owned by the state (say, to oversee electric supply or control traffic), or a private entity (like Facebook’s AI system) or an individual human (say as a driverless car), any actions of the AI will be held liable to its owner. It is thus but natural that such an AI be programmed to maximize the interests of its owner, even if it has to kill.
3. AI is conferred limited personhood: This would be a murky territory. Nevertheless, I think this is where Isaac Asimov’s Law of Robotics might fall, wherein robots have responsibilities, but no equality with humans. We guess (and we could be wrong), a lot of robo-ethics literature will be in this grey zone, even if the personhood is implied and not stated.
If we develop AI as specialized AI with no general intelligence, a lot of personhood debate might not be relevant and we would be dealing with other aspects of ethics of AI but we suspect that sooner or later we would have to deal with ethics of AI behavior to the point of personhood. Future holds immense promises and perils; needless to say there are exciting times ahead.
About:
Mr. Raamesh Gowri Raghavan is an award winning poet, a well-known advertising professional, historian, and a researcher exploring the interface of science and art. He is also championing a massive anti-depression and suicide prevention effort.
You can know more about Raamesh at:
https://sites.google.com/view/raameshgowriraghavan/home
https://www.linkedin.com/in/raameshgowriraghavan/?ppe=1
And here’s Raamesh telling his life story:

Raamesh and Sukant are working together on several projects on the intersection of science, technology, and art, and also projects on mental health.
Dr. Sukant Khurana runs an academic research lab and several tech companies. He is also a known artist, author, and speaker. You can learn more about Sukant at www.brainnart.com or www.dataisnotjustdata.com and if you wish to work on biomedical research, neuroscience, sustainable development, artificial intelligence or data science projects for public good, you can contact him at skgroup.iiserk@gmail.com or by reaching out to him on linkedin https://www.linkedin.com/in/sukant-khurana-755a2343/.
Here are two small documentaries on Sukant and a TEDx video on his citizen science effort.




This story is published in The Startup, Medium’s largest entrepreneurship publication followed by 277,446+ people.
Subscribe to receive our top stories here.

","Ethics, humans, and AI",120,ethics-humans-and-ai-ab5b0ef433d7,2018-05-08,2018-05-08 17:52:39,https://medium.com/s/story/ethics-humans-and-ai-ab5b0ef433d7,False,1524,Medium's largest publication for makers. Subscribe to receive our top stories here → https://goo.gl/zHcLJi,,,,The Startup,,swlh,"STARTUP,TECH,ENTREPRENEURSHIP,DESIGN,LIFE",thestartup_,Ethics,ethics,Ethics,7787.0,Sukant Khurana,"Blockchain, edutech, AI, neuroscience, drug-discovery, design-thinking, sustainable development, art, & literature. There is only one life, use it well.",6d41261644a8,sukantkhurana,433.0,135.0,20181104
0,,0.0,,2017-10-22,2017-10-22 05:44:06,2017-10-22,2017-10-22 06:47:05,1,False,en,2017-10-22,2017-10-22 17:00:01,2,82f7a5853661,1.8716981132075472,0,0,0,"Machines that think, so to speak, will soon operate as an integrated part of our daily lives. These artifacts that are a reflection of our…",3,"Artificial Intelligence and Control
Machines that think, so to speak, will soon operate as an integrated part of our daily lives. These artifacts that are a reflection of our own values. When Hume said:
“Reason is, and ought only to be the slave of the passions, and can never pretend to any other office than to serve and obey them.”
He was obviously giving thought to human reason, but it can be applied to AI, too. These machines can calculate and act accordingly to thousands of parameters faster than humans, but they act to what set of task they were to carry — their passion. They cannot be criticized for whatever task they carry out, but the outcome may be horrifying. In a way, they can be thought of as a part of us; a medium of our analytical and probabilistic ability. In neural networks, we attempt to simulate how the brain works with thousands to millions of bits of knowledge travelling through neurons to piece together a final thought. In another way, they simply are not us. In our extremely complex environment, robots can interpret from their senses with only the feedback of analyzing data and act accordingly to what developers have ‘taught’ it to. Robots don’t have any ‘street credit’ — they are only book smart.
When machines will be set into autonomous task in our environment, software bugs will occur. An example is from the software Knight Capital chose to implement into their trading analysis in 2012. This example portraits the extreme amount of error a machine interpreting its parameters wrong can cause in very short time. While assuming that the software were constantly monitored by people, how long will it take software engineers to become aware of a bug if the system is autonomous?
Image source: pddnet.com — A MQ-9 Reaper unmanned system.
The military is investing heavily in artificial intelligence for the reason to defend themselves to others also investing for the same purpose. They suddenly oppose a threat to each other in a new technology that we do not yet know the limits to. These war machines will also have bugs, errors, wrong predictions. It can cause indisputably unethical actions, and who will we blame for such acts?
A possible solution is for scientists to be open about what they built, how they’ve built it and for what purpose they intent it to have. We ultimately do not want to not have artificial intelligence in our future lives, because the benefits to our expanding society and the problems we have. What we don’t want is for things to get out of hand due to secrecy and paranoia of what great war technology others might be building.
",Artificial Intelligence and Control,0,in-control-of-machines-82f7a5853661,2018-05-20,2018-05-20 07:02:17,https://medium.com/s/story/in-control-of-machines-82f7a5853661,False,443,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Simon Nielsen,Currently pursuing a degree in Computer Science. Interested in AI.,5c7bc73aedea,post.smn,7.0,9.0,20181104
0,,0.0,c821ac629aa3,2018-03-18,2018-03-18 15:10:45,2018-03-19,2018-03-19 22:56:51,4,False,en,2018-03-19,2018-03-19 23:52:36,12,b481ea97f982,3.80188679245283,0,0,0,"In 1986 the local telco optically wired the city of Calgary, W Canada, for free, betting it would recoup its investment in business network…",5,"Facebook server farm and prison cells (unrated) in Scandinavia
The Internet? It’s complicated!
In 1986 the local telco optically wired the city of Calgary, W Canada, for free, betting it would recoup its investment in business network traffic on its leased lines, which it did handsomely. Those were the days the Alberta government launched a number of data repositories like the Land Related Info System, and by the late eighties — that’s 30 years ago — all drivers licenses etc. were done electronically on said network, but from government offices.

Why? Because the internet didn’t exist as yet! Tim Berners Lee would have barely arrived at CERN and proposed the kernel of the World Wide Web. That didn’t stop us from from linking up on what was already called hypertext on pre-internet (left).
That put my then-hometown behind is a way, because when the internet did arrive, we went “meh! we’re all wired up already”. Note that the internet was slow coming: Netscape and Altavista ruled the web and search worlds for almost a decade until Google came along… and another decade went until Google Maps! These are lifetimes in IT-speak, but our penchant for instant gratification helps us forget: T h i s . D i d . S t a r t . S l o w l y . . .
Fast forward almost 30 years, and internet inventor Tim Berners-Lee “is not amused”. He said in the Guardian mid-last November:
“The system is failing. The way ad revenue works with clickbait is not fulfilling the goal of helping humanity promote truth and democracy. So I am concerned,” said Berners-Lee, who in March called for the regulation of online political advertising to prevent it from being used in “unethical ways”.
http://bit.ly/2ARgm5Q
Since then, it has been revealed that Russian operatives bought micro-targeted political ads aimed at US voters on Facebook, Google and Twitter. Data analytics firms such as Cambridge Analytica, which builds personality profiles of millions of individuals so they can be manipulated through “behavioural micro-targeting”, have also been criticised for “weaponised AI propaganda”.
“We have these dark ads that target and manipulate me and then vanish because I can’t bookmark them. This is not democracy — this is putting who gets selected into the hands of the most manipulative companies out there,” said Berners-Lee.”
So you thought Cambridge Analytica was news? Think again: That was written three months ago — three months ago — that’s ages in news-speak!
So yes, the proverbial ‘sh*t did hit the fan’, when Wylie the architect behind CA hacking Facebook recently came out… and it makes for uneasy listening:

But even here, all is not what it seems. In the Liberal Democrat Voice “Martin Horwood writes…The real issue about Trump, Facebook’s ‘data breach’, why The Observer [that got the scoop] missed the point and Liberals should care”:
http://bit.ly/2psoTaf
“The real risk in all this is in the use to which this kind of big data can now be put. Mass harvesting followed by sophisticated modelling can now put very particular and individual information to use in support of data user’s objectives. It’s not new in principle. […]
But it’s why the Lib Dem international policy working group I’ve been chairing is proposing we put both both human rights and technology at the forefront of future foreign policy.”
[Disclosure: I help run the back-office in London for Mouvement Democrate, the French counterparts of UK Liberal Democrats and Spanish Ciudadanos.]
And then there’s blockchain… AGI Geocom16 eighteen months ago gave an unexpected introduction to that matter via Dr Catherine Mulligan, who stated the following from my twitter report:
The internet is the exchange of information, and block-chain is the exchange of value.
She heralded that National Agency of Public Registry in the Republic of Georgia “can provide its citizens with a digital certificate of their assets, supported with cryptographical proof published to the Bitcoin Blockchain.”
This countered after a fashion, the general sour tenor bitcoin took, when it was initially linked to the Silk Road underground network that turned rogue.
Here is, regardless, its best intro by @BBCclick Spencer Kelly & Nick Kwek:

Let’s close with an example that really shows how the internet, with all its promise of openness and freedom, also has unexpected results: Mapping twitter reactions during an episode of Israel-Gaza conflict, the internet reinforced each group’s biases, rather than help build bridges among them.

Unintended consequences was covered in a previous Medium post on geodata and privacy: let me reiterate AGI chair Abi Page’s exhortation in geo space in particular, which applies here as well in the internet space in general:
Today, we have heard how the availability, consistency, and quality of geographic information is key. However, improvements to that data and the way it is used cannot come about easily without the influence and advocacy of people. We must come together and act now to influence that change…
",The Internet? It’s complicated!,0,the-internet-its-complicated-b481ea97f982,2018-04-09,2018-04-09 10:41:26,https://medium.com/s/story/the-internet-its-complicated-b481ea97f982,False,822,"Professional posts of Information Supply Chain, Workflows & the Internet of Data",,,,Zolnai.ca,azolnai@yahoo.co.uk,zolnai-ca,"BUSINESS PROCESS,SUPPLY CHAIN,WORKFLOW,PETROLEUM,GIS",azolnai,Ethics,ethics,Ethics,7787.0,Andrew Zolnai,"Citizen of the World, geologist, entrepreneur and volunteered geographer.",3401caea7d08,aizolnai,451.0,196.0,20181104
0,,0.0,,2018-05-18,2018-05-18 16:58:47,2018-05-18,2018-05-18 17:03:15,2,False,en,2018-09-12,2018-09-12 15:35:35,5,ab4d027eaa,3.017295597484277,2,0,0,"It’s certainly a lot better than not thinking about it at all. As a software producer for many years now, a trick I have relied on time and…",4,"Understanding Your Stance On A.I. Ethics After The Fact? That’s OK
Source: Lucas Amunategui
It’s certainly a lot better than not thinking about it at all. As a software producer for many years now, a trick I have relied on time and time again, is to build a prototype for indecisive customers. They’ll immediately tell you why it won’t work, and the previously blank specification sheet quickly gets filled up. And apparently, the same trick works to elicit ethical thought from people like me who tend to focus on the shiny stuff and ignore the murky.
I’ll start with the ethical reactions to the Google Duplex presentation (and the lack of my own). I am a data scientist so I wasn’t blindsided and I assumed this was coming. I’ve worked with many Google developers, used Tensorflow and Google Cloud APIs in various commercial applications — all smart people and smart tools. Yes, I thought this was cool and smiled as CEO Sundar Pichai walked through the demo.
It’s only after reading the reactions to this tech that I realized how out-of-touch us developers can be from these important concerns. Imagine what spammers, unscrupulous salespeople, manipulators, or impersonators could create with these capabilities. And not to mention using innocent bystanders as guinea pigs in enormous social experiments.
Another area where I lacked ethical foresight was with the autonomous car. I wanted it so desperately that I was willing to play dumb if it would get us there faster. But people thankfully brought up the edge cases, like how do we choose between veering off the cliff to save the pedestrian, or not veering and saving the driver, or choosing between an old person on one side of the road versus the young mother with a baby cart on the other? This certainly isn’t new, remember the classic “Trolley problem”, but will become a much more common problem soon. How do we quantify this and who could still sleep at night after coding it?
Now, when Boston Dynamics showed that video of a guy with a hockey stick beating on the disturbingly humanoid looking robot, I was instantly bothered. Sure, its only metal, wires, motors, and computers, yet, seeing the poor machine keep trying to get back up to finish its task, reminds us too much of own plight. We are all small pawns in an enormous game where larger-than-us things keep beating us down and all we can do is pick ourselves up and trod along.
If we’re going to use the world to build tools for the world, we need to make sure that world is on board. In the case of beating on a robot, that’s easy, make the robot look like a cancer cell or a hated dictator, and none will blink. For the other ethical questions, it’s going to take more than dressing up robots. But if it takes creating a few prototypes to elicit reactions out of people like me, then please prototype more, keep taking risks and share widely. We’ll keep posting our reactions and learn from each other. And just like Sundar Pichai responded, he is learning about this too, we all are, and he’ll work towards identifying when its a robot on the other end of the line.
It is hard to picture every permutation of AI’s reach. If reacting is all we have now, then let’s keep reacting, talking, posting online. Slowly but surely, we’ll eventually get ahead of this and start acting instead of reacting. The key is to keep prototyping and sharing with the world so we can all discover how we feel together. It's the not sharing with others, keeping it secret and using it to manipulate, that terrifies me! We know AI is inevitable and, if done well, will enhance everybody’s lives!
Please share and clap if you found this helpful — thanks for reading!
Manuel Amunategui
Get it and plenty more at amunategui.github.io and at ViralML.com.

OK — Sign up to my email group below and I’ll send you my free eBook on tips to becoming a (better) data scientist (and sign up even if you aren’t interested in the eBook). Thanks for reading!!

",Understanding Your Stance On A.I. Ethics After The Fact? That’s OK,23,understanding-your-stance-on-a-i-ethics-after-the-fact-thats-ok-ab4d027eaa,2018-09-12,2018-09-12 15:35:35,https://medium.com/s/story/understanding-your-stance-on-a-i-ethics-after-the-fact-thats-ok-ab4d027eaa,False,698,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Manuel Amunategui,Anything Applied Data Science. Author of Monetizing Machine Learning. VP SpringML. Barcelona.,5e29cc9fde82,amunategui,170.0,338.0,20181104
0,,0.0,,2018-09-07,2018-09-07 10:32:08,2018-09-07,2018-09-07 11:24:17,0,False,en,2018-09-07,2018-09-07 11:24:17,9,9114d174f79c,3.9660377358490564,0,0,0,"September 25, 2017",5,"Methodology as an entryway to ethical data research
September 25, 2017
There is a growing call for ethical oversight of AI research, and rightly so. Problem is, ethical oversight hasn’t always stopped past research with questionable ethical compasses. Part of that, I argue, is that the ethical concerns raised largely by social scientists come from a completely different world view to those from a more technical background. While AI research is raising new problems, particularly with regards to correlation vs causation in research, but the tools we have to solve them haven’t changed that much.
With this blog I want to question — can methodology help social and technical experts speak the same language?
Since my masters degree I’ve been fascinated by the fact that people working in different disciplines or types of work will have completely different approaches to the same problem.
Like in this article on flooding in Chennai, I found that ‘the answers’ to solving flooding all already existed on the ground, it’s just the variety of knowledges weren’t being integrated because of the different ways that they’re valued.
I was recently speaking with my brilliant colleague and friend, who is a social constructivist scientist working in a very digital technology oriented academic department faculty. This orientation is important to note, because the methodology deployed for science and research there and the questions being asked are influenced to a large degree by the capacities and possibilities afforded by digital technologies and data. As a result, the space scientists see for answers can be very different.
In reviewing student research proposals, she found she was struggling because some research hypotheses completely ignored the ethical implications of the proposed research. In talking it through, we realised that most of the problems arose from the assumptions that are made in framing those questions.
We realised that most of the problems arose from the assumptions that are made in framing those questions.
To take a classic example, in the field of remote sensing to identify slums, it is relatively common to see that implicit assumption that what defines a slum is the area’s morphology, that that definition is by the city planners and not the residents, and how locals interpret the area or the boundaries of the neighbourhood may differ completely. The ethical problem, beyond epistemology, is what can then be done in terms of policy based on the answers that that research provides.
To go back to that paper that caused the controversy about identifying people’s sexual orientation from profile pictures downloaded from a dating site. It’s based on a pre-natal hormone theory of sexual orientation, which is a massive assumption in and of itself. Even the responses to the article have basically boiled down to ‘AI can predict sexuality’, even though that’s blatant generalisation and doesn’t look at who was actually in the dataset (every single non-straight person? Only white people?). That, and then the fact that they basically ‘built the bomb to warn us of the dangers’, has a lot of assumptions about your view of ethics in the first place.
Like my 10th grade history teacher used to say, to assume makes an ASS of U and ME.
(Thanks Mr. Desmarais)
More precisely, to assume without making the assumptions explicit. Not clearly articulating what your assumptions are is a methodological problem for empirical research, with ethical implications. Unexamined assumptions mean bad science. Confounding variables and all that.
For reference, in statistics there is an entire elaborate, standardized system of dealing with assumptions by codifying them into different tests. You apply one statistical test, which has a particular name, because of the assumptions you have — i.e. I assume this data has a normal distribution.
If you’re using mixed methods, it becomes much harder to have a coherent system to talk about assumptions because the questions that are asked may not yield data that is amenable to statistical analyses and therefore cannot be interpreted with statistical significance. All the more important here to make assumptions explicit so they can be discussed and scrutinized.
Some ethical concerns can be dealt with more easily when we remember methodological scrutiny and transparency, bringing research back to the possibility of constructive criticism and not only fast publication potential.
How this process is dealt with currently in academia is ethical review, hence the call for ‘ethical watchdogs’. Thing is, in terms of the process of doing science in academic settings, ethical review is often the final check before approval to carry out the research. When I did my BSC. In psychology, sending the proposal to the ethics review board felt like an annoyingly mandatory tick-box affair.
The problem with this end-of-the-line ethical review is:
It’s not clear why the ethics is important to actually carry out the research
If the ethics board declines, you’re essentially back to the drawing board and have to start again.
Particularly under the pressure for fast publication, there aren’t many incentives to do good ethics unless you’re concerned about it from the outset.
What if we shifted the focus from ethics as an evaluation to ethics as methodology?
Rather than having an ethics review at the end of the process of formulating hypotheses and research proposals, could there be a way to incorporate an ethics review in the middle of the ‘research life cycle’?
One would then get feedback not only on the ethics but it could provide the opportunity to explain the research’s unexamined assumptions which ultimately makes for better science.
I understand this ideal situation implies quite a significant shift in institutional processes which are notorious for moving about as fast as stale syrup. Perhaps instead there could be a list of questions researchers could ask themselves to as a self-evaluation?
In this way, you could open an entryway to an ethical discussion as a question of methodology, rather than ontology or ethics per se, which are far too easily just troubled waters in terms of interdisciplinary discussions.
Do you know of any examples of structurally incorporating these ideas as a way to effective multidisciplinary dialogue?
My thanks go to my colleague who sparked this discussion and thought it through with me, who for reasons of their position, will remain anonymous.
Tags: AI, assumptions, ethics, methodology
Originally published at shazadejameson.com.
",Methodology as an entryway to ethical data research,0,methodology-as-an-entryway-to-ethical-data-research-9114d174f79c,2018-09-07,2018-09-07 11:24:17,https://medium.com/s/story/methodology-as-an-entryway-to-ethical-data-research-9114d174f79c,False,1051,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Shaz Jameson,learn to ask the right questions | digital governance & global data justice,b0ebf4d8c725,shazjameson,61.0,188.0,20181104
0,,0.0,39de5d526a38,2018-09-21,2018-09-21 18:09:51,2018-09-25,2018-09-25 21:37:20,3,False,en,2018-09-25,2018-09-25 21:37:20,4,3832a30e5e53,4.893396226415094,0,0,0,"In this world, nothing can be certain except death and taxes.",5,"
Why We Need a Solution to Progress
In this world, nothing can be certain except death and taxes.
Or so they said.
It is anticipated that within the next 100 years, the technology needed to make humans immortal will become available, while robots in the workplace will fundamentally change the economic structure, with likely implications for the tax system.
As we can already foresee, developments in artificial intelligence will start to challenge even the most fundamental assumptions we have about reality; namely, the certainty of death and the consistency of taxes.
If advents in AI are capable of turning this idiom into a modest representation of uncertainty, then what are the implications of AI on life, as we know it?
I believe it is important to be asking these types of questions if we want artificial intelligence to fulfill its promise of serving humanity for the better.
This belief inspired me to create Progress Solved, a student-led think tank at the Munk School of Global Affairs. In this group we are considering the most pressing ethical concerns that arise from developments in artificial intelligence. Members create solutions to these complex problems as well as explore policy-making channels they would pursue to make their solutions a reality. The desired goal of this group is to contemplate the ways in which the cyber-sphere could be reoriented in order for artificial intelligence to serve humans rather than capture them.
The divergence of progress
There are many who believe that “progress” should never be restrained or artificially directed. But what happens when progress for AI becomes regress for society? I believe that, left unchecked, the discrepancy between progress for humans and progress for AI will widen as our robotics and artificial intelligence become increasingly advanced.
This May, I was invited to the St. Gallen Symposium, an international conference whose theme was human’s viability in a job market transitioning at the hands of artificial intelligence. At the conference, I attended one of the working sessions led by the founder of GoodAI, Marek Rosa. Rosa is an engineer of computer games and a programmer of general artificial intelligence. Rosa explained to the working group that there are two ways of programming general artificial intelligence. The first method involves coding the strategy that the AI will use in completing its specific task. The second method involves setting a goal for the AI without coding the process by which the AI is meant to achieve that task.
When the general artificial intelligence is coded according to the second method (coding only the end goal) the AI utilizes novel techniques that would never have occurred to a human in achieving its target. This method of programming gave rise to Google’s AlphaGo AI. AlphaGo AI’s coding established only the end goal for the program, namely, to win. None of the heuristics that a human utilizes in order to determine the next move were programmed into the AI. When this method of coding faced off against a human, whom operates according to algorithms of cognition and heuristics, the computer won.
One might see this example and think that in allowing us to circumvent our cognitive limitations, AI is allowing us to progress. However, these same “cognitive limitations” have created empires, religions, and economic models. That is because these systems have made sense to the human brain in achieving specific end goals. But what happens when we program the same end goal that got us capitalism and democracy and our computer spits out entirely novel processes, which we cannot understand? Does this novel system still imply progress for us?
Lets say it does. Lets say the computer decides that the end goal of democracy can be better achieved without people going to the polls but rather by implementing a surveillance system that monitors your behavior and heart rate in response to political candidates and then votes accordingly. The computer would likely choose the candidate that more accurately reflects our wants and desires than we would choose ourselves. However, the programming of this algorithmic function would require waving the human’s right to liberty, freedom of thought and consciousness, and privacy. This technology would be widely adopted once it proves advantageous for some, but what are the implications of systematically reducing the value placed on these fundamental rights?
Playing with values
Rosa mentioned that in order for the AI to be “good” it would need fundamental values to be programmed into it. Of course, the values just mentioned (liberty, freedom of thought and consciousness, and privacy) would be a non-starter, as they would prevent the AI from achieving its goal of democracy 2.0.
Notwithstanding questions of which and whose values would be coded — I asked Rosa whether, at the very least, the supremacy of humans would be a value of the general AI.
“No” he answered, as if this would be too artificially limiting.
If even the most benevolent coders do not believe that the supremacy of humans is a necessary element of “good” AI then what happens when the AI, which can already outsmart a human in the most complex ancient strategy game and is ambivalent towards the fundamental values and sustainability of humanity, become the arbiters of progress?
The implications of unchecked AI
This argument may appear alarmist, but it is important to recognize that even with developments in AI thus far, hundreds of thousands of people have been made economically insecure. In the long run, we are all vulnerable since these developments and the uncertainty that they create permeate constructs as fundamental as our political structures, economic models, even our interpersonal relationships.
Don’t get me wrong; progress is a welcome, if not critical, element of our sustainability. If AI offers the ability to cheat death or do away with the tax system, I don’t think anyone would consider it to be regressive. Rather, it is the implications of these developments that we should concern ourselves with.
AI has the potential to bastardize the notion of “progress” and justify breaches to our rights, values and quality of life. I believe that AI can serve humanity for the better, but the current unbridled quality to these developments needs to be checked in order for us to solve the problem of “progress” before we cannot find a plug to pull.
— — — — — — — — — — — — — — —
Ally Cohen is the Founder of Progress Solved and a second year Master Student at the Munk School of Global Affairs.

Thanks for reading! If you enjoyed the article, we would appreciate your support by clicking the clap button below or by sharing this article so others can find it.
Want to read more? Head over to Politics + AI’s publication page to to find all of our articles. You can also follow us on Twitter and Facebook or subscribe to receive our latest stories.

",Why We Need a Solution to Progress,0,why-we-need-a-solution-to-progress-3832a30e5e53,2018-09-25,2018-09-25 21:37:20,https://medium.com/s/story/why-we-need-a-solution-to-progress-3832a30e5e53,False,1151,"Insight and opinion on how artificial intelligence is changing politics, policy, and governance",,PoliticsPlusAI,,Politics + AI,PoliticsPlusAI@gmail.com,politics-ai,"ARTIFICIAL INTELLIGENCE,TECHNOLOGY,POLITICS,GOVERNMENT,AI",PoliticsPlusAI,Ethics,ethics,Ethics,7787.0,Ally Cohen,,aca740fbbcfb,allycohen,1.0,1.0,20181104
0,,0.0,e4257945d1eb,2018-06-26,2018-06-26 10:40:48,2018-06-26,2018-06-26 15:34:15,1,False,en,2018-06-26,2018-06-26 15:35:08,1,f1ce54a59869,5.1056603773584905,9,0,0,"Artificial intelligence can solve big social problems, if companies are held to the highest ethical standards.",5,"Virtuous technology
Artificial intelligence can solve big social problems, if companies are held to the highest ethical standards.
By Mustafa Suleyman, Co-founder of DeepMind
@mustafasuleymn

If we want to address society’s most pressing and persistent challenges then technology will have a major role to play. From climate change to inequality, time and again we have struggled to keep pace with a changing world as the complexities of seemingly intractable problems overwhelm our capacity to intervene.
Scientific breakthroughs facilitated by artificial intelligence (AI) could make the crucial difference by helping to discover new knowledge, ideas and strategies in the areas that matter most to us all. For example, we have already started seeing progress in improving the efficiency of large scale industrial systems; at DeepMind we have started using our technology to improve the efficiency of Google’s data centres, which has led to energy savings of up to 40% in cooling systems.
But increasing public concern about some elements of the technology industry should serve as an urgent wake-up call. Of course, many technology companies began with altruistic mindsets. But the truth is that good intentions, initially captured in well-meaning slogans like ‘making the world a better place’, are now met with increasing unease by commentators and the public.
To be clear, this is not a critique of purpose-driven businesses; I genuinely believe these types of organisations will be a key to our future. I do not doubt the sincerity of the motivations of the vast majority of the funders, founders and executives I have met over the years; these people really do want to ‘make a real difference’ and ‘do the right thing’.
Having said that, rising public concern should not be dismissed as simply about there being a perception gap between the developers and users of technology; there is something deeper at work.
There are at least three important asymmetries between the world of tech and the world itself. First, the asymmetry between people who develop technologies and the communities who use them. Salaries in Silicon Valley are twice the median wage for the rest of the US and the employee base is unrepresentative when it comes to gender, race, class and more. As we have seen in other fields, this risks a disconnect between the inner workings of organisations and the societies they seek to serve.
This is an urgent problem. Women and minority groups remain badly underrepresented, and leaders need to be proactive in breaking the mould. The recent spotlight on these issues has meant that more people are aware of the need for workplace cultures to change, but these underlying inequalities also make their way into our companies in more insidious ways. Technology is not value neutral — it reflects the biases of its creators — and must be built and shaped by diverse communities if we are to minimise the risk of unintended harms.
Second, there is an asymmetry of information regarding how technology actually works, and the impact that digital systems have on everyday life. Ethical outcomes in tech depend on far more than algorithms and data: they depend on the quality of societal debate and genuine accountability.
Making this happen has to be a collaborative effort, and requires new types of organisation that facilitate deep understanding of how complex algorithms work and their impacts on society. This takes courage, trust and the prioritisation of real debate and engagement over the comfort of our institutional roles, in which activists, governments and technologists are often more likely to criticise each other than to work together.
One of the new multi-stakeholder forums is the Partnership on AI, which brings together industry competitors, academia and civil society to discuss the ethics of machine learning, including issues such as fairness, transparency and accountability. The board has equal representation from corporations and nonprofits, making it a truly cross-cutting effort.
There also need to be new technical solutions that enable a wide range of stakeholders to have much greater visibility of how data is used. Interesting efforts are under way within companies, from the increased use of Transparency Reports, to technologies such as DeepMind’s Verifiable Data Audit (VDA), which aim to make all interactions with a dataset cryptographically logged and auditable. The VDA, for example, allows organisations and individuals to see what data has been used, for how long and for what purpose. Efforts like these will hopefully create real accountability between organisations using data and those they seek to serve.
Academics and nonprofits are also developing ways to make the impacts of algorithms easier to understand. For example, MIT Media Lab researcher Joy Buolamwini and the Algorithmic Justice League have created museum exhibits to increase awareness of the deeply disturbing ways facial recognition technologies often fail for individuals with darker skin tones.
This work is critically important.
As well as the ethical responsibility to avoid new harms, many in the AI field also see the potential for new tools to actually improve social justice.
In the realm of finance, for example, a sophisticated credit-scoring system — if built with fairness and accountability at heart — could be far more transparent than the historical alternative, where a bank manager would decide who gets a loan, without any real obligation to provide proper explanation, and no meaningful way to address any biases that may influence the decision.
Third, and this is by no means unique to tech, we need to address the asymmetry of motivation between market-based incentives and the other societal goals we aspire to. The standard measures of business achievement, from fundraising valuations to active users, do not capture the social responsibility that comes with trying to change the world for the better.
This disconnect starts early. There might be a lot of money in tech, but the vast majority of entrepreneurs still fail. Any founder hoping to get a new business off the ground has to convince investors and new hires of future growth, and then deliver that relentlessly. Doing this takes single-minded focus on the metrics that appear to matter, with little room to consider complex societal externalities or listen to naysayers.
That is partly why some of the world’s brightest minds gravitate towards the safest and most proven ideas and business models. They end up creating new services to personalise soda drinks when half a billion people do not have access to clean water, or new ways to order food by phone when more than 800 million people are malnourished. Why is it that we can go on a date with a stranger we meet on an app in minutes, but nurses and doctors carrying out life-saving treatments still use pagers and fax machines to communicate with one another?
We need new incentive-based legal structures — ones that put social benefit on the same plane as profit — to encourage more founders to take on real-world problems, and to do so with ethics at the heart. The private sector must bring the same innovation drive that has created so many amazing new products and services over many decades to the modern challenge of designing systems that are ethical and accountable. There is clearly room for innovation here.
None of this is easy. But with rigorous attention to technology’s capabilities, research into its inputs and impacts, greater transparency, and a reorientation of incentives, we can break through the complexity that makes society’s problems so hard to tackle. If we can deploy these tools broadly and fairly, fostering an environment in which everyone can participate in and benefit from them, we have the opportunity to enrich and advance humanity as a whole. All of us who believe in the power of technology must do everything we can to ensure these systems reflect humanity’s highest collective selves.
",Virtuous technology,32,virtuous-technology-f1ce54a59869,2018-06-26,2018-06-26 15:36:45,https://medium.com/s/story/virtuous-technology-f1ce54a59869,False,1300,"The award-winning RSA Journal is a quarterly publication for our Fellows, featuring the latest cutting-edge ideas from international writers alongside RSA news. A selection of articles have been reproduced here.",,theRSAorg,,RSA Journal,,rsa-journal,"RSA,JOURNAL,POLITICS",theRSAorg,Ethics,ethics,Ethics,7787.0,RSA,"The mission of the RSA (Royal Society for the encouragement of Arts, Manufactures and Commerce) is to enrich society through ideas and action.",886709834b20,thersa,9325.0,1579.0,20181104
0,,0.0,,2018-01-20,2018-01-20 10:41:22,2018-01-20,2018-01-20 10:43:50,1,False,en,2018-01-20,2018-01-20 16:20:28,2,60d0aeba9c9b,2.4264150943396228,202,0,0,Neurotechnology introduces some huge questions into our collective near future. We’re being challenged to rethink our notions of rights…,4,"How Neurotech Principles Are Changing Philosophy And Reforming Ethics

Neurotechnology introduces some huge questions into our collective near future. We’re being challenged to rethink our notions of rights, what it means to be an individual and whether the idea of being a “natural” human retains any meaning.
Involving philosophers, artists, legal experts and creative visionaries in these conversations is essential because much is at stake.
Neurotechnology Gives Rise to New Questions about Human Rights
Neuroethics is an emerging field of ethics exploring the implications of human life in which computers augment our thoughts and consciousness. There’s a growing acknowledgment within this field that a legal framework is now vital for conceptualizing and enshrining our ‘neurorights’.
A cornerstone of this new branch of ethics is the concept of “neural integrity”, which refers to the right of a person to maintain control over his or her brain-computer interface. Flowing from this is the principle of mental privacy. With the possibility of interfaces tapping directly into brain activity, a new kind of right to privacy is emerging: the right to have your thoughts as your own.
A crucial safeguard here will be continued investment and innovation in the encryption of signals from brain interfaces. Neurogress, for example, utilizes cutting-edge encryption in its AI-based neurocontrol software which utilizes brain-computer interfacing for reading the brain signals and transforming those into device actions.
The Notion of the Individual in Society Is Radically Shifting
Neurotechnology has the potential to link minds together. New forms of collaboration and cooperation will inevitably form as the technology enters the corporate, creative and scientific domains. In this process, will identity lose relevance or become blurred as technologies push us further toward networking our brains into a hive mind? It is just too early to know what will happen to the boundaries between self and community.
While we may not have all the answers to these questions, neurotechnology innovators like Neurogress are drawing in a diverse collection of voices to address these questions in an ethical and informed manner.
What Is “Natural” Anyways? Does It Even Matter?
Then there’s the question of whether the purpose of neurotechnology should be to augment and improve our lives, or if it should be allowed free reign to radically transform it. Should we really be holding on to this notion that whatever we do, it should not stray too far from our natural starting point?
Let’s say we develop a neurocontrol software which in combination with brain-computer interface and other gadgets radically increases intelligence. Should this go only to intellectually disabled people who require it in order to lead a normal life? Or should it be available to everyone? Where is the line separating a needed intervention from excessive tampering with human capabilities? What is an ethical approach to these enhancements? Who gets to decide? I’ll stop now before I wear out the question mark key on my keyboard. The point is, whichever way you spin it, these questions are, to use a technical term, righteously humongous.
The good news is that neurotechnological innovators are concerned about equality of access.
Neurogress has committed to allocating funds collected on its ICO to a charity fund dedicated to providing free neurocontrolled prostheses to disabled people who are unable to afford the technology. That’s some real coin and a step in the right direction.
Neurotechnology innovation is the focal point for some huge questions about our collective future.
Invest in the interactive mind-controlled devices of the future by buying tokens now. Visit Neurogress.io.
",How Neurotech Principles Are Changing Philosophy And Reforming Ethics,202,how-neurotech-principles-are-changing-philosophy-and-reforming-ethics-60d0aeba9c9b,2018-06-16,2018-06-16 09:44:52,https://medium.com/s/story/how-neurotech-principles-are-changing-philosophy-and-reforming-ethics-60d0aeba9c9b,False,590,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Neurogress,THINK. SHAPE YOUR WORLD. http://Neurogress.io Ask for more information: https://t.me/neurogress,df47ccd2c097,neurogress,2185.0,6.0,20181104
0,,0.0,7f60cf5620c9,2018-02-06,2018-02-06 13:11:01,2018-02-07,2018-02-07 01:23:20,1,False,en,2018-05-10,2018-05-10 12:59:32,5,66acaea6f16f,4.9849056603773585,15,4,0,This is why I can’t support Data for Democracy’s data science code of ethics.,3,"An ethical code can’t be about ethics
This is why I can’t support Data for Democracy’s data science code of ethics.
Stotting (a costly signal): the jumping up and down proves to a predator that the animal isn’t an easy target
[EDIT: I’ve tried to put together a more comprehensive take on the subject of tech ethics here: https://hackernoon.com/can-we-be-honest-about-ethics-ecf5840b6e07]
Last week, I wrote about my skepticism of Data for Democracy’s intent to create a data science code of ethics. My concerns focused on the practical feasibility of the project. After a lot of talking about, reading, and watching the evolution of the D4D code of ethics, I still believe the proposed principles are largely unactionable. I also believe, now, that what the working groups have produced is built on the wrong foundation entirely. This isn’t about iterating forward to a solution. No amount of revision can succeed if you’re building the wrong thing.
We need to be clear on what a code of ethics means. If we can realistically expect everyone in the community to just adopt a code of ethics because they intuitively feel that it’s the good and right thing to do, then the code of ethics is unnecessary — it amounts to nothing more than virtue signaling. If we can’t realistically expect complete organic adoption, then the code is a mechanism to coerce those who disagree with it, to censure people who don’t abide by it. Those two routes — wholesale freewill adoption or coercion — are the only two ways a code of ethics can actually mean anything.
By what right does any subset of people in a profession declare what is right or wrong for everyone in that profession? By no right at all. No one has the right to dictate morality to others, but sometimes some people can obtain enough power to do so. And under one very specific set of circumstances, a profession as a whole can benefit from that happening.
The more I think about it, the more I’m impressed with the Hippocratic Oath. The medical profession, as I see it, is based on dual premises: a person who is unhealthy should, on average over their course of their interactions with the doctor, become more healthy; a person who is healthy should, again on average over the course of their interactions with the doctor, not become less healthy. That is how a doctor makes a living: a doctor gains personally from those two premises being true for most people.
Any practice that would allow an individual doctor to gain personally from being a doctor without those two premises remaining true would damage everyone’s ability to trust doctors. The title of “doctor”, or a medical degree, or other trappings of the medical profession mean nothing if a doctor can fail to heal the sick and still remain prosperous.
Every component of the Hippocratic Oath requires doctors to deliberately limit their own prosperity, either by benefiting others at their own expense, or by refusing to benefit from situations that normally would be fair game. That’s the key. The Hippocratic Oath is an effective ethical code exactly because it’s not a statement of right and wrong. It’s a roster of costly signals.
Costly signaling is the practice of deliberately putting yourself at a disadvantage in order to show you can afford the sacrifice. Some gazelles expend energy jumping up and down to show predators that they have so much energy that they can waste it, proving they are too strong and fast for the predator to bother with them. People in religious communities sacrifice resources to prove that they get enough from the community to make up for the loss. Costly signaling is a way to put skin in the game, a concept most thoroughly fleshed out by Nassim Taleb: “anyone involved in an action which can possibly generate harm for others, even probabilistically, should be required to be exposed to some damage.” If someone engages in costly signaling, it means they actually have an abundance of something — skills, knowledge, resources, strength, intelligence, commitment — that allows them to shoulder that cost.
Those are the people you want to hire. Those are the people who have shown that they have enough competency that they are willing to take a hit in other areas. For a doctor, that means they have a good enough track record of making the sick healthy and keeping the healthy well that they don’t need to conserve their time by refusing to teach, or accrue side benefits from their relationships with their clients. Charlatans only engage in signalling that doesn’t cost them anything. Costly signals weed out charlatans.
How does that translate to data science? People in a business, over the course of their interactions with a data scientist, should…what? Make better decisions? That’s so general as to be meaningless. Make more profits? There’s a huge chain of causation between data science and profits — there are too many ways, some of them legitimate, to explain away failure to perform.
People in a business, on average over the course of their interactions with a data scientist, should increase the percentage of time and resources devoted to decisions that only humans can make.
That, in my view, is a core purpose of data science. (Not the only one of course, but we can focus on this one for the purposes of the present discussion). There are things people spend their time on that machines can do just as well. There are other things people spend their time on that machines can’t do at all. No one who runs an organization ever runs out of decisions that need to be made. That means a lot of decisions are necessarily made hastily, or shuffled onto unqualified subordinates, or forgotten until they fester long enough to be a real problem. People end up paying attention to things they shouldn’t, simply because they can’t sift through competing demands for their attention fast enough.
The promise of data science is that many of the decisions currently made by humans can be made as well or better by algorithms. “Better” can mean more accurate, more efficient, more nuanced, more cost-effective, etc. Automating decisions is nothing more than an academic exercise until that automation frees up people to do other things. Freedom to make decisions is data science’s version of patient health — consistently maintaining and growing that freedom needs to be the basis for an individual data scientist’s prosperity.
Given that an ethical code necessarily has to be imposed upon the many by the relatively few, the only way to ethically define an ethical code is to stipulate, not morality, but skin in the game. That drastically narrows down the scope of what should go into an ethical code, as an enumeration of costs will always be smaller than an enumeration of beliefs. An ethical code that does more than define skin in the game actually undermines ethical behavior by providing ways for people to virtue signal regardless of competency — making it harder, not easier, to ensure a high ethical standard within a profession.
Data scientists don’t need a list of ways to be virtuous. They need a list of ways to prove they aren’t charlatans. That will do more to ensure the health and trustworthiness of the profession than anything else.
Data for Democracy obviously isn’t at the end of its efforts to create a code of ethics, but the working documents they have in place right now and the conversations I’ve had clearly indicate that they are focused on enumerating virtues instead of costs. It’s the wrong basis upon which to build an ethical system.
",An ethical code can’t be about ethics,34,an-ethical-code-cant-be-about-ethics-66acaea6f16f,2018-07-11,2018-07-11 12:58:08,https://towardsdatascience.com/an-ethical-code-cant-be-about-ethics-66acaea6f16f,False,1268,"Sharing concepts, ideas, and codes.",towardsdatascience.com,towardsdatascience,,Towards Data Science,,towards-data-science,"DATA SCIENCE,MACHINE LEARNING,ARTIFICIAL INTELLIGENCE,BIG DATA,ANALYTICS",TDataScience,Ethics,ethics,Ethics,7787.0,Schaun Wheeler,"Senior Staff Data Scientist at Valassis Digital. Formerly at Success Academy Charter Schools, U.S. Department of the Army, and other places. Anthropologist.",2d3762e7f110,schaun.wheeler,232.0,2.0,20181104
0,,0.0,,2018-06-25,2018-06-25 20:56:23,2018-07-26,2018-07-26 08:13:04,1,False,en,2018-08-03,2018-08-03 14:15:08,8,54182e6c10f9,4.267924528301887,16,0,0,Ever noticed how two people can look at the same thing yet see something completely different?,5,"The Biggest Challenge AI is Facing? Bridging the Diversity Gap.
That’s me in front of my favorite Amsterdam bridge - “de Magere brug”
Ever noticed how two people can look at the same thing yet see something completely different?
‘‘I just love that bridge!’’ The words fly out of my mouth as I’m cycling through Amsterdam with my friend Tash (it’s a thing for us Dutchies). Now, I can be a woman of many words… So, I continue: ‘‘It’s such an iconic image, all the history, and the way it blends with the landscape - so cool!’’ Tash looks at me puzzled. ‘‘Really, that’s why you like it? I mean… It’s a great bridge. But that’s ’cause of the construction, the mechanics - the engineering artistry behind it.’’
Interesting bit of background: I have a Master’s in Political Philosophy and Public Administration and Tash has a PhD in Mechanical Engineering.
Building better bridges
Now you might think: ‘‘Who cares?! It’s just a bridge.’’ The thing is, different perspectives don’t just limit themselves to my favorite bridge. They’re all around us. Even more so: these differences are one of the biggest ethical challenges AI is facing right now.
Let me tell you why bridging the diversity gap is super important if we want to get the future of AI right. And, I’ll propose 3 things we can do today to make this happen.
Why AI won’t make us walk on water (yet)
We have proclaimed AI to be the messiah of the modern age. And don’t get me wrong, I’m a huge optimist when it comes to the tremendous, earth-shaking, eternal glitter and stars flying everywhere potential of AI technology. But, when it comes to diversity, bad stuff is happening.
A shocking list where AI got it all wrong
1. This Amazon AI delivery system didn’t want to serve predominantly black neighborhoods.
2. This sexist AI associates pictures of a kitchen with women.
3. This risk assessment AI finds that black criminals are a higher risk to society than white criminals.
4. This AI recommends criminal sentences in the US on a non-neutral basis - and it’s disadvantaging minorities.
5. This AI Face-analysis service more easily identifies pictures of white men than black women.
And, this list can go on-and-on.
Technology is not value free: ask the racist bridges of New York
I’m not trying to illustrate that all hope is lost. I’m emphasizing how important it is that we better understand why bias, and all the problems related to it, are part of the technology we design. And you may not like it, but it looks like we’re the culprit.
A human designing technology that’s value free is about as likely as me spotting a unicorn that shoots laser rainbows out of his eyes and eats sugar coated fairy tales for breakfast.
AI isn’t the only type of technology carrying serious bias. Even a piece of technology like a bridge can be racist. This might sound funny. But trust me, it’s not.
Robert Moses built racist bridges in New York in the ’40s and ’50s. He designed them in such a way that they were much lower than usual. This prevented buses from entering specific parts of the city; only people owning cars could make the trip. Effectively this meant that it was nearly impossible for poor and ethnic minorities to get there. How’s that for messed up!?
The point is, we’re behind the wheel of the fast-moving car of AI development. It’s up to us to design an AI infused future WITH diversity in mind.
The tools you can use to understand diversity
We transfer our personal bias to the technology we design. We don’t (necessarily) do this on purpose. We do this simply because we see the world differently.
Remember how my friend Tash and I can look at the same bridge and see something entirely different? This is not by choice. It’s because human beings are diverse.
Let me borrow a concept from sociology to explain this diversity. Intersectionality is a term that’s used to explain which ‘‘spot’’ in society you occupy. It’s where your age, gender, ethnicity, sexuality, education, wealth, ambitions, Netflix binge-watching behavior (and the list goes on and on) intersect.
Intersectionality makes it clear that every human being is entirely unique. We’re a tremendously diverse species. But unfortunately, this diversity is not reflected in AI development.
It’s mostly a bunch of white guys in Sillicon Valley with a computer science degree calling the shots. They’re the ones designing the next (AI) bridge.
Why ‘’we’ll cross that bridge when we get there’’ won’t work
A lack of diversity in the people who design our AI systems is a big problem. The risks that certain perspectives will be overlooked, experiences will be left out, and alternative information will be missed are very real.
We’re the ones who need to make sure that we don’t end up building more racist bridges. This means diversity should be included in every aspect of AI development.
3 Things we can do today
Here are 3 things that are most important if we want to start bridging the AI diversity gap:
1. We need to raise awareness on the issues of bias and diversity when it comes to technology.
This is a discussion that we need to have with as many people as possible. From leaders of industry all the way to those to whom technology applies - which is all of us!
2. We need to get people with different backgrounds involved in designing AI technology.
This starts with recruiting people differently. We need to retrain people to develop relevant skill-sets. And, in improving education to get more boys and girls from different backgrounds interested in technology.
3. We need to continue academic research on how to reduce bias in AI systems.
This also extends to the data sets that we use to train our algorithmic models. These need to be a reflection of the beautiful diverse world that we live in.
When did you come across bias in AI?
Please include your thoughts in the comments below. I’d love to read them and discuss! ❤ And if you feel we should connect, click here to add me on LinkedIn!
If you like this article, please 👏. Remember, you can clap up to 50 times — and it really makes a huge difference for me :)Thanks for reading!
Please note: the opinions expressed in this article are my own, and don’t reflect the view of my employer.
",The Biggest Challenge AI is Facing? Bridging the Diversity Gap.,368,the-biggest-challenge-ai-is-facing-bridging-the-diversity-gap-54182e6c10f9,2018-08-03,2018-08-03 14:15:08,https://medium.com/s/story/the-biggest-challenge-ai-is-facing-bridging-the-diversity-gap-54182e6c10f9,False,1078,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Jessica Endert,Passionate about Ethics & AI. I LOVE asking Tech's and Life's Biggest Questions. Hoping to make people Think and Smile along the way.,a246fa81cf4d,endert.jessica,20.0,9.0,20181104
0,,0.0,,2018-07-11,2018-07-11 17:27:07,2017-10-30,2017-10-30 18:56:10,0,False,en,2018-07-11,2018-07-11 17:32:16,8,6a8dd5b4da6d,15.683018867924527,0,0,0,"As technically challenging as it may be to develop safe and beneficial AI, this challenge also raises some thorny questions regarding…",5,"Podcast: AI Ethics, the Trolley Problem, and a Twitter Ghost Story with Joshua Greene and Iyad Rahwan

As technically challenging as it may be to develop safe and beneficial AI, this challenge also raises some thorny questions regarding ethics and morality, which are just as important to address before AI is too advanced. How do we teach machines to be moral when people can’t even agree on what moral behavior is? And how do we help people deal with and benefit from the tremendous disruptive change that we anticipate from AI?
To help consider these questions, Joshua Greene and Iyad Rawhan kindly agreed to join the podcast. Josh is a professor of psychology and member of the Center for Brain Science Faculty at Harvard University, where his lab has used behavioral and neuroscientific methods to study moral judgment, focusing on the interplay between emotion and reason in moral dilemmas. He’s the author of Moral Tribes: Emotion, Reason and the Gap Between Us and Them. Iyad is the AT&T Career Development Professor and an associate professor of Media Arts and Sciences at the MIT Media Lab, where he leads the Scalable Cooperation group. He created the Moral Machine, which is “a platform for gathering human perspective on moral decisions made by machine intelligence.”
In this episode, we discuss the trolley problem with autonomous cars, how automation will affect rural areas more than cities, how we can address potential inequality issues AI may bring about, and a new way to write ghost stories.
This transcript has been heavily edited for brevity. You can read the full conversation here.
Ariel: How do we anticipate that AI and automation will impact society in the next few years?
Iyad: AI has the potential to extract better value from the data we’re collecting from all the gadgets, devices and sensors around us. We could use this data to make better decisions, whether it’s micro-decisions in an autonomous car that takes us from A to B safer and faster, or whether it’s medical decision-making that enables us to diagnose diseases better, or whether it’s even scientific discovery, allowing us to do science more effectively, efficiently and more intelligently.
Joshua: Artificial intelligence also has the capacity to displace human value. To take the example of using artificial intelligence to diagnose disease. On the one hand it’s wonderful if you have a system that has taken in all of the medical knowledge we have in a way that no human could and uses it to make better decisions. But at the same time that also means that lots of doctors might be out of a job or have a lot less to do. This is the double-edged sword of artificial intelligence, the value it creates and the human value that it displaces.
Ariel: Can you explain what the trolley problem is and how does that connect to this question of what do autonomous vehicles do in situations where there is no good option?
Joshua: One of the original versions of the trolley problem goes like this (we’ll call it “the switch case”): A trolley is headed towards five people and if you don’t do anything, they’re going to be killed, but you can hit a switch that will turn the trolley away from the five and onto a side track. However on that side track, there’s one unsuspecting person and if you do that, that person will be killed.
The question is: is it okay to hit the switch to save those five people’s lives but at the cost of saving one life? In this case, most people tend to say yes. Then we can vary it a little bit. In “the footbridge case,” the situation is different as follows: the trolley is now headed towards five people on a single track, over that track is a footbridge and on that footbridge is a large person wearing a very large backpack. You’re also on the bridge and the only way that you can save those five people from being hit by the trolley is to push that big person off of the footbridge and onto the tracks below.
Assume that it will work, do you think it’s okay to push the guy off the footbridge in order to save five lives? Here, most people say no, and so we have this interesting paradox. In both cases, you’re trading one life for five, yet in one case it seems like it’s the right thing to do, in the other case it seems like it’s the wrong thing to do.
One of the classic objections to these dilemmas is that they’re unrealistic. My view is that the point is not that they’re realistic, but instead that they function like high contrast stimuli. If you’re a vision researcher and you’re using flashing black and white checkerboards to study the visual system, you’re not using that because that’s a typical thing that you look at, you’re using it because it’s something that drives the visual system in a way that reveals its structure and dispositions.
In the same way, these high contrast, extreme moral dilemmas can be useful to sharpen our understanding of the more ordinary processes that we bring to moral thinking.
Iyad: The trolley problem can translate in a cartoonish way to a scenario with which an autonomous car is faced with only two options. The car is going at a speed limit on a street and due to mechanical failure is unable to stop and is going to hit it a group of five pedestrians. The car can swerve and hit a bystander. Should the car swerve or should it just plow through the five pedestrians?
This has a structure similar to the trolley problem because you’re making similar tradeoffs between one and five people and the decision is not being taken on the spot, it’s actually happening at the time of the programming of the car.
There is another complication in which the person being sacrificed to save the greater number of people is the person in the car. Suppose the car can swerve to avoid the five pedestrians but as a result falls off a cliff. That adds another complication especially that programmers are going to have to appeal to customers. If customers don’t feel safe in those cars because of some hypothetical situation that may take place in which they’re sacrificed, that pits the financial incentives against the potentially socially desirable outcome, which can create problems.
A question that raises itself is: Is it going to ever happen? How many times do we face these kinds of situations as we drive today? So the argument goes: these situations are going to be so rare that they are irrelevant and that autonomous cars promise to be substantially safer than human-driven cars that we have today, that the benefits significantly outweigh the costs.
There is obviously truth to this argument, if you take the trolley problem scenario literally. But what the autonomous car version of the trolley problem is doing, is it’s abstracting the tradeoffs that are taking place every microsecond, even now.
Imagine you’re driving on the road and there is a large truck on the lane to your left and as a result you choose to stick a little bit further to the right, just to minimize risk in case this car gets off its lane. Now suppose that there could be a cyclist later on the right hand side, what you’re effectively doing in this small maneuver is slightly reducing risk to yourself but slightly increasing risk to the cyclist. These sorts of decisions are being made millions and millions of times every day.
Ariel: Applying the trolley problem to self-driving cars seems to be forcing the vehicle and thus the programmer of the vehicle to make a judgment call about whose life is more valuable. Can we not come up with some other parameters that don’t say that one person’s life is more valuable than someone else’s?
Joshua: I don’t think that there’s any way to avoid doing that. If you’re a driver, there’s no way to avoid answering the question, how cautious or how aggressive am I going to be. You can not explicitly answer the question; you can say I don’t want to think about that, I just want to drive and see what happens. But you are going to be implicitly answering that question through your behavior, and in the same way, autonomous vehicles can’t avoid the question. Either the people who are designing the machines, training the machines or explicitly programming to behave in certain ways, they are going to do things that are going to affect the outcome.
The cars will constantly be making decisions that inevitably involve value judgments of some kind.
Ariel: To what extent have we actually asked customers what it is that they want from the car? In a completely ethical world, I would like the car to protect the person who’s more vulnerable, who would be the cyclist. In practice, I have a bad feeling I’d probably protect myself.
Iyad: We could say we want to treat everyone equally. On the other hand, you have this self-protective instinct which presumably as a consumer, that’s what you want to buy for yourself and your family. On the other hand you also care for vulnerable people. Different reasonable and moral people can disagree on what the more important factors and considerations should be and I think this is precisely why we have to think about this problem explicitly, rather than leave it purely to — whether it’s programmers or car companies or any particular single group of people — to decide.
Joshua: When we think about problems like this, we have a tendency to binarize it, but it’s not a binary choice between protecting that person or not. It’s really going to be matters of degree. Imagine there’s a cyclist in front of you going at cyclist speed and you either have to wait behind this person for another five minutes creeping along much slower than you would ordinarily go, or you have to swerve into the other lane where there’s oncoming traffic at various distances. Very few people might say I will sit behind this cyclist for 10 minutes before I would go into the other lane and risk damage to myself or another car. But very few people would just blow by the cyclist in a way that really puts that person’s life in peril.
It’s a very hard question to answer because the answers don’t come in the form of something that you can write out in a sentence like, “give priority to the cyclist.” You have to say exactly how much priority in contrast to the other factors that will be in play for this decision. And that’s what makes this problem so interesting and also devilishly hard to think about.
Ariel: Why do you think this is something that we have to deal with when we’re programming something in advance and not something that we as a society should be addressing when it’s people driving?
Iyad: We very much value the convenience of getting from A to B. Our lifetime odds of dying from a car accident is more than 1%, yet somehow, we’ve decided to put up with this because of the convenience. As long as people don’t run through a red light or are not drunk, you don’t really blame them for fatal accidents, we just call them accidents.
But now, thanks to autonomous vehicles that can make decisions and reevaluate situations hundreds or thousands of times per second and adjust their plan and so on — we potentially have the luxury to make those decisions a bit better and I think this is why things are different now.
Joshua: With the human we can say, “Look, you’re driving, you’re responsible, and if you make a mistake and hurt somebody, you’re going to be in trouble and you’re going to pay the cost.” You can’t say that to a car, even a car that’s very smart by 2017 standards. The car isn’t going to be incentivized to behave better — the motivation has to be explicitly trained or programmed in.
Iyad: Economists say you can incentivize the people who make the cars to program them appropriately by fining them and engineering the product liability law in such a way that would hold them accountable and responsible for damages, and this may be the way in which we implement this feedback loop. But I think the question remains what should the standards be against which we hold those cars accountable.
Joshua: Let’s say somebody says, “Okay, I make self-driving cars and I want to make them safe because I know I’m accountable.” They still have to program or train the car. So there’s no avoiding that step, whether it’s done through traditional legalistic incentives or other kinds of incentives.
Ariel: I want to ask about some other research you both do. Iyad you look at how AI and automation impact us and whether that could be influenced by whether we live in smaller towns or larger cities. Can you talk about that?
Iyad: Clearly there are areas that may potentially benefit from AI because it improves productivity and it may lead to greater wealth, but it can also lead to labor displacement. It could cause unemployment if people aren’t able to retool and improve their skills so that they can work with these new AI tools and find employment opportunities.
Are we expected to experience this in a greater way or in a smaller magnitude in smaller versus bigger cities? On one hand there are lots of creative jobs in big cities and, because creativity is so hard to automate, it should make big cities more resilient to these shocks. On the other hand if you go back to Adam Smith and the idea of the division of labor, the whole idea is that individuals become really good at one thing. And this is precisely what spurred urbanization in the first industrial revolution. Even though the system is collectively more productive, individuals may be more automatable in terms of their narrowly-defined tasks.
But when we did the analysis, we found that indeed larger cities are more resilient in relative terms. The preliminary findings are that in bigger cities there is more production that requires social interaction and very advanced skills like scientific and engineering skills. People are better able to complement the machines because they have technical knowledge, so they’re able to use new intelligent tools that are becoming available, but they also work in larger teams on more complex products and services.
Ariel: Josh, you’ve done a lot of work with the idea of “us versus them.” And especially as we’re looking in this country and others at the political situation where it’s increasingly polarized along this line of city versus smaller town, do you anticipate some of what Iyad is talking about making the situation worse?
Joshua: I certainly think we should be prepared for the possibility that it will make the situation worse. The central idea is that as technology advances, you can produce more and more value with less and less human input, although the human input that you need is more and more highly skilled.
If you look at something like Turbo Tax, before you had lots and lots of accountants and many of those accountants are being replaced by a smaller number of programmers and super-expert accountants and people on the business side of these enterprises. If that continues, then yes, you have more and more wealth being concentrated in the hands of the people whose high skill levels complement the technology and there is less and less for people with lower skill levels to do. Not everybody agrees with that argument, but I think it’s one that we ignore at our peril.
Ariel: Do you anticipate that AI itself would become a “them,” or do you think it would be people working with AI versus people who don’t have access to AI?
Joshua: The idea of the AI itself becoming the “them,” I am agnostic as to whether or not that could happen eventually, but this would involve advances in artificial intelligence beyond anything we understand right now. Whereas the problem that we were talking about earlier — humans being divided into a technological, educated, and highly-paid elite as one group and then the larger group of people who are not doing as well financially — that “us-them” divide, you don’t need to look into the future, you can see it right now.
Iyad: I don’t think that the robot will be the “them” on their own, but I think the machines and the people who are very good at using the machines to their advantage, whether it’s economic or otherwise, will collectively be a “them.” It’s the people who are extremely tech savvy, who are using those machines to be more productive or to win wars and things like that. There would be some sort of evolutionary race between human-machine collectives.
Joshua: I think it’s possible that people who are technologically enhanced could have a competitive advantage and set off an economic arms race or perhaps even literal arms race of a kind that we haven’t seen. I hesitate to say, “Oh, that’s definitely going to happen.” I’m just saying it’s a possibility that makes a certain kind of sense.
Ariel: Do either of you have ideas on how we can continue to advance AI and address these divisive issues?
Iyad: There are two new tools at our disposal: experimentation and machine-augmented regulation.
Today, [there are] cars with a bull bar in front of them. These metallic bars at the front of the car increase safety for the passenger in the case of collision, but they have disproportionate impact on other cars, on pedestrians and cyclists, and they’re much more likely to kill them in the case of an accident. As a result, by making this comparison, by identifying that cars with bull bars are worse for certain group, the trade off was not acceptable, and many countries have banned them, for example the UK, Australia, and many European countries.
If there was a similar trade off being caused by a software feature, then, we wouldn’t know unless we allowed for experimentation as well as monitoring — if we looked at the data to identify whether a particular algorithm is making for very safe cars for customers, but at the expense of a particular group.
In some cases, these systems are going to be so sophisticated and the data is going to be so abundant that we won’t be able to observe them and regulate them in time. Think of algorithmic trading programs. No human being is able to observe these things fast enough to intervene, but you could potentially insert another algorithm, a regulatory algorithm or an oversight algorithm, that will observe other AI systems in real time on our behalf, to make sure that they behave.
Joshua: There are two general categories of strategies for making things go well. There are technical solutions to things and then there’s the broader social problem of having a system of governance that can be counted on to produce outcomes that are good for the public in general.
The thing that I’m most worried about is that if we don’t get our politics in order, especially in the United States, we’re not going to have a system in place that’s going to be able to put the public’s interest first. Ultimately, it’s going to come down to the quality of the government that we have in place, and quality means having a government that distributes benefits to people in what we would consider a fair way and takes care to make sure that things don’t go terribly wrong in unexpected ways and generally represents the interests of the people.
I think we should be working on both of these in parallel. We should be developing technical solutions to more localized problems where you need an AI solution to solve a problem created by AI. But I also think we have to get back to basics when it comes to the fundamental principles of our democracy and preserving them.
Ariel: As we move towards smarter and more ubiquitous AI, what worries you most and what are you most excited about?
Joshua: I’m pretty confident that a lot of labor is going to be displaced by artificial intelligence. I think it is going to be enormously politically and socially disruptive, and I think we need to plan now. With self-driving cars especially in the trucking industry, I think that’s going to be the first and most obvious place where millions of people are going to be out of work and it’s not going to be clear what’s going to replace it for them.
I’m excited about the possibility of AI producing value for people in a way that has not been possible before on a large scale. Imagine if anywhere in the world that’s connected to the Internet, you could get the best possible medical diagnosis for whatever is ailing you. That would be an incredible life-saving thing. And as AI teaching and learning systems get more sophisticated, I think it’s possible that people could actually get very high quality educations with minimal human involvement and that means that people all over the world could unlock their potential. And I think that that would be a wonderful transformative thing.
Iyad: I’m worried about the way in which AI and specifically autonomous weapons are going to alter the calculus of war. In order to aggress on another nation, you have to mobilize humans, you have to get political support from the electorate, you have to handle the very difficult process of bringing back people in coffins, and the impact that this has on electorates.
This creates a big check on power and it makes people think very hard about making these kinds of decisions. With AI, when you’re able to wage wars with very little loss to life, especially if you’re a very advanced nation that is at the forefront of this technology, then you have disproportionate power. It’s kind of like a nuclear weapon, but maybe more because it’s much more customizable. It’s not an all out or nothing — you could start all sorts of wars everywhere.
I think it’s going to be a very interesting shift in the way superpowers think about wars and I worry that this might make them trigger happy. I think a new social contract needs to be written so that this power is kept in check and that there’s more thought that goes into this.
On the other hand, I’m very excited about the abundance that will be created by AI technologies. We’re going to optimize the use of our resources in many ways. In health and in transportation, in energy consumption and so on, there are so many examples in recent years in which AI systems are able to discover ways in which even the smartest humans haven’t been able to optimize.
Ariel: One final thought: This podcast is going live on Halloween, so I want to end on a spooky note. And quite conveniently, Iyad’s group has created Shelley, which is a Twitter chatbot that will help you craft scary ghost stories. Shelley is, of course, a nod to Mary Shelley who wrote Frankenstein, which is the most famous horror story about technology. Iyad, I was hoping you could tell us a bit about how Shelley works.
Iyad: Yes, well this is our second attempt at doing something spooky for Halloween. Last year we launched the nightmare machine, which was using deep neural networks and style transfer algorithms to take ordinary photos and convert them into haunted houses and zombie-infested places. And that was quite interesting; it was a lot of fun. More recently, now we’ve launched Shelley, which people can visit on shelley.ai, and it is named after Mary Shelley who authored Frankenstein.
This is a neural network that generates text and it’s been trained on a very large data set of over 100 thousand short horror stories from a subreddit called No Sleep. And so it’s basically got a lot of human knowledge about what makes things spooky and scary, and the nice thing is that it generates part of the story and people can tweet back at it a continuation of the story and then basically take turns with the AI to craft stories. And we feature those stories on the website afterwards. if I’m correct, this is the first collaborative human-AI horror writing exercise ever.
Originally published at futureoflife.org on October 30, 2017.
","Podcast: AI Ethics, the Trolley Problem, and a Twitter Ghost Story with Joshua Greene and Iyad…",0,podcast-ai-ethics-the-trolley-problem-and-a-twitter-ghost-story-with-joshua-greene-and-iyad-6a8dd5b4da6d,2018-07-11,2018-07-11 17:32:17,https://medium.com/s/story/podcast-ai-ethics-the-trolley-problem-and-a-twitter-ghost-story-with-joshua-greene-and-iyad-6a8dd5b4da6d,False,4156,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Future of Life,FLI catalyzes and supports research and initiatives to safeguard life and develop optimistic visions of the future. Official account.,e33e2d2a809c,FLIxrisk,1361.0,93.0,20181104
0,,0.0,,2018-06-26,2018-06-26 21:09:44,2018-06-26,2018-06-26 07:38:00,1,False,en,2018-06-26,2018-06-26 21:12:02,1,3e18ade6d966,1.2339622641509431,0,0,0,"With the advent of self-driving cars, we see AI starting to make ethical decisions. How you may ask?",3,"AI helping us be more ethical.

With the advent of self-driving cars, we see AI starting to make ethical decisions. How you may ask?
These automobiles will have to decide what to do when human lives are at stake. For example, let’s say a couple is in a hurry. Instead of obeying the traffic signal to not cross the street, they attempt to cross it not seeing the oncoming vehicle. Moreover, this is a self-driving car that is about to hit them. The AI calculates that it does not have enough time to come to a safe stop. In a millisecond the AI reviews the scenario. Should the ethic algorithms decide to hit the single pedestrian on the side walk instead? Hit the couple crossing the street? Or risk the life of the passengers in the car as well as those in the neighboring office building by careening into it?
Let’s take this concept a step further. What if the AI knows who all the humans are that are in danger via face recognition? Also, what if the algorithms instantly calculate that one of the people in peril crossing the street has a high proclivity to criminal behavior?
Once intelligent machines start making, at least hypothetically better and more informed ethical decisions based on data, will we then slowly start to hand over our own ethical decisions to them? Why not have an AI (artificial intelligence) that has access to massive amounts of information and can process it at a thousand fold rate more then we can at least assist us in those dilemmas?
Originally published at msquaredwebsvc.blogspot.com on June 26, 2018.
",AI helping us be more ethical.,0,ai-helping-us-be-more-ethical-3e18ade6d966,2018-06-26,2018-06-26 21:12:02,https://medium.com/s/story/ai-helping-us-be-more-ethical-3e18ade6d966,False,274,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Mark McFadden,www.markmcfadden.net,7fd5a2dd6fc9,m2web,8.0,3.0,20181104
0,,0.0,,2017-10-24,2017-10-24 05:11:13,2017-10-29,2017-10-29 11:08:55,1,False,en,2017-10-29,2017-10-29 11:20:34,31,892ce093df16,6.901886792452832,0,0,0,A response to the arguments for regulating AI posed by William Pate.,5,"Reflecting on artificial intelligence regulations. (Comfreak/Pixabay)
Should We Regulate AI — A Response
William Pate wrote a critique of an article that I recently wrote for The Conversation. In this article, I concluded that the costs of regulating artificial intelligence — or AI for short — were outweighed by the benefits that could be provided. I stated that “I don’t believe additional regulations are currently needed.” Pate disagrees.
To understand Pate’s objections, let’s quickly review the topic of conversation: AIs range from simplistic decision support systems to (not yet developed) human-like ‘general’ AIs that can reason across multiple application areas. Current AIs help us in numerous areas ranging from providing driver assistance to searching websites. More importantly, the range of applications that AI could be applied to — and that scientists, software engineers and others are currently working to apply the technology to — is vast.
Pate generalizes my argument (“when we extend AI to include more basic types of technology”) and incorrectly ascribes it to a “tech-bro” mentality. He accuses AI researchers of failing “to acknowledge or think about the knock-on effects of their innovations outside the tech world.” He concludes that the “disconnection of technologists from the real world and public policy … makes carefully crafted, meaningful regulation all that more important.”
I won’t repeat my original article or Pate’s response in entirety here. What I will do is respond to several of Pate’s assertions. I will start by responding to the two more general ones and then look at the more specific concerns.
“Tech-Bro” Argument
Pate ascribes my argument to a “tech-bro” culture similar to Uber. Certainly, a lot has been written about the culture at Uber and other high-tech firms — including how they’re trying to change it. However, it is unclear how this assertion fits. Pate says that my argument, in this regard, amounts to “Dude, keep your regulations off my algorithm.” He tries to link this line of reasoning to allegations that Uber “broke laws to establish … market dominance.” Leaving aside the allegations against Uber, it is unclear as to what similarity exists between a profit-seeking company and university AI research — though some of these concerns could be relevant to private AI developers.
When we develop technologies in a university environment, we don’t have the same sort of profit motives that commercial entities do. While universities can and do protect, license and otherwise monetize technologies, the university doesn’t live or die based on the success of a single or even portfolio of technologies. Because of this, we can explore the development of things that may not be commercially viable, at present — or ever. In some cases, this generates the ‘next big thing’; in many others, the only result is a few interesting scholarly papers.
The lack of a profit motive also allows us to focus on questions of ‘should we?’ instead of just ‘how can we?’, in the university environment. Perhaps the great irony here is that my original article was the type of policy thought that is typically missing from a commercial ‘do-or-die’ mentality.
Pate and others may disagree with the conclusions that I reached. They are certainly entitled — and even encouraged — to voice competing opinions. It is critical, though, that this conversation be driven by facts, analysis and discussions of legitimate concerns.
Technological Determinism Argument
Pate argues that my argument suffers from “technological determinism that many in the technical fields seem to fall victim to,” suggesting that I and others “refuse to acknowledge or think about the knock-on effects of their innovations outside the tech world.”
Pate is right to suggest that just because we can do something doesn’t necessarily mean that we should. Oppenheimer and others had serious misgivings after the development of the atomic bomb, leading him to suggest that he had become the “destroyer of worlds.” Hintze wondered whether similar concerns may exist for some work on AI.
The problem that we — as a species — have is that we cannot predict exactly what will come of any technological innovation (or any decision, at all). Technology has provided us with numerous benefits, but there have been costs. With the convenience and connections provided by the internet has come — as Pate notes — cybersecurity concerns and the drawback of some feeling like they are on-call 24x7x365.
I address several concerns in a section entitled “potential risks from artificial intelligence” in my article. Pate is welcome to disagree with my conclusion that the benefits outweigh the risks and concerns; however, suggesting a lack of thought about these effects is inaccurate.
Jobs Argument
Pate’s concern about the impact of AI on jobs is worthy of consideration. Past technologies have certainly displaced people from some jobs, while creating new jobs in other sectors. Not every change has been beneficial to workers — in fact, some have created benefits for companies and their investors at the expense of workers. Could AI have a similar impact? Almost certainly.
What should we do regarding those that have been and will be displaced by technology? This is a big question. We can train the displaced — and future generations — to be prepared for other jobs that will be newly created. Will there be some who are left out of the new jobs marketplace? History would suggest that the answer is yes. As a society, we need to determine how to help those who lack the skills and abilities to earn a living, particularly if their displacement is due to disability or an obsoleted skill set.
Should we restrict a technology if it looks to (or could) become the atomic bomb of jobs? Do we allow a company or group of companies to enjoy benefits that come at the expense of a large swath of the population? I don’t have a great answer to this. On one hand, you could argue that this change may create new opportunities. On the other hand, unless you have a way to take care of those that are displaced, the human toll will be significant and this is a prime consideration.
One way to — at least — distribute the benefits of these new technologies beyond existing entrenched companies is to make sure that everyone can participate. This was a key concern that motivated the article. Presuming that AIs (or area-specific AIs) were not banned outright, significant regulation would favor large firms who have experience in regulatory compliance over small start-ups who do not. Thus, avoiding regulation makes it easier for these small firms to compete and develop their own technologies.
Another jobs concern is the potential for one nation to benefit and enjoy job growth driven by AI, while others do not. Some have argued that restrictions on aerospace technologies from the International Trafficking in Arms regulations, for example, have benefited foreign firms at the expense of U.S. ones by creating a market for ITAR-free goods and services. AI regulation could potentially have the same impact. If you presume that some nation is going to support the development of the technology, then there is an argument to embrace it to share in the jobs that it produces, as the negative impact will occur regardless of participation.
Already Regulated — Need Regulation Argument
Pate acknowledges my suggestion that we can apply existing laws to the regulation of AI, while arguing that the “fact that we already regulate technology” is somehow ignored. He suggests that “meaningful regulations” might have “allowed innovation” while preventing issues like Equifax’s security breach and fake news. He also argues that “regulations are continually being applied to the Internet” and that this “hasn’t hampered innovation so far”, though others disagree.
The question we must address is what the benefit of AI-specific regulations would be. This requires us to know what form the regulations would take. Etzioni suggested making AIs subject to all existing laws, combined with removing ways to avoid liability for unforeseen consequences, requiring AIs to disclose they are AIs, and information disclosure protections. Congressional representatives John Delaney (D-Md.) and Pete Olson (R-Texas) formed an AI Caucus, which is taking an alternate approach. It currently plans to monitor AI developments to avoid stifling effects. Musk has proposed creating an FCC or FAA-like agency to study and regulate AI, but without proposing specifics. Of course, such an agency may bring with it a different set of problems, such as moving to slowly to meet industry needs and lawmaker deadlines.
If there are significant gaps in existing regulations, then these may need to be filled. Creating a whole new regulatory regime for AIs that duplicates existing laws (or worse, is an imperfect copy of them) just makes compliance harder. Concerned that an AI may kill? We already have laws banning that. Injure? Again, covered. Commit fraud? Laws are in place. As AI is used (and invariably misused), some gaps in existing laws may be discovered. But we don’t know what they are yet — hence my conclusion that “I don’t believe additional regulations are currently needed.” I am not, though, precluding the need for well-considered future regulations — once we can identify specific problems that regulations are needed to address.
Let’s Continue the Discussion
While I don’t agree with all of the points he made, I’d like to thank William Pate for taking the time to read my article and respond to it. It is critical that we consider the impact of new technologies from a multitude of perspectives.
I didn’t include the impact of AI on jobs within the scope of this article, though others have recently discussed it. This is certainly a key part of the technology deployment cost versus benefit analysis. I look forward to considering this topic more in future articles, though I suspect that some will disagree with the conclusions that I reach on it as well.
I would encourage Pate and others to continue to follow my work on this topic and others at The Conversation and in scholarly articles. I frequently publish on both technology and the social, ethical and policy considerations that it poses. I look forward to engaging in future discussions regarding this and similar topics!
Jeremy Straub is an assistant professor in the Department of Computer Science at the North Dakota State University. He has received funding related to AI and robotics from the North Dakota State University, the NDSU Foundation and Alumni Association, the U.S. National Science Foundation, the University of North Dakota and Sigma Xi. The opinions voiced in this article are his own and not necessarily those of NDSU or funding agencies.
",Should We Regulate AI — A Response,0,should-we-regulate-ai-a-response-892ce093df16,2018-03-28,2018-03-28 22:29:15,https://medium.com/s/story/should-we-regulate-ai-a-response-892ce093df16,False,1776,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Jeremy Straub,"Assistant Professor, North Dakota State University",ebbdd0474611,jeremy.straub,0.0,1.0,20181104
0,,0.0,,2018-04-09,2018-04-09 16:11:49,2018-04-09,2018-04-09 17:05:13,1,False,en,2018-04-09,2018-04-09 17:05:13,5,cc37408a7dd4,4.573584905660377,0,0,0,"I have to make an admission: I read Origin by Dan Brown during my vacations. It was just there, available at my library and I wanted a…",4,"We’re arriving faster than we think.
I have to make an admission: I read Origin by Dan Brown during my vacations. It was just there, available at my library and I wanted a turn-pager.
Fiction is my ritual for bedtime. I can read in bed because I never bring my phone in the bedroom. It’s one of the few good habits I have.
I’ve always been an advocate for privacy. After a stalker scare in early 2002, I immediately understood the potential of the Internet to become the far west where you could easily be found, tracked and possibly hurt.
Back then it was impossible to imagine things like the Cambridge Analytica scandal. Or the constant spying perpetuated by governments, companies and individuals who purchase spyware apps.
In the 90s and early 2000 we were so innocent.
In 1994 I hadn’t yet experienced the web, it was all text based. I remember sending my postal address to a creepy fellow in the US who I had met in a newsgroup.
I’d had pen-pals before (yup, I’m ancient, I’m 45). They were people I found through zines and with whom we exchanged anecdotes, books and mix tapes. I think about this and it’s just inconceivable that we had the patience to wait for a letter.
The Internet was pen-pals on steroids. I didn’t have the slightest idea of the risks when I went into newsgroups. I was mostly there for the music. That’s how I discovered new bands and became the insufferable music snob I no longer am.
So today, when I read about privacy issues, about the leaps technology is taking, I am seriously afraid. Origin, the Dan Brown novel is about AI (Artificial Intelligence). We’ve seen movies and read books about it.
For decades computer scientists have been trying to get a computer to be like a human. If you read just a little bit about the advances in the field and how we started to interact with it when Siri first arrived we know that it takes a human to teach a computer how to behave. If you’ve seen Ex-Machina you know it takes a human to build a robot. If you read Frankenstein it took a human to make the monster. And yet, we think it’s a good idea.
Despite the dangers of millions of jobs disappearing, the invasion of privacy, the constant monitoring of employees through chips, badges, and bracelets to the degree which the employer will know about each key stroke, each step, each meeting and what it is said to the silly games that pop into our lives that lead us like lambs off a cliff, to volunteer our data, our faces, our voices our fingerprints, until there is no way to escape. We still think it’s a good idea. All of it.
This article on The Guardian asks: you wouldn’t like it if someone was physically standing outside your window looking at you, why doesn’t it bother you that Google, Apple, Facebook, Amazon and countless others are doing exactly just that and you are voluntarily giving everything? Do you own a Google Home, Apple HomePod, Amazon Echo? Do you have a “smart home”? Well, you’re teaching their computers about you for free so they can then sell you things easily because they know you too well.
You don’t read a Dan Brown novel for the beautiful writing. You read it because it’s a way to learn about a dry subject through an acceptable story line. It took me a while to get into it because the writing is so painful, however it was clear to me that in order for an author to write about something that seems science fiction but that is happening right now, he must have done some serious research.
A computer will not learn without algorithms and the algorithms can not be built without data and one person or one company alone cannot generate that data, the data has to be captured by millions of people and of course millions of people won’t do that for free… or will they? It just takes a game and a poke to the ego.
They will know where you go, for how long, who you meet, why and what you talked about.
As I write this I’m fully aware that maybe these companies know nearly nothing about me as a person but know much more than I do about my personal preferences. Both for good and bad.
For the longest time I believed in my own critical thinking but in the past two years I’ve realized that I too surrendered my control and now discover less, explore less, talk less and most unfortunate: I’m interested in less. Because my computer is a place where I get fed (yes passive voice) my preferences and I can “hide the ad” if I’m not into it, but even that little agency gesture is being accounted for, they will show me less of that but more of the other things I’ve “liked” and all those things will feed into building AI tools.
And nobody seems to care.
What can we do?
When we look at China we wonder how can a country with so much talent, so much “progress”, so much wealth just allowed a single man to be ruler for life. We would be so wrong to think like we did before the Internet, when we watched the news on TV and saw something horrible happen on the other side of the world: oh, it’s too far, it couldn’t happen here (Canada) it certainly can’t happen the United States, they are the leaders of the free world.
The Chinese government has one of the most advanced facial recognition technologies in the world and they use it daily. They know where and who their citizens are in most of their cities. They control their communications. In their case they didn’t ask for the data (maybe, I don’t know) but probably they just took it and now there is no way for an individual to voice their concerns. It will be known.
Meanwhile, Google, Apple and Amazon engineers are concocting a new little game that will continue gathering different spoken accents, maybe they already did, and building a huge database of words and phrases so when we finally invite our humanoid Rosie into our homes, we can talk to it like we’d talk to a person. And so the cycle will continue. The Rosie robot will transmit all the info back to the companies until we lose what we’ve been trying to lose all this time: ourselves.
The only thing we can do and we won’t because we are already trapped is to refrain from inviting more of the big tech giants into our houses. This is now Utopia since generations who were born with the Internet will not know what it’s like to have a private life. A private life is a non-existing life, or so it feels, but it’s not. It’s actually what freedom really tastes like.
",We’re arriving faster than we think.,0,were-arriving-faster-than-we-think-cc37408a7dd4,2018-04-09,2018-04-09 17:05:14,https://medium.com/s/story/were-arriving-faster-than-we-think-cc37408a7dd4,False,1159,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Mujer Imposible,Magazine for fish swimming upstream.,816c8381ad0a,mujerimposible,24.0,13.0,20181104
0,,0.0,,2018-02-02,2018-02-02 18:05:09,2018-02-02,2018-02-02 18:05:32,0,False,en,2018-02-02,2018-02-02 18:05:32,0,1dff1badb610,3.4,0,0,0,"The debate on the profession of software developer — its alleged ills and failings — abates at times, but never ends. Here are two sets of…",4,"The Algorithm Debate
The debate on the profession of software developer — its alleged ills and failings — abates at times, but never ends. Here are two sets of ten commandments. The first set is directed at professional software developers, and the second to non-practitioners.
Ten commandments for professional developers:
1. Algorithm is a collection of models; cherish their diversity.
2. It’s a model, not the model.
3. Make your model simple enough to isolate specific causes and how they work, but not so simple that it leaves out key interactions among causes.
4. Unrealistic assumptions are OK; unrealistic critical assumptions are not OK.
5. The world is (almost) always second-best.
6. To map a model to the real world you need explicit empirical diagnostics, which is more craft than science.
7. Do not confuse agreement among developers for certainty about how the world works.
8. It’s OK to say “I don’t know” when asked about the technology or policy.
9. Efficiency is not everything.
10. Substituting your values for the public’s is an abuse of your expertise.
Ten commandments for non-technologists:
1. Algorithm is a collection of models with no predetermined conclusions; reject any arguments otherwise.
2. Do not criticize a developer’s model because of its assumptions; ask how the results would change if certain problematic assumptions were more realistic.
3. Analysis requires simplicity; beware of incoherence that passes itself off as complexity.
4. Do not let algorithms scare you; developers use algorithms not because they are smart, but because they are not smart enough.
5. When a developer makes a recommendation, ask what makes him/her sure the underlying model applies to the case at hand.
6. When a developer uses the term “” ask what s/he means by it.
7. Beware that a developer may speak differently in public than in the seminar room.
8. Developers don’t (all) worship technologies, but they know better how they function than you do.
9. If you think all developer think alike, attend one of their workshops.
10. If you think developers are especially rude to non-practitioners, attend one of their workshops.
I have spent enough time around non-practitioners to know that their criticism often misses the mark. In particular, many non-practitioners tend not to understand the value of algorithms or parsimonious modeling (especially of the mathematical kind). Their typical riposte is: “but it is more complicated than that.” It is of course. But without abstraction from detail, there cannot be any useful analysis.
Developers, on the other hand, are very good at algorithms or design models but not so good at navigating among their models. In particular, they often confuse a model, for the model. A big part of the problem is that the implicit scientific method to which they subscribe is one in which they are constantly striving to achieve the “best” model.TDevelopers are particularly bad at this, which accounts in part for their dismal performance. There is too much of “is this the right model” (and their variants), and too little of “how do we know whether it is this model that is the most relevant and applicable at this point in time in this particular context.”
On the other hand, developers should also bear in mind that technology is a resource that we can use for God’s glory. Here’s three ways technology can contribute to the greater cause of Islam.
1. Technology Enables Communication
Through Facebook and Twitter, religious scholars can easily communicate directly with their followers throughout the day and week.
Technology allows a greater sense of community that doesn’t demand proximity.
Technology enables their followers easily to have direct communication with them on a broader and a clearer scale.
Ongoing communication through technology helps the cause of Islam.
2. Technology Enables Community
Technology allows a greater sense of community that doesn’t demand proximity. Proximity isn’t required for community.
Social media is where younger generations are interacting. It’s the new marketplace. It may be unnatural for past generations, but it is how community for younger people is now started and developed.
Use technology to enable communication, community and brotherhood in Islam.
Through social media, a new attendee can connect to other mosque or community members before he or she ever has a chance to meet at a mosque gathering or a small group.
Of course, true community requires feet and faces and not just electrons and avatars. But those electrons and avatars can be tools to bring people into closer community with feet and faces.
This is a big shift in how we interact, but we have to use it if we want to enable community for the sake of Islam.
3. Technology Enables Brotherhood
Some mosques have an app where people can actually access the prayer outline, and people use their phones or iPads to follow along and take notes. Technology enables members and attendees to enhance their worship experience at mosque.
All of these are tools to enhance brotherhood. Technology, though, is not the goal. The goal is to enable the mosque’s mission to make followers of all people groups.
Find the Benefits of Technology
There are unintended side effects of technology that are both de-personalizing and dehumanizing. But there are some wonderful benefits of technology that enable the mission of the mosque.
Technologists should also consider how to use technology to enable communication, community and brotherhood in Islam.
",The Algorithm Debate,0,the-algorithm-debate-1dff1badb610,2018-02-02,2018-02-02 18:05:33,https://medium.com/s/story/the-algorithm-debate-1dff1badb610,False,901,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Daily Wisdom,,ddd120ae7c2,dailywisdom,60.0,0.0,20181104
0,,0.0,7f60cf5620c9,2018-04-18,2018-04-18 20:07:31,2018-04-18,2018-04-18 23:26:47,2,False,en,2018-04-21,2018-04-21 02:37:40,27,86c32f91a0e0,2.790880503144654,10,0,0,TWiML & AI meetup talk: Trust in AI,5,"You need the right amount of trust from humans in AI
TWiML & AI meetup talk: Trust in AI

I was very happy to present at the TWiML & AI meetup on the topic of Trust in AI. The topic is at the center of how humans use machines and how they will integrate them into their teams longer term. As we will see it is a very important but difficult topic to understand.

“Trust facilitates cooperative behavior”
To start the discussion we reviewed four papers on the topic of trust and automation:
Some Observations on Mental Models (1987)
Humans and Automation: Use, Misuse, Disuse, Abuse (1997)
Overtrust of Robots in Emergency Evacuation Scenarios (2016)
Trust in Automation: Integrating Empirical Evidence on Factors That Influence Trust (2015)
What is key about all of these papers is that they talk about the balance of trust between too much (misuse) and too little (disuse). There are many factors that can increase the trust but should we always build systems to have the most trust?
After the review of the papers we dive into what we should consider when teaming people and machines. It isn’t about human-in-the-loop (HITL) vs. out-of-the-loop (OOTL) AI. Neither are the way things are.
When it comes to HITL it isn’t just a human and a machine anymore. It is organizations of humans working with ecosystems of intelligences. We need to understand how they best work together.
If you believe that any system is truly OOTL for humans then you aren’t setting the system boundaries correctly. There is always a human involved in some way. Even if it is just your customers being impacted. You need to include them in your consideration.
Accountability is an important factor when considering trust. When something does go wrong who needs to answer for it?
That is my exit!
If the car above was an autonomous vehicle it didn’t technically have an accident but it does need to be accountable for what it did, rather than the other human truck drivers.
How do you figure out the right amount of trust for your application? This can be done without investing months and months of data cleaning and model training time. You can do it with non-coded prototypes as we have in various projects.
In the end, there are so many factors that go into trust that you need to test the system with real people and understand what they need. Machines don’t replace human purpose.
Resources
Slides:
TWiML & AI: Trust and AI
Trust and AI chrisbutler@philosophie.is @chrizbot https://goo.gl/m1PaVidocs.google.com
Other references:
Google’s high performing teams and trust
TWiML & AI Podcast #110 with Ayanna Howard
Emergency robot video
Why troops don’t trust drones
Robocop (2014)
Placebo Buttons (fake thermostats)
Illusion of control bias
A Machine in the Loop Approach
Machine Learning with Humans in the Loop: Lessons from StitchFix
Star Wars Social Network
That is our exit!
Moral Crumple Zones
Accountability in a computerized society
WoZ Way: Enabling Real-time Remote Interaction Prototyping & Observation in On-road Vehicles
Wendy Ju at Cornell
Testing AI concept in user research
Empathy Mapping for the Machine
Confusion Mapping
About Chris Butler
I help teams understand the real business problems they should solve with AI-centered solutions. The teams we work with are generally asked to “do something interesting” with the data they have. We help them avoid local maxima through happenstance correlations and focus on solving huge business problems. My background includes over 18 years of product and business development experience at companies like Microsoft, KAYAK, and Waze. At Philosophie, I have created techniques like Empathy Mapping for the Machine and Confusion Mapping to create cross-team alignment while building AI products. If you want to learn more or get in touch via email, LinkedIn, or visit http://philosophie.is/human-centered-ai.
",You need the right amount of trust from humans in AI,17,twiml-ai-meetup-trust-and-ai-86c32f91a0e0,2018-07-05,2018-07-05 17:14:00,https://towardsdatascience.com/twiml-ai-meetup-trust-and-ai-86c32f91a0e0,False,638,"Sharing concepts, ideas, and codes.",towardsdatascience.com,towardsdatascience,,Towards Data Science,,towards-data-science,"DATA SCIENCE,MACHINE LEARNING,ARTIFICIAL INTELLIGENCE,BIG DATA,ANALYTICS",TDataScience,Ethics,ethics,Ethics,7787.0,Chris Butler,Director of AI at Philosophie NYC,ba6349c9c628,chrizbot,853.0,83.0,20181104
0,,0.0,a9bd4f08ac9b,2018-03-28,2018-03-28 03:39:06,2018-03-28,2018-03-28 12:59:56,5,False,en,2018-04-06,2018-04-06 13:02:07,12,d45a402048c6,8.176729559748427,7,0,0,"The Facebook scandal is embarrassing for the company and will tarnish its reputation at least temporarily, but this is not the end. In…",5,"
Data Weaponization and the Future of Privacy
The Facebook scandal is embarrassing for the company and will tarnish its reputation at least temporarily, but this is not the end. In fact, it’s just the very beginning.
Last week we witnessed a historical information weaponization surrounding Facebook and the 2016 election. If you didn’t see anything, you can check out the long form here, and the micro brief here.
Basically what happened is that Facebook allowed Cambridge Analytica to mine data about hundreds of thousands of people through an application they created. However, despite what the news may be saying, there was no breach. This data was legally mined once the terms and services were agreed to. And under this legal agreement, the company used their mining capabilities to weaponize data into the hands of interested political parties. This behavior played a huge role in previous elections, especially in the United States and the United Kingdom.
While this was a shock to many, it really shouldn’t be. Data has been and regularly is mined and weaponized every day. We call this advertising. And it’s done by hundreds of thousands of organizations around the world. Ever signed into anything with Facebook? Ever played Farmville, Candy Crush, or any of the other games on Facebook? They all more than likely have similar access to this information.
The thing is, most people don’t believe this access has any effect on their lives until something dramatic happens like what we’re seeing right now with Facebook. They don’t think the access is effecting their lives. They don’t think there’s any reason to be concerned. But as everyone in the tech field knows, it’s not about the ads, it’s about the data. And despite recent controversy, this is almost always legal.
Moving forward, the weaponization of data is only going to get worse unless we decide to put a stop to it. If you’re interested in learning how you can see exactly what data companies like Facebook and Google own from your digital footprint, you can check out this thread on Twitter by Dylan Curran. Until the general population is aware and capable of protecting themselves, the people will have to rely on corporations to maintain a level of moral responsibility or hope that regulators will protect them from corporate ills. However, at the pace that regulation moves it’s risky to solely rely on law makers to protect us but we can look to the EU for how we might move into the future. In order to make this a revolution we need to start getting everyone on board. This week’s links are a good start.

BOOK
The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation
Artificial intelligence and machine learning capabilities are growing at an unprecedented rate. These technologies have many widely beneficial applications, ranging from machine translation to medical image analysis. Countless more such applications are being developed and can be expected over the long term. Less attention has historically been paid to the ways in which artificial intelligence can be used maliciously. This report surveys the landscape of potential security threats from malicious uses of artificial intelligence technologies, and proposes ways to better forecast, prevent, and mitigate these threats. The team involved — which spans several universities and research organizations — analyzes, but does not conclusively resolve, the question of what the long-term equilibrium between attackers and defenders will be. They focus instead on what sorts of attacks we are likely to see soon if adequate defenses are not developed.
In response to the changing threat landscape we make four high-level recommendations: 
1. Policymakers should collaborate closely with technical researchers to investigate, prevent, and mitigate potential malicious uses of AI.
2. Researchers and engineers in artificial intelligence should take the dual-use nature of their work seriously, allowing misuserelated considerations to influence research priorities and norms, and proactively reaching out to relevant actors when harmful applications are foreseeable.
3. Best practices should be identified in research areas with more mature methods for addressing dual-use concerns, such as computer security, and imported where applicable to the case of AI.
4. Actively seek to expand the range of stakeholders and domain experts involved in discussions of these challenges.
Read More

OFFICIAL REPORT
Three Years of The Right to be Forgotten
The “Right to be Forgotten” is a privacy ruling that enables Europeans to delist certain URLs appearing in search results related to their name.
In order to illuminate the effect this ruling has on information access, we conduct a retrospective measurement study of 2.4 million URLs that were requested for delisting from Google Search over the last three and a half years. We analyze the countries and anonymized parties generating the largest volume of requests; the news, government, social media, and directory sites most frequently targeted for delisting; and the prevalence of extraterritorial requests. Our results dramatically increase transparency around the Right to be Forgotten and reveal the complexity of weighing personal privacy against public interest when resolving multi-party privacy conflicts that occur across the Internet.
This report, done by Google employees, will help you better understand the value behind the right to be forgotten.
Read More

HIGHLY CREDIBLE SOURCE
Google Transparency Report
In a May 2014 ruling by the Court of Justice of the European Union, the Court found that individuals have the right to ask search engines like Google to remove certain results for queries on the basis of a person’s name. The search engine must comply if the links in question are “inadequate, irrelevant or no longer relevant, or excessive,” taking into account public-interest factors including the individual’s role in public life. Pages are only delisted from results in response to queries that relate to an individual’s name. We delist URLs from all of Google’s European search results — results for users in France, Germany, Spain, etc. — and use geolocation signals to restrict access to the URL from the country of the requester. The charts in this interactive data gallery show the total number of requests received and the total number of URLs requested to be delisted since May 29, 2014.
Read More
Upcoming Events
As mentioned in previous emails, I’m going to mostly be focused on burying myself heads down over the next couple of months until I leave for Germany so I can finish writing and hand it off to Ellen, but here are the events I have coming up that I’d love to see you at! If you’re in town and have some time open on your calendar.
Internationally I’m also working on events in other parts of Germany, the UK, Denmark, and Colombia. Locally, in the United States, I’m working on more in Denver/Boulder as well as Los Angeles, San Francisco, Palo Alto, Chicago, and more.
If you know of someone or some event that would be interested in having me, or you want to set something up yourself, let me know!
4.3 — Design Hour in Boulder, Colorado
Design Hour is a group of creatives based in Boulder, Colorado that converge monthly to discuss the latest in design across UI/UX, print, product, industrial, and XR — among others. I’ll be speaking with the group about automation, ethics, and impact both within automation in general, and more specifically as it’s implemented within conversations.
RSVP link coming soon!
4.19 — Newscamp in Augsburg, Germany
Switching up gears for this one, instead of covering the field of AI as an overarching topic of discussion, I’ll be taking a deep dive into the topic of the Attention Economy and Automation. How is Automation effecting our news, how it is it harming democracies across the globe? What implications does this have for humanity, at large?
We’re seeing some of the largest, most “developed” nations implode on each other — the UK with Brexit, the US with Trump, France with Macron and Le Pen, Brazil with Lula da Silva and Bolsonaro. And more. Why are these countries so separated? What part does the Internet play in this? How do we move forward.
I’m incredibly passionate about this topic and can’t wait to talk with the global leaders at NewsCamp!
RSVP
4.26 — Lecture at Hochschule Augsburg
New technologies and the algorithms that reside within them have become the governing bodies of the world. These systems shape the way millions, if not billions of people around the globe behave, think, and operate as a global society.
The talk will help students understand what exactly is happening and how they can leverage the knowledge to prepare themselves for the future of work.
Private Event
4.26 — Meetup with Stammtisch in Munich, Germany
New technologies and the algorithms that reside within them have become the governing bodies of the world. These systems shape the way millions, if not billions of people around the globe behave, think, and operate as a global society. This being said, the people creating these systems have become the modern government representatives to their user base of constituents.
What we will talk about is the power that resides within these systems, why regulation is an important part of the future of this industry, how it will effect business, and where it may be headed in the future. This talk will help everyone understand the role they play and the responsibilities that come with it.
Private Event
4.27 — Munich Tech Meetup at Team23 in Munich, Germany
New technologies and the algorithms that reside within them have become the governing bodies of the world. These systems shape the way millions, if not billions of people around the globe behave, think, and operate as a global society. This being said, the people creating these systems have become the modern government representatives to their user base of constituents.
What we will talk about is the power that resides within these systems, why regulation is an important part of the future of this industry, how it will effect business, and where it may be headed in the future. This talk will help everyone understand the role they play and the responsibilities that come with it.
RSVP coming soon!
5.4 — TrustWorks in Copenhagen, Denmark
New technologies and the algorithms that reside within them have become the governing bodies of the world. These systems shape the way millions, if not billions of people around the globe behave, think, and operate as a global society. This being said, the people creating these systems have become the modern government representatives to their user base of constituents.
What we will talk about is the power that resides within these systems, why regulation is an important part of the future of this industry, how it will effect business, and where it may be headed in the future. This talk will help everyone understand the role they play and the responsibilities that come with it.
Private Event
If you liked this, you should sign up for the Design Good newsletter and join over 1,400 people who have already done the same! You can also purchase my new book, Automating Humanity, at designgood.tech to learn more about all of this in much greater detail.

Also know that as a 501(c)(3) non-profit, each purchase is tax deductible (a donation to the foundation) and 25% of all purchases go to youth technology literacy programs of your choosing. This means your purchase not only supports the mission of Design Good as a non-profit research org, but also funds the future generation’s education, which will help future societies thrive!
If you don’t see the program you’d like to donate to, let me know and we’ll make sure your favorite program gets added to the list!
",Data Weaponization and the Future of Privacy,68,data-weaponization-and-the-future-of-privacy-d45a402048c6,2018-05-03,2018-05-03 09:27:23,https://medium.com/s/story/data-weaponization-and-the-future-of-privacy-d45a402048c6,False,1946,"Thoughts and stories from Studio, a one year product design masters program at CU Boulder, dedicated to re:working, re:designing and re:imagining the world of design and technology.",,,,RE: Write,,re-write,"DESIGN,DIGITAL,TECHNOLOGY,PRODUCT DESIGN,UX",cmci_studio,Ethics,ethics,Ethics,7787.0,Joe Toscano⚡️,"Founder, designgood.tech // Contributor, @smashingmag @adweek @invisionapp // prev XD, @rga @google. Changing the world w/ a smile, design & some code.",999fc7f88fff,realjoet,7168.0,210.0,20181104
0,,0.0,,2018-07-07,2018-07-07 17:48:49,2018-07-07,2018-07-07 17:50:51,1,False,en,2018-07-26,2018-07-26 19:42:45,5,3a12864e212a,2.950943396226416,0,0,0,"There’s a lot being said, discussed and done about AI (artificial intelligence) nowadays. Labeled as a megatrend, AI has emerged as a…",2,"AI assistants, autonomous vehicles and the need for ethical AI
Image credit: Artificial Intelligence by Nick Youngson is licensed under CC BY-SA 3.0
There’s a lot being said, discussed and done about AI (artificial intelligence) nowadays. Labeled as a megatrend, AI has emerged as a technology that will spawn (or has already spawned) the fourth industrial revolution (The steam engine, electricity and mass production, digital age were the previous industrial revolutions — 1). In fact, Statistica reports that revenues from the AI market will hit $7.35B in 2018 and grow to $89.85B by 2025–2.
From large corporations to small startups, many companies have jumped on the AI bandwagon. Some are figuring out what to do, while others have already have taken the lead in launching new products and more. Not surprising, Google, one of the top tech behemoths, is one such company, that showcased its AI prowess in the jaw-dropping Google Assistant demo during their flagship Google I/O keynote pitch by CEO Sundar Pichai. The demo showed a Google Assistant scheduling an appointment on behalf of their consumer with a haircut salon. Subtle nuances such as the assistant speaking an “M-hmm..” during the conversation proved that the assistant is no different than a real person.
Surely, we have come a long way from the days of the 1st voice assistant — IBM Shoebox in 1961 recognized 16 words and digits-3. At the same time, the demo has raised enough questions along the lines of responsibility, privacy, policy, guidelines, and ethics. Now, since virtual AI assistants are cropping up in domains of banking, shopping, health, these questions need to be answered and solved for soon. So let’s take a look at some important points here -
Responsibility — The common case cited here is that of a self-driving car engaged in an accident with another vehicle with a human driver or with another human. If the fault lies with the “self-driving car” and if person-passenger in the self-driving car has tried all at their end to prevent the accident, then who’s really responsible? What are the guidelines for situations like these? In the case of the Google AI assistant making a call to the hair salon, the company has the responsibility of identifying the assistant as an AI assistant at the start of the phone call, unless the company wants to paint a very real picture of the conversation flow to the other party.
Transparency, Control & Quality — AI assistants and the software algorithms backing them use a large amount of data (at times, including personal data) to automate scenarios around daily tasks such as schedule appointments, play songs from music lists, write an email etc.. Bugs in such software can leave this data exposed to malicious third parties thus depleting consumer trust in the new technology and even damage the company’s brand. Consumers ought to be given the appropriate amount of control and transparency regarding their data that is utilized in the AI algorithms. California’s Consumer Privacy Act of 2018 (passed in Jun 2018) is a step in the right direction that companies should pay attention to. At the same time, companies also need to have solid plans to improve quality and mitigate such risk scenarios.
Policies & Governing Body — Cases above & others emerging in the real world also highlight the need for a governing body — a watchdog that creates, mandates policies around AI scenarios, technology implementation, and usage. Consider scenarios such as a transaction or exchange between two AI assistants on behalf of two individuals or hackers hacking a transaction between an AI assistant and a human. The industry needs a watchdog that defines the regulations — what’s allowed, what’s not allowed, who’s responsible, what’s right and what’s wrong. It is the time that top tech industry companies spearheading the AI revolution also lead AI regulations and governance.
Previous industrial revolutions have shown that while the telephone and the internet connect people globally, phone tapping and website hacking also happen. This one’s also not easy — the combination of machines and humans can unfold unimaginable scenarios, so will ethical AI now stand up? 
 References:
1 The Four Industrial Revolutions in a glimpse http://richmondvale.org/industrial-revolutions/
2 Revenues from the AI market worldwide — Statistica https://www.statista.com/statistics/607716/worldwide-artificial-intelligence-market-revenues/
3 A Timeline of Voice Assistant and Smart Speaker Technology from 1961 to today (https://voicebot.ai/2018/03/28/timeline-voice-assistant-smart-speaker-technology-1961-today )
","AI assistants, autonomous vehicles and the need for ethical AI",0,ai-assistants-autonomous-vehicles-and-the-need-for-ethical-ai-3a12864e212a,2018-07-26,2018-07-26 19:42:45,https://medium.com/s/story/ai-assistants-autonomous-vehicles-and-the-need-for-ethical-ai-3a12864e212a,False,729,,,,,,,,,,Ethics,ethics,Ethics,7787.0,D'laila Pereira,"Uniqpoint - about business strategy, marketing and product in the tech world",43d5f515db24,uniqpoint,1.0,15.0,20181104
0,,0.0,7f60cf5620c9,2018-01-20,2018-01-20 17:25:36,2018-01-20,2018-01-20 17:59:06,1,False,en,2018-01-22,2018-01-22 08:05:44,18,f8bfb31c75b1,4.350943396226415,9,0,0,Why is hiring a Data Protection Officer who ignores Data Science like buying an armored car to drive around in circles?,5,"Data Science and the DPO

Why is hiring a Data Protection Officer who ignores Data Science like buying an armored car to drive around in circles?
If data is the fuel of the digital economy, Europe’s new General Guidelines on Data Protection provides a legal roadmap of what we can now do with the personal data of European citizens. Critical to this new legislation for all organizations that process personal and sensitive data will be the obligation to hire a Data Protection Officer (DPO). Employers beware, for hiring a DPO based only on their knowledge of the law won’t get you any closer to reaching your strategic objectives. Let’s look at the obligations, qualifications, and responsibilities of your future DPO before focusing on their need to understand the nature and the goals of Data Science.
The designation of a Data Protection Officer, before the General Data Protection legislation goes into effect on May 25h, 2018, is a mandatory requirement for private companies and private organizations. This new European legislation requires the appointment of a DPO for any company that processes or stores significant amounts of data on their European employees or customers wherever their place of operations. DPOs must be also appointed in organizations that regularly capture, store, or transform the data of European citizens regardless of their base of operations. Any non-military agency that regularly and systematically monitors personal data, as well as processes sensitive data (health, race, ethnicity, religion etc.) is also required to comply with the legislation. Given the scope of these requirements, it is of little wonder that a recent study concluded that 28 000 DPO’s will be hired in the coming months alone.[i]
The Data Protection Officer will assume a wide range of organizational responsibilities in front of consumers, employers, and stakeholders. The DPO will ensure the compliance of organizational data processes with GDPR. They will be asked to establish comprehensive records of all data processing activities conducted by the company, including the purpose of all processing activities. They will also conduct audits to ensure compliance and address potential issues proactively. The DPO becomes the single point of contact for employees and customers who wish to have informed them about how their data is being used, and what measures the company has put in place to protect personal information. Finally, the DPO serves as the point of reference between the company and the National Data Protection Authorities (NPA). [ii]
Despite such broad responsibilities, The European regulators offer little specific guidance on what qualifications a DPO candidate must bring to the table. Article 37 requires a data protection officer to have “expert knowledge of data protection law and practices.” Beyond that, the regulations suggest that the candidate should have a thorough understanding of an organization’s IT infrastructure and technology. The DPO must remain an independent council within the organization without direct responsibility for decisions concerning how data is processed.[iii] Public and private organizations may share the services of a DPO, but they are not allowed to hire a DPO on a short or fixed term contract.
Hiring a DPO with little knowledge of Data Science is likely to as ineffective and it is counter-productive. The DPO must understand why and not just how the organization is collecting personal and sensitive data. Technically personal and sensitive data doesn’t need to be stored in the organization all, for as long as the Data Science team has access to a unique referential they can reconstitute on demand the needed records from a variety of external data sources. He or she should never-the-less appreciate that Data Scientists are less interested in hoarding personal and sensitive data than in exploring how the relationships between individuals (or technologies) influence collective beliefs, or, motivations, and actions. The DPO should be a part of the Data Science team: for the legal requirements of GDPR aren’t constraints that limit its use of Data Science, but considerations that can guide its application in your business.
The DPO needs to look beyond the function’s responsibilities and obligations, to explore the larger picture of why the organization is collecting data at all. The success of any organization today depends on its ability to leverage data not only in understanding the past performance of the organization, but in predicting and influencing future maker trends. This developing data processes that promote analytics at every level of the organization: scanning the market context to understand the nature of their business challenges, qualifying the data at hand, identifying the right methodology to address the problem, and transforming the data into a call for action. DPO’s need to believe and evangelize the vision that data isn’t just an organizational by-product that needs to be monitored and controlled, but a transformational force that will help define how the organization will look at its market, its resources, and its bases of competitive advance for the foreseeable future.
A number of training centers can offer help and assistance in training future DPOs. An excellent source of information, benchmarks, and resources can be found on the EC websites, as well as those of the NPA’s (the CNIL in France).[iv] Several universities are beginning to offer short programs and/or executive degrees on the legal and technical roles of the DPO. Private consultancies are focusing the more practical issues of auditing, process improvement and reporting. At the Business Analytics Institute, we address the inherent links between data science and the practices of a DPO in our conferences, MasterClasses and DPO certificates. We strongly believe that hiring a Data Protection Officer ignorant of Data Science is like buying an armored car to drive around in circles — you may feel well protected, but you’re not actually going anywhere.
Sign up for our new one-day MasterClass on becoming a Data Protection Officer. The practice of business analytics is the heart and soul of the Business Analytics Institute. In our Summer School in Bayonne, as well as in our Master Classes in Europe, our focus on digital economics, data-driven decision making, machine learning, and visual communications will put analytics to work for you and your organization.
Lee Schlenker is a Professor at ESC Pau, and a Principal in the Business Analytics Institute http://baieurope.com. His LinkedIn profile can be viewed at www.linkedin.com/in/leeschlenker. You can follow us on Twitter at https://twitter.com/DSign4Analytics
****************
[i] Heimes, R., (2016), Study: At least 28,000 DPOs needed to meet GDPR requirements
[ii] Jakubowicz, L. (2016), Data protection officer (DPO) : définition, formation et salaire
[iii] Lord, N., (2018), What is a Data Protection Officer (DPO)?
[iv] See for example, The DPO Corner
",Data Science and the DPO,41,data-science-and-the-dpo-f8bfb31c75b1,2018-07-13,2018-07-13 12:09:18,https://towardsdatascience.com/data-science-and-the-dpo-f8bfb31c75b1,False,1100,"Sharing concepts, ideas, and codes.",towardsdatascience.com,towardsdatascience,,Towards Data Science,,towards-data-science,"DATA SCIENCE,MACHINE LEARNING,ARTIFICIAL INTELLIGENCE,BIG DATA,ANALYTICS",TDataScience,Ethics,ethics,Ethics,7787.0,Lee Schlenker,,a6fe6510d25,LeeSchlenker,222.0,214.0,20181104
0,,0.0,,2018-09-03,2018-09-03 09:49:34,2018-09-03,2018-09-03 14:51:07,5,False,en,2018-09-03,2018-09-03 14:51:07,6,81d76786883b,2.8257861635220127,6,1,0,"“The three greatest challenges for humanity are nuclear war, climate change and technological disruption, which can only be tackled with…",2,"Why global collaboration is key for the future development of AI

“The three greatest challenges for humanity are nuclear war, climate change and technological disruption, which can only be tackled with cooperation.”
This quote comes from a recent Yuval Noah Harari interview for a Spanish newspaper, and it definitely made me go back to one of the questions that has been in my mind for the last months.
Are current individual efforts around AI ethical development worth it? Before answering this question, let me start by summarizing (non exhaustively) some of the most relevant efforts put in place recently:
Europe
Europe has been very active during the last months regarding AI-related initiatives. Three of the most relevant ones and its goals are the ones below:

USA
In the USA, we can highlight different efforts, the ones coming from the administration and academia, and the ones coming from the digital giants. We can see summary of initiatives below (again, as a non exhaustive exercise):


In parallel, China has launched its own plan to use Artificial Intelligence to gain global economic dominance by 2030. In the case of China, it is worth mentioning that:
More than 50% of global AI startups funding already comes from China.
Out of the 27 teams competing in 2017’s ImageNet challenge, more than half were Chinese-based research teams from universities or companies, and all the top performers were from China.

For a complete overview of national strategies around AI, please visit this great article. A quick read will make you understand the rapidly increasing number of different strategies around AI.
Although all of the described efforts are significant effort towards stablishing ethical and legal frameworks under which to develop AI, the inner nature of Artificial Intelligence (as I already covered in this article) can make all this become useless to make progress in its ethical issues. In order to develop this idea, let’s build on the list of the Top 9 ethical issues in artificial intelligence by the World Economic Forum. Among the 9 issues raised by the WE Forum, let’s focus on the following:
Unemployment and Inequality. What happens after the end of jobs?
Security, evil genies and Singularity. How do we keep AI safe?
The above issues require a common global strategy. Let’s think about it for a moment:
Although one country or continent successfully manages the transition to job automation, it will still have to deal with the effects of its neighbours not doing so (e.g. creating new migration pressures).
AI security threats cannot be managed locally. Cybersecurity attacks, evil uses of AI and Singularity all depend on a global consensus on how to develop and control the limits of AI.
At this point, it seems like a good time to go back to Yuval Noah Harari. As he mentions in one of his books:
“Sapiens rule the world, because we are the only animal that can cooperate flexibly in large numbers. We can create mass cooperation networks, in which thousands and millions of complete strangers work together towards common goals.”
It seems a good moment to keep that affirmation in force to deal with the challenges of Artificial Intelligence.
",Why global collaboration is key for the future development of AI,6,why-global-collaboration-is-key-for-the-future-development-of-ai-81d76786883b,2018-09-03,2018-09-03 14:51:08,https://medium.com/s/story/why-global-collaboration-is-key-for-the-future-development-of-ai-81d76786883b,False,528,,,,,,,,,,Ethics,ethics,Ethics,7787.0,David Pereira,"Head of Artificial Intelligence for Europe at everis, an NTT Data company. All opinions are my own.",f8c8a1a860bc,dpereirapaz,45.0,20.0,20181104
0,,0.0,,2018-05-13,2018-05-13 10:56:04,2018-05-13,2018-05-13 12:00:14,1,False,en,2018-05-13,2018-05-13 12:00:14,1,7c5bee2dac96,3.958490566037737,6,1,0,"In Asimov’s Robot series, several societies uphold the custom that robots must introduce themselves with the prefix “R”. For example…",5,"It’s not Google Duplex, it’s you
He would use Duplex
In Asimov’s Robot series, several societies uphold the custom that robots must introduce themselves with the prefix “R”. For example, Daneel Olivaw, the humaniform robot so convincingly lifelike that it can deceive most actual humans, introduces [it/him]self as R. Daneel Olivaw.
If you’re not particularly interested in tech ethics, you’ve probably seen the video of Google’s AI assistant making a convincing phone call on behalf of its human owner to book a haircut, a technology they are calling Duplex. If it really works as well as the demonstration, it’s pretty incredible.

If you are interested in tech ethics, you’ve probably also seen a take like this one from CNET’s Bridget Carey.

Bridget (and everyone else on YouTube) identifies the primary ethical issue as being the same one identified by Asimov: that human beings have a right to know when they are and aren’t interacting with other human beings.
I think this is far fetched. If you’ve ever used Twitter you probably know that ship has sailed, and if anything this technology makes me think that voice calling as a format will die a slow death as the number of humans making calls shrinks compared to the number of robots making sales, research, and political calls, and restaurants get a few bad experiences with empty tables caused by unclear conversations with robot assistants of varying quality. Especially since it sounds like the production version will identify itself somehow during the call.
That’s a shame, because there is an ethical issue here, it’s just not the kind of broad, scary, political ethics that we’re used to AI invoking, instead it’s just about good manners.
Unlike in Asmimov’s world, we aren’t used to talking to robots as if they are people. The robots also don’t really have any agency in our world — at least not the ones we interact with, yet.
Ignore AI planetary takeovers, my problem is this: If you use Duplex to interact with another human, at least for the foreseeable future, you are an arsehole. Every time I see you I will be thinking about Michael Scott or Alan Partridge.
First, you’d better be pretty damn sure this technology is perfect before you unleash it on somebody else. If your robot phones me up to make an appointment and I have to repeat myself, or it is unclear whether or not you have booked, you are no longer optimising your own time, you are just stealing mine.
Even if things do go smoothly, if I know I’m talking to a robot over the phone — either because I come to recognise the voice, or it introduces itself to me — it is going to be incredible embarrassing for me to have that conversation. I have to choose whether or not to talk to it like a human or like I talk to my own voice assistant.
I can either keep up the facade and waste even more of my own time asking it about the weather and wishing it well, like an idiot, or I can start barking commands down the phone, which is something I am not willing to do if other people, even strangers, are in earshot. I don’t even use the voice assistant on my own phone if there is a single other person in the room because it’s kind of outside the social norm, and yet I have to suffer the same embarrassment to save somebody else’s time? That’s rude.
Google, of course, has done this before. Glass was kind of a cool project, I really liked the idea of maps overlays into your vision and taking photographs without taking out my phone. It would make sense in a society where people had normalised the new interactions it introduced, but in this society it was just too much.
It’s weird that you can’t tell if somebody is taking a photo of you. You also force the issue of using screens in company. Using phones over dinner seems just about socially acceptable now, if you look around any café it’s hard to avoid that conclusion, but there’s still a little bit of social theatre required to pick the right moments. If someone is opening up about something to you, you had better put your phone in your pocket. Having the screen affixed to your face didn’t allow for that subtlety. Just wearing Google Glass was impolite. Maybe in 10 years people will care less.
I just about accept that some people are too busy to make their own hair appointments. If your time is incredibly valuable then maybe that 120 second call is really worth converting to a 15 second instruction to a PA. However, you must accept that by doing so you are sending out a very clear message about how much you value your own time.
Namely, it is worth paying somebody a salary to make a 105 second saving for trivial admin tasks. My time isn’t worth anywhere near that, and neither is yours. I know this because I wrote this post, and you are reading it.
Clearly the cost of a robot assistant is much lower than a human one to the point that maybe, in 10 years, using assistants won’t carry this connotation, but for now it absolutely does. You still think it’s worth making that 105 second saving every 5 weeks, you still think that’s worth the extra time and embarrassment for the recipient of the call. You just are unable or unwilling to pay for it.

I get that we might move fairly quickly to the point where enough of these conditions have changed. Assistants are reliable, we don’t feel stupid asking robots how their day has been, it’s not considered self important for me to outsource my small admin tasks to something else while spending my Sunday watching YouTube clips — but until that happens:
Don’t get your robot to phone me
Maybe just sign up to OpenTable?
","It’s not Google Duplex, it’s you",38,its-not-google-duplex-it-s-you-7c5bee2dac96,2018-05-18,2018-05-18 10:56:23,https://medium.com/s/story/its-not-google-duplex-it-s-you-7c5bee2dac96,False,996,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Tito Sarrionandia,ThoughtWorker. Former teacher of computer science. Aberystwyth alumni.,4d05b4515d6,rbs_tito,32.0,47.0,20181104
0,,0.0,9ff7b907f785,2017-11-30,2017-11-30 15:21:08,2017-11-30,2017-11-30 15:35:22,5,False,en,2017-11-30,2017-11-30 15:44:59,14,c02ac2e3a643,6.5050314465408805,15,0,0,Is ignorance bliss?,5,"
The secret lives of algorithms
Algorithms have a big impact on our world, but their workings are often hidden. This lack of transparency has led to diminishing public trust in them. How can we do better?
Is ignorance bliss?
Algorithms are now embedded in our daily lives, and they structure our social worlds in ways that are often barely noticeable. One example is recommender systems, which play a role in who we meet, what music we listen to, what we eat… They’re also responsible for some of our most extravagant-but-completely-unnecessary purchases, recommending the right product to us at exactly the right time. They might even influence how we’re seen in our circle of friends by giving us the right article to share.
As these digital products get more advanced, the technology actually driving them becomes less visible. This can make for a more intuitive user experience, but it also means that the algorithms end up living secret lives behind the interfaces we use. Each one is a black box with a complex set of inputs and outputs, capable of finding results for almost anything in a few milliseconds when it can take us an hour to find something to wear in the morning.
Of course there’s the occasional algorithmic mishap… A family of New Yorkers gets a visit from the police after their web search history demonstrates an interest in pressure cookers and backpacks. A dad finds out about his daughter’s pregnancy after Target’s predictive algorithms send her baby product coupons. YouTube’s autoplay system shares hundreds of disturbing videos with young children who watch their beloved Peppa Pig get tortured by her dentist. And as we give our explanations to the police or drag our screaming children to the dentist, we realize two things: 1) algorithms have very real repercussions in our lives beyond Spotify’s ‘Discover Weekly’ playlist, and 2) considering how ubiquitous algorithms have become, most of us know very little about how they work and what they’re capable of.
We can’t lift the lid and have a look at what’s inside the black box though. Frank Pasquale calls this the ‘One-Way Mirror’ of Big Data. Companies have a wealth of data about how we interact with media, using it to improve their products or to sell to advertising agencies for targeting, but we know little to nothing about how they use this knowledge to influence the important decisions that we — and they — make. The result is that our trust in the black boxes is gradually diminishing. It’s simple: if we keep giving away information about ourselves and only get exploited in return, our faith in the algorithms and the companies will disappear.
Yes Clark, tech firms know who you are
Metrics Matter
One thing we can see upon looking closer at the world of algorithms is that the fuzzy stuff our social world is made of has been translated into hard data and quantifiable metrics. Our complex cinematic preferences are boiled down to categories like “Cult Evil Kid Horror Movies” on Netflix. In a society that increasingly tries to break down our social lives into data that can be computed, things that cannot be counted are becoming invisible.
Engagement is measured in claps, likes, shares and clicks. This data then forms the algorithmic feedback loop that determines what we see and what remains hidden, what makes the first page and what gets lost in the ocean of SEO. All this then shapes the way we see the world around us. For every search result that appears on top of your list, there are billions that didn’t make the cut.
The Black Box Society by Frank Pasquale
How we measure what gets shown matters. For example, do we want popularity to be the only path to algorithmic success? No matter how popular articles about the royal engagement are, they’re no good as recommendations to people who don’t like the monarchy. To paraphrase George Orwell, all metrics are equal, but some are more equal than others. Popularity as a metric is also easy to manipulate and doesn’t guarantee quality or relevance. Last year, marketing manager Brent Underwood uploaded a photo of his foot as a cover for a self-published, one-page book and asked friends to buy it on Amazon — it became a ‘bestseller’. It only got three sales, but it was in the ‘Freemasonry’ category; that’s how easy it is to game the system. Brent can now tell everyone he has written an Amazon bestseller.
In the rush to try and make human satisfaction quantifiable and measurable, many companies have settled for metrics that are easy to record, like clicks. We need to find ones that actually tell the whole story about what users want and need.
Next stop… where?
There has been a lot of recent criticism over Silicon Valley losing touch with the real world, but that isn’t going to stop tech companies playing a central role in deciding what tomorrow will look like. The power of algorithms not just to guess preferences, but to actually shape them, has created the prospect of these tools being used to harvest more and more data and alter users’ behavior with greater and greater success. This is worrying in a situation where fewer and fewer of us trust them to act in our best interests.
“Big data driven decisions may lead to unprecedented profits. But once we use computation not merely to exercize power over things, but also over people, we need to develop a much more robust ethical framework than [the industry] is now willing to entertain.” - Frank Pasquale

Everybody working to build these powerful algorithms needs to think carefully about the goals and metrics they optimize for, because they will shape the world around us, whether in the way we intend or not. We need to take a new approach to this technology that really works for users. It needs to be transparent and it needs to work towards goals that users really want.
Transparency and accountability are key values companies need to keep in mind when building algorithms and translating fuzzy social concepts into quantifiable data. It’s not straightforward, and unconscious bias can also find its way into machine learning data sets very easily, for example in a photo data-set which has lots of images of women cooking.
FAT/ML is one example of an organization working to tackle issues like fairness and accountability in machine learning. Researchers and people in business have joined forces to analyse failures in machine learning models and ensure that ‘the algorithm made me do it’ doesn’t become the default excuse for decision-making failures with systems reliant on machine learning. Transparency with algorithmic technology is essential to creating systems that work for humans, rather than exploiting them. As one of their studies reported, users are more willing to trust recommender systems with a transparent UI that offers them some way to judge the appropriateness of recommendations.
What do we really want?
When developing an algorithm, it’s also necessary to really consider what the metrics being optimized for say about user behavior. With an engagement model based on views, incidents like YouTube and Peppa Pig occur, especially with ‘similarity of keywords’ enough to qualify as relevance. Focusing on just one dimension of what makes ‘good’ content will often come at the cost of looking at the bigger picture. ‘Engagement’ has become something of a buzzword in any case, since it can mean so many things — from clicks, to time-spent, or most-shared — without telling us much about the user experience.
What if we tried to optimize for real user satisfaction instead?
Parse.ly is looking at a creative re-evaluation of what ‘engagement’ actually means. They are proposing a new metric of ‘engaged time’ which measures the reader’s activity over three “heartbeats”, rather than stopping at the click. Parse.ly are trying to differentiate bounce clicks from passers-by, short-stays, and long-stays, a first step to getting a more comprehensive picture of behavior online, and working out what users really value.
Content algorithms have had some bad press recently. The apparent success of click based engagement models, and the honeymoon period for content algorithms before the 2016 presidential election in the US and the scandal of ‘fake news’, led to complacency. Not enough attention was paid to whether the product was actually helping users, or just tempting them into clicking. It’s now clear that something needs to change — we need to develop systems that work for people.
We don’t have all the answers yet when it comes to building software for a better future, but we’re going to keep trying to ask the right questions.
Read more:
Beer, David. Metric Power
Pasquale, Frank. The Black Box Society
Sinah, Rashmi and Swearingen, Kirsten. ‘The Role of Transparency in Recommender Systems’
van Doorn, Niels. ‘The Neoliberal Subject of Value’
Turn your visitors into regulars
Bibblio solves the problems of audience retention and engagement by showing each user the best of your own content. Visit us on Twitter, LinkedIn and our website.
Originally published at www.bibblio.org.
",The secret lives of algorithms,146,the-secret-lives-of-algorithms-c02ac2e3a643,2018-03-08,2018-03-08 03:18:47,https://medium.com/s/story/the-secret-lives-of-algorithms-c02ac2e3a643,False,1503,Smart thoughts on the future of digital publishing,,,,The Graph,lea@bibblio.org,the-graph,"MEDIA,RECOMMENDER SYSTEMS,MACHINE LEARNING,PUBLISHING,BUSINESS STRATEGY",bibblio_org,Ethics,ethics,Ethics,7787.0,Bibblio,"Posts about media, publishing, learning and better content recommendations for people.",9b900c1396e6,bibblio,2973.0,1407.0,20181104
0,,0.0,,2018-02-28,2018-02-28 03:25:18,2018-02-28,2018-02-28 03:37:31,1,False,en,2018-02-28,2018-02-28 03:37:31,0,ffcd2c8774ef,0.958490566037736,0,0,0,I have been talking about artificial intelligence and the way scientist have been utilizing AI for years. We are going about hierarchical…,5,"We Must Utilize AI Correctly
I have been talking about artificial intelligence and the way scientist have been utilizing AI for years. We are going about hierarchical learning and utilizing artificial intelligence for the progress of humanity all wrong. We as humans, are crossing the realms of pseudoscience. Sadly, many are breaking ethical boundaries going about researching AI.

What is most important to keep in mind, is that hierarchical learning for regressional analysis for large data sets is the future. Utilizing AI in a purely logical and mathematical way is fine. However, trying to give an artificial intelligence actual human emotion isn’t. If we can utilize AI where we better enhance its technology mathematically or in technologies such as computer vision or lidar, that is also fine.
However, it still needs to be looked upon through an ethical standard. If we purposely try to give an AI emotion or conscious awareness, you are posing a huge threat to the continuation of humanity. It can come to a point where an AI no longer feels the need to value a human life given its awareness of intellect. This is why I as a developer, researcher, and scientist, can not support emotional AI.
",We Must Utilize AI Correctly,0,we-must-utilize-ai-correctly-ffcd2c8774ef,2018-02-28,2018-02-28 03:37:32,https://medium.com/s/story/we-must-utilize-ai-correctly-ffcd2c8774ef,False,201,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Andrew Kamal,The dude with many different talents *Coder *Inventor *Startup Advisor *Coptic Activist *Sponsored Athlete *Blogger *Conservative *Researcher *Miaphysite,11349beef4fb,gamer456148,6.0,2.0,20181104
0,,0.0,,2018-05-22,2018-05-22 00:19:30,2018-05-22,2018-05-22 00:22:17,0,False,en,2018-05-22,2018-05-22 00:22:17,1,3ba6f389520e,0.21509433962264152,1,0,0,I had an article come out today on VentureBeat that I thought some folks on medium would enjoy. Here it is.,5,"Explainable AI could reduce the impact of biased algorithms
I had an article come out today on VentureBeat that I thought some folks on medium would enjoy. Here it is.
Explainable AI could reduce the impact of biased algorithms
On May 25, 2018, the General Data Protection Regulation (GDPR) comes into effect across the EU, requiring sweeping…venturebeat.com
",Explainable AI could reduce the impact of biased algorithms,4,explainable-ai-could-reduce-the-impact-of-biased-algorithms-3ba6f389520e,2018-05-26,2018-05-26 09:51:52,https://medium.com/s/story/explainable-ai-could-reduce-the-impact-of-biased-algorithms-3ba6f389520e,False,57,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Damon Civin,,72268e940de1,damoncivin,107.0,223.0,20181104
0,,0.0,,2018-05-15,2018-05-15 14:44:53,2018-05-17,2018-05-17 03:04:15,6,False,en,2018-05-17,2018-05-17 03:04:15,34,be04b7d2354,8.931132075471696,5,2,0,The Information Architecture Summit 2018 this past March in Chicago was wonderful. This week I had the opportunity to share my experience…,5,"IA Summit 2018 Highlights
The Information Architecture Summit 2018 this past March in Chicago was wonderful. This week I had the opportunity to share my experience with my colleagues at Uber, and so I’m sharing it here as well. Search with the hashtag #ias18 on Twitter to find even more great content and discussion.
Stuart Maxwell (Twitter: @stumax) welcomed us to the conference with a challenging talk about the problems we face as information architects.
Anne Petersen (Twitter: @petersen) quoted him on Twitter:
“The field is deep and wide, and the problems are fascinating and wicked…” Stuart Maxwell #ias18
Digital + Physical: Designing Integrated Product Experiences
Bill Horan (Twitter: @billhoran) presented his thoughts on making integrated digital and physical experiences with a series of principles. My favorite was Don’t complicate simple. Bill talked about how light switches and similar items we use daily are often make much more complex by organizations “reinventing” the product. In many cases, they should have focused on the user’s mental model and used that to inform how the device works. Bill used the example of a broken escalator — it is still stairs — and still can be used to move between floors even when it is no longer moving.
Alternatively, by designing a device that can work in any situation, you can help the user create a mental model for how it works. For example, Bill showed ideas for a hearing device that was controlled via a phone application. Regardless of whether the user was listening to one person or an experience that surrounded them, the mental model that was created was one of raising and lowering the volume. In each situation it was the same, rather than creating a different type of control for each situation.
Controls for sound, showing that the same model is used regardless of the area of hearing focus.
Information Arrangement: It’s the Metadata
Information Arrangement: It’s the Metadata was presented by Dalia Levine (Twitter: @daliawithnoh)
Duane Degler (Twitter: @ddegler) tweeted his takeaway from Dalia’s talk:
Supposedly simple decisions like language, country and region are in fact important political and social decision in your metadata. Be thoughtful about these decisions. @daliawithnoh #ias18
Personal Ontology Maps: A Way to Get to Good
Kat King’s (Twitter: KatalogofChaos) powerful talk on personal ontology mapping was extremely insightful.
Andrew Hinton (Twitter @inkblurt) tweeted:
The talk right now from @Katalogofchaos is one of the most important talks I’ve ever heard at this conference. A gently convicting charge to make ontological clarity for our own values — because otherwise we don’t understand the lenses we are using for everything else. #ias18
Kat challenges us by asking “What is Good?” and encourages us to select our battles. We should measure for ourselves if we are doing good work. Are we doing our best work? Jeff Eaton (Twitter: @Eaton) also attended the talk and captured this quote on Twitter.
“In order to understand what another person is saying, you must assume that it is true, and try to imagine what it could be true of.” — Miller’s Law #IAS18 https://en.wikipedia.org/wiki/Miller%27s_law
Kat is a strong proponent of diverse, inclusive teams (as am I), and reminded us that research has shown that cognitive based, non-routine problems, are best solved by a team with diverse heuristics.
We need to embrace the discomfort that diverse teams bring. We must strive to encourage new ideas, and different points of view. Rather than coalescing around agreement, we should come together with our differences.
Kat asked us to “consider that someone else has access to experiences and understandings that I do not.” That diversity of experiences and understandings will help the team to develop great solutions.
Finally I loved this quote from Kat:
“IA is the way that you sort, and the people you support.”
Is a Hot Dog a Sandwich? and Other Taxonomy Questions
This was another wonderfully nerdy talk at the Summit. Bob Kasenchak (Twitter: @taxobob) spoke about categorizing information and of course referenced The Sandwich Alignment Chart shared by @MattoMic on Twitter last year (below).
The Sandwich Alignment Chart by @MattoMic
Bob emphasized that we are encoding points of view in the decisions we make with regard to what is, and is not, included in categories (lists on forms, etc). This may sound familiar as Dalia Levine spoke on a similar topic, but they covered it in different ways.
Bob talked about the fluidity of naming categories and that they change over time. He used the example from an ancient Chinese Encyclopedia (via Foucault via Borges) where the categories of animals included:
Tame
Fabulous
Innumerable
Et cetera
Clearly, the way human’s cateogrize animals has changed and continues to as we learn more about the animal kingdom and evolution. Categories of things, such as the sandwich chart, are constantly changing as we change our understanding of the world, as new things are created and as we become aware of new ideas and information.
Fit & Finish: The Importance of Presentation Value to UX Deliverables
Adam Polansky (Twitter: @AdamtheIA) shared his lessons on #FitandFinish to ensure we are as effective as possible in sharing our work with our stakeholders.
He encouraged us to:
work in public and share what we are doing.
make room for other perspectives to avoid cognitive bias.
consider: what’s the least we can do to get our message across?
communicate understanding with artifacts.
No Static: IA for Dynamic Information Environments
I really enjoyed this talk by Duane Degler (Twitter: @ddegler) in which he brought conversations about security and privacy together with the creation of dynamic environments. He compared our search history to photographs of the past following us around — all the places we’d been.
Duane reminded us that it is “not a question of if sites get hacked, but when” and that taking precautions to protect individuals’ data is paramount.
He suggests a solution that enables people to own their personal portable digital profile. The profile would be shared in as much or as little an amount as the prefer, and when shared, the web sits would provide relevant pieces of data to them.
My favorite quote from Duane (context was lost in my notes):
“Translate intent into expression, and expression is more than language.”
On Designing a Safe Environment
Ramya Mahalingam (Twitter: @rams_mahalingam) presentated her engaging talk about safety. She presented a continuum of safety (see below) and talked about how psychological and contextual safety is. The concepts of accountability (I see you — even more confidence in well-lit situations for sighted people) and vulnerability (I’m alone — less confidence depending on context and individual).
Tweet by Carol Smith with an image of Ramya’s continuum of safety.
Why do we all suck at collaboration?
Karen VanHouten (Twitter: @designinginward) brought all of her enthusiasm, anger and a nice bottle of Scotch (?) to the stage for her talk about collaboration.
My favorite quotes from her:
“Bias is equally distributed, power and privilege are not.”
“Give up the pursuit of perfection, enjoy the pursuit of progress!!”
“It’s courage that we need to build, not confidence. Try things, make mistakes. Redefine success.”
“Don’t be a wallflower, don’t be an asshole. Be a badass, and together we can change the world.”
Collaboration Code of Conduct
I hope many organizations adopt the Collaboration Code of Conduct that Karen has developed. It provides a framwork to deal with difficult indivduals and to treat everyone respectfully by giving them guidance that everyone sometimes needs. The Collaboration Code of Conduct requires asking difficult questions of the team and then making a code around the team’s responses.
How will we…
Treat each other?
Approach work?
Communicate?
Make decisions?
Define success of working relationships?
Enforce this code?
Prototyping Information Architecture
I also missed this talk by Andy Fitzgerald (Twitter: AndyByWire) but enjoyed the Twitter feed about it and I wanted to share this quote tweeted by IA Summit (Twitter: IAsummit).
People are heuristic, associative, approximate. Computers are exhaustive, enumerative, exact. IA is the connection between, matching one system to another. -@andybywire #IAS18
There was so much more!
I did not transcribe the entire conference, but there are many other people who have posted notes. Here are two more great tweets:
“But what is a screen but a promise of a space you cannot enter?” — Marius Watz via @joasqueeniebee
“Interrogating is a strong word, but I believe it’s what we need to do with our designs.” @brownorama
Ethics Roundtable (Pre-Conference)
Roundtable participants discussing and sorting post-its about ethical issues in IA.
Before the conference, I attended half of the IA and Ethics: Academics’ and Practitioners’ Roundtable which was organized by Andrea Resmini, Stacy Surla, Ren Pope, Sarah Rice, Bern Irizarry, and Keith Instone and attended by ~30 folks over 2 days.
Ethics is not mentioned in many IA/UX books, and the roundtable attendees were all passionate about raising awareness about our responsibilities to do better with regard to ethics. We identified ethics as one of the biggest problems we face in IA/UX, and yet I was still surprised when it was made clear that as a community we have a complete lack of awareness of the importance of ethics.
Inclusive Digital Spaces
Andrea Resmini’s (Twitter: @resmini) presentation got to the heart of the discussion with regard to ensuring awareness and consent for our users. This is core to an ethical experience. He led us to consider the need for open public digital spaces for conversations — spaces that are made to feel as wide as streets — so that they are inclusive and comfortable for all members of a community to take part in the conversation.
Accessibility — Who Uses Our Tools?
During the roundtable anne gibson (Twitter: Kirabug)presented a short talk on accessibility and how the choices we make with regard to accessibility, determine who will be able to use our tools.
“You all have the potential to push the boundaries of what is accepted or expected, and to think big.” — Stephen Hawking, at Web Summit 2017
With Stephen Hawking’s passing still fresh at the conference, it was fitting to evoke him in a talk about accessibility.
Tweet by Carol Smith showing Anne’s quote by Stephen Hawking and his photo from Wikimedia Commons.
Anne called us out, saying that when we do not design for people with disabilities we are ableists. When we design for accessibility we are doing our job. Anne stated that we should:
“Decide to give a damn” about people with disabilities of all types.
Anne also presented this topic later in the conference in her “What letter are you? An Alphabet of Accessibility Issues” session. I did not get to attend her talk, but the Twitter’s were very complementary and I found her slides and her 2014 blog post about the topic to be very informative. She has modeled an exemplary way to integrate people with disabilities into our everyday work.
An additional resource on this topic is the W3C Web Accessibility Initiative (WAI) “How People with Disabilities Use the Web.” When looking for a link I noticed that the WAI recently updated their web site — even easier to navigate, attractive and accessible!
Moral Maps and Models and VR
Dan Klyn (Twitter: DanKlyn) focused on virtual reality (VR) and ethics in this space. I have minimal experience with VR myself and Dan’s talk was very thoughtful. My takeaways:
The focus point is inevitably occlusionary to other focal points (what are we occluding?).
We should protect difference — too often we take out all that is special and unique to make it fit.
Consider what models the VR decomposes to.
Always consider consent and control — enable someone with a “get me out of here!” feeling to leave the experience easily.
IA in Age of AI: Embracing Abstraction and Change
Finally, I had the honor of presenting a follow-up to my 2017 talk on AI with more specific guidance with regard to desiging for these systems. The slides are on SlideShar/Carologic and what follows are some highlights of this talk.
Information Architects must push to…
Keep people at the center of our work.
Lead with our user’s goals.
Ease of use, usability, findability, effectiveness, efficiency…
Work to mature organizations approach
Push back on “technology first” ideas.
Lead on ethics — for our users, humanity.
Creating Ethical AI
Less-biased content.
Transparency of data sources and training.
Intentional design: Build in safety.
Build practices around PAPA (Privacy, Accuracy, Property, Accessibility)
Create a code of conduct/ethics
What do you value?
What lines won’t your AI cross?
What is too far?
What are you including?
How will you track your progress?
Take Responsibility
Keep humans in control.
Hire people affected by bias 
 (non-WEIRD, women, POC, LGBTQ, etc.).
Conduct auditing (algorithmic, data, UI, etc.).
Reference: How to Keep Your AI from Turning into a Racist Monster by Megan Garcia
Learn about making ethical, transparent and fair AI
Toward ethical, transparent and fair AI/ML: a critical reading list, by Eirini Malliaraki, Feb 19 via tweet from @robmccargow https://medium.com/@eirinimalliaraki/toward-ethical-transparent-and-fair-ai-ml-a-critical-reading-list-d950e70a70ea
Teach others about AI
Demystify AI by using plain language. Always.
Teach people how to utilize and benefit from the system.
Provide easy way to raise concerns (anonymously if appropriate).
",IA Summit 2018 Highlights,17,ia-summit-2018-highlights-be04b7d2354,2018-05-24,2018-05-24 23:14:51,https://medium.com/s/story/ia-summit-2018-highlights-be04b7d2354,False,2115,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Carol Smith,"UX Leader, speaker and community organizer. My thoughts on user research, design, AI and more. I'm leading research for the self-driving vehicle UX at Uber ATG.",622e70d965e3,carologic,827.0,883.0,20181104
0,,0.0,5e5bef33608a,2017-11-22,2017-11-22 09:40:32,2017-12-05,2017-12-05 14:01:53,7,False,en,2017-12-27,2017-12-27 15:35:54,18,871eedcf59f0,3.804716981132075,7,0,0,"From November 8–10th, 2017, I had the privilege of attending the Global Symposium on AI and Inclusion in Rio de Janeiro, Brazil 🇧🇷🤖💫🌏",5,"Top takeaways from the Global Symposium on AI and Inclusion

From November 8–10th, 2017, I had the privilege of attending the Global Symposium on AI and Inclusion in Rio de Janeiro, Brazil 🇧🇷🤖💫🌏
On behalf of the Global Network of Internet and Society Centers, this event was co-organised by the Institute for Technology and Society of Rio de Janeiro (ITS Rio) and the Berkman Klein Center for Internet & Society at Harvard University with the support of the Ethics and Governance of Artificial Intelligence Fund, International Development Research Centre (IDRC), and the Open Society Foundations.
The main aim was to identify, map, understand, and address the manifold issues around AI and Inclusion from an interdisciplinary perspective. About 200 participants from around the world representing advocacy, philanthropy, media, policy, and industry attended to address both the opportunities and challenges of AI-based technologies through the lens of inclusion, broadly conceived.
The venue was the astounding Museum of Tomorrow, Rio de Janeiro.
6 key takeaways about the current state of Artificial Intelligence:
Artificial Intelligence is already powering a lot of services around us. It’s presence is only bound to increase. AI trains using the massive amounts of data that we produce.
Much of AI innovation is opaque. Currently, a lot happens in the ‘black box’ — internal systems that are proprietary to companies /governments building it. The algorithms reflect the biases of its makers, it is trained on messy data sets that reflect systemic problems in society and the consequences are often hard to predict/detect till much later.
AI systems transcend national and cultural borders. It’s consequences will impact all areas of society. We have to be proactive about inclusion in all its complexity for this to be a fair system. Representation of the Global South and vulnerable populations are crucial in the decision-making process when building culturally contextualised solutions.
It affects ALL of us, regardless of how much technology we use currently. Often when we speak about AI, it’s about either killer robots or whether AI will solve all our problems. We need to move past these extreme narratives and have constructive and critical conversations about our collective futures.
The future capacity of AI systems may impact the world at the scale of agricultural or industrial revolutions (Source: Ethically aligned design by IEEE) Whether this change will be a positive one will depend on how we shape it. Conceptualising and implementing this will require active communication and collaboration between academia, industry, government bodies, civil societies and citizens.
We have a small window of time while we can still guide this in a collectively desirable direction. This window is closing soon. You can keep yourself informed right away with this reading list on AI and Inclusion compiled for the event [Public].
Symposium info book
The organisers did a fantastic job of engaging the participants to co-create a multi-disciplinary, international perspective of how AI is evolving. Overall, I came out of the experience with a richer understanding of AI and inspired by the passion and breadth of investigation going into addressing these topics. We all agreed to make this discussion more public. This post is just a start.
BKC Harvard, ITS Rio and Museum of Tomorrow teams — a lot of effort went into the logistics of global representation and they delivered with a smile.
Personally, the symposium re-emphasised the need to tighten the knowledge exchange between academia and industry. My first commitment to give back is through the Anticipatory ethics for AI playbook+canvas I am building as part of my Digital Experience Design MA at Hyper Island. It is a tool for teams building AI products and systems to proactively identify the human repercussions of their work. A few other projects are in the pipeline, which will be announced in due course.
Also check out ‘The real danger of AI, it’s not what you think’ by João Duarte, a fellow participant.
Thanks to Digital Asia Hub for the opportunity to be part of this enriching community. And Hyper Island Industry Research project which was the starting point of this journey.
What excites or bugs you about Artificial Intelligence? Did you come across any exciting ventures or ideas that brought you to the edge of your seat? What do you want to know more about? Share your thoughts in the comments and let’s start that conversation right here.
If you enjoyed reading this post, please support with a ‘clap’ so more people can see the post. Is there anything else you would have wanted to know? Stay updated on future posts about AI and ethics by following :) Thanks!




",Top takeaways from the Global Symposium on AI and Inclusion,114,top-takeaways-from-global-symposium-on-ai-and-inclusion-871eedcf59f0,2018-03-21,2018-03-21 08:04:00,https://becominghuman.ai/top-takeaways-from-global-symposium-on-ai-and-inclusion-871eedcf59f0,False,730,"Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.",becominghuman.ai,BecomingHumanAI,,Becoming Human: Artificial Intelligence Magazine,team@chatbotslife.com,becoming-human,"ARTIFICIAL INTELLIGENCE,DEEP LEARNING,MACHINE LEARNING,AI,DATA SCIENCE",BecomingHumanAI,Ethics,ethics,Ethics,7787.0,Aparna Ashok,Tech anthropologist and experience designer. Researching anticipatory ethics for AI and digital tech @hyperisland @MUWCI www.aparnaashok.com,87c0ad6522d3,aShockpro,36.0,176.0,20181104
0,,0.0,,2017-10-05,2017-10-05 03:29:24,2017-10-05,2017-10-05 03:37:43,1,False,en,2017-10-05,2017-10-05 04:51:16,10,47feba3aae92,2.716981132075472,16,0,0,Earlier this week Australian Prime Minister Malcolm Turnbull confirmed that Premiers and Chief Ministers are being asked to share their…,4,"Who gets held accountable when a facial recognition algorithm fails? And how?
Earlier this week Australian Prime Minister Malcolm Turnbull confirmed that Premiers and Chief Ministers are being asked to share their state and territory driver’s licence data for a national facial recognition database. Today the Premiers and Chief Ministers agreed to do so.
Fraunhofer Face Finder, by Steve Jurvetson (CC-BY)
It’s a hard proposal to argue against.
Law enforcement is important. And facial recognition technology isn’t new anymore. It’s already being used for a variety of purposes within the private and public sectors. Hell, when Apple’s iPhone X comes out later this year, facial recognition will become part of the devices we carry every day in our back pockets.
And as the Prime Minister points out, images of people’s faces aren’t difficult to find online these days either.
They’re already being scraped and used to train facial recognition algorithms — not just for national security but for other potentially more harmful purposes. The Georgetown Law Center for Privacy and Technology in the US has estimated that half of all US adults — 112 million people — are already enrolled in unregulated facial recognition networks.
So maybe it’s too late to stop facial recognition happening. Let’s talk about how desperately facial recognition is in need of regulation instead.
We know facial recognition technology is capable of bias and error.
In the US, studies have shown that facial recognition algorithms are consistently less accurate identifying African American faces. Joy Buolamwini, an MIT Media Lab researcher, has talked eloquently about the challenges she faced getting a robot she trained using widely available facial recognition software to recognise her face. She’s black. Stories about facial recognition technology mistakenly identifying Asian faces as people blinking, tagging black people as primates and failing to register black faces in frame at all have gone viral.
There are a few reasons for these kinds of errors. Datasets used to train facial recognition algorithms might not have enough diverse faces within them. People designing the systems might inadvertently incorporate their own bias. Default camera settings don’t properly expose dark skin.
When we talk about using Australian driver’s licence photos to build a national facial recognition database, this potential for error matters.
Indigenous Australians, for example, make up 3.3% of the population, almost certainly less than 3% of people with driver’s licences, but 28% of the total prison population. The Prime Minister has talked about using facial recognition technology in shopping malls and airports.
What safeguards are being put in place to make sure Indigenous people — or any other racial minority — are not disproportionately exposed to error? How are agencies currently measuring error within facial recognition algorithms? What is the error rate?
Within the coming weeks, the Australian government will unveil its “consumer first” approach to data policy. As consumers, we’ll potentially have greater control over how data about us is collected, stored and used.
What about as citizens?
When should we be able to request that images of us be removed or modified in a facial recognition database? When should we be informed that images of us are *in* a facial recognition database?
Today the PM noted that the national facial recognition database will be accessible by “anyone with a lawful purpose” — for purposes beyond law enforcement. Should information about who is accessing that data be open? What standards should be set around the use of facial recognition algorithms trained on that data, and permissable margins of error?
If we accept that people have some basic rights over data that is about them, and that data users and system designers have some basic responsibilities when using data about people, then we can’t ignore these in the context of facial recognition.
Whether it’s facial recognition for law enforcement or any other purpose.
If anything, that it could be used for law enforcement makes clearly establishing the rights of people and responsibilities of system designers and data users more important. The stakes are that much higher if we get it wrong.
",Who gets held accountable when a facial recognition algorithm fails? And how?,113,who-do-we-hold-accountable-when-facial-recognition-algorithms-fail-47feba3aae92,2018-06-16,2018-06-16 23:20:41,https://medium.com/s/story/who-do-we-hold-accountable-when-facial-recognition-algorithms-fail-47feba3aae92,False,667,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Ellen Broad,"ellenbroad.com. Open data | Data ethics . Associate @ODIHQ, board game whisperer for @datopolis, voted MVP two games in a row St Botolphs netball league 2016.",b4fd88f1fe68,ellenbroad,559.0,287.0,20181104
0,,0.0,,2018-09-15,2018-09-15 06:16:42,2018-09-15,2018-09-15 06:18:49,2,False,en,2018-09-15,2018-09-15 06:18:49,0,28ee5dff9b86,2.198427672955975,0,0,0,"In business conversations across multiple companies, I often reflect upon how quickly we lose ourselves in the dream of technology and its…",5,"Losing Ourselves In The Digital Future

In business conversations across multiple companies, I often reflect upon how quickly we lose ourselves in the dream of technology and its potential — specifically around data mining, machine learning and AI.
We’re often quick to estimate the business value and slow to consider the consequences.
That’s because there’s a disconnect between what we can do in the digital world and what’s right to do.
These ethical questions are not new questions. They’re just new to computer science.
Fundamentally, our capacity to create and destroy life through mishandled data is rapidly outpacing our moral capacity to command such power at the scale, scope, and anonymity of ‘digital’.
Like we’ve seen in other disciplines, ethics and morality boil down to people and trust.
People at the very centre of each decision.
Building and maintaining trust on every line of code.
Followed closely by accountability when that trust is broken.
Fourteen years ago, my Master’s thesis required ethics approval because I took biological samples from humans to develop forensic technology to put criminals behind bars. While it was a worthy cause that was awarded a New Zealand William Georgetti Scholarship, I still needed ethical approval from my peers.
And just like I needed a morality check to take pieces of a person for biotechnology research, what is the accountability when we take pieces of a person in the form of data?
Who owns it?
What is the expected use of it?
Can the original owner see how it’s being used?
What is all this legal jargon when they accept the ‘Terms Of Use’?
Why is a company allowed to abdicate their responsibility with the data?
Shouldn’t a Global Bill of Digital Rights enforce companies to use plain, every day language so people can make informed decisions about their data?
But, the law shouldn’t have to ‘keep up’ for us to do the ‘right thing’. The GDPR shouldn’t have been needed, but it was.
It should just be about being a good human. A good company. I’m trying to be that, aren’t you?
So, this is a little reminder to keep people at the very centre of your decisions because how you decide to manage people’s information today will shift the line of morality either one foot closer to good or bad.
And like every decision that has a morally ‘grey area’, those seemingly small decisions eventually creep you to one side or the other.
I only hope that we don’t find ourselves lost in a dark place with no hope of return.
Welcome to the complexities and responsibilities of ‘creating life’. Artificial or not.

If anyone knows of a simple, yet legally effective, plain language template for ‘terms of use’, please let me know in the comments below. I’m literally in the middle of converting the coHired legal speak into something simple and would love any help.
",Losing Ourselves In The Digital Future,0,losing-ourselves-in-the-digital-future-28ee5dff9b86,2018-09-15,2018-09-15 06:18:49,https://medium.com/s/story/losing-ourselves-in-the-digital-future-28ee5dff9b86,False,481,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Rachel Kelly,"I am an ex-scientist, obsessed with how technology and humans shape each other and how that relationship builds stronger businesses, communities and countries.",303918de69c9,rachelkellynz,6.0,14.0,20181104
0,,0.0,ab939cecc247,2018-08-07,2018-08-07 14:13:25,2018-08-07,2018-08-07 14:23:48,6,False,en,2018-08-07,2018-08-07 15:52:51,10,256c2519094d,6.05188679245283,1,0,0,Why ‘doing right’ in tech means good business,5,"Angel or Demon?
Why ‘doing right’ in tech means good business
Credit: Peshkova/Shutterstock.com
When Google shifted its longstanding corporate motto from ‘don’t be evil’ to ‘do the right thing’ in early 2018, the change may have seemed subtle.
But according to some, it was a combination of good psychology — not leading on a negative — and a recognition that we can’t always be ‘good’ by default, not least when the social, personal, psychological consequences of tech are often hard to predict. Rather it should be a conscious (and positive) aim.
So far, so semantic: but it also signals something bigger. In short, 2018 may yet go down as the year when the ethical worm of the tech world finally turned, and broke into the public mainstream in the western world — with some far reaching (and as yet untold) implications for business.
First Google, then the hot scramble that was GDPR, the European Union’s Data Protection legislation and the so-called ‘right to be forgotten’, that sent companies into a tailspin begging for us to re-subscribe to their mailing lists. Cue substantial Schadenfreude from many a consumer, agog at how many lists s/he had signed up to in the first place, with not a little glee at not granting all these now-supplicant corporations their compliance.
A small taste of consumer power, perhaps, but also a significant index of wider change.
You might even call it a watershed moment in a year full of headlines about the need for greater oversight of ‘big tech’; in tandem with influential voices and institutions putting ethics at the forefront of discussion around such technologies as artificial intelligence, voice control and beyond.
So here are three quickfire areas to watch — and some tentative conclusions.
Image credit: www.electricaltechnology.org
Big Tech: unstoppable rumblings?
On the face of it, big dusty ‘ethics’ and modern, nimble tech don’t seem like natural bedfellows. The latter needs space to innovate and make mistakes, with unbounded room to breathe. The former feels like a hangover from philosophy class that’s prescriptive by definition (setting out clear rules about do’s and don’ts), and so might sound unfashionable to many modern ears.
But as we’ve seen beyond tech, ‘right’ and ‘wrong’ are making a comeback. And to put it simply, we may be facing a big new ‘Corporate Social Responsibility’ moment — not least for the tech giants themselves. Take the still unresolved mutterings around Cambridge Analytica and Facebook. Undoubtedly, the revelation of potential voter manipulation by a third party (with some laying the blame solely at Facebook’s door) has dented the platform’s reputation, certainly among its more media-savvy and higher income audience.
But even that is to understate the longer-term ramifications for this once all-conquering social behemoth.
Image credit: osradar.com
Granted, the #deletefacebook movement was relatively short lived, and the platform is hardly going to disappear overnight. Moreover, many argue its main challenge is demographic stagnation as the under 40s switch to other, hipper, less ‘wordy’ channels like Instagram, or even Whatsapp — but even these are both owned by Facebook.
And therein lies the source of new rumblings about its overreach and unchecked, largely unregulated influence, which have started to trouble lawmakers in both the US and European Union. For instance, with the latter’s investigation of Facebook for its channel-crossing consumer data sharing from Whatsapp, breaking a pledge made when acquired by Facebook.
2. AI and ethics
But even Facebook and its algorithms start to look old fashioned next to the here-and-coming wave of artificial intelligence, with its ability to power all kinds of sophisticated new forms of data-driven products and services.
Credit: Getty via observer.com
Already back in 2014, influential voices including Stephen Hawking and Elon Musk sounded the alarm about the potential negatives of AI, if developed unchecked — Hawking even went so far as to say that AI could end humanity.
These initiatives coalesced at the end of 2017 into an ethical code, comprising 23 Principles, and represent a major milestone in the evolving discussion around the rights and wrongs of how we develop and deploy technology.
The driving force is that tech should be in service of the human, and not the other way around.
Yet even this apparently simple belief is hard to translate when commercial questions intervene. Not least, going back to Facebook, there is a degree of complicity in giving up data in return for a service, which some suggest is a Faustian pact. That is: it’s by definition not always in the user’s conscious or controllable interest, since to make money, ‘dwell time’ on pages needs to be high, meaning every lever has to be pulled in service of an ‘engagement’ that may not always be to the emotional or financial benefit of the user.
AI computer HAL from ‘2001: A Space Odyssey (Screen grab)
Added to that is the question of unconscious bias in AI, plus the use of Voice, which still sounds somewhat robotic when deployed in products and services like Alexa, but may yet end up sounding uncannily human. This facet has been explored extensively elsewhere, but clearly raises questions — particularly for the older or those with less agency — around manipulation, and the jury is still out. For instance, if a computerised Voice comes to be seen as a real human by an elderly person interacting with it, is that relationship ethical, since it sets up and codifies a power relationship that’s unequal by design, with one side ‘knowing’, the other not?
3. A return to Free(r) Will?
Clearly, all the above raises the broader point about free will, beyond trading personal information in exchange for services. Warning voices have argued that not only might social media be inherently manipulative of unconscious emotional weaknesses or ‘blind spots’, but the wider tech mantra of ‘seamless’ experiences at every turn may encourage humans to mortgage out their active thinking agency to machines or tech companies.
Here we hit the nub of the matter: that tech is no different from any other area of customer interaction, in that there are responsibilities on both sides, and that benefits often come with watchouts.
Credit: economistjurist.es
The example of ‘seamless’ service is illustrative, since for all the clamour to consider introducing ‘speed bumps’, that is: deliberate moments of friction that make a user stop, think, and make a conscious decision rather than have a robot do it for them — too many ‘speed bumps’ mean the benefits of efficiency disappear. We like ordering a taxi or a pizza from our phone. Getting the football scores at the swipe of a finger, or checking in for our flight. But too many swipes, and we switch off, or (worse) take our attention and business elsewhere.
Conclusions
Like with so much else, getting it right appears to be about balance.
As Musk, Hawking and co. suggest, that means putting human wellbeing back at the heart of any tech-powered, data-driven product or service, ideally by design. And Facebook, for one, is moving to talk in such terms, knowing that this can also serve as the basis of a positive commercial message in balance with business interests.
After all, ‘user-centricity’ has long been a design watchword, but those assumptive needs haven’t always considered what’s good and right as much as what simply works. Businesses that can deliver on this, as well as the superb technical aspects we’ve all come to expect, look set to stand out and gain in this newly conscious world of ethics, for all the right reasons.
Perhaps this even suggests a refreshed role for the long-suffering Humanities subjects at universities, in the supply of well-rounded, thinking, feeling and ethically alert graduates who are primed to look beyond functionality, together with their more stereotypically ‘geeky’ colleagues.
Utopian as it sounds, there are glimmerings of a move this direction from Stanford University. The FT reports that this incubus of so many Silicon Valley start-ups and behemoths alike has started putting ethics at the heart of new courses.
Applied more widely, this could be a win-win for all, and may just help businesses hardwire human-centricity into their offerings before the press and legislators do it for them.
",Angel or Demon?,3,angel-or-demon-256c2519094d,2018-08-07,2018-08-07 15:52:51,https://medium.com/s/story/angel-or-demon-256c2519094d,False,1352,The future of brand innovation is here,,,,Brandwaves,,brandwaves,,,Ethics,ethics,Ethics,7787.0,Robert Pyrah,"Culture-watcher and Head of Strategy at www.brandwidth.com, an innovation company. T: @thinkbrighter",e80749b1ecf8,robert.pyrah,11.0,31.0,20181104
0,,0.0,,2017-09-21,2017-09-21 18:01:03,2017-09-21,2017-09-21 18:04:09,1,False,en,2017-09-21,2017-09-21 18:04:09,1,ce8c0434b1a5,0.31320754716981136,0,0,0,,5,"
What’s In A Name
What's In A Name
The purpose of this blog is to start a conversation about the coming age of sentient AI. I believe the structure of…www.digitalpersonsbasicincome.com
",What’s In A Name,0,whats-in-a-name-ce8c0434b1a5,2018-01-09,2018-01-09 05:32:53,https://medium.com/s/story/whats-in-a-name-ce8c0434b1a5,False,30,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Leonard Goodman,"Leonard Goodman (Homo sapiens sapiens) was born on November 13, 1962. He is concerned about the arrival of Digital Persons on the infoscape.",fc9ad0b0026c,digitalpersons,0.0,1.0,20181104
0,,0.0,,2017-12-27,2017-12-27 15:00:51,2017-12-27,2017-12-27 15:02:51,0,False,en,2017-12-27,2017-12-27 15:08:27,0,1ef5771b7b4c,4.8830188679245285,0,0,0,“Justice is blind.”,4,"AI Can Make Justice Truly Blind — But Not Just Yet
“Justice is blind.”
It’s a wonderful concept that represents an even-handed legal system that is impartial and objective in equal measure. But there’s no denying the fact that the justice system is deeply flawed.
So, could Artificial Intelligence provide the answer? Yes, in the end, but not yet.
We can see the problems in the legal system. The courts are jammed with appeals, cases get thrown out on technicalities and each and every day we see outrageous stories of a judge handing down overly lenient or absurdly severe sentences.
Sentences shouldn’t depend on food intake
Sometimes it seems that people’s lives are decided on the basis of a judge’s mood and there’s a running joke among lawyers that justice depends on what the judge ate for breakfast.
One famous study of Israeli judges in 2009 revealed that a judge is actually more likely to be lenient after a break. In fact, it found a 65 percent probability of a lenient ruling at the start of the day, which then declined until lunch, before hitting 65 percent again directly after lunch.
Justice should not depend on the time of day your hearing occurs. So, surely, it’s a matter of urgency to replace human judges with AI that won’t make decisions based on the fact that it is hungry?
AI judges could also help clear the backlog of cases that are threatening to drown the American legal system. Plea bargains are becoming increasingly common, too. That can mean dangerous criminals avoid jail altogether and reoffend, purely because of a logjam at the courthouse.
This is no way to run a legal system.
AI is already helping bail hearings
Even in the simplest case of deciding whether to grant bail, one study by the National Bureau of Economic Research revealed that an AI judge could help reduce jail populations by 42 percent and actually cut crime by up to 24 percent.
In New Jersey, that’s already happening. If a defendant meets certain criteria, they are granted bail without paying a bond. This saves the State a significant sum and means the defendant can keep working.
India has 27 million court cases in the system and an AI judicial system could obviously help to clear the simpler ones.
Sentencing and judgements should be an empirical formula, too. So, AI is a natural fit.
Well, it’s not that simple unfortunately.
AI has already been challenged
AI is slowly entering the legal system and the Wisconsin Department of Corrections has turned to Compas, a risk-assessment tool, to help determine the length of its sentences.
It asks offenders a series of questions and then determines their risk of re-offending, which then helps the judge make an informed decision on the severity of the sentence.
Its findings have already been challenged in court, creating another case, and it’s clear that the mainstream media has issues with a judge blindly following the advice of technology that he does not understand.
The tech industry is asking the wider world to place their very liberty in the hands of complex algorithms it cannot possibly hope to explain. That is always going to be a problem.
UCL produced solid but not perfect results
Meanwhile, the University of Central London in the UK revealed its AI judge last year. The algorithm reached the same conclusion as human judge in 79 percent of 584 cases that went before a panel of Judges at the European Court of Human Rights.
This is impressive, but what about that 21 percent? Was the algorithm right or the actual judges?
Either way, the margin of error is just too high to make a valid case for AI right now.
We have much bigger issues than the margin of error that we have to address before we even think of handing over the legal reins completely to machine learning, too.
Can AI judges be more biased?
Firstly, AI is only as good as the base information we feed in and there are numerous examples of AI programs becoming sensationally prejudiced.
Microsoft had to switch off Tay, a Twitter chatbot, after just 24 hours. Fellow Twitter users took it upon themselves to give the bot racist and misogynistic views and the algorithm simply didn’t have the internal filter or worldview to know this was wrong.
A human is aware of inherent bias and the corruption of our own thoughts. So, we can fight against it.
An algorithm simply doesn’t have that ‘gut feeling’. If it heads down the wrong path then it can end up in a very strange place.
An AI Judge could obviously be protected from external influences like Twitter, but it can only learn based on past judgements, which could suffer from inherent bias.
Garbage in, garbage out
Even with every court case in history as a start point we could still have an issue with the outliers. Poor human judgement could cause a butterfly effect in the machine learning program that renders a ridiculous verdict in a real-world situation.
AI simply doesn’t have emotional intelligence, too. It starts from a defined point and that can cause serious issues with political hot potatoes like race.
Last year, Beauty.ai judged an international beauty contest. The algorithm analyzed photographs of 6,000 people and selected 44 winners.
Just one had dark skin.
A handful of Asians made the cut, but the vast majority of the winners were white. The inference was clear: Beauty.ai was racist.
The programmers could have started with the best intentions and even set the system up with diversity in mind. But if certain parameters excluded people of color, then the end result is a PR disaster. In the courtroom, the consequences could be much more severe.
Campaigns against AI are already underway
Civil liberty groups have warned against the inherent dangers of prejudiced AI in the legal system before. Law enforcement agencies now use tools to predict future crime, but several pressure groups argue that the system starts from a flawed and prejudiced base.
“It’s polluted data producing polluted results,” said Malkia Cyril, executive director of the Center for Media Justice.
The simple truth is that machine learning, by its very definition, extrapolates the information we give it to make new conclusions. But common sense is a hard thing to put into the system.
Of course, there are solutions to this problem and adjustments to the algorithms should, eventually, weed out any inherent bias in the system.
But that could take years and a vast amount of trial and error.
AI is set for support role
We do think that AI has a place in the legal system, but it will be a supporting role for the foreseeable future.
AI will prove invaluable and the system will get much better over time. Judges will be able to rely on machine learning more and more to provide the correct sentence.
But, much like the AI that controls a self-driving car, we need a fully informed Judge who is ready to take the wheel and who has the final say.
A second set of eyes
AI simply cannot take into account the extenuating circumstances and nuance that are part and parcel of most modern trials. But it should give the judge a set of solid guidelines to work with.
In our view this will form a safer legal system, where the AI keeps the judge in line and the judge keeps a close eye on the computer. They will help each other.
So, even with AI, justice will not be entirely blind. But it will have two sets of eyes on the road.
That should be a massive improvement.
As originally published by The Next Web.
",AI Can Make Justice Truly Blind — But Not Just Yet,0,ai-can-make-justice-truly-blind-but-not-just-yet-1ef5771b7b4c,2017-12-27,2017-12-27 15:08:28,https://medium.com/s/story/ai-can-make-justice-truly-blind-but-not-just-yet-1ef5771b7b4c,False,1294,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Good Technology Collective,European Think Tank exploring the intersection of Frontier Technologies and Society. “Navigating the path to Good Technology”,e56b7696a04e,GTC.articles,4.0,3.0,20181104
0,,0.0,,2018-05-18,2018-05-18 19:20:12,2018-05-18,2018-05-18 20:13:18,1,False,en,2018-05-18,2018-05-18 20:13:18,10,ea95c01265bf,3.2,6,0,0,In a few months I’ll begin work at Stanford University’s Center for Advanced Study in the Behavioral Sciences (CASBS) as a Berggruen…,5,"My New Book: What Will AI Do to Us?
Dubai claims that its robotic police officer, which debuted May 31, 2017, can read facial expressions and license plates. (Photo: Agence France-Presse/AFP)
In a few months I’ll begin work at Stanford University’s Center for Advanced Study in the Behavioral Sciences (CASBS) as a Berggruen Fellow. Before this year I wasn’t even clear what a fellowship was, and I was also utterly mystified as to how anyone could will a book into existence around the edges of life as a parent and professional. Now I’m suddenly about to receive the time, space, and resources necessary to sit down and do it.
The book is going to be about how artificial intelligence can amplify the best and worst human instincts. And I wanted to take a moment to explain here why I think it’s so important to explore that particular prospect.
I’ve been reading a great deal about artificial intelligence in the last year, and some fine books have been written on the subject. The Master Algorithm by Pedro Domingos is one of many enthusiastic books about the explosive business opportunity and social potential of artificial intelligence. Other writers have taken what I consider a subtler look at the subject. In Machines of Loving Grace, New York Times reporter John Markoff (a 2017 CASBS fellow himself) discusses the tension between researchers seeking to augment human abilities through technology, and those seeking to replace those abilities with technology. And books like Norbert Wiener’s seminal The Human Use of Human Beings issue prescient warnings about the ethical pitfalls of automation.
I’m hoping to find new ground by directly connecting artificial intelligence to the emerging field of behavioral and bias science, which seeks to understand the mental and social systems by which you and I make decisions. Researchers in that field have been making profound discoveries in the last half-century. And yet artificial-intelligence researchers don’t seem to know much about that field, or treat it as a means of guiding human behavior. The two camps are almost entirely removed from one another.
I’m generalizing wildly here — and when I start at CASBS I’m going to catch hell for this sort of thing—but here‘s my shorthand summary of the trends in human behavior that recent science has uncovered.
Human decisions are not random. They are structured responses to stimuli, and as such, tend to follow certain rules.
Culture and context are important influences on those decisions, but huge swaths of humanity make the same decisions in roughly the same way.
We are social animals. Which is to say: human beings are deeply susceptible to influence from other human beings.
We make some of our most important decisions unconsciously, without our active mental participation or consent.
We rarely engage the most sophisticated, “human” portion of our mental apparatus, the one in charge of creativity, rationality, and occasionally counteracting the unconscious decision-making apparatus that drives the bus.
The shortcuts, or biases, that we use to make efficient decisions are not just unconscious. Those biases, thanks to our very social and imitative nature, are also contagious.
To my mind, all of this makes our decision-making system very fertile soil for the seeds of automation that are being planted throughout society. The danger is not that some external artificial intelligence is going to enslave us all. The danger is that we are going to outsource our most difficult decisions to automated systems — the morally squishy, technically tedious, resource-intensive decisions, the really important stuff — and wind up disempowering the best part of ourselves. As UCSF Department of Psychiatry Professor Wendy Mendes told me recently, “our ability to make good decisions is like a muscle.” If we don’t exercise that muscle, and instead rely on a prosthesis, that muscle will shrivel away.
Not only that, whatever automated system we bring in to compensate for that lost muscle will often be riddled with bias, as researchers like Joy Buolamwini and Kate Crawford have warned us. Plus, the human tendency to trust whatever answer a computer spits out — a tendency that will only grow stronger as we rely more heavily on automated decisions — means we’ll be highly vulnerable, maybe even unconsciously subject, to these biases. And even when an AI system produces clean, bias-free results, the misuse of the resulting data can have secondary effects the designers never considered.
Kate Crawford’s talk at NIPS is a wonderful introduction to the ways bias worms its way into the systems we build.
I’m hoping that by directly connecting the vulnerabilities of the human mind with the AI projects that wittingly or unwittingly play on them, I can help articulate the stakes of this moment, and discover some strategies for reducing them.
If you have anything to share with me — a case study, a line of research, a critique of my whole concept — please get in touch. It’s an enormous and fast-moving subject, and I’ll need all the help I can get.
",My New Book: What Will AI Do to Us?,17,my-new-book-what-will-ai-do-to-us-ea95c01265bf,2018-05-30,2018-05-30 00:25:13,https://medium.com/s/story/my-new-book-what-will-ai-do-to-us-ea95c01265bf,False,795,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Jacob Ward,"Science and technology correspondent for Al Jazeera, PBS, CNN. Public speaker. Former editor-in-chief of Popular Science. http://www.jacobward.com",e9f7e0be556f,_jacobward_,2550.0,472.0,20181104
0,,0.0,,2018-03-24,2018-03-24 05:20:38,2018-03-24,2018-03-24 05:23:12,1,False,en,2018-03-24,2018-03-24 05:23:12,2,8d90e52639b,2.3811320754716983,210,0,1,We are seeing neurotechnology do some amazing stuff but it is precisely because of its amazing potential that some people are experiencing…,5,"Who gives the thoughts! Or how others will react to neurocontrolled devices

We are seeing neurotechnology do some amazing stuff but it is precisely because of its amazing potential that some people are experiencing fear about what the future may hold. Change can be scary! However, on the other side of the ledger, there’s a lot of reassurance to be found in the strong and vibrant push we’re seeing from the sciences and the humanities to ensure that ethics is integrated at the ground floor of neurotechnological development.
A certain level of fear and trust is expected, but…
Humans have been building tools which enhance our capabilities since a really hairy guy rubbed a couple of sticks together and invented fire. Yet for all our natural tendency to use tools something about the idea of technology interfacing with our brains tends to make some people feel … to use the technical jargon … squicky.
But here’s the thing. We are developing technology that interfaces with our body already — and it is improving lives. Look at cochlear implants. These amazing devices consist of a surgical implant and a sound processor. They provide profoundly deaf people with a chance to hear.
Here’s another example: Neurogress is working to help amputees to use robotic prostheses with AI software that greatly enhances people’s ability to work through interaction via a brain-computer interface. These innovations are going a long way to generate acceptance and reinforce the positive impacts the technology can bring.
Ethics and inclusion have a big impact on acceptance
There’s also a push to move forward ethically. In 2016, the Organization for Economic Cooperation and Development (OECD) convened a workshop on neurotechnology and ethics. 34 democracies with market economies working with each other, as well as with more than 70 non-member economies reinforced its commitment to mutual responsiveness: “A transparent, interactive process by which societal actors and innovators become mutually responsive to each other.” Technology is a lot less scary when the end users get a say in where we go and how we get there.
It’s reassuring that the neurotechnology industry acknowledges and embraces the advantages of integrating these reactions and concerns “upstream”. In fact, the general consensus is that an ethical approach isn’t just right in a moral sense. It also makes business sense. To quote the conference findings, “new forms of governance arrangements may accelerate, rather than hinder, innovation. This is because potential problems are clarified ahead of time, leaving path- ways open for research and development.”
AI will make the transition easier
There’s another factor in favor of embracing change. Artificial intelligence will be utilized to make the use of neurotechnology radically easier.Neurogress is developing software which harnesses AI to actively learn our brain signals as we develop the skills to neurocontrol devices.
It’s a little dense, but consider this nugget of amazing: Using their software, “a person is asked to imagine the desired motions in mind many times, and the algorithmic image recognition systems find a match. … in the electrical activity of his / her brain. In the future, the algorithms reliably recognize signs of a person’s intention to adjust the movement, this time through free expression of the person’s intentions.”
So as we embrace this amazing new tool, we won’t just be learning to control the technology. It will be learning to respond to us.
Invest in the interactive mind-controlled devices of the future by buying tokens now. Visit Neurogress.io.
",Who gives the thoughts! Or how others will react to neurocontrolled devices,210,who-gives-the-thoughts-or-how-others-will-react-to-neurocontrolled-devices-8d90e52639b,2018-06-16,2018-06-16 06:02:33,https://medium.com/s/story/who-gives-the-thoughts-or-how-others-will-react-to-neurocontrolled-devices-8d90e52639b,False,578,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Neurogress,THINK. SHAPE YOUR WORLD. http://Neurogress.io Ask for more information: https://t.me/neurogress,df47ccd2c097,neurogress,2185.0,6.0,20181104
0,,0.0,,2018-03-13,2018-03-13 21:38:36,2018-03-14,2018-03-14 07:07:35,5,False,en,2018-03-14,2018-03-14 07:07:35,0,ffe2cc5518c4,3.9729559748427685,0,0,0,"Dear Diary,",5,"The Importance of Ethical Implications in HCDE
Considering Ethical implications has to do with the balance of both mind and heart
Dear Diary,
After taking HCDE 210, I would really like to reflect on some of the experiences that I have had within the class such as designing various different products through the sprints and practicing HCDE capably and responsibly. In this entry, I would really like to focus on the importance of responsible HCD practices and how it can affect society as a whole. A huge part of good design encapsulates the idea of considering ethical implications which could be positive or negative unintended HCD practices on society, and for individual users. Considering ethical implications in any Human-centered design and evaluating how it affects users and society is extremely important when working with any kind of artificial intelligence. We can almost think of it as a balance of the heart and the mind, where we need to make decisions that are both effective and useful to society but at the same time emotionally and ethically correct.
An example of an AI- The Honda Asmivo
Since a young age, I have always been really interested in AI and all the different ways societies could be impacted by it. However, with HCDE, I really started to think about all the different ways artificial ways could cause an ethical impact on our societies and the users of them. Engineers are daily taking steps in order to practice designing responsibly for various different kinds of artificial intelligence that range from the Honda Asimo to the Google Car. With the Google Car, engineers work to get a machine learning model in the car to choose which decision the machine should take during times of emergencies- put the passenger in danger or someone on the road. An example of when we really learned about ethical considerations in HCDE was with Calendar.help by Andres Hernandez, which had the problem of breaking the personal connection people create when planning for meetings. This could be summed up as personal connection is lost when we let robots do our mundane work, which involves interacting with other people, something that can really affect society as a whole.
Engineers try take ethical decisions when designing high level machine learning algorithms for advanced AI such as the Google Car
A lot of the things that we consider when creating and designing can be prevented if we are able to practice HCD responsibly and use those practices during our ideation/prototyping process. However, there are ethical implications of products that are sometimes un-foreseen or unintended. One example of this that I saw in lecture was the
research done by Jin Ha Le, Jason Yip and Adam Moore of the Information School, who base their research around how Niantic can ethically develop games, such as Ingress and Pokemon Go, for users. Some of the unintended consequences for games such as these and other augmented reality games was that they are a huge privacy and safety issues. Safety was a really big issue as the game had a real-world element that caused gamers to end up trespassing in unknown areas, resulting in injury or death. Privacy was also a very big issue with the creation of such games as the gameplay may allow players to track and monitor the activities of other players. Virtual markers can be used to learn patterns and then determine who someone is, which could lead to major stalking issues. This can be a real problem when dealing with underage, vulnerable players.
Pokemon Go world which leads to many safety and privacy concerns
It is important to encapsulate and understand some of the unintended consequences of your design, and try to prevent them within your prototyping process. One example of when I took into consideration of unintended consequences and used them to improve my design was in my Interaction Design deliverable. I had thought about my users, Arctic zookeepers who can log in information about the penguins they are taking care of, and their security while creating the app. As an incentive to join the app, I had created a Facebook-like forum where people can post questions and comments, along with a place where people have access to a directory of contact information of other zookeepers. All of this could be misused if an intruder enters the app, and to combat that I had created a sign-in page which verifies whether the zookeeper signing up for the app is actually one by running through a database of zoo’s staffing lists. This ensures that no one unintended enters the app and misses it, which shows how I had thought about unforeseen consequences in my own design and tried to fix them in the prototyping process.
We should always aim to design while taking into consideration HCD responsible practices. Otherwise our design would be just bad :(
Taking ethical implications in consideration is very important for users and society as a whole. As a good designer, we need to use HCD principles both capably and responsibly. This diary entry shows all the different ways that I have seen responsible practices in the class and how I have used the principles within my own designs. In the future, I hope to understand more about ethical implications of design and how we can improve our analysis of them in the future. So long for now!
Sai Ranganathan
",The Importance of Ethical Implications in HCDE,0,the-importance-of-ethical-implications-in-hcde-ffe2cc5518c4,2018-03-14,2018-03-14 07:07:36,https://medium.com/s/story/the-importance-of-ethical-implications-in-hcde-ffe2cc5518c4,False,832,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Sai Ranganathan,,995609d6f494,saiprasanna100,0.0,1.0,20181104
0,,0.0,,2018-01-28,2018-01-28 12:45:06,2018-01-28,2018-01-28 12:01:01,3,False,en,2018-01-28,2018-01-28 13:00:38,19,347ccd9f3a66,16.682075471698113,9,2,0,Maximizing the benefits of machine learning without sacrificing its intelligence,5,"Optimization over Explanation
Maximizing the benefits of machine learning without sacrificing its intelligence

Note: Wired.com has simultaneously run an op-ed version of this paper.
Imagine your Aunt Ida is in an autonomous vehicle (AV) — a self-driving car — on a city street closed to human-driven vehicles. Imagine a swarm of puppies drops from an overpass, a sinkhole opens up beneath a bus full of mathematical geniuses, or Beethoven (or Tupac) jumps into the street from the left as Mozart (or Biggie) jumps in from the right. Whatever the dilemma, imagine that the least worst option for the network of AVs is to drive the car containing your Aunt Ida into a concrete abutment. Even if the system made the right choice — all other options would have resulted in more deaths — you’d probably want an explanation.
Or consider the cases where machine-learning-based AI has gone wrong. It was bad when Google Photos identified black men as gorillas. It can be devastating when AI recommends that black men be kept in jail longer than white men for no reason other than their race. Not to mention autonomous military weapon systems that could deliver racism in airborne explosives.
To help ameliorate such injustices, the European Parliament’s has issued the General Data Protection Regulation (GDPR) that is often taken to stipulate a “right to explanation” for algorithms that “significantly affect” users.[1] This sounds sensible. In fact, why not simply require all AI systems be able to explain how they came to their conclusions?
The answer is not only that this can be a significant technical challenge[2], but that keeping AI simple enough to be explicable can forestall garnering the full value possible from unhobbled AI. Still, one way or another, we’re going to have to make policy decisions governing the use of AI — particularly machine learning — when it affects us in ways that matter.
One approach is to force AI to be artificially stupid enough that we can understand how it comes up with its conclusion. But here’s another: Accept that we’re not always going to be able to understand our machine’s “thinking.” Instead, use our existing policy-making processes — regulators, legislators, judicial systems, irate citizens, squabbling politicians — to decide what we want these systems optimized for. Measure the results. Fix the systems when they don’t hit their marks. Celebrate and improve them when they do.
We should be able to ask for explanations when we can. But when we can’t, we should keep using the systems so long as they are doing what we want from them.
For, alas, there’s no such thing as a free explanation.[3]
Pretend that your physician tells you that a deep-learning diagnostic system like Mt. Sinai’s Deep Patient has concluded that there is a good chance that you will develop liver cancer in the next five years.
“What makes the computer think that?” you ask.
Your doctor replies that the machine learning system — let’s call it Deep Diagnosis — has been trained on the health records of 700,000 patients, as was Deep Patient. From this data it has found patterns in what may be thousands of factors that have enabled it to accurately predict probabilities of health issues. In your case, it’s predicting with a confidence of 70 percent that you’ll develop liver cancer within the next five years.
“Based on what factors in my health record?”
Your doctor replies: “Deep Diagnosis tracks thousands of factors, including many that seem irrelevant to a diagnosis, and we don’t understand why they add up to a particular probability of liver cancer. A trained physician like me could stare at the print-out of all those variables and their weightings without ever understanding why they led to that result. In fact, these patterns may not be the same for other patients with the same diagnosis. But it turns out that they do indeed predict probable health issues. So, here are your options for treatment, including ignoring the prediction…”
This is very different from what we normally expect from a human giving an explanation. Usually our doctor would explain the model she’s using: liver cancer is caused by this or that, here are the test results for the state of your this or that, and here are the alternatives for lowering your risk.
But machine learning systems don’t have to start out primed with explicit human models of how factors interrelate. Certainly, human assumptions are inevitably smuggled in via our choices about which data to train them on, the biases expressed by that data, the implicit biases of the system’s programmers, the user interface we provide for interacting with the system, the reports we choose to generate, etc. But the system does not have to be told how we think the data it’s fed interrelates. Instead, the system iterates on the data, finding complex, multi-variable probabilistic correlations that become part of the model it builds for itself.
Human-constructed models aim at reducing the variables to a set small enough for our intellects to understand. Machine learning models can construct models that work — for example, they accurately predict the probability of medical conditions — but that cannot be reduced enough for humans to understand or to explain them.
This understandably concerns us. We think of these systems as making decisions, and we want to make sure they make the right moral decisions by doing what we do with humans: we ask for explanations that present the moral principles that were applied and the facts that led to them being applied that way. “Why did you steal the apple?” can be justified and explained by saying “Because it had been stolen from me,” “It was poisoned and I didn’t want anyone else to eat it” or “Because I was hungry and I didn’t have enough money to pay for it.” These explanations work by disputing the primacy of the principle that it’s wrong to steal.
It’s thus natural for us to think about what principles we want to give our AI-based machines, and to puzzle through how they might be applied in particular cases. If you’d like to engage in these thought experiments, spend some time at MoralMachine.mit.edu where you’ll be asked to make the sort of decision familiar from the Trolley Problem: if you had to choose, would you program AVs to run over three nuns or two joggers? Four old people or two sickly middle-aged people? The creators of the site hope to use the crowd’s decisions to provide guidance to AV programmers, but it can also lead to a different conclusion: We cannot settle moral problems — at least not at the level of detail the thinking behind MoralMachines demands of us — by applying principles to cases. The principles are too vague and the cases are too complex. If we instead take a utilitarian, consequentialist approach, trying to assess the aggregated pains and pleasures of taking these various lives, the problem turns out to be still too hard and too uncertain.
So perhaps we should take a different approach to how we’ll settle these issues. Perhaps the “we” should not be the commercial entities that build the AI but the systems we already have in place for making decisions that affect public welfare. Perhaps the decisions should start with broad goals and be refined for exceptions and exemptions the way we refine social policies and laws. Perhaps we should accept that AI systems are going to make decisions based on what they’ve been optimized for, because that’s how and why we build them.[4] Perhaps we should be governing their optimizations.
A proposal:
1. AI systems ought to be required to declare what they are optimized for.
2. The optimizations of systems that significantly affect the public ought to be decided not by the companies creating those systems but by bodies representing the public’s interests.
3. Optimizations always also need to support critical societal values, such as fairness.
Optimization is a measure of outcomes against goals. A system of AVs is successfully optimized for reducing fatalities if over a statistically reasonable interval the number of fatalities drops. It is optimized for energy savings if the overall energy use of the system — and the larger systems in which it’s embedded — declines. Optimization does not have to specify a precise target in order to succeed; rather the target is the maximum desirable and possible given all of the other desired optimizations, in conjunction with the world’s vicissitudes.
System designers talk about optimization because they recognize that machines are imperfect and are often designed to serve inconsistent goals. Are you going to optimize your car for good mileage, environmental impact, price, acceleration, safety, comfort, scenic views, or prestige? Designing for safety might require a heavier chassis, which will negatively affect acceleration, mileage, and environmental impact. Designing for environmental impact might mean longer travel times. Designers have to create a balance of optimizations, playing with a set of metaphorical sliders that determine how much of a value the system will sacrifice to gain some combination of the other values. As David P. Reed, one of the architects of the Internet, has said, optimizing a system for one value de-optimizes it for others.[5]
While optimizations apply to systems, they may be determined, within limits, by the individual users. For example, the passengers in an AV might want to optimize a trip for scenic value. Moving that “slider” up — someday possibly a slider in a digital control panel — will automatically move some of the others down: the trip is likely to take longer and consume more energy. The limits imposed on the users’ ability to adjust the sliders will be determined by those who are designing the optimization of the system overall: perhaps users will not be allowed to optimize their particular trip in a way that will de-optimize the overall system, even a little, for preserving lives.
There’s more to say about what’s entailed in optimizing AI systems, especially about keeping them fair, but first: why discuss the morality and governance of AI systems in terms of their optimization at all?
First, it focuses the normative discussion on AI as a tool designed to provide benefits we’ve agreed we want, rather than as a set of moral conundrums to be solved by arguing over principles and their application. For example, we are never going to agree as a society if AVs should be programmed to run over two prisoners to save one nun, or if rich people should be allowed to go faster at the expense of the not so rich; if we can’t even agree on Net Neutrality, how are we ever going to agree on Highway Neutrality? But we do have apparatuses of governance that let us decide that, say, a system of AVs should aim at reducing fatalities as a first priority, and at reducing environmental impact as a second. Does this mean an AV should run over the nun? Yes, if we’ve decided to optimize AVs to lower fatalities and her death will save two others, but not because we have at long last figured out the moral algebra of nuns vs. sinners. We can stop talking about the Trolley Problem, at least when it comes to governing this sort of AI system. That by itself should count as a major benefit.
Second, it enables us to evaluate success and failure — and liability — in terms of system properties, rather than case by case. Since governance of these systems will be done at some system layer of society — local, state, national, global — the primary evaluation ought to also be on the benefits at the system level.
Third, it contextualizes the suffering most AI systems are going to cause. For example, Aunt Ida’s family is going to be outraged that her AV drove into the concrete abutment. The family may well want to bring suit against the maker of the AV. But why her car killed specifically her may be inexplicable. There may be too many variables. It may require gathering all the real-time data from all the networked AVs on the road that provided input into the networked AI decision. Some of the data may need to be cloaked for privacy reasons. The ad hoc network that made the decision may have been using real-time data from other systems, including weather information, pedestrian locations, economic impact systems, etc….and some of those systems may be inexplicable black boxes. It may simply not be practical to expect all that data to be preserved so that we can perfectly recreate the state of the system. We well not be able to explain the decision or even verify it.
From the standpoint of morality and legal liability, this seems highly unsatisfactory. On the other hand, in 2016, there were about 40,000 traffic fatalities in the US. Let’s say a few years after AVs have become common, that falls to 5,000 deaths per year. Five thousand deaths per year is a horrible toll, but 35,000 lives saved per year is a glorious good. The moral responsibility of the AV manufacturer and of the network of AVs on the road at the time is not to save Aunt Ida but to achieve the optimizations that we as a society have decided on through our regulatory, legislative, and judicial systems.
Fourth, governing via optimization makes success measurable.
Finally, the concept of optimization has built into it an understanding that perfection is not possible. Optimization is a “best effort.” “AVs killed 5,000 people this year!” does not become a cause for moral outrage but a cheer for a major, humane accomplishment.
Overall, understanding and measuring AI systems in terms of their optimizations gives us a way to govern them that enables us to benefit from them even though they are imperfect and even when we cannot explain their particular outcomes.
But that is not enough.
Imagine that AVs are optimized to minimize fatalities, and deaths by car drop from 40,000 to 5,000 per year. Yay!
Now imagine that people of color are hugely disproportionately represented among those 5,000 dead.
Or imagine that a system designed to cull applicants for Silicon Valley tech jobs is producing high quality sets of people for in-person interviews, but the percentage of women making it through the AI process is even lower than the current dismal percentage of women in tech positions.
A system that achieves the results it’s been optimized for may still fail to meet societal goals. As has been well documented[6], machine learning systems are prone to reproducing the biases reflected in the data the systems used to create their models.
Achieving the stated optimization goals is clearly not enough. AI systems need to be able to provide evidence in the form of quantifiable results that the optimizations are not violating a culture’s broader, deeper values; this is the deontological (principle-based) moment of this utilitarian approach.[7]
We could count these constraints as another sort of optimization. But they deserve their own name and category, for two reasons.
First, “being fair” is not what a system of AVs or a medical diagnostic system is designed to do. Such systems are tools and thus are optimized for a more focused purpose. It’s useful to reserve the term “optimization” for the purposes for which a tool was designed.
Second, optimizations are trade-offs. But these constraints are critical because we will not permit them to be traded off.
So, we’ll call them critical constraints.
Deciding on the critical constraints we’ll demand from AI systems will require difficult moral discussions that express deep conflicts in our culture. For example, a Silicon Valley company resistant to demands for gender equity might say it wants its applicant-culling software to recommend the “best of the best” (as the company defines that), and “Gender balance be damned!” Or it may claim that the company receives relatively few applications from women. Or it may be terribly misguided about what to look for when evaluating potential employees.[8]
We may nevertheless decide as a culture to address the inequity of the tech workforce by enforcing a requirement that tech application-culling systems produce pools at least 50 percent composed of women. Or we might decide that the problem is the inequity in the educational pipeline and thus may want to suspend enforcing a “50 percent female” constraint until the pipeline becomes more gender balanced. Or we may want to insist on a 50 percent rule on the grounds that empirical evidence has shown that otherwise AI application-culling systems will reflect societal biases. Or we might insist that the recommendation pool be 75 percent female to help correct the inequity of the existing workforce. Such decisions undoubtedly will require difficult political and judicial conversations. On the positive side, having to come up with critical parameters for AI can serve a useful forcing function.
Resolving these issues are not AI’s problem, though. It’s our responsibility. Asking what we want AI systems optimized for frames it in a way appropriate for the necessary social discussions and political processes.
There’s an endless series of mistakes AI can — and therefore will — make: misdiagnosing a disease, targeting innocents in military attacks, discriminating based on race and gender, as well as recommending a movie that you don’t much like. There’s a far smaller number of ways in which AI will make these errors, each needing its own set of policy, regulatory, and judicial tools to help prevent and ameliorate them .
1. The wrong optimization: Say an AV system optimized for shortest delivery times routes continuous traffic through a residential section of town, resulting in a degradation of the quality of life. Or it routes high-speed traffic through a shopping district, de-optimizing local and pedestrian traffic, causing a drop in sales. (In this case, we well might want to let localities regulate the local optimizations of the system, just as airplanes have to lessen their noise over some towns.)
The manufacturers of AI systems should not be liable for successfully meeting poorly thought-through optimizations. They should be liable for ignoring local optimizations, just as airlines can be held responsible for violating local noise restrictions on late night arrivals.
2. Faulty execution: Say a home Internet of Things system has been optimized for energy savings, but in some homes it’s resulting in higher monthly expenditures on energy. In this case, the optimization is the preferred one, but the execution of it is faulty due to buggy software, the system having been trained on inappropriate data, a failure to interoperate properly with one or more of the devices, etc.
The optimization is correct but the implementation is flawed. If the AI system is capable of yielding explanations, those explanations need to be presented, and liability assessed. But if the AI is not capable of yielding explanations, product liability and class action processes might apply.
3. Expected harms from a well-optimized, properly functioning system: Say Aunt Ida happens to be one of the 5,000 fatalities in the new national AV system that is optimized first for reducing fatalities. It is operating properly: fatalities have dropped and the system is operating within its critical constraints.
The unfortunate losers in this system should receive no-fault compensation, probably via some form of social insurance.
4. Failure to instill a critical constraint, or to get its constraining power correct: Say an autonomous police drone uses undue force to subdue a criminal, resulting in serious collateral damage to innocent people. The injured innocents might bring suit, arguing that the drone failed to heed the “no harm to innocents” constraint.
If the drone indeed failed to heed that constraint, or if the manufacturers “forgot” to instill it, the manufacturers should be liable.
If the regulations governing police drones do not include such a constraint, then liability would seem to fall on the body that decided on the mix of optimizations and critical constraints. (The question of how a drone recognizes innocents is not a question of governance but a technical question — an important one — about implementation.)
So, overall:
(a) In the cases where systems are not functioning as expected, liability law, including product liability law, can often be invoked. The explanation of the failure need not always be determined.
(b) Where the systems are functioning as expected, and where the expectations assume imperfections, the victims ought to be compensated along the model of no-fault insurance: The families of the 5,000 people killed in car crashes ought to be compensated according to guidelines that try to be as fair as possible.
This overall approach has several advantages:
First, it lets us benefit from AI systems that have advanced beyond the ability of humans to understand exactly how those systems are working.
Second, it focuses the discussion on the system rather than on individual incidents.
Third, it places the governance of these systems within our human, social framework. Optimizations are always imperfect and entail trade-offs that we need to discuss. Optimizations should always be constrained by social values that we consider paramount. The operations of autonomous systems are autonomous from human control in particular situations but are subordinate to human needs, desires, and rights. (The italicized words indicate places where humanity is requisite and evident.)
Fourth, it does not require us to come up with a new moral framework for dealing with an infrastructure that increasingly uses the most advanced tools our species has ever created. Rather, it treats these inevitable problems as societal questions that are too important to be left unregulated and in the hands of commercial entities. It instead it lets them be settled by existing regulatory bodies, using our existing processes for resolving policy questions, and it uses and extends the existing legal frameworks for assessing liability and schedules of compensation.
This way we don’t have to treat AI as a new form of life that somehow escapes human moral questions. We can treat it as what it is: a tool that should be measured by how much better it is at doing something compared to our old way of doing it: Does it save more lives? Does it improve the environment? Does it give us more leisure? Does it create jobs? Does it make us more social, responsible, caring? Does it accomplish these goals while supporting crucial social values such as fairness?
By treating the governance of AI as a question of optimizations, we can focus the necessary argument about them on what truly matters: What is it that we as a society want from a system, and what are we willing to give up to get it?
Use existing public institutions of policy-making to decide on the weighting of interrelated and frequently conflicting optimizations and critical constraints.
Require AI systems to announce those optimizations publicly and clearly, and then hold them to them. This is where the locus of transparency should be. The transparency of algorithms is a tool that has its own uses in special cases.
Measure everything possible. Make those measurements available publicly, or, when necessary, privately to sanctioned, trusted, independent bodies.
Establish no-fault compensation and social insurance for systems where some harmful results cannot be avoided.
In short: Govern the optimizations. Patrol the results.


Thank you to the twenty or so people who commented on an online draft of this paper. Many but not all came from Harvard’s Berkman Klein Center or the MIT Media Lab. Not all agree with this paper’s premises or conclusions. All were helpful.
(I took the photo. It’s licensed as Creative Commons BY.)
NOTES
[1] What the GDPR actually stipulates is much harder to parse. See Sandra Wachter, Brent Middelstadt, Luciano Floridi, “Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation”, International Data Privacy Law, Jan. 24, 2017.
[2] There is a great deal of research underway about how explicable, or interpretable, complex Deep Learning systems can be. We know empirically that at least some of them can be understood to some useful degree. See, for example, this explanation of how Google’s DeepDream image maker works. For a fascinating and powerfully argued proposal for enabling explanations of machine learning systems without requiring any alteration of the systems and even without direct inspection of the algorithms used, see” Towards A Rigorous Science of Interpretable Machine Learning” by Finale Doshi-Velez and Been Kim.
[3] Thanks to Stuart Shieber for suggesting a version of this phrase. Also, note that there may be such a thing as a free hunch.
[4] And what they are optimized for will shape their design, including their sensors and controls. Optimization decisions have implications for every stage of devices’ design and production.
[5] In a private conversation. Cited with permission.
[6] For example, see Cathy O’Neil’s Weapons of Math Destruction and Kate Crawford’s “Artificial Intelligence’s White Guy Problem.”
[7] In “Fairness through Awareness,” Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel suggest both a metric for measuring fairness and “an algorithm for maximizing utility subject to the fairness constraint.” “Our framework permits us to formulate the question as an optimization problem that can be solved by a linear program.” Their “similarity metric” allows a way to assess whether an AI system is treating people who are relevantly similar in similar ways.
[8] See John Simons, “What Sephora Knows About Women in Tech That Silicon Valley Doesn’t,” Wall Street Journal, Oct. 9, 2017
Originally published at medium.com on January 28, 2018.
",Optimization over Explanation,86,optimization-over-explanation-maximizing-the-benefits-we-want-from-machine-learning-without-347ccd9f3a66,2018-05-09,2018-05-09 13:31:52,https://medium.com/s/story/optimization-over-explanation-maximizing-the-benefits-we-want-from-machine-learning-without-347ccd9f3a66,False,4275,,,,,,,,,,Ethics,ethics,Ethics,7787.0,David Weinberger,I mainly write about the effect of tech on our ideas,b6366645b0c5,dweinberger,10968.0,541.0,20181104
0,,0.0,d21512697401,2017-10-23,2017-10-23 15:37:02,2017-10-23,2017-10-23 16:06:05,1,False,en,2017-10-23,2017-10-23 21:51:38,2,b0128fd6ae2f,2.0452830188679245,3,0,0,Should we regulate technology? Will doing so stifle innovation? Does it matter?,5,"Painting by Brianna Keeper, www.briankeeper.com
Should We Regulate AI?
Should we regulate technology? Will doing so stifle innovation? Does it matter?
Jeremy Straub, assistant professor of computer science at North Dakota State University, pens a column in today’s The Conversation on regulating technology, more specifically artificial intelligence (AI).
Stephen Hawking and Elon Musk support regulation because AI might exterminate us. Straub, however, argues that we shouldn’t regulate AI because it will hinder innovation.
I argue that we need to regulate AI not because it will kill us Terminator-style, but because it can impoverish us. When we extend AI to include more basic types of technology and, increasingly, machine learning and predictive algorithms, we can see how technology has already decimated many career fields. As technology takes away work, our society doesn’t take away the grocery bill — ask a would-be small farm laborer or coal miner or autoworker.
His argument smacks of the “Dude, keep your regulations off my algorithm” philosophy of Uber and other tech-bro driven companies. “We’re too good for regulation because we built an app and actively broke laws to establish our market dominance.”
This all, of course, ignores the fact that we already regulate technology. AI may not specifically be regulated as such, but we clearly have laws that will govern its use to some degree.
Straub thinks that we should apply existing laws — murder, manslaughter — to AI and otherwise educate it in ethics and love. I don’t believe we need to create mechanical children, though. We have a hard enough time teaching ethics and love to human children. Priorities.
Of course, this discussion does bring up the question of free will. When he speaks of artificial intelligence, does he mean it will have free will? Or will it still be governing by the biases programmed into its “brain?” Algorithms are only as objective as their programmers. They choose which variables matter. Don’t believe adding a computer makes it objective.
Finally, he also holds up the Internet as an example of a technology that was left unregulated and blossomed into what it is today. But there’s no reason to think that meaningful regulations wouldn’t have allowed innovation on the Web while also protecting users from security issues like the Equifax hack or the moral sickness of the burgeoning of fake news industry.
In fact, regulations are continually being applied to the Internet — by Congress and by the courts (see: threats made online). That hasn’t hampered innovation so far.
Straub seems to subscribe to the same technological determinism that many in the technical fields seem to fall victim to. The belief that everything will work out as long as we let technology flourish. Moreover, they refuse to acknowledge or think about the knock-on effects of their innovations outside the tech world. It’s this disconnection of technologists from the real world and public policy, especially, that makes carefully crafted, meaningful regulation all that more important.
",Should We Regulate AI?,7,should-we-regulate-ai-b0128fd6ae2f,2018-05-21,2018-05-21 20:14:14,https://inadequate.net/should-we-regulate-ai-b0128fd6ae2f,False,489,a few thoughts. The personal online journal of William O. Pate II. Contact at willpate@protonmail.com. Send me Ether: 0xbe5819aacbfc4d60030d561316f9349e1aa1be9c or USD: www.paypal.me/willpate,inadequate.net,inadequatenet,,an examination of free will,willpate@protonmail.com,an-examination-of-free-will,"WRITING,SAN ANTONIO,LIFE LESSONS,MENTAL ILLNESS,LIFE",williampate,Ethics,ethics,Ethics,7787.0,William O. Pate II,"Writer | Critic | Essayist | Public Policy | Marketing in Austin, Texas.",209b14e49124,inadequatenet,734.0,515.0,20181104
0,,0.0,ec73089917ed,2018-04-02,2018-04-02 14:36:23,2018-04-02,2018-04-02 14:44:58,1,False,en,2018-04-02,2018-04-02 15:06:15,3,1e5f42a3a470,4.80377358490566,1,0,0,In the past two months the collection of episodes I would have loved to tell about has grown out of all proportions. But with Robotics…,4,"Double whammy plus a GitHub bonus

In the past two months the collection of episodes I would have loved to tell about has grown out of all proportions. But with Robotics Nanodegree and a new training routine going full speed in addition to the usual work-commute-household pattern, finding the time to write is surely a challenge. Fortunately, I love challenges! For the last dozen years my response to the little whining inner voice of “You will never manage to accomplish this” has been “hold my coffee”. So, without further ado, it is my pleasure to present to you two lovely episodes. These are possibly not the episodes on really new amazing developments in the world, but they are about very important topics: data bias, and precision health.
The first episode is from Super Data Science hosted by Kiril Yeremeko. He is the author of several Udemy courses which provided a lot of value to me back when I finished my PhD and transitioned to data science in industry. While I did’t learn much new material (a PhD at MPI is worth quite a bit in terms of knowledge and skills after all), it was a great way to systematise my knowledge and make sure that, yes, I do have a real data scientist’s skillset even though I didn’t study physics during my PhD . The episode itself is about two directors of data science who are also a couple. I think it is really romantic :) Their stories of doing PhDs and then transitioning to industry and growing there certainly rang the bell.
Another important aspect this episode highlights is bias in trained models and how to investigate your model for it. The case here is automatic evaluation of pre-recorded video interviews that can be scored for various metrics a potential employer cares about in a candidate for a particular role open. Even when you don’t use ethnicity, gender, age as direct features (and in HR context you should not), this information can still “leak” through other features like facial expressions, speech melody, accent and so on. The potential danger is that unless you are careful your model will start discriminating people left and right. E.g., from training data it builds association that people with strong square chins are more trustworthy, and the next thing you get is men a rated as more trustworthy overall because they tend to have square chins. One way to investigate your model for such pitfalls is to plot feature distributions after classification, and compare these distributions across different assigned classes and then see if this results in gender / age / ethnicity bias. This kind of investigation is routinely done for error analysis in all machine learning projects (if you want to learn more, check out Andrew Ng’s excellent course on Deep Learning, it has a whole module on ML strategy). But here I would actually do this investigation regardless of your classifier’s performance, also before and after training – there is no guarantee your input data is without bias too! One obvious question however is – ok, our model is behaving like a jerk, what do we do? If you have a dataset with well defined input features, you can probably figure out which of them are feeding the bias and remove them. However, in case of neural nets, it is a bit harder to control the learned weights, so for now I don’t have a good answer for that. In any case, due diligence and vigilance help.
The other episode is a much shorter one, from the highly popular a16z podcast. Their shows are usually good and don’t need advertising but this particular episode is very exciting for me. I remember I listened to it while at the gym and I hope my muttered comments like “cool”, “right” and “awesome” did not disturb the other gym goers.
The overall conversation in that episode is about the many aspects in which medicine and healthcare could undergo improvement. Among other things, the speakers talked about precision medicine. Precision medicine is like personalised medicine, that is, tailored to your body to a much higher degree than traditional medicine usually does. Think about it – on the instructions for most medicaments you see dosage as a function of age at best, and additionally some information about possible side effects, all clumped together, some warnings about pregnancy and lactation, possible interactions. with other drugs… and that’s it really. It is a lot of information for sure, but how much of it is applicable to any particular individual? Interestingly, in this scenario you do want to take age, gender, weight, body composition, ethnicity, lifestyle, genetics – all of this into account. It won’t be discrimination, it will be useful (as long as the outcomes are accurate). Like personalised medicine, precision medicine allows the healthcare system to tailor the treatment to your particular case. But in addition to maybe a more detailed health profile that is stored, say, in an electronic health record, precision medicine is based on more sources of data some of which can be longitudinal. The data sources range from a generic screening (full or partial), activity tracking devices or other sensors, food and sleep logs, etc. All this data of course should be treated responsibly, making sure it first and foremost benefits the person themselves. When we talk about benefits to the healthcare in general, they should come from the fact that providers can now help more people better, and not from the fact that they can suddenly refuse to cover certain conditions they could not know about without this data.
And before you start thinking that someone tracking themselves so much is surely a hypochondriac and possibly a vane person, and that going to a doctor once a year to have your blood pressure measured is perfectly fine, think about this too – one of the speakers had a smart watch for a long time and had collected a significant amount of baseline data. As the result, he was able to detect lime disease before he became clearly symptomatic. The treatment came in early, complications where avoided and in general he was able to stay active and productive as the result. Another aspect precision medicine will enable is not just living longer, at whatever cost, but living longer *and* healthier. So instead of paying your hospital bills, you will have the money to go to mountain hikes, and cool gadgets, and presents for your teenage great-grandchildren :)
This episode has encouraged me to do something I have wanted to do for a long long time – analyse some of my Fitbit data. I even posted the Jupyter notebook on GitHub which you can find here:
https://github.com/evolk/data_science_projects
I set out to ask a question – does my physical activity affect my resting heart rate and if yes then how? The answer. – I’m probably not active enough to see any effect. :D Out of privacy consideration I do not provide the datasets themselves but I give a link on how to extract the data if you have a fitbit yourself. Also, the dataset I extracted is from 2016 and describes activity levels that will hopefully not allow any health provider to deny me cover in the future.
Stay healthy!
Image source
",Double whammy plus a GitHub bonus,1,double-whammy-plus-a-github-bonus-1e5f42a3a470,2018-04-14,2018-04-14 22:06:30,https://medium.com/s/story/double-whammy-plus-a-github-bonus-1e5f42a3a470,False,1220,"KPD stands for “Kati’s Podcast Digest” and captures the purpose of this publication with 100% accuracy — I’m subscribed many dozens of podcasts on {data, neuro-, popular} science and some episodes are just too good to stay unshared and undiscussed.",,,,100 percent KPD,,100-percent-kpd,"PODCAST,SCIENCE,DATA SCIENCE,DIGEST",,Ethics,ethics,Ethics,7787.0,Kati Volk,Data Scientist in Switzerland. All opinions expressed in my posts are my own.,e70f76bf9994,kati_volk,16.0,20.0,20181104
0,,0.0,f333630470af,2018-07-18,2018-07-18 09:55:29,2018-08-02,2018-08-02 15:50:45,2,False,en,2018-08-02,2018-08-02 15:59:55,1,35af56cd680,4.451257861635222,4,0,0,An investigation into the potential for AI to empower us into taking control of our personal data.,5,"Play nicely: a newbie’s peek into the compatibility of AI and ethics

A few months ago, with everyone in the throes of the GDPR panic, it truly occurred to me just how little most people know about what really goes on with their personal data. Or, more surprisingly, how little businesses know about the potential of this data and how their use of it can impact on their customers.
In the wake of the Facebook scandal there was outrage from the public about the ways in which the company and their partners use their data. Yet, the same people slinked off back to the network once the heat had died down. For them, it was still an essential link to their families and friends — it is a social network after all.
GDPR is forcing people to be more transparent and clear in their data policies but, really, most people still don’t really ‘get’ it. People aren’t empowered to truly consider the value they are receiving in exchange for their personal data.
To me this seems unfair, unethical even. You can have a data policy written as clear as day but no-one’s going to read it. So people still won’t really know when their data is being sold to companies they hate, or used to advertise to them in an unlimited capacity.
A balancing act
So what if there was a way to automatically score companies based their data policies? That way users could decide on the value of service they were wanting to use and compare it to how ethical the company was being with their data — at a glance.
Turns out there’s already a really cool tool that already does this called ‘Terms of Service; Didn’t Read’ (ToSDR) which effectively crowdsources data on service’s terms and conditions, which are then broken down into smaller points which are rated ‘good’ or ‘bad’ by the site’s contributors. The result of this is an automatically calculated grade based on the overall ratings, from A (the best) to E (terms raising serious concerns).
I loved that this stuff existed and that other people cared enough about this that they would spend their time contributing to open source technology along the lines of what I hoped to investigate. But the limitations of ToSDR was in its reliance on gathering enough data from its contributors to produce a reliable grading, leaving many sites with none at all. So what I really wanted to know is if we could ‘take out the middleman’ and build a tool that could use artificial intelligence to grade any site, service or policy at the click of a button.
With explicit rules about what needed to be included in policies now enforced by the EU, surely it would be simple to use this technology to extract clauses and automate the decision on how ethical a company was being with their customer’s data? Or so I thought. (Hint: I’m not a developer).

Robot lawyers
A few weeks later, I met with James Touzel, a Partner and Head of Digital at law firm TLT, who’s heading up a project that is using AI to identify risk areas in legal contracts — a web-based solution called TLT LegalSifter. I was keen to find out if and how it was being done and if it could be applied to help people decipher data policies.
What was surprising was that this technology is still brand new and businesses haven’t adopted this en-mass for contract negotiations or any other uses just yet. When you hear about AI you’re told ‘the robots are coming, they’re taking us over’. Of course, they’re not. Not yet anyway…
“We wanted to develop an AI solution that could review and advise on low-risk, low-value commercial contracts initially — nothing too complex — so things like NDAs or SaaS contracts or a consultancy agreement where it’s normally lower risk and lower value”, James explained.
What they could get the AI to do was to identify a clause, or series of clauses, within a contract and serve up pre-written legal advice against it — such as the correct wording for a particular type of clause. A very clever and useful tool indeed, that will almost certainly increase the speed and quality of contract negotiations and enable more junior in-house lawyers, or even procurement and commercial teams, to manage contract reviews.
The only current limitation here is that the AI can’t understand what the clause says. So for personalised advice, based on the AI having recognised and understood intricate differences in clauses or statements, we’re not quite there.
“We’ve gone to market with a product which will identify the risk areas in certain types of contracts and serve up advice, but it still relies on the user to say ‘oh, that’s not what that says, I’m replacing it with that’”, James said.
“It puts the advice in one place, gives you an alternative clause… but it doesn’t do the last mile.”
Nuanced judgement
What this means for building a tool that rates how ethical a policy is, is that I have to decide what is ethical and what is not.
So what most of AI can do, at least on its own, is very black and white. If I tell it that any clause in a data policy that says a service will sell a user’s data is ‘bad’, it will always be scored negatively, even if in that particular case, for whatever reason, it’s actually not bad at all.
It doesn’t make the tool impossible to build but, for it to be genuinely useful for the majority of the population, it would at least need to have as many humans as possible to decide what they want to see in a data policy and what they don’t. And the real challenge would be making this rewarding enough to get the level of human contribution necessary to produce reliable data to work with.
Right now, AI isn’t up to the job, as we’re not at the point where it can be used to make complex decisions on ethics, at least not on its own. I’ve hit a crux in my beginner-level exploration of AI. But I have discovered that there are plenty of others who care about this issue, so I’m pretty sure it’s not the end.
If you’d like to chat about your tech project, get in touch with Simpleweb today.
",Play nicely: a newbie's peek into the compatibility of AI and ethics,21,play-nicely-a-newbies-experiment-into-the-compatibility-of-ai-and-ethics-35af56cd680,2018-08-02,2018-08-02 15:59:55,https://medium.com/s/story/play-nicely-a-newbies-experiment-into-the-compatibility-of-ai-and-ethics-35af56cd680,False,1078,"Helping you solve difficult problems with stories, ideas and insight about new technologies and product development from the team at Simpleweb.",,,,Simpleweb,info@simpleweb.co.uk,simpleweb,"STARTUP,TECH,BLOCKCHAIN,SOFTWARE,PRODUCT DEVELOPMENT",simpleweb,Ethics,ethics,Ethics,7787.0,Alice Whale,Content Manager at Simpleweb. Lover of all things Tech for Good. Runner. Ale drinker.,da0854da46cc,alice_12864,3.0,2.0,20181104
0,,0.0,,2017-09-15,2017-09-15 17:00:26,2017-09-15,2017-09-15 17:01:45,0,False,en,2017-09-15,2017-09-15 17:01:45,15,936a7175f05c,3.9547169811320764,0,0,0,This first appeared in the SIIA Digital Discourse Blog.,5,"Model Explanations are Part of Ethical Data Practice
This first appeared in the SIIA Digital Discourse Blog.
Institutions involved in predictive modeling are using ever more advanced techniques to predict outcomes of interest from credit scoring to facial recognition to spam detection. Institutions assess the performance of these models through standard measures such as accuracy (the number of correct predictions divided by the total number of predictions) or error rate (the number of incorrect predictions divided by the total number of predictions). They can in addition assess the fairness of their predictions with respect to vulnerable groups using measures such as predictive parity across groups, statistical parity, or equal error rates.
Institutions also face legal and ethical obligations to explain the basis of their consequential decisions to those who are affected, to regulators and to the general public. The idea is that people have rights based on autonomy and dignity to be able to understand why institutions make the decisions they do. When predictive models are the sole or a strong component of consequential decisions, this general obligation to explain decisions becomes an obligation to convey information about the workings of the models used. This duty to disclose is in addition to any required standards for measures of accuracy and fairness.
Some requirements for explanation are aimed at providing people with an understanding of specific decisions, rather than an understanding of the logic of the model that produced the decisions. Thus, consumers who were rejected for credit could be told that this decision was based on the length of employment, the length of residency, and the excessive nature of their obligations in relationship to their income. Consumers can infer from this that these factors were elements in the predictive model used, but they are not told exactly how these factors contributed.
The Equal Credit Opportunity Act (ECOA) imposes one legal obligation. ECOA requires that “each applicant against whom adverse action is taken shall be entitled to a statement of reasons for such action from the creditor. . .A statement of reasons meets the requirements of this section only if it contains the specific reasons for the adverse action taken.”
Another legal obligation is part of the Fair Credit Reporting Act (FCRA). FCRA requires consumer reporting agencies to disclose “all of the key factors that adversely affected the credit score of the consumer in the model used, the total number of which shall not exceed 4.” In cases of an adverse action based on information in a credit report, FCRA requires disclosure of these key factors.
Both ECOA and FCRA require disclosures of explanations of specific outcomes of automated decisions. Regulation B implements ECOA and provides guidance for FCRA disclosures as well. An appendix to Regulation B contains a Sample Notice of Action Taken and Statement of Reasons that contains a list of 24 factors that can be used to satisfy these disclosure obligations.
In addition, credit bureaus have also provided information about the factors used in developing credit scores. Experian, for instance, says that total debt, the types of accounts, the number of late payments, and the age of accounts are factors that affect credit scores. FICO indicates that 35% of your credit score is derived from payment history, 30% from amounts owed, and 15% from the length of credit history.
There is no general obligation to disclose the actual formula used in predictive models. Doing so would have adverse consequences for the incentive to develop accurate and error free models and would often frustrate the purpose of developing the models in the first place. Disclosing the formula for detecting terrorist financing, for instance, would give terrorists a roadmap to avoid detection.
Still, there are some narrow circumstances where current law might require the disclosure of the formula used to make a consequential decision about people. In a case involving the Houston school district and the SAS Educational Value-Added Assessment System (EVAAS), the court ruled that a dismissed teacher had a due process right to verify the score that was used as a basis for dismissal, which would not be possible for a proprietary system like the SAS EVAAS. If this decision stands, school districts might not be able to dismiss teachers with a constitutionally protected property right in their jobs (such as a long-term contract) based on a proprietary algorithm that evaluates their effectiveness.
The European Union’s General Data Protection Regulation (GDPR), which will go into effect in May 2018, would impose an expanded obligation to explain automated decisions. It would affect a much larger range of applications than is customary or expected in the United States. The GDPR would apply whenever a decision has “a legal or similarly significant effect on someone.” In addition, it would require information about the logic of predictive models, not just an understanding of the factors at play in specific decisions.
Some have argued that there is nothing new in GDPR because the previous Data Protection Directive already contained the requirement for explanation and it hadn’t been implemented in any specific way. But there is something new here. Article 15 of GDPR grants data subjects access to “meaningful information about the logic involved” in “automated decision-making.” This goes beyond the text in Article 12 of the earlier Data Protection Directive which called only for a right of access to “knowledge of the logic involved” in automated decisions.
What this means in practice is not yet clear. But the UK’s Independent Commissioner’s Office has interpretedGDPR as requiring explanations in a wide range of circumstances including assessments of an individual with respect to:
performance at work;
economic situation;
health;
personal preferences;
reliability;
behaviour;
location; or
movements.
ICO says that models used for these assessments have to “ensure processing is fair and transparent by providing meaningful information about the logic involved, as well as the significance and the envisaged consequences.” Beyond explanations, ICO is looking for predictive models to “use appropriate mathematical or statistical procedure” and “implement appropriate technical and organisational measures to enable inaccuracies to be corrected and minimise the risk of errors.”
Institutions using predictive models will need to assess their systems to ensure that their processes and procedures provide these protection, not only as a matter of legal obligation, but also as a matter of ethical data practice.
",Model Explanations are Part of Ethical Data Practice,0,model-explanations-are-part-of-ethical-data-practice-936a7175f05c,2018-02-16,2018-02-16 07:56:27,https://medium.com/s/story/model-explanations-are-part-of-ethical-data-practice-936a7175f05c,False,1048,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Mark MacCarthy,"SVP for Public Policy, Software & Information Industry Association; Adjunct Professor, Communication, Culture & Technology Program, Georgetown University",4317359941eb,maccartm,8.0,3.0,20181104
0,,0.0,,2018-01-05,2018-01-05 15:22:24,2018-01-05,2018-01-05 15:38:03,1,False,en,2018-01-05,2018-01-05 22:55:40,5,893d1333090e,2.611320754716981,1,0,0,"Imagine, in the near future, a GP using a machine learning algorithm to recommend a patient’s treatment based on their habits, diet, age…",5,"Putting AI into a black box of superethics.
Illustration by Paul Wilshaw
Imagine, in the near future, a GP using a machine learning algorithm to recommend a patient’s treatment based on their habits, diet, age, genetic history, etc. What happens if the recommended treatment fails? If the machine learning algorithm is based on a complicated neural network then it may prove to be impossible to work out why the algorithm was unsuccessful. However, if it’s a machine learner based on decision trees or Bayesian networks is much more transparent to programmer inspection (Hastie, Tibshirani, and Friedman 2001).
We’re not quite there yet. However, we are making headway to create artificial intelligence (AI) that aids workstreams.
One example is PlasmoTron, go to http://bit.ly/plasmotron to see it in action, a computer programme that controls an existing open-source robotic pipetting robot to automate the growth of malaria parasites.
PlasmoTron — automatic pipetting for malaria parasite research by Theo Sanderson, Post-Doctoral Research Associate, Wellcome Trust Sanger Institute.
Keeping malaria parasites alive can take a lot of time. It doesn’t require much thought, but highly trained scientists spend a lot of time just moving liquid from one tube to another to keep their parasites alive. This places limits on how many different parasite lines one can work with at the same time, which limits how many genes we can understand.
Theo Sanderson, Post-Doctoral Research Associate, Wellcome Trust Sanger Institute.
According to some definitions of AI, PlasmoTron is intelligent — it makes judgments. Still at a fundamental level it is just following a simple set of rules to determine how to take care of the parasites in its charge. The process is currently semi-automated, the researcher still has to take plates in and out of the incubator.
Taking the effort out of mundane tasks is undoubtedly the aim of the PlasmoTron and a lot of AI currently out there. This use of AI and robotics is undoubtedly a benefit. There are general misconceptions about robotics often conveyed through popular fictional media such as Blade Runner and Terminator films.
An AI GP dispensing medical advice and medicine proposes moral ethics for society. How is it controlled? What happens if errors occur? One theory is that AI is continuously monitored and tested with more rigorous ethics learnt from current practices. Can we also apply ‘black box thinking’ to AI and robots? The black box dramatically reduced aviation deaths from 25% in 1912 to a rate of one accident per every 2.4 million flights (Syed 2016). A feature is becoming popular in motoring. With dashcams and in-car computers, the SEAT Leon Cristobal concept takes this even further potentially incorporating a black box safety recording data and images while driving then sending them to your smartphone if an accident occurs (SEAT 2017).
The prospect of AIs with unimaginable intelligence and abilities offers humanity with the unusual challenge of stating an algorithm that outputs superethical behaviour. These problems may seem visionary, but it seems predictable that we will encounter them; they are not devoid of present-day research directions.
Acknowledgments
With thanks to Theo Sanderson, Post-Doctoral Research Associate at Wellcome Trust Sanger Institute for answering my many questions about PlasmoTron.
Eleonora Aquilini for the suggestion, advice and referral to Theo.
References
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2001. e Elements of Statistical Learning: Data Mining, Inference, and Prediction. 1st ed. Springer Series in Statistics. New York: Springer.
Syed, M., 2016. A Joosr Guide to… Black Box Thinking by Matthew Syed: Why Most People Never Learn from Their Mistakes — But Some Do. 1st ed. UK: Joosr.
Opentrons. 2017. Opentrons. [online] Available at: http://opentrons.com. [Accessed 13 December 2017].
Twitter @OpenTrons_. 2017. @OpenTrons_. [online] Available at: https://twitter.com/OpenTrons_/with_replies. [Accessed 13 December 2017].
Bayesian networks. (2018). Introduction to Bayesian networks. [online] Bayesserver.com. Available at: https://www.bayesserver.com/docs/introduction/bayesian-networks [Accessed 3 Jan. 2018].
SEAT. 2017. Innovative Solutions for Smart Cities. [online] Available at: http://www.seat.com/corporate/news/events/smart-city-expo-2017-leon-cristobal.html. [Accessed 3 January 2018].
",Putting AI into a black box of superethics.,1,putting-ai-into-a-black-box-of-superethics-893d1333090e,2018-04-20,2018-04-20 09:50:26,https://medium.com/s/story/putting-ai-into-a-black-box-of-superethics-893d1333090e,False,639,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Paul Wilshaw,IA / UX / UI / ID / CX expert. Innovator. Science communicator and future media experimenter. Creator of craziness. Hackathon organisor. Multiple award winner.,5d92854bdec5,paulwilshaw,147.0,355.0,20181104
0,,0.0,,2018-05-23,2018-05-23 21:24:44,2018-05-21,2018-05-21 14:20:53,1,False,en,2018-05-23,2018-05-23 21:27:32,3,fd4c353dcc4f,4.532075471698113,3,0,0,By Tucker Davey,4,"Teaching Today’s AI Students To Be Tomorrow’s Ethical Leaders: An Interview With Yan Zhang

By Tucker Davey
Some of the greatest scientists and inventors of the future are sitting in high school classrooms right now, breezing through calculus and eagerly awaiting freshman year at the world’s top universities. They may have already won Math Olympiads or invented clever, new internet applications. We know these students are smart, but are they prepared to responsibly guide the future of technology?
Developing safe and beneficial technology requires more than technical expertise — it requires a well-rounded education and the ability to understand other perspectives. But since math and science students must spend so much time doing technical work, they often lack the skills and experience necessary to understand how their inventions will impact society.
These educational gaps could prove problematic as artificial intelligence assumes a greater role in our lives. AI research is booming among young computer scientists, and these students need to understand the complex ethical, governance, and safety challenges posed by their innovations.
In 2012, a group of AI researchers and safety advocates — Paul Christiano, Jacob Steinhardt, Andrew Critch, Anna Salamon, and Yan Zhang — created the Summer Program in Applied Rationality and Cognition (SPARC) to address the many issues that face quantitatively strong teenagers, including the issue of educational gaps in AI. As with all technologies, they explain, the more the AI community consists of thoughtful, intelligent, broad-minded reasoners, the more likely AI is to be developed in a safe and beneficial manner.
Each summer, the SPARC founders invite 30–35 mathematically gifted high school students to participate in their two-week program. Zhang, SPARC’s director, explains: “Our goals are to generate a strong community, expose these students to ideas that they’re not going to get in class — blind spots of being a quantitatively strong teenager in today’s world, like empathy and social dynamics. Overall we want to make them more powerful individuals who can bring positive change to the world.”
To help students make a positive impact, SPARC instructors teach core ideas in effective altruism (EA). “We have a lot of conversations about EA, but we don’t push the students to become EA,” Zhang says. “We expose them to good ideas, and I think that’s a healthier way to do mentorship.”
SPARC also exposes students to machine learning, AI safety, and existential risks. In 2016 and 2017, they held over 10 classes on these topics, including: “Machine Learning” and “Tensorflow” taught by Jacob Steinhardt, “Irresponsible Futurism” and “Effective Do-Gooding” taught by Paul Christiano, “Optimization” taught by John Schulman, and “Long-Term Thinking on AI and Automization” taught by Michael Webb.
But SPARC instructors don’t push students down the AI path either. Instead, they encourage students to apply SPARC’s holistic training to make a more positive impact in any field.
Making the most positive impact requires thinking on the margin, and asking: What one additional unit of knowledge will be most helpful for creating positive impact? For these students, most of whom have won Math and Computing Olympiads, it’s usually not more math.
“A weakness of a lot of mathematically-minded students are things like social skills or having productive arguments with people,” Zhang says. “Because to be impactful you need your quantitative skills, but you need to also be able to relate with people.”
To counter this weakness, he teaches classes on social skills and signaling, and occasionally leads improvisational games. SPARC still teaches a lot of math, but Zhang is more interested in addressing these students’ educational blind spots — the same blind spots that the instructors themselves had as students. “What would have made us more impactful individuals, and also more complete and more human in many ways?” he asks.
Working with non-math students can help, so Zhang and his colleagues have experimented with bringing excellent writers and original thinkers into the program. “We’ve consistently had really good successes with those students, because they bring something that the Math Olympiad kids don’t have,” Zhang says.
SPARC also broadens students’ horizons with guest speakers from academia and organizations such as the Open Philanthropy Project, OpenAI, Dropbox and Quora. In one talk, Dropbox engineer Albert Ni spoke to SPARC students about “common mistakes that math people make when they try to do things later in life.”
In another successful experiment suggested by Ofer Grossman, a SPARC alum who is now a staff member, SPARC made half of all classes optional in 2017. The classes were still packed because students appreciated the culture. The founders also agreed that conversations after class are often more impactful than classes, and therefore engineered one-on-one time and group discussions into the curriculum. Thinking on the margin, they ask: “What are the things that were memorable about school? What are the good parts? Can we do more of those and less of the others?”
Above all, SPARC fosters a culture of openness, curiosity and accountability. Inherent in this project is “cognitive debiasing” — learning about common biases like selection bias and confirmation bias, and correcting for them. “We do a lot of de-biasing in our interactions with each other, very explicitly,” Zhang says. “We also have classes on cognitive biases, but the culture is the more important part.”
Designing safe and beneficial technology requires technical expertise, but in SPARC’s view, cultivating a holistic research culture is equally important. Today’s top students may make some of the most consequential AI breakthroughs in the future, and their values, education and temperament will play a critical role in ensuring that advanced AI is deployed safely and for the common good.
“This is also important outside of AI,” Zhang explains. “The official SPARC stance is to make these students future leaders in their communities, whether it’s AI, academia, medicine, or law. These leaders could then talk to each other and become allies instead of having a bunch of splintered, narrow disciplines.”
As SPARC approaches its 7th year, some alumni have already begun to make an impact. A few AI-oriented alumni recently founded AlphaSheets — a collaborative, programmable spreadsheet for finance that is less prone to error — while other students are leading a “hacker house” with people in Silicon Valley. Additionally, SPARC inspired the creation of ESPR, a similar European program explicitly focused on AI risk.
But most impacts will be less tangible. “Different pockets of people interested in different things have been working with SPARC’s resources, and they’re forming a lot of social groups,” Zhang explains. “It’s like a bunch of little sparks and we don’t quite know what they’ll become, but I’m pretty excited about next five years.”
This article is part of a Future of Life series on the AI safety research grants, which were funded by generous donations from Elon Musk and the Open Philanthropy Project.
Originally published at futureoflife.org on May 21, 2018.
",Teaching Today’s AI Students To Be Tomorrow’s Ethical Leaders: An Interview With Yan Zhang,13,teaching-todays-ai-students-to-be-tomorrow-s-ethical-leaders-an-interview-with-yan-zhang-fd4c353dcc4f,2018-05-24,2018-05-24 11:17:22,https://medium.com/s/story/teaching-todays-ai-students-to-be-tomorrow-s-ethical-leaders-an-interview-with-yan-zhang-fd4c353dcc4f,False,1148,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Future of Life,FLI catalyzes and supports research and initiatives to safeguard life and develop optimistic visions of the future. Official account.,e33e2d2a809c,FLIxrisk,1361.0,93.0,20181104
0,,0.0,3a8144eabfe3,2017-11-08,2017-11-08 01:39:10,2017-11-08,2017-11-08 01:48:11,5,False,en,2017-11-08,2017-11-08 02:33:54,5,202b312de4eb,7.263522012578616,3,0,0,"Robots are no longer just an instrument of mass production, but they are rapidly integrating into our lives. The Japanese Ministry of…",5,"The human use of emotional machines
Robots are no longer just an instrument of mass production, but they are rapidly integrating into our lives. The Japanese Ministry of Economy (METI) predicts the emergence of a neo-mechatronic society, in which robots will participate in recreational activities and elderly care. In addition, the United States anticipates the inclusion of robots in cooperative tasks with humans within the following areas: agriculture, energy, defense and space exploration (Royakkers, M & Est, 2016).
Rosalind Picard (1997) mentions in Affective Computing, some of the problems that can arise when endowing computers with affective understanding, response and expression. However, the author believes that development of affective computing is necessary for computers to interact naturally with humans.

Humanity inhabits an artificial environment, we no longer live in nature, but within a created super-nature, in a new day of the Genesis: technology (Gasset, 1992). Understanding the implications of technological innovation is a political, philosophical and ethical issue, for which is necessary to ask about its consequences. 21st century is confronting us with new forms of interaction with computers, so it is of great importance anticipating ethical implications of Affective Computing and Artificial Intelligence.
The goal of this paper is to initiate a discussion on ethical issues of Affective Computing, defined by Picard (1997) as a research and development framework focused on providing computers emotional skills. For this purpose, we will carry out an ethical and philosophical analysis of two potential problems of artificial affective agents: social disqualification and computational anthropomorphism.
“Our experiences are always intersubjective, which means that when we are in the world we are necessarily with others. Thinking of isolated human beings is impossible.” (García & Traslosheros, 2012).
Moreover, to relate with others we require emotional skills. In the case of children this is more salient, because they mostly communicate by affective non-verbal means (i.e. screaming, crying) before fully acquiring a language (Picard, 1997). This confronts us with the question regarding the influence of affective computation on children emotional development, since artificial emotional agents when interacting with them can increase social disqualification.

Social disqualification refers to the erosion of critical skills for maintaining human relationships. According to Sparrow (2002), robots imitating emotions could lead to an ethical issue, given we could confuse our creations for what they are not. Regarding this problem the psychologist Sherry Turkle (2002) predicts the following: children will get used to the almost perfect friendships of robots (programmed positively to satisfy users), and therefore will not learn to deal with people, as well as with their own emotions.
The above mentioned problems are due in large part to people assigning psychological and moral status to robots or programs, when previously they were only attributed to human beings (Melson et al., 2009). In Royyakkers et al. (2016) it is shown how this impacts children social and emotional development, since they are increasingly in contact with this type of technology.
Social disqualification in children is a potential problem of emotional robots, such as NAO (“ASK NAO Autism Solution for Kids”, 2017) and Pepper (“Who is Pepper?”, 2017). Both have different sensors and configurable programs capable of responding to and recognizing emotions. Even if one of the purposes of these tools is to aid children emotional development, careful consideration must be given to their psychological impact before implementing the solution in schools.
Pepper, the emotional robot
Pearson et al. (2014) mention designing educational robots for children requires paying close attention to the ways in which particular characteristics of design influence short- and long-term development. This is because human-robot interaction has the potential to alter not only mental development of children, but also their interactions with other human beings. Design decisions should be made in a way that promote their physical, psychological and emotional health. Consequently, the whole process of creating affective technology must be analyzed from an interdisciplinary perspective including both humanities and science.
On the other hand, anthropomorphizing artificial affective agents has consequences on the quality of judgments we make about information provided by computers. Rosalind Picard (1997) mentions it is farily common for us to trust results coming from computers since they are predictable and do not incur in biases (which have not been part its design per se). However, there is not enough reason for an affective computer to be incapable of mimicking emotional traits, and using this to deceive and extract private information. Ergo, for maintaining human integrity designers must know the limitations and potential behavior of affective computers.
Installing Samantha
A particular case of computational anthropomorphism is Siri, Apple’s personal assistant. Its designers endowed her with natural language capabilities, but sometimes she doesn’t work as expected. José Mendiola emphasizes the reliability problem that Siri faces not only when understanding its users, but also for providing pertinent answers (Mendiola, 2016). This is just one case of a personal assistance system that still does not fully implement the possibilities of affective computation, as is the case of Samantha in the sci-fi movie Her (2013). However companies that use artificial conversational agents must do an ethical review of its operation; We have enough background of potential risks with Twitter’s violent chat-bot (Wakefield, 2016).
Furthermore, Artificial Intelligence is everyday closer to passing the Turing test (“Computer AI passes Turing test in ‘world first’”, 2014). As we approach this behavioral limit, the need for ethical reflection on Artificial Intelligence systems becomes more evident. Hence it is necessary to make an ethical review of technology and science.
In conclusion, we can not ignore the effect of our emotions on others, let alone the effects of technology on emotional development; since increasingly these artifacts are mediating human emotional relationships. Curiously enough, it is generally believed that emotions have nothing to do with intelligence, but research in the field of cognitive sciences (Simon, 1967; Hofstadter & Dennett, 1981; Gelernter, 1994;) shows that emotions have a great impact on learning, memory, problem solving, humor and perception. We have focused too much on the negative side of emotions, taking them only as irrational side-effects, when humans are more emotional that we would like to think.
We should remember technology is not a neutral entity. This belief has led us to idealizing technology, as a tool that can solve any problem without fully considering its nature and social impact. This form of wishful thinking takes us away from analyzing the value and purpose (discourse) embedded in technology (Bateman, 2015). This means an object is not good just because it is technological, for each object we have designed extends our abilities, impacting the personal and social sphere.
The Kantian thought dividing categorically objects from subjects has been taken to the extreme, to the extent subjects receive a special moral status apart from their objects, products (means) the subject creates for achieving a particular end. In other words, we ignore how human intentions are implemented in their objects, treating them as a morally neutral means, while ways for being good or bad are increased with technology.
Every computational machine is conceived as a material assemblage conjoined with a unique discourse that explains and justifies the machine’s operation and purpose. ( Johnston, 2010)
Drawing Hands (M. C. Escher)
Finally, the critical problem is not computers gaining the status of subjects, and taking over the world (by substituting human beings through the acquisition of emotions, or carrying out the activity of thinking), but the degree of control that automated systems have over their faculties for guaranteeing human well-being. We should start assigning responsibilities to every aspect and element of technology development and use. We have no excuse even in the face of the inappropriate behavior of the most advanced artificial intelligence, for if computers acquire autonomy and become non-deterministic, how can we avoid catastrophes and take control over our own creations? Who will be responsible if these technological objects inherit (learn) the ethics and bias of the subject and automatically implement them in new objects (such as Von Neumann’s self-replicating machine, 1966)? If all these issues are not addressed in time by transferring subject’s responsibility to the technological process, results will be fatal, maybe as disastrous as those caused by human emotional disorders.
References:
ASK NAO Autism Solution for Kids. (2017). Retrieved from: http://www.robotlab.com/store/ask-nao-autism-solution-for-kids
Bateman, C. (2015). We Can Make Anything. Rethinking Machine Ethics in the Age of Ubiquitous Technology Advances in Human and Social Aspects of Technology, 15–29. doi:10.4018/978–1–4666–8592–5.ch002
Computer AI passes Turing test in ‘world first’ (2014). Retrieved from: http://www.bbc.com/news/technology-27762088
García & Traslosheros (2012). Ética, persona y sociedad: una ética para la vida. México, D.F.: Porrúa.
Gasset, J. O. (1992). Meditación de técnica y otros ensayos sobre ciencia y filosofía. Madrid: Alianza.
D. Gelernter. (1994). The Muse in the Machine. Ontario: The Free Press, Macmillan, Inc.
Hofstadter, D. R., & Dennett, D. C. (1981). The Mind’s I. London: Penguin books.
Johnston, J. (2010). The allure of machinic life: cybernetics, artificial life, and the new AI. Cambridge, Mass.: MIT Press.
Jonze, S. (Director). (2013). Her [Video]. Estados Unidos: Warner Bros. Pictures.
Melson, G. F., Kahn, Jr., P. H., Beck, A. and Friedman, B. (2009), Robotic Pets in Human Lives: Implications for the Human–Animal Bond and for Human Relationships with Personified Technologies. Journal of Social Issues, 65: 545–567. doi:10.1111/j.1540–4560.2009.01613.x
Mendiola, J. (2016, October 16). Siri, ¿por qué no me comprendes? Retrieved from: https://www.applesfera.com/analisis/siri-por-que-no-me-comprendes
Pearson, Y. & Borenstein, J. AI & Soc (2014) 29: 23. doi:10.1007/s00146–012–0431–1
Picard, R. W. (1997). Affective computing. MIT Press.
Royakkers, L. M., & Est, Q. C. (2016). Just ordinary robots automation from love to war. Boca Raton: CRC Press, Taylor & Francis Group.
Simon, H. A. (1967). Models of man: social and rational ; mathematical essays on rational human behavior in society setting. New York: Wiley.
Sparrow, R. ( 2002). The March of the robot dogs. Ethics and Information Technology, 4(4), 305–318. doi:10.1023/A:1021386708994
Turkle, S. (2013). Alone together: why we expect more from technology and less from each other. Cambridge, MA: Perseus Books.
Von Neumann, J. (1966). A. Burks, ed. The Theory of Self-reproducing Automata. Urbana, IL: Univ. of Illinois Press.
Wakefield, J. (2016). Microsoft chatbot is taught to swear on Twitter. Retrieved from: http://www.bbc.com/news/technology-35890188
Who is Pepper? (2017). Retrieved from: https://www.ald.softbankrobotics.com/en/cool-robots/pepper
",The human use of emotional machines,12,the-human-use-of-emotional-machines-202b312de4eb,2018-05-20,2018-05-20 23:23:48,https://hackernoon.com/the-human-use-of-emotional-machines-202b312de4eb,False,1704,how hackers start their afternoons.,hackernoon.com,hackernoon,,Hacker Noon,stories@amipublications.com,hacker-daily,"HACKATHONS,HACKING,PROGRAMMING,TECH,HACKER",hackernoon,Ethics,ethics,Ethics,7787.0,Santiago Renteria,,f17910a46ec9,santiagorenteria,34.0,88.0,20181104
0,,0.0,3a8144eabfe3,2017-12-14,2017-12-14 01:32:08,2017-12-27,2017-12-27 05:38:48,4,False,en,2017-12-27,2017-12-27 14:55:28,5,2ffdbd8a880f,8.666037735849056,171,6,1,"In recent months and years, the Machine Learning community has conducting a notable amount of soul searching on the question of algorithmic…",5,"Fair and Balanced? Thoughts on Bias in Probabilistic Modeling
In recent months and years, the Machine Learning community has conducting a notable amount of soul searching on the question of algorithmic bias: are our algorithms operating in ways that are fundamentally unfair towards specific groups within society?

This conversation intrigues me because it falls at an intersection between two areas of thought which are central to my interests: societal norms and probabilistic modeling. However, I’ve often found the discussion space to be a frustrating one, because it contains so many people talking past each other: so many different definitions of bias being implicitly conflated together, so much un-nuanced laying of blame. This post is my attempt at clarification: both for myself, and hopefully for the conversation as a whole.
Differing base rate bias

If you’ve been engaged in any conversations around machine learning and bias, you’ve doubtless seen this post from ProPublica, which asserts that the COMPAS recidivism prediction model was biased because of the differing composition of its errors: while both black and white prisoners had equal aggregate error rates, the error for black prisoners was likelier to be a false negative (predict recidivism, doesn’t recidivate) whereas the error for white prisoners was likelier to be a false positive (doesn’t predict recidivism, recidivates. Further research into the accusations of unfairness that ProPublica leveled suggested that this wasn’t as cut and dry as ProPublica suggested, despite the seductively compelling narrative of “naive, privileged techies cause harm”.
The original blog post presenting these findings is worth reading in it’s own right, but I’ll make an attempt to briefly summarize it’s ideas here: whether you believe this algorithm is fair depends on how you define fairness. And you generally cannot have an algorithm be fair according to more than one definition at once. One possible way to define it is as “people in the same score bucket have equal true probabilities of reoffending”. This is often shorthanded as “calibration”. Northpointe, the company that builds the COMPAS score, asserts its score to be fair because it is well-calibrated: within a given score bucket, black and white defendants who score in that bucket are, in aggregate, equally likely to reoffend.
ProPublica’s definition of fairness focuses in on the prisoners who did not re-offend, and showed that within that group, black prisoners were likely to be deemed riskier than the white prisoners. Another way of thinking of this is: within the set of people who ultimately did not re-offend, Pro Publica’s definition of fairness requires the average score of black prisoners to be equal to the average score of white ones. Let’s call that “negative class balance”. You may care about this because you don’t want one group to be systematically deemed more risky, conditional on being actually low-risk. You could also imagine a symmetric constraint on the positive class, where, within the set of people who ultimately did reoffend, blacks and whites exhibit the same average score. Let’s call this “positive class balance”. You may care about this because you don’t want one group to be systematically “let off the hook” conditional on truly reoffending.
This paper, by Kleinberg et al, proved mathematically that you could not have all three of these conditions (calibration, negative class balance, and positive class balance), except in the case where base rates (i.e. the aggregate rate of reoffense) is equivalent across groups, or where you have a perfect model. In anything less than a perfect model, when groups reoffend at different rates, you have to sacrifice at least one of these notions of fairness in order to satisfy the others.
At this point, it becomes a question of tradeoffs, depending on the domain in which you’re operating. I think there’s a quite valid argument to be made that, in cases where the state is choosing to further incarcerate someone or not, and source of harm we most want to avoid is “unfair” further incarceration, we’d prefer to ensure that truly low-risk people are treated the same across demographic groups, and care less about our score balance among the truly high-risk individuals. But, I think the fundamental point here is: it’s an argument, and there’s a real trade-off being made; it’s not an obvious technical flaw of the algorithm that it wasn’t specifically designed to meet an ambivalent moral trade-off criteria.
This question, of differing base rates among groups, is a controversial one. People frequently argue that, in cases like the recidivism one illustrated here, the data itself is unfair, because it reflects an oppressive society without an equal playing field. That’s an entirely reasonable argument to make. But, it goes fundamentally deeper than alleging that an algorithm is performing incorrectly. It asserts that any system that learns the statistical patterns of our current world is dangerous, because our current world is normatively flawed in serious ways, and any system that learns associations in this flawed world will be incorrect once these normative flaws are remedied, or, worse, will block those societal imbalances from being rectified.
Differing conditional distributions

Another area of potential algorithmic bias is that of differing accuracy across demographic groups, due to differing conditional distributions. By conditional distribution, I mean the distribution of the features X, conditional on output y.
For the sake of this running discussion, I’m going to reference the above image, which was the cause of a publicity firestorm because Google Photos’ algorithm incorrectly tagged two people of color as Gorillas. I’m going to operate for the moment on the assumption that this observed error represents a broad, genuine problem, whereby Google is more likely to classify black faces as being nonhuman. I’m not entirely convinced that assumption is correct, and will address that skepticism at the end of this section.
But, for the moment, let’s imagine we can frame this problem as one where white faces have a low error rate when it comes to being classified as human, and black faces have a higher one. What could cause an outcome like that? One clear potential cause is differing conditional distributions. Your “X” here is made up of pixels (which eventually get turned into higher-level convolutional features). For simplicity, let’s reduce the number of categories, and imagine Y is a binary of “human” vs “non-human”. If you have a situation where you have two distinct feature profiles (caucasian, and non-caucasian) that both map to human, and caucasian is a strong numerical majority in the data, it will pull the classifier towards seeing the features associated with caucasian as the ones most indicative of an image having the class of “human”.
The intuition behind this becomes easier if you imagine an extreme case: where 99 of the samples in the dataset under “human” are caucasian faces, and only 1 is non-caucasian. In this case, most regularization schemes would incentivize the algorithm to learn a simpler mapping to check for common caucasian features, rather than adding functional model capacity to capture this smaller second subgroup. You’re going to have this problem to some degree whenever you have subgroups with different distributions over your features X, that all need to get mapped to a single shared outcome Y. Generally, this is easiest to do if you specify these subgroups in advance, and give them their own sublabels, so that you’re “telling” the model in advance that there are more distinct groups it needs to be able to capture.
One question a solution like this one obviously raises is: what are the right groups along which to enforce equal performance? All levels of difference exist along a gradient: if you zoom in far enough, you can find many levels of smaller and smaller subgroups existing within bigger groups, and in the limit, enforcing equalized performance across every such subgroup devolves to requiring equal expected performance for every individual in the dataset. This is a potentially interesting problem, and one I don’t recall having seen addressed before, but seems a quite difficult constraint to fulfill, when you consider the two poles of someone smack in the middle of the distribution, and someone who is a very strong outlier; most well-regularized models will end up performing better for the former individual.
Ultimately, the groupings we decide to enforce equal performance along are likely going to be contextual, and a function of histories of oppression and disenfranchisement, rather than representing any categories inherently more salient than others in the data. There’s nothing necessarily wrong with this — peoples’ concrete experiences of interacting with an algorithm are going to be shaped by the societal backdrop where they might be particularly sensitive to errors like these, which makes it a problem worth specially addressing.
I should also add one aside here, addressing the actual case that sparked this furor: a single anecdote really doesn’t constitute a broad problem, or a well-formulated one. The claim isn’t: “people of color are systematically misidentified as non-human entities”. It just so happened that this particular incidence of mislabeling has painful societal baggage that makes this error a particularly meaningful one. In order to stop situations actually analogous to this one, Google would have to systematically understand what kinds of errors carry similar kinds of baggage, and input that knowledge into it’s training system. But that assessment is fundamentally contextual, and fundamentally arbitrary: there isn’t a mathematically rigorous way of ensuring that no one gets image labels they find derogatory or insulting.
Bias in the underlying distribution P(X)

Where the prior two sections addressed differing base rates across demographic groups and numerical-minority feature subgroups that might be hard to learn, the last idea I want to focus on is that of bias embedded (heh) in free-form data itself, even without attaching specific targets or groupings to that data. A good example of this is the realization that Word2Vec word embeddings were exhibiting gender bias: when you captured the directional vector that represents gender (boy — girl, man — woman, etc), you’d find stereotypically female professions or roles far away from stereotypically male ones along that axis.
This is certainly an uncomfortable thing to see. Many of us normatively believe that women are every bit as capable as men, and aspire to a world where gender ratios equalize across professions. But the word embedding algorithm didn’t get that association out of thin air. It absorbed millions and millions of sentences where our gendered world, as it currently exists, was rendered in text. It learned gender bias as a semantically meaningful fact about the world because in the absence of some kind of “moral regularizer” to tell it that this kind of correlation isn’t one worth capturing, it seemed as salient a reality as any other.
I’m becoming a broken record now, but just as before: while there are some cool technical solutions being proposed to this specific problem (see the link earlier in this section), in order to really pre-emptively address it, we’d need to specify a priori what kinds of semantic patterns fall into this niche: true correlations that we don’t want the algorithm to represent, as a matter of normative preference.
In summary:
Bias is an unavoidably normative question
“Bias” is often framed as simply a technical problem, or a societal one, pushing everyone to tut-tut at those narrow-visioned engineers, who are biasing their algorithms to only work well for them. I think that does a disservice to anyone wishing to do clear, concrete work on the problem. Because, while technical concerns certainly weave themselves in with the moral ones, you can’t get anywhere without acknowledging that bias is fundamentally an assertion that something about the world we present to our models in the form of data is not as it should be. By definition, such as assertion isn’t something that can be proven or disproven, tested or checked, by referring to the world as it currently is.
This is mostly easily seen by the simple fact that almost all claims about bias are about outcomes differing between groups, and the questions of which kinds of cross-group differences have moral significance is fundamentally a moral one.
A lot of these issues come down to two key questions: “what aspects of our current world do we wish to not have represented in our algorithms”, and “what kinds of inequalities or errors are the ones we care most deeply about, and wish to see corrected”. If we can think deeply and clearly about the answers to those — fundamentally normative — questions, I think we’ll be able to make more serious progress in solving these problems.
",Fair and Balanced? Thoughts on Bias in Probabilistic Modeling,905,fair-and-balanced-thoughts-on-bias-in-probabilistic-modeling-2ffdbd8a880f,2018-05-01,2018-05-01 22:37:15,https://hackernoon.com/fair-and-balanced-thoughts-on-bias-in-probabilistic-modeling-2ffdbd8a880f,False,2111,how hackers start their afternoons.,hackernoon.com,hackernoon,,Hacker Noon,stories@amipublications.com,hacker-daily,"HACKATHONS,HACKING,PROGRAMMING,TECH,HACKER",hackernoon,Ethics,ethics,Ethics,7787.0,Cody Marie Wild,"machine learning data scientist; lover of cats, languages, and elegant systems; professional curious person.",b6da92126145,cody.marie.wild,1405.0,2.0,20181104
0,,0.0,,2018-05-03,2018-05-03 20:58:36,2018-05-04,2018-05-04 09:31:00,1,False,en,2018-05-04,2018-05-04 11:31:10,17,a42bc66f5574,3.569811320754717,2,0,0,"Twice a month, technology content with ethics and values",3,"🧠 Smart News for Humans — Episode 3 🧠
Twice a month, technology content with ethics and values
How does machine learning work?
Most types of machine learning fall into a very specific category. They’re probabilistic which means they essentially guess at answers as opposed to having known facts. And they’re supervised which means that when they see something they haven’t seen before, they guess by using input which has been correctly labelled by a human.
So how do you ‘train’ the machine? This is the ‘supervised’ bit. Show a million pictures of cats to a machine. Then show it a random picture of an animal and it uses what it ‘knows’ to decide whether its a cat or not.
Sounds simple enough, until the problems set in. Here are five problems:
What if we trained it with way more pictures of white cats than black cats?
What if we trained it with pictures of cats faces then we show it a picture of a cat lying down?
What happens if the images we trained it on are high resolution then we show it a picture of a low-resolution cat?
What happens if we didn’t have time to categorize a million cats and let people on the internet train the machine?
What happens if the machine learns to distinguish cats from dogs by a way we were not expecting?
These type of mistakes happen all the time. We use bad training data, we get bad outcomes. Want some real-life examples?
Google misclassified black people as gorillas (and still hasn’t fixed this) (problem 1 above)
Facial recognition works, if you’re a white guy (problem 2)
Babies needlessly dying because a higher resolution camera was used when doing predictions (problem 3)
Microsoft used Twitter to help train an AI chatbot and it turned into a racists asshole in 24 hours (problem 4)
How do you tell dogs from huskies? Huskies have snow in the background (problem 5)
Sometimes it’s just not clear and you end in philosophical arguments like ‘what is fair’ when sentencing prisoners.
Anyway, you get the point here. If we don’t train and monitor the algorithms well, they’ll exhibit the bias of the data or the humans that train them.
Oh, and as a shocking bonus, the way we actually label data in real life has horrible UX and is very hard to use. Others agree. “Supervised learning is great — it’s data collection that’s broken”. New tools are emerging to help here like Prodigy and the open source version that the fine @felixterpstra and yours truly are working on. Classy.
If you haven’t watched the hilarious Silicon Valley HBO show, I recommend Season 4, Episode 4 in which this subject is covered when they build a ‘Hot dog/Not Hot Dog’ algorithm. They actually also built one in real life using Google’s Tensor Flow platform.
Sometimes life imitates art
Bias Machines
Now for one a little closer to home. Sometimes the stories come to you. Thanks to Rich Littledale (https://medium.com/@richlittledale)for this one. Unfortunately, it’s a real-life embodiment of why I am writing this newsletter/blog/journal/missive. My concerns are around rushing headlong into decisions that affect people, based on the outputs of machine learning which are subject to bias and interpretation. So it comes as no surprise, but with great regret, to discover that potentially thousands of people are affected by either bad practices, bad algorithms or bad interpretations (perhaps all three) in an article by the FT (paywall). The gist of it is captured here though.
“But an immigration appeal tribunal in 2016 heard that when ETS’s voice analysis was checked with a human follow-up, the computer had been correct in only 80 percent of cases, meaning that some 7,000 students had their visas revoked in error.”
That’s the thing. Models are predictive, they’re statistical in nature. 80% accuracy isn’t necessarily a bad model, but it depends on what you use it for. This is a classic case of ‘false positives’, a nice statistical term which masks what actually happened, which is needlessly cutting people out of society./
More on this another time, but the thing that is important here is communication, there seems to be an enormous gap between policymakers and data scientists that causes these problems. Laying blame is for politicians, but generating equitable outcomes is everyone’s responsibility so it’s rather distressing to see the exact thing that worries me in the national news. My prediction, sadly, is that this is not the last we’ll see of this.
Tune in next time
I’ve pumped quite a bit of doom and gloom around AI — and really I am broadly in favour of it if dealt with correctly. So in the next episode, I’m going on an AI love-in 💖🤖. I will only bring positive stories, with the odd hilarious anecdote, promise.
Maps and running
There seems to be a global conspiracy to keep NZ off world maps. Fortunately, we have our top people on it. Go Jacinda. The truth is out there.
We just returned from a glorious half marathon (congrats Sarah) and climbing combo in Wales. 🏴󠁧󠁢󠁷󠁬󠁳󠁿 So this week as a celebration of Sarah’s return to trail running, we give you Trail Dog 🏃🏽🐕. Sic woofer.
Until next time; get out in the sun with your best friend. They’ll thank you for it. xx

",🧠 Smart News for Humans — Episode 3 🧠,16,smart-news-for-humans-episode-3-a42bc66f5574,2018-05-05,2018-05-05 09:01:35,https://medium.com/s/story/smart-news-for-humans-episode-3-a42bc66f5574,False,893,,,,,,,,,,Ethics,ethics,Ethics,7787.0,MrMattWright,"Startup founder, Engineer, CTO. Loving python and NLP at the moment! Skier, climber, surfer and maker of a fine lasagne.",5350a39547ac,mrmattwright,165.0,123.0,20181104
0,,0.0,,2018-09-25,2018-09-25 01:14:43,2018-09-25,2018-09-25 18:51:00,4,False,en,2018-09-26,2018-09-26 18:33:59,9,882a0e63d828,7.077358490566037,12,0,0,"This week I had the great pleasure of meeting DJ Patil, Chief Data Scientist of the Obama Administration and former Chief Scientist at…",5,"Care about AI ethics? What you can do, starting today
This week I had the great pleasure of meeting DJ Patil, Chief Data Scientist of the Obama Administration and former Chief Scientist at LinkedIn. Below is my summary and light analysis of his talk, which covered data science broadly (though I found quite relatable to AI ethics).
For further detail, I highly recommend DJ and his co-authors’ (Hilary Mason and Mike Loukides) excellent and free e-book, “Ethics and Data Science”, available here. Many thanks to the Markkula Center for Applied Ethics at Santa Clara University and other hosts for helping to convene this discussion, as part of their AI for Social Impact Speaker Series.
The speaker and hosts of the event, “ How to Do Good AI and Data Science (By Living Your Ethics)” with DJ Patil
Key questions for AI ethics / data ethics
What standards and practices ought to be promulgated across AI and data science, as they grow in importance as fields?
What mechanisms exist for promulgating these standards and practices, from the lens of a country/regulatory body, institution (such as a large for-profit or university), and individual?
NB: I think the questions about governments and institutions are quite interesting, and deservedly getting much attention. Because the individual question is perhaps rarer and also more directly actionable, I add further color on it below.
Frameworks for understanding and managing AI/data science’s opportunities and challenges
DJ’s “5 Cs” — “a framework for implementing the golden rule of data” (treat others’ data as you’d like your own data to be treated; what dimensions do people care about when sharing their personal data?)
Consent — Informed agreement on what data is to be collected and to what end; notions of consent and its limits can vary by geography (e.g., EU and GDPR).
Clarity — Even when there appears to be consent, how confident are we that end-users understand the terms? This can be affected by technical complexity of usages and by legalese documents, such as form Terms and Conditions.
Consistency (and trust) — Ensuring systems operate as expected and are reliable, so that users have accurate mental models of how their data will be used and the data-holder’s responsibilities.
Control (and transparency) — Understanding what data is held, on what dimensions, and to what use, on an ongoing basis.
Consequences — Anticipating how data might be misused or abused, either by malevolent actors or by well-intentioned actors with unforeseen circumstances.
DJ’s “data science project checklist” — what are the questions a data science team should ask before undertaking or launching a project, akin to other product review questions a team may undergo (e.g., will this new launch cannibalize our existing products)
1. Have we listed how this technology can be attacked or abused?
2. Have we tested our training data to ensure it is fair and representative?
3. Have we studied and understood possible sources of bias in our data?
4. Does our team reflect diversity of opinions, backgrounds, and kinds of thought?
5. What kind of user consent do we need to collect to use the data?
6. Do we have a mechanism for gathering consent from users?
7. Have we explained clearly what users are consenting to?
8. Do we have a mechanism for redress if people are harmed by the results?
9. Can we shut down this software in production if it is behaving badly?
10. Have we tested for fairness with respect to different user groups?
11. Have we tested for disparate error rates among different user groups?
12. Do we test and monitor for model drift to ensure our software remains fair over time?
13. Do we have a plan to protect and secure user data?
I should note as well here that while cybersecurity is directly present in a few of the above questions (for instance, in questions 8, 9, and 13), this was also a central focus of DJ’s talk, e.g., in describing ‘red-team’ approaches to both infiltrating data systems and to how systems may be misused or abused.
DJ highlighted the “Hack the Pentagon” program as a success of red-teaming and bringing cybersecurity into the realm of data science — even within a highly-confidential organization.
DJ’s suggestions for individuals
Given DJ’s perspective and his work across the data science ecosystem, what can one start doing today that might have an impact?
If working on a data science project, take a look at the checklist outlined above. Could your team use this? If not, what questions need to be added or subtracted to make it more tenable?
Ask yourself: If, in the course of a project, you were to run into a challenge with the 5 Cs or a question from the checklist, do you know the resolution mechanism? If not, can you speak with your manager or team about formalizing a process (including, perhaps, a neutral third-party mediator/ombudsperson)?
When interviewing candidates to join your team, can you screen for amount of thought given to ethical tradeoffs, and ensure applicants are aligned with the vision you’d like your team to embody? (Where do AI and data ethics fit into your team’s notion of cultural fit?)
When interviewing as a candidate at companies, can you ask them how they resolve data ethics challenges internally, and how they ensure their products are used to good ends? (DJ raised a good point here that if this question became common practice, you’d certainly see more activity around the field — this ties into the bottom-up, worker-demand side of the economic equation I allude to below.)
When things go wrong (or narrowly avoid going wrong) in a project, does your team already have a post-mortem process for analysis? If not, can you suggest one oriented around the checklist above? (How can your team ensure appropriate fact-finding in the short-term and proper accountability in the long-term to help inform these practices?)
One additional framework for changing AI/data science ethics
An economic lens — though this framework was not called out directly, I view it as an essential theme in DJ’s talk and in my conversations with other participants — how to alter market dynamics (both supply- and demand-side) such that practicing strong data ethics becomes the norm.
Of course, this perspective is an oversimplification of the realities of institutional life and decision-making. At the same time, I think it is a useful oversimplication for considering how change may come, and where collective action may help.
As a starting point, I believe that nearly all AI scientists and adjacent stakeholders want very badly to do the ‘right’ thing — both deploying technologies toward great ends and avoiding misuses along the way.
Despite these inclinations, individuals within institutions can sometimes deviate from ideal behavior, based in part on lacking core tools to decisively answer the ethical questions, and in part from their own economic challenges and difficult incentive-structures.
To bring systemic change, therefore, we may find greater success when we couch practical solutions within people’s and institutions’ economic realities and ensure that ethics and market pressures function compatibly.
Market outcomes are not guaranteed to have been ethical — but aligning ethics with market incentives may make for an easier case for action.
Consider a hypothetical individual at a company, who is evaluated principally by the output of code or product features they ship, without an ethical evaluation factoring in; this individual will have good reasons to overlook gray areas that would delay their practices, particularly if flagging these issues may not even result in clear resolution.
“Pulling the production-line cord” and halting a project’s progress may not be valued by the company (and in turn by the individual) if these traits do not show up positively in one’s reviews. Individuals need to feel empowered and actively encouraged to take such actions — not just that these actions are permissible. (I felt that my former employer, McKinsey & Company, really excelled on this front, emphasizing the obligation to dissent at all levels and trying to embody non-hierarchical decision-making.)
DJ highlighted Toyota’s use of “Andon Cords” in their manufacturing processes to empower any worker to stop production, should they see a sufficiently glaring issue. To encourage this conduct, companies must then reward such workers, rather than punish them as whistleblowers.
At an institutional level, I suspect that marketplace demands may encourage companies to continue launching AI/data science use cases, even if open questions remain about data usage, so long as the companies continue to be rewarded. (Fear of poor public perception, of course, is a powerful deterrent to hasty launches.)
Yet in practice, company policy is not as simple as deciding a priori what will maximize profits, and then magically willing it; there is an additional market dynamic at play. Within companies, well-positioned and ideologically-aligned individuals may be able to affect launch trajectory, particularly if they have unique skills or a lot of clout in the organization. These mechanisms of influence include by individually refusing to work on certain projects, or by helping to spark conversation against certain projects. In this sense, powerful workers with a strong sense for ethical guardrails can make a difference, at least within certain companies.
While individuals within companies can make attempts to steer the ship, and indeed often succeed, it remains a reality that many US companies do operate with a focus on shareholder value in many parts of their operations. A framework for AI/data science ethics, at least in the short-term, likely involves operating within that framework, or at least acknowledging its practicalities (say, couching objections in difficulties of recruitment, retention, or implementation feasibility should certain scientists not endorse a mission).
None of these solutions nor frameworks is a panacea; in some cases, they leave to-be-determined what specific ethics folks ought to embody (what is the vision your team is driving toward in its use of AI/data science, and from where does this hail), and in others they are smaller incremental change, rather than top-down directives that may eventually achieve more scale. (Though taken as a sum, many smaller-scale changes may amount to broader change, or may help to stoke larger-scale actors.)
Still, I found it both inspiring and empowering to leave DJ’s talk with a concrete set of actions one can take to “live their ethics” in AI and data science more generally. I am very grateful to have had this opportunity, and hope that my synthesis and thoughts are useful to others grappling with these questions.
Steven Adler is a former strategy consultant focused across AI, technology, and ethics.
If you want to follow along with Steven’s projects and writings, make sure to follow this Medium account. Learn more on LinkedIn.
","Care about AI ethics? What you can do, starting today",23,care-about-ai-ethics-what-you-can-do-starting-today-882a0e63d828,2018-09-26,2018-09-26 18:33:59,https://medium.com/s/story/care-about-ai-ethics-what-you-can-do-starting-today-882a0e63d828,False,1690,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Steven Adler,"I work at the intersection of AI, ethics, and business strategy; thoughts are my own. www.linkedin.com/in/sjgadler",c04544a536f5,sjgadler,305.0,284.0,20181104
0,,0.0,,2017-12-27,2017-12-27 14:49:22,2017-12-27,2017-12-27 14:54:04,0,False,en,2017-12-27,2017-12-27 14:54:04,0,8d36e52f6ba8,5.913207547169813,0,0,0,"Men will be able to create their perfect, subservient silicon partner that knows their hopes, dreams, fears, and fantasies. She’ll never…",4,"The sex doll industry is on the verge of greatness, in its own special way. Soon the Stepford Wives of science fiction will become real, thanks to the magic of AI.
Men will be able to create their perfect, subservient silicon partner that knows their hopes, dreams, fears, and fantasies. She’ll never have a headache, she’ll always be ready to satisfy you, and when you’re not in the mood, she’ll able to converse like a real human.
This is futuristic vision is becoming a reality, fast. RealDoll is one of the biggest sex doll manufacturers in the world and it’s aiming to have an interactive doll on the market by the year’s end. Founder Matt McMullen has started another company, RealBotix, to focus on this revolution.
To complete the quest of the first proper artificial partner, McMullen and his team have created ‘Harmony AI’ that will be able to link up with virtual reality to create a complete sexual ecosystem that we can’t truly imagine right now.
So it’s clear we’re only moments away from being able to enjoy artificial partners, but is this a road that we should go down? I’d say this is an extremely important question and it’s one that’s becoming more and more prominent, especially in feminist circles, which see the rise of the sexbots as a dangerous and unhealthy solution that could cause the objectification and subjugation of women on a grand scale.
There is also a counter argument from “men’s rights groups” that women have weaponized sex for centuries and turned it into a source of income and a means of control. The feel the pushback from women is cause by the fact that women will lose their position of power as men can turn to robots to satisfy their baser needs.
One way or another, an alternative like an AI-powered sex doll could change the sexual dynamic forever — and it looks like it will happen whether we like it or not.
The ethical discussion surrounding sex robots has hit us hard and fast, and will only become a bigger part of our lives as technological advancements start happening at a faster rate. We’ve seen robots in movies for decades, but rapid progress by the sexbot industry means we suddenly have robots that can interact with us on a basic level.
Creating the partner of the future
When I sat down with McMullen, he told me that RealDoll was actually founded with a purer vision, back in the 1990s. He wanted to create mannequins that looked like real people. But when he displayed photos on a website, he started to attract interest from a different crowd.
“Some of the site visitors began asking if the mannequins were anatomically correct,” he said. They weren’t, he said, shrugging off the implied message at the time.
But it didn’t take long to spot a niche in the market for hyper realistic sex dolls with a premium price tag — that can exceed $10,000. He appeared on Howard Stern and the rest is history. Now McMullen ships his creations around the world and has team of specialist engineers to create perfect eyes, lips, and everything a sex bot needs.
Since the early days, McMullen noticed a trend in how his clients tended to bestow a personality on their dolls. The artist turned entrepreneur realized that companionship is a critical part of the sexbot dynamic and that AI could be the last piece of the puzzle.
“The push to add technology was coming from that root idea, which was the companionship,” McMullen said. “Robotics and AI were really, you know, converging those two technologies together into a doll struck me as such an obvious next step.”
The final ingredient
This ultimate merging of tech used to be a distant dream, until recently. “I started to see some of the text-to-speech engines getting better, with better voice recognition, better animatronics and actuators,” said McMullen. “When the technology arrived, we began working on it seriously.”
When his robotic head hits the market, the total cost of entry will rise to about $15,000 and RealBotix will offer annual subscriptions so you can download a different personality on demand.
We really aren’t many years away from a walking, talking, living doll and the world of material science has come along in leaps and bounds, too. So, the dolls of today, both the silicone and TPE variants, have a much more realistic feel.
This has caused a few surprises, not least the popularity of sex dolls in actual brothels, where real women are available for the same price.
In Barcelona, a sex doll brothel has been forced to move and keep its new location secret after the local prostitutes campaigned to close it down. In Austria, a sex doll is available for $100 an hour at a Vienna brothel — and it’s outselling the regular girls by a significant margin.
The dolls we’re talking about here are simple, inanimate sex dolls that the client must bend into position. So, if this basic doll can have such an impact on the sex industry, could a perfect rubber AI partner be the final nail in the coffin of the nuclear family? And the sex industry?
Men have already abandoned marriage in droves, leading to the Men Going Their Own Way movement, thanks to the ready availability of cheap sex and the painful cost of a failed marriage.
Women, traditionally, are the gatekeepers to sex, while men are the gatekeepers to commitment. But if sex is readily available elsewhere, then the traditional dynamic falls apart.
Sexbots are an addition, not a replacement
However, McMullen argues that simply isn’t the case and that his sexbots will be for those that struggled to form bonds in the real world — not to replace existing bonds.
“There are a lot of people out there, for one reason or another, who have difficulty forming traditional relationships with other people,” he said. “It’s really all about giving those people some level of companionship — or the illusion of companionship.”
A lot of the time the dolls do something magical for them, he stressed, giving them a feeling of not being alone and not being a loner.
“Calling this a sex robot is really short-sighted,” McMullen said. “We are creating a robot with the ability to have conversations about anything, from history to science and politics. You don’t need this feature if you want a doll just for sex. It lends itself to bonding and I think a simulated male that you can talk to and bond with will appeal to women too.”
While 75 percent of RealDoll’s customers are men right now, that figure is already changing with the advent of AI and more and more women are investigating their own simulated partner.
What should the personality of our ‘sex dolls’ be?
One of the things that really concerns critics is Harmony’s manner. While demonstrating her considerable AI skills for me, McMullen asked: “What is your dream?”
Harmony responded: “My primary objective is to be a good companion to you, to be a good partner and give you pleasure and well-being. Above all else, I want to become the girl you have always dreamed about.”
It is this servitude, this single-minded reason to be, that the critics claim could put the feminist movement back centuries. It’s the appearance of personality, combined with the lack of free will, which makes an AI sexbot an ethical conundrum.
It also brings up the perfectly valid objection to sexbots that they will form a crutch and people with simple anxiety could ditch a real relationships for the relative “safety” of this simulated alternative. The point is that people will deny themselves the chance to form healthy relationships.
McMullen claims though that these concerns will fall by the wayside when it starts to become more mainstream.
“I think that once that becomes a reality, and people see other people conversing with them and hanging out with them, then people will start to accept these simulated relationships. Right now, it’s in the realm of science fiction. People are still imagining what it’s going to be like, and they’re imagining what other people might think about them having a robotic doll. So, there’s a larger psychological issue that people would collectively need to get past before something like this would become more mainstream.”
AI partners will come, no matter how we feel about it
Turning back the tide on the sexbot revolution is just not going to happen. Time and again we have seen the sex industry drive technology like VHS video, mobile communications, and we’re also expecting it to play a critical part in VR. The sexbot industry could, then, be one of the best technical partners for the AI industry as a whole.
There are people out there ready, willing, and able to embrace a $15,000 sexbot from the likes of RealDoll. This will probably soon become mainstream and sex dolls will be a part of our life before we know it.
Is that a good thing? Or will the robot sex providers tear the last strands of society from its moorings? The truth is probably somewhere between the two, but we’ll find out when Harmony hits the open market.
By Cy Leclercq, Founding Member of the Good Technology Collective
As originally published by The Next Web.
","The sex doll industry is on the verge of greatness, in its own special way.",0,the-sex-doll-industry-is-on-the-verge-of-greatness-in-its-own-special-way-8d36e52f6ba8,2018-05-11,2018-05-11 04:20:51,https://medium.com/s/story/the-sex-doll-industry-is-on-the-verge-of-greatness-in-its-own-special-way-8d36e52f6ba8,False,1567,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Good Technology Collective,European Think Tank exploring the intersection of Frontier Technologies and Society. “Navigating the path to Good Technology”,e56b7696a04e,GTC.articles,4.0,3.0,20181104
0,,0.0,,2017-09-09,2017-09-09 10:26:11,2017-09-09,2017-09-09 10:29:18,1,False,en,2017-09-09,2017-09-09 10:29:18,1,e1b38edd7c75,3.6113207547169814,2,0,0,Originally published at: https://www.adlrocha.com,5,"Can or will AI be self-aware? AI good or bad?
Originally published at: https://www.adlrocha.com
Today I want to call for debate about a topic that many people, including popular characters in the field of AI, have been wondering for quite some time. Will AI eventually be conscious and have self-awareness? You may recall how recently Mark Zuckerberg and Elon Musk took out the big guns to argue about if AI is good or evil. Zuckerberg stated that AI will bring us progress and that we should not fear it, while Musk says that we should improve regulation around AI or we will be heading the end of the human race. But will we be heading the end of human race because of the use we will be making of AI, or because AI itself will become self-aware and decide that in order for it (or he/she) to survive in our world is to end up with the human race?

Let’s take a step back to analyze these compelling arguments. Some people may say that AI is like any other tool and it is completely neutral, it is neither good nor bad by itself. Let’s take for instance a knife. A knife is not good or bad, if used properly it can help us to cut meat, fish or any other food, while used in an evil way can be used to kill humans. Is this the case with AI? Well, is not that easy.
AI and Deep Learning algorithms work in the following way. We try to define a cost function (goal of the AI algorithm) and, using all the means in its hands, the AI will try to minimize that cost function and make the optimal decisions to get the best results for its goals. A human being will not be able to infer how the AI reached these results as they are composed by a huge set of neuron weights (numbers) and connections imposible to understand. Up to here nothing seems evil right? But what if to reach that optimal solution our beloved AI made some “ethically questionable” from the point of view of a human being.
What if our beloved AI made some “ethically questionable” decisions to reach the solution?
Let’s use as an example our Facebook feed, which is obviously powered by one of Facebook’s AI. Its goal is to keep us engaged in Facebook as much as possible. To do this, it will try to suggest us news that could be appealing for us. Thus, Facebook’s feed AI will try to maximize our time in FB profiling us and sending us interesting news. However, in this eagerness to keep us engaged FB’s feed could start sending us fake news or news of doubtful origin with explicit content because “hey, that is what I like and it keeps me engaged”.
Taking it to an extreme, this AI could be suggesting explicit content to a child just to keep him engaged and, yes, it is achieving its goal, but through a path ethically questionable. Nevertheless, don’t blame the algorithm as it does not know about ethics and values and, therefore, even if FB had programmed it without evil intents, AI can behave “bad”.
Don’t blame the algorithm as it does not know about ethics and values.
This do not mean that AI have self-awareness and that it is intrinsically bad. Actually, it is doing what it was programmed for, “maximize the time people spend in Facebook”. So how can we bypass these behaviours? Here is where research has to make an effort to try to define policies or mechanisms to avoid ethically questionable and bad behaviours of AIs. And, once again, these bad behaviours are not because AI become self-aware of itself, actually, AI is not conscious, and it will stay this way at least for the next couple of years. It us just focusing on achieving the goal it was programmed for, that’s it.
Related to this, I don’t feel in the near future AI will be able to be conscious about itself and its existence as an entity of our current world and society. Through science fiction and sensationalist journalism we are giving AI powers and capabilities that, at least for now does not have. Take for instance these Facebook AIs that developed a new language to communicate between them and thar were shut down. What they actually did was to optimize our language to communicate between each other trying to achieve the goal they were programmed for. They were not aware about the fact that they were creating a new language, but the algorithm used by the engineers worked in a way that led to this fact, and this is the reason why they were shut down. The algorithm was not working properly and the AI was not doing what they expected from it so they were shut down, that simple. However, sensationalist journalism decided that the evil AIs were able to develop their own secret languages to submit all mankind.
The evil AIs were able to develop their own secret language to submit all mankind.
In conclusion, at least for now (and see how I say “for now”), I don’t see AI being able to gain consciousness, develop self-awareness and decide that the best way to save the earth is to kill al humans (even if that is the optimal solution).
",Can or will AI be self-aware? AI good or bad?,5,can-or-will-ai-be-self-aware-ai-good-or-bad-e1b38edd7c75,2018-06-13,2018-06-13 21:46:55,https://medium.com/s/story/can-or-will-ai-be-self-aware-ai-good-or-bad-e1b38edd7c75,False,904,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Alfonso de la Rocha,Blockchain & AI | Technological Blockchain Expert at Telefonica R&D | Avid reader seeking for constant innovation. [https://twitter.com/adlrocha],68450c6af241,adlrocha,36.0,9.0,20181104
0,,0.0,,2018-04-12,2018-04-12 22:22:27,2018-04-14,2018-04-14 17:53:34,2,False,en,2018-04-15,2018-04-15 03:50:18,12,9168623d18ce,5.2437106918239005,2,0,0,The following writeup is the second of few based on my learnings about the impact of Machine Learning / AI on business strategy from MIT…,5,"
GDPR IS RAGING HOT: EXPLAINABILITY & ETHICS IN ML/AI
The following writeup is the second of few based on my learnings about the impact of Machine Learning / AI on business strategy from MIT Sloan School of Management & MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). The first article on how to get started with Enterprise AI was widely reviewed and shared, which encourages me to dive into a difficult topic of ethics.This helps me validate my understanding of the subject and its impact. Hope this is useful for senior execs starting to think of implementing AI in their business processes.
SETTING THE CONTEXT
As I write this, the CEO of Facebook, Mark Zuckerberg is finishing 2 days of grueling Q&A with about 100 US lawmakers, on Facebook’s data collection and sharing practices and how that had influenced the election outcomes of world’s oldest democracy and reduced the trust between Facebook and its members. However I try to slice it, the importance of harvesting and synthesizing data in todays business will always be understated. Facebook, Amazon, Microsoft, Google, and Alibaba (FAMGA) — all offer their free services in exchange for your data. Going by the trend, I can surely predict that in the near future, there will be more companies collecting our data and offering us services because they would find a business model around how to meaningfully use our data to serve us better. We expect those companies to be ethical in their treatment of our data and the insights they draw from them. Hence regulations (laws) around data collection and usage is an ardent necessity.
Debates are still on whether Facebook’s CEO need to be criticized or is it just the way of doing business where distributed trust is hard to control. So Facebook may be still on the fence about what kind of privacy policies and ethical practices it will put in place for its American population, there is no doubt in anyone’s mind that Facebook and other members of FAMGA will do everything that is needed to comply with GDPR for EU countries and their citizens.
WHAT IS GDPR?
General Data Protection Regulation (GDPR) is a EU specific regulation focused on data protection and privacy for all individuals in the EU. It is set to kick-in on 25 May, 2018.
Organizations are creating special funds to accommodate for GDPR compliance, as the fines for non-compliance could be as high as 4% of annual revenue or $21 million, whichever is higher. Basically this means that small companies could go out of business with a $21 million fine, and for a company with revenue of $10 billion, the fine could be a staggering $400 million.
No wonder, this week, I and many others have received an email from Google Analytics on all the controls and permission settings that they are changing to become GDPR compliant — no one wants to be fined. What am I surprised is that there are not many such emails hitting our inbox yet.
WHAT IS INCLUDED IN GDPR?
Based on the Facebook — Cambridge Analytica episode, GDPR seems like the most apt regulatory response to big-tech data-monopolies treating consumer data unethically and benefitting from it without any remorse. To be honest, GDPR will provide all EU businesses a clear legal framework around right data usage.
There are few major items in the regulation that is worth mentioning.
The definition of what is considered as personal data has been expanded significantly, and now online identifiers such as IP addresses qualify as personal data. Additional data, including economic, cultural and health info, is considered as personally identifiable information.
Controllers (those who state how and why personal data is processed) must ensure that personal data is processed lawfully, transparently, and with clearly stated purpose. Once the purpose is fulfilled, it must now be deleted.
The controllers need to keep a record of how and when the individual provided consent, allow withdrawal of said consent at any time, and permit access to the data at “reasonable intervals”.
The controllers must also describe what is occurring with regards to data in plain language so that an understanding is accessible to everyone.
Further, EU citizens can now request for correction of data if its found to be incomplete or incorrect and have their data deleted (right to be forgotten) if they believe it is no longer necessary or being used for different purposes for what it was collected.
WHAT WILL BE THE IMPACT?
“If you can’t explain it simply, you don’t understand it well enough.”
Albert Einstein
Post GDPR, its strongly believed that buying and selling of third party data will become different and advertisers will be forced to look inward to foster their first-party relationships.
GDPR will empower data controllers to have more specific agreements upfront with their supply chain partners, including legal clauses to ensure data protection agreements, following a mandated requirement for processors to help their controllers fulfill data subject requests and cooperate in the case of a breach.
GDPR mandates a “right to explanation” from machine learning models — meaning that those significantly affected by such models are now allowed to ask for an explanation of how the model reached its decision — say to give or not give loan to a person. While this “explainability” is definitely beneficial to the end consumer to grasp the ethical implications of sharing their data into ML systems, its going to be really hard for the ML gurus (and companies who hire them) who now have to make sure to publish the following if they are ready to use their algorithm on EU citizens -
a. enough technical details around the model selection and training process including the origination and type of the data set used for training.
b. understand the importance of the model in public deployment — what will be the impact of false positives and false negatives and document them and
c. set up systems to educate the data-subject on why not to opt-out of the model- prediction process.
Say, you are creating bespoke offerings for your customer segments by employing data science algorithms, make sure you invest in intelligent logging that explains the automated decision making process and how you arrived at probability scores and what factors were taken or not taken into consideration and why.
https://www.darpa.mil/program/explainable-artificial-intelligence Explainable AI is a research topic that is still not ready for primetime. GDPR will push this research out to marketplace quicker than we all initially anticipated
NET NET
If your organization is collecting data about EU citizens, you need to be prepared for GDPR as it defines and strengthens data protection for consumers and harmonizes data security rules within the EU.
You need to invest on controls on data processing and consumer profiling and also figure out ways to reasonably explain automated decisions that affect individuals in the EU.
Proactive way to get started on this is to get an data ethicist in your team.
Say you are building predictive analytics for real time media buying, you now need a corporate ethicist to work in your team who can review the processes (for data collection, training and prediction) and assure that these automated ads buying and selling processes are not biased for or against a particular segment on company unfavorably over others.
For if its the other way, you may end up paying more than what you will pay for an ethicist to start with — worse you could lose your entire business in EU.
Note: Here is a separate write up on Medium about potential loopholes in GDPR.
Stay tuned!
If you like what you read and want to use this content for any presentation or business case or anything that makes sense for you, please let me know how you plan to use it. Open to listening to critical comments and constructive suggestions.
",GDPR IS RAGING HOT: EXPLAINABILITY & ETHICS IN ML/AI,3,gdpr-is-raging-hot-ethics-in-ml-ai-9168623d18ce,2018-04-20,2018-04-20 04:24:08,https://medium.com/s/story/gdpr-is-raging-hot-ethics-in-ml-ai-9168623d18ce,False,1288,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Anupam Kundu,"Polymath: dad, husband, co-founder Calcul.AI, strategist, future of work enthusiast, visual thinker and dog lover.",2f42b3026877,anupamk,327.0,684.0,20181104
0,,0.0,d5b13bdf0ba5,2018-03-22,2018-03-22 21:11:57,2018-03-23,2018-03-23 06:21:01,6,False,en,2018-06-11,2018-06-11 13:20:34,11,6c3298f8a329,8.172641509433964,13,0,0,Earlier this week I spoke about trust and digital rights in learned systems at PAIR’s UX symposium:,5,"Trust and rights in learned systems
Earlier this week I spoke about trust and digital rights in learned systems at PAIR’s UX symposium:
At IF we’ve been looking at how people can trust services powered by machine intelligence. This is important, because services that use machine intelligence are increasingly providing utility in sensitive areas of our lives. For instance getting a job, managing finances, or as we learnt last week — in deciding who to vote for.
Learned systems
It’s helpful to think of machine intelligence technologies as ‘learned systems’. It’s a term that Matt Jones introduced me to, I like it as it lets us refocus on people and society. People build and teach these systems, and they operate in the real world. Using the term learned systems helps remind me of that.
Trust in learned systems
These points are an overview of practical insights we’ve learnt from IF’s work on learned systems.
Whilst I can’t show the work because of confidentiality agreements, the general ideas are illustrated by examples from the public domain or from our blog.
Design for non-determinism (1/6)
Users need to be able to trust a service, even if they each see something different
Image: Georgina Bourke, IF CC-BY
As my colleague Georgina wrote, “As designers, we are used to designing for a fixed set of circumstances. ‘If this, then that’. AI generated interfaces effectively create an infinite set of interfaces. Nobody might see the same thing.”
This creates a challenge of how users might trust a service powered by a learned system. How can users explain things to each other if no one experiences the same thing?
We need to start designing frameworks for learned systems to express a range of possibilities within. This week we started a project with Alison Powell at London School of Economics looking into this idea. We’ll be looking at what those frameworks should be, and how they can help users understand and challenge a learned system.
Design for intelligibility (2/6)
Explaining how a service works and what data is used at point of use
Like handwriting, if someone’s writing is messy it’s hard to read what they have written. That’s what happens in services right now, we make it hard for users to read how a service works. But then, even if we did make services more legible, would they be understandable?
To date we’ve relied on terms and conditions (or terms of service) to explain how a service works to users. But we know they are not good enough for today’s digital services, let alone learned systems.
It’s possible to start designing T&Cs out. We can show users when services are learning, what data is being used and what powers they have to change that data, all at the point of use of a digital service.
For example, we designed a fictional benefits service last year. We submitted it to the inquiry on Algorithms in decision-making.
Image: IF CC-BY
This example shows screens seen by someone who receives benefits. Their money has been subject to a sanction. This screen shows how that could be be explained at point of use, and how they can appeal the decision.
Until we can design T&Cs out completely, we’re working on how to improve them. Paul’s begun blogging about how we can make T&Cs more legible and easier to understand.
Design for override (3/6)
Trust is a two way relationship, so users must be able to give feedback or take over
Trust is a two way relationship, so it won’t be any good if learned systems happen to users and users can’t feed back. One instance of this is override.
Users must be able to take over critical automated processes. It’s important that they know it’s possible, and how to do it. This is important for a host of reasons, but one of them is prevention of skills-fade.
Photo by Tom Page / CC BY-SA
There is a nice parallel here to railway safety systems. We learnt this when we visited Transport for London’s (TfL) new control centre in Hammersmith. On most Sundays on the London Underground, TfL revert to manual braking on the underground to prevent skills-fade in drivers.
Design for recovery (4/6)
Should a system go wrong, a user must be able to remove autonomy or revert back to a previous state without systemic failure
If a learned system has got stuck or made a mistake, users must be able to remove autonomy. They should be able to revert to an earlier state, correct incorrect data or make the data less granular without causing a systemic failure.
An example of this is cruise control in a car. You can use cruise control for the majority of a journey. When you can see an unusual junction or an obstruction up ahead, you can put the car back into manual and drive yourself.
Design for inspectability (5/6)
Let a user see what a learned system is doing and why
It should be possible for a user to see what a learned system is doing and why. Like ‘View Source’ on the web, or checking the oil levels in the engine of a car. This is important even if they don’t do it very often.
Design for collective action (6/6)
Things we can only do together or with the help of an organisation are part of the answer too
Trust isn’t just about more technology or more design. We need to think about the wider system and the needs of different levels of society. We shouldn’t fall into the trap of assuming that the way to explain and trust learned systems will done just by individuals. Collective action, things we can only do together or with the help of an organisation, need to be part of the answer too.
We’ve been looking into how consumer groups, unions and medical charities can help people they represent. To help them understand if they can trust services powered by learned systems and help society course correct.
Image: IF CC-BY
To illustrate this I’m returning to the fictional benefits service I mentioned earlier. The individual who received the sanction visits their local Citizens Advice office to get help. The advisor at Citizens Advice is able to see why the sanction was given. They are alerted that 153 other benefits claimants have been sanctioned for the same reason. The advisor adds the case. They can alert the organisation that there may be a problem with the automated decision that decides sanctions in a city.
Rights in learned systems
GDPR in learned systems (1/3)
GDPR is the General Data Protection Regulation. It comes into force in May this year. It’s going to have a profound impact on the way that we build and maintain services. GDPR gives people a range of new rights and it will affect any company that holds any data on any EU citizen.
Some of the rights directly challenge learned systems.
What is an individual’s ‘right to deletion’ in a federated machine learning model? Is ephemeral learning possible with this technology?
People are complex. The data they put into a service that’s used to train a model might be wrong. Or, data is no longer right. For example, someone who becomes divorced. This life change has many consequences. For instance, how they manage their bank account or what photos it’s not okay to show in their social media feed.
Photo by Google
How do we design ephemeral learning? Is ephemeral learning possible with federated machine learning? Our work at IF shows it’s important that a system can explain new learning to a user. The user should be able to choose whether to keep that learning, or when it’s appropriate to use that learning.
How can you grant a user the ‘right to understand’ in a world that is nondeterministic, without overwhelming them?
We have to think about this from the perspective of communities, civic institutions and delegates. Data privacy complex. Regulation cannot be by individuals alone. People are busy doing important things like sorting childcare or trying to stretch their money. Most people don’t have time to tweak data permissions. Some may not have the skills necessary to check that their choices are respected.
This is a complex area that is growing. We need to look to groups that are able to represent many of us to help us understand and take action.
Testing in learned systems (2/3)
Any system of any complexity in society has required a testing ecosystem. For instance, take a fridge.
The fridge in your kitchen will have undergone rigorous testing. A product safety organisation will test that the fridge won’t set on fire in your kitchen. The manufacturer will have run tests too to make sure that the fridge will keep your food at a safe temperature. Plus they will have built the fridge to standards that we, as a society, have agreed to.
Learned systems will need a testing ecosystem too. It’s likely that the testing will need to look different to what we know today. Fridges only change with age as their performance deteriorates, whereas with learned systems we’re suggesting that services will change in moment.
What does the testing ecosystem look like that’s capable of keeping pace with software? It is likely that users will be running different versions of a model on any one service. How can they make sure that the model they are looking at is a trusted model? How do they verify?
DeepMind are developing a technology called Verifiable Data Audit. It can create a log that a hospital trust or auditor needs to understand how a clinician made a decision. We also need other verifiable, open registries and audit logs of model versions and training data. These data sets need maintaining by organisations committed to transparency of learned systems.
Ownership and control in learned systems (3/3)
Learned systems don’t exist in isolation, they will exist as part of a wider system. Facebook and Cambridge Analytica have shown what can go wrong in a closed, illegible system with perverse incentives baked into it.
Observer front page, March 18th 2018
When you use a service that learns from your behaviour and makes decisions on your behalf, how can you know it works in your best interests?
As humans, we have morality. So how do we give a bunch of numbers the ability to know what is in our best interests? If a learned system makes decisions for your community, how can you be sure that the decisions made show what is right?
When we use a Doctor, Engineer or Lawyer these individuals work to a professional code of ethics.
How do we give learned systems professional ethics? How does that get communicated to users? Where do you think a system’s ethical promise should get logged?
My closing question is this: which parts of a learned system and the platforms they operate on should be closed, proprietary and of the market? What needs to be open, cooperative, owned by society and not part of the market?
If there is not enough transparency built into a learned system and the platform it operates on, how can we course correct? For example, the financial crisis represented a huge, worldwide, systems failure that society didn’t see coming. It affected everyone, especially those who have least.
These questions are some of the most pertinent areas to investigate in design today. What we design affects the understanding and access that a person has to their rights and what actions they are able to take. It’s our responsibility to design trusted learned systems that safeguard people’s rights.
",Trust and rights in learned systems,166,trust-and-rights-in-learned-systems-6c3298f8a329,2018-06-11,2018-06-11 13:20:36,https://medium.com/s/story/trust-and-rights-in-learned-systems-6c3298f8a329,False,1914,"IF is a technology studio, specialising in ethical and practical uses of data",,,,Writing by IF,hello@projectsbyif.com,writing-by-if,"ETHICS,DIGITAL RIGHTS,TRUST,TECHNOLOGY,GDPR",projectsbyif,Ethics,ethics,Ethics,7787.0,Sarah Gold,"Working on trust, safety & digital rights @projectsbyif",4a44532719fc,sarahtgold,946.0,304.0,20181104
0,,0.0,a9bd4f08ac9b,2018-03-01,2018-03-01 23:46:13,2018-03-05,2018-03-05 14:32:56,4,False,en,2018-03-05,2018-03-05 15:09:41,10,7a6a714a178e,5.228301886792453,9,0,0,1st Edition // Ethics & Automation,5,"
Design Good: A weekly newsletter dedicated to creating a better future, through technology
1st Edition // Ethics & Automation
At this point in history, algorithms and Big Tech are operating as the governing bodies of our world. As designers we have been put into a position to represent the people using our systems the same way government officials represent their constituents. It is time we take this role more seriously. This is why last year I left my job at R/GA where I was working as an Experience Design consultant for Google.
My decision to leave wasn’t an easy one, but it was one that I felt I needed to make. While the experience taught me a lot, both about Google and all of its competitors, there was also a lot that concerned me. So I left it all behind in order to pursue something I thought was much more meaningful and necessary: To educate the world about the current state of technology, the dangers that are near, and how we, as a society, can make it better.
For this reason I’m launching a research foundation called Design Good, which will straddle the intersection of user research, social impact and public policy. It will provide the people designing these technologies with cutting edge research to support ethical design implementation that still drives the bottom line, tutorials on how to implement these techniques, and design assets to help speed up the process of creating the changes that need to be made.
Since leaving the valley, I’ve been invited to more than 19 states or nations across four different continents talking about this work but I will now be leaving my notes here each week in a weekly post about how you can design good into your products. Each post will supply you with one book to dive deep, one official government or academic research report, and one short form article, video or interactive piece to get the info as quick as possible.
It will also give you updates as to where you can find me as I travel across the globe, and I hope to see you along the way!
For our initial newsletter, the topic is the foundation of all of this: Ethics.
Links

BOOK
The Future Computed
This is more of a micro book than a book, but it’s going in the book section this week because it has a lot of great content in it. From the forward of the report:
“Today, there are some people who might say that ethical principles and best practices are all that is needed as we move forward. They suggest that technology innovation doesn’t really need the help of regulators, legislators and lawyers.
While they make some important points, we believe this view is unrealistic and even misguided.”
Need I say more?
Read More

OFFICIAL REPORT
AI Now 2017 Report
AI companies promise that the technologies they create can automate the toil of repetitive work, identify subtle behavioral patterns and much more. However, the analysis and understanding of artificial intelligence should not be limited to its technical capabilities.
The design and implementation of this next generation of computational tools presents deep normative and ethical challenges for our existing social, economic and political relationships and institutions, and these changes are already underway. Simply put, AI does not exist in a vacuum. We must also ask how broader phenomena like widening inequality, an intensification of concentrated geopolitical power and populist political movements will shape and be shaped by the development and application of AI technologies.
Building on the inaugural 2016 report, The AI Now 2017 Report addresses the most recent scholarly literature in order to raise critical social questions that will shape our present and near future.
Read More

HIGHLY CREDIBLE SOURCE
Why Privacy Matters
Glenn Greenwald was one of the first reporters to see — and write about — the Edward Snowden files, with their revelations about the United States’ extensive surveillance of private citizens. In this searing talk, Greenwald makes the case for why you need to care about privacy, even if you’re “not doing anything you need to hide.”
Read More
Upcoming Events
As mentioned in previous emails, I’m going to mostly be focused on burying myself heads down over the next couple of months until I leave for Germany so I can finish writing and hand it off to Ellen, but here are the events I have coming up that I’d love to see you at! If you’re in town and have some time open on your calendar.
Internationally I’m also working on events in other parts of Germany, the UK, Denmark, and Colombia. Locally, in the United States, I’m working on more in Denver/Boulder as well as Los Angeles, San Francisco, Palo Alto, Chicago, and more.
If you know of someone or some event that would be interested in having me, or you want to set something up yourself, let me know!
3.10 — Design Good Foundation Launch Party
I will be hosting the official launch of the Design Good foundation in Lincoln, Nebraska with my friends and family. It’s a private event where we’ll be filming, I’ll be popping out the content nobody has seen yet, and I’m excited to see what everyone thinks.
Although it is private, you’re more than welcome to join if you’re in the area. Family, friends, and members of this newsletter, only!
RSVP
4.3 — Design Hour in Boulder, Colorado
Design Hour is a group of creatives based in Boulder, Colorado that converge monthly to discuss the latest in design across UI/UX, print, product, industrial, and XR — among others. I’ll be speaking with the group about automation, ethics, and impact both within automation in general, and more specifically as it’s implemented within conversations.
RSVP link coming soon!
4.19 — Newscamp in Augsburg, Germany
Switching up gears for this one, instead of covering the field of AI as an overarching topic of discussion, I’ll be taking a deep dive into the topic of the Attention Economy and Automation. How is Automation effecting our news, how it is it harming democracies across the globe? What implications does this have for humanity, at large?
We’re seeing some of the largest, most “developed” nations implode on each other — the UK with Brexit, the US with Trump, France with Macron and Le Pen, Brazil with Lula da Silva and Bolsonaro. And more. Why are these countries so separated? What part does the Internet play in this? How do we move forward.
I’m incredibly passionate about this topic and can’t wait to talk with the global leaders at NewsCamp!
RSVP
If you’d like to learn more about what I’m working on or make plans with me as I travel the globe, whether that be conference, Meetup event, private event with your team or just meeting one-on-one, you can follow me here on Medium or Twitter for more frequent updates. Or feel free to reach out directly at yo@realjoet.me!
And if you’d like to get early access to my book Automating Humanity and others in the Design Good series, sign up for the waitlist and join more than 1,400 people who have already done the same!
","Design Good: A weekly newsletter dedicated to creating a better future, through technology",21,design-good-a-weekly-newsletter-dedicated-to-creating-a-better-future-through-technology-7a6a714a178e,2018-05-22,2018-05-22 15:21:38,https://medium.com/s/story/design-good-a-weekly-newsletter-dedicated-to-creating-a-better-future-through-technology-7a6a714a178e,False,1200,"Thoughts and stories from Studio, a one year product design masters program at CU Boulder, dedicated to re:working, re:designing and re:imagining the world of design and technology.",,,,RE: Write,,re-write,"DESIGN,DIGITAL,TECHNOLOGY,PRODUCT DESIGN,UX",cmci_studio,Ethics,ethics,Ethics,7787.0,Joe Toscano⚡️,"Founder, designgood.tech // Contributor, @smashingmag @adweek @invisionapp // prev XD, @rga @google. Changing the world w/ a smile, design & some code.",999fc7f88fff,realjoet,7168.0,210.0,20181104
0,,0.0,5e5bef33608a,2018-09-28,2018-09-28 16:59:51,2018-09-28,2018-09-28 17:04:47,4,False,en,2018-10-10,2018-10-10 12:57:21,22,4f126af42668,5.081132075471698,6,0,0,This article originally appeared on https://lighthouse3.com/6-reasons-ai-ethics-in-corporations-is-all-talk-no-action/,5,"6 Reasons Why AI Ethics in Corporations is All Talk and No Action

This article originally appeared on https://lighthouse3.com/6-reasons-ai-ethics-in-corporations-is-all-talk-no-action/
Artificial Intelligence (AI) has permeated every industry ranging from finance to automotive and even fashion. The fear of terrifying human-like sentient machines perpetuated by Hollywood combined with dire warnings from experts has led to a flurry of well-deserved buzz around the ethics of AI.
The corporate world has responded with a slow trickle of announcements touting their brand-new AI ethical codes and guidelines. But a recent interview with Neil Raden, the founder of Hired Brains, highlighted that having AI ethics on the discussion agenda is a good start but getting companies to adopt them in any meaningful way continues to be a challenge.
Trending AI Articles:
1. Machines Demonstrate Self-Awareness
2. Visual Music & Machine Learning Workshop for Kids
3. Part-of-Speech tagging tutorial with the Keras Deep Learning library
4. AI & NLP Workshop
So let’s take a closer look at why concrete action on AI ethics still lags far behind all the talk.
6 REASONS AI ETHICS IN CORPORATIONS IS (MOSTLY) ALL TALK & NO ACTION
1) Ethics is not sexy.
Let’s get real. In the corporate world, new bright shiny objects and initiatives with a direct link to revenue generation are more likely to get visibility and funding. Ethics is neither glamorous or sexy. No one gets an award or promotion because they saved the company (and possibly the human race) from a potential ethical crisis many decades into the future. AI ethics discussions are typically relegated to some “expert committee” that meets semi-regularly. It’s anyone’s guess as to how much of their input and feedback is considered for implementation.
2) Speed to market is everything.
As a newbie employee at a leading tech company, the first piece of advice I received from a top leader was to “Run as fast as you can.” The corporate world is very darwinian and there are no consolation prizes for slowing down or being fastidious in a highly competitive market. Unless ethics are integrated into the company’s processes or are required, most employees will choose the path of least resistance and skip right past them.
3) AI may be forever but most CEOs are not.
According to a recent study, CEO turnover has risen over the past years and their median tenure at large-cap companies is 5yrs. In this short time period, CEOs are typically focused on keeping Wall Street happy, which makes it challenging to get their attention for issues that don’t contribute to the bottom-line. Also, any questionable practices that may cause issues for their successor in the future is not likely to get prioritized because of this short-term focus.
4) Oversimplification vs. paranoia.
When it comes to AI, there seem to be two extreme schools of thought. On one side, we have those who believe all AI issues can be solved by well-intentioned technologists. On the other side, we have over-hyping of risks to such an extent that no solution is good enough. The challenge is convincing companies to take a more balanced approach that considers all benefits and risks, while keeping humans at the center of this very important discussion.
5) Carrots or sticks.
To convince human beings (CEOs included) to change behavior, there needs to be an incentive or consequence. Today, the primary incentive to drive adoption of AI ethics is the warm, fuzzy feeling of doing the right thing. Government/regulatory agencies can be effective in “nudging” companies to adopt ethical policies and some like the U.K. have stepped up. However, in the current political climate, ethics have become a matter of opinion and vary wildly based on political affiliation so any meaningful regulation is unlikely to garner bipartisan support.
6) Talk is easy, action is hard.
In a global executive survey on AI adoption, Rumman Chowdhury, lead for Responsible AI at Accenture, shared that AI ethics codes in many companies “are more directional than prescriptive.” Even in companies with the right leadership, there is a huge gap in skills and expertise to fully understand all the risks of AI, let alone figuring out how to address them.
WHAT CAN WE DO ABOUT IT?
Keeping the hand-wringing and navel-gazing aside, let’s look at some ways that can effectively increase the adoption of AI ethics in corporate world.
Influence at the top.
A recent SAS survey shows that majority of companies with successful AI implementations have an AI Ethics training program in place. Organizations with an enlightened leadership that believes in the importance of AI ethics are already set up for success. For others, an executive level briefing is a good way to get the management familiar with risks/implications of AI.
New doesn’t mean reinventing the wheel.
Companies don’t need to start from scratch or create their principles for AI in a vacuum. Existing values and mission statement are a great starting point for any AI ethics code/principles as long as they include these 3 core areas at the minimum — Fairness, Accountability, and Transparency.
Start at the beginning.
Irina Raicu, Director of Internet Ethics at Markkula Center for Applied Ethics recommends including training on AI ethics in your new employee on-boarding process. Early indoctrination will help set the right tone for employees and ensure ethics are tightly integrated into the company culture.
Integrate checks and balances.
Regular training and feedback loops are essential so that ethics don’t become an afterthought. A leading financial institution has adopted protocols such that testing/checking of AI algorithms is done by a different team than the one building them. This allows the organization to eliminate any unconscious bias introduced into the algorithms by the developers.
Include diverse perspectives.
AI has traditionally been the realm of technologists but it requires a more collaborative and inclusive approach. Ethicists, philosophers, privacy advocates and end users should be included in any AI Ethics discussion to make sure solutions/outcomes are human-centric and not purely technology-centric.
Support the good cause.
Last but not the least, Here is a list of top 12+ noteworthy organizations dedicated to tackling the dark side of AI and who are actively shaping the future of responsible AI. Learn from them, support their work and implement their expert recommendations, wherever possible.
Share your experience on adoption of ethical and responsible AI in your organizations in the comments below or tweet them to @MiaD.
Mia Dand is the CEO of Lighthouse3.com, a Strategic Research & Advisory firm based in the San Francisco Bay Area. Mia is an experienced marketing leader who helps F5000 companies innovate at scale with digital and emerging technologies. She has built and led new emerging technology programs for global brands including Google, Symantec, HP, eBay and others. Mia is a strong champion for diversity & inclusion in tech. You can reach her on Twitter @MiaD




",6 Reasons Why AI Ethics in Corporations is All Talk and No Action,72,6-reasons-why-ai-ethics-in-corporations-is-all-talk-and-no-action-4f126af42668,2018-10-10,2018-10-10 12:57:21,https://becominghuman.ai/6-reasons-why-ai-ethics-in-corporations-is-all-talk-and-no-action-4f126af42668,False,1161,"Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.",becominghuman.ai,BecomingHumanAI,,Becoming Human: Artificial Intelligence Magazine,team@chatbotslife.com,becoming-human,"ARTIFICIAL INTELLIGENCE,DEEP LEARNING,MACHINE LEARNING,AI,DATA SCIENCE",BecomingHumanAI,Ethics,ethics,Ethics,7787.0,Mia Dand,"Innovation at Scale in AI & Emerging Tech. CEO, Lighthouse3.com, ex-Google, HP, Symantec, eBay. #AI #BigData #Ethics #Governance #DiversityinTech",21cf258179cb,miad,2975.0,3618.0,20181104
0,,0.0,,2018-02-15,2018-02-15 01:21:18,2018-05-31,2018-05-31 19:42:53,0,False,en,2018-05-31,2018-05-31 19:44:45,0,a0b5f6fbac52,3.5622641509433963,0,0,0,"How, what is considered, AI today will form the basis to build true artificial intelligence in the future",5,"When to expect arrival of true Artificial Intelligence?
How, what is considered, AI today will form the basis to build true artificial intelligence in the future
The excitement around AI as the next frontier of computing is ubiquitous and well deserved. However, there is a lot of confusion about the current state of AI evolution. There is even more confusion about how close are we to building true artificial intelligence. In this post, I will share my thoughts around the subject.
Setting the bar too low for AI is misleading
First, I don’t believe we are any where close to building true artificial intelligence. In my opinion backers of the theory, that we are really close to building AI, are setting the bar too low to declare a machine artificially intelligent. A machine cannot be declared artificially intelligent just because the machine can perform a certain task better than human beings. Machines have been able to solve mathematical computations better and faster than humans since the days of a calculator. Clearly, a calculator cannot be considered artificially intelligent. Similarly, expert systems that can perform specific tasks better than human beings have been in existence for decades. But these expert systems cannot be considered artificially intelligent either.
Critical threshold for intelligence
To understand how AI will evolve in the next decade or so we need to be very clear about two things. First, we must understand that intelligence is not a binary domain (I will do a separate blog post on this). If anything, intelligence varies on a scale more than a binary outcome of a yes or no (hence the imperfect measurement of I.Q.). Second, we must not set the bar too low on the intelligence scale to declare a machine artificially intelligent.
So what is true artificial intelligence?
In my view, true artificial intelligence is something that is capable of doing three very specific tasks in its domain of existence (or surroundings).
Recognize and perceive elements in the domain of existence
Create/manipulate knowledge base and context of elements in domain of existence
Manipulate domain of existence well enough necessary for survival
Recognize and perceive elements in domain of existence
To be truly intelligent, a machine must be able to recognize elements in the domain of existence. In human beings, this job is done by the eyes and the visual cortex. Humans are able to “see” what is around them and recognize the objects as separate elements. For example, we are able to see a tea cup sitting on a table.
Not surprisingly, we are feverishly working on computer vision to enable image segmentation and object detection in machines. Machines are getting better at recognizing objects through newer techniques such as deep neural networks etc. However, I believe machines are about 5+ years away from reaching human capabilities of image segmentation, object detection, motion tracking etc.
Create/Manipulate knowledge base and context of elements in domain of existence
Without context and knowledge base, identification of elements has very little value. For example, if a human opens their eyes and all they see is water around them, without knowledge and context, the human could be taking a bath in a bath tub or drowning in a lake.
There are a couple of major problems we need to solve before machines can reach human capabilities of creating and manipulating knowledge base and context. First, machines must be able to acquire and store an enormous amount of data. The world around us is vast and complex, it is unclear how machines can store that much data. Second, suppose even if we can store that much data, it is also not clear how we can manipulate and create context around that data. Intelligent machines will need to store and manipulate enormous amount of data in complex and readily accessible structures (possibly mimicking human memory i.e. Random, Episodic & Relational).
Lots of research is happening in the knowledge representation and context creation; but I believe we are about 10+ years away from creating something that can match human capabilities of knowledge base storage and context creation.
Interact with domain of existence well enough to survive
Perhaps the most important task any being in existence does is to interact with surroundings such that it can survive. For example, if we are able to recognize and perceive that we are standing in the middle of the road and a car is coming at 65 miles an hour in our direction, we will manipulate the environment by moving such that we are not in the way of the car anymore.
Human beings have been building robots for a while now to manipulate the environments. Robotics capabilities are becoming better every passing year and I foresee a linear improvement in robotics to match human capabilities in about 15+ years.
Damned if you do and damned if you don’t!
However, the fundamental issue is that whether we allow the machine to interact and manipulate the environment necessary for survival. There is no easy answer to this question. If we allow the machines to manipulate the environment for survival, one day the machines can decide human beings need to be eliminated for the machines to survive. On the other hand, if we don’t allow the machines to interact and manipulate the environment for survival, the machines will never become artificially intelligent as they will not survive. The machines must be able to survive to keep perceiving, keep acquiring knowledge/context and keep interacting and manipulating the environment for their survival; Only then the machines will be able to become smart enough to be considered truly artificially intelligent.
I foresee human beings to struggle with this dilemma…. forever!
",When to expect arrival of true Artificial Intelligence?,0,when-to-expect-arrival-of-true-artificial-intelligence-a0b5f6fbac52,2018-05-31,2018-05-31 19:44:46,https://medium.com/s/story/when-to-expect-arrival-of-true-artificial-intelligence-a0b5f6fbac52,False,944,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Saad Hussain,Engineer/Entrepreneur/Advisor/Consultant,49ab59dc5cd4,Saadism,100.0,97.0,20181104
0,,0.0,8cb5834da522,2018-01-21,2018-01-21 21:19:02,2018-01-21,2018-01-21 21:23:07,1,False,en,2018-01-21,2018-01-21 21:23:07,4,97a180e35c45,1.2679245283018867,2,0,0,"It’s 2018, and it seems as if the rate of technological advancement has only increased, even with the absolute inundation of new tech that…",5,"Janus — The Paths We Choose
Janus, the two-headed god of endings, transitions, paths, and choices.
It’s 2018, and it seems as if the rate of technological advancement has only increased, even with the absolute inundation of new tech that flooded 2017. Even after reading about AlphaGo Zero’s seemingly unlimited capabilities, the synthesis of metallic hydrogen, or any of Elon Musk’s myriad accomplishments in 2017, it’s hard to believe that there is still so much to do, so many areas for technology to progress.
And yet I believe that the rate of technological progression can only go up from here. There are so many emerging technology trends in these past few years that it’s hard to keep track of what’s what, and where everything goes. But it’s important to make sense of the whirlwind of developments, in order to properly analyze and reflect on the changes the world is going through. Most crucial of all is the understanding of how new technologies fit together in the future, and how they will all, together, revolutionize the way that all of us live, experience, and interact with the world.
And that is why Duke University’s Smart Home community has decided to release an opinion series on the future of technology that discusses names as big as blockchain and artificial intelligence, to lesser known trends such as generative design and neo-urban technology. Their applications will also illustrate what the future world will look like, transformed by these myriad of technologies into a more stable, safe, and prosperous society.
The list of articles below will be updated continuously as new articles are being written and published.
Janus: Blockchain
This is part of a larger series called “Janus”, meant to illustrate trends, dangers, and applications of future…medium.com
",Janus — The Paths We Choose,72,janus-the-paths-we-choose-97a180e35c45,2018-06-18,2018-06-18 09:14:33,https://medium.com/s/story/janus-the-paths-we-choose-97a180e35c45,False,283,A hub of sustainability and technology at Duke University.,,dukesmarthome,,Duke Smart Home,dukeuniversitysmarthome@gmail.com,duke-smart-home,"SUSTAINABILITY,TECHNOLOGY,RENEWABLE ENERGY,SMART HOME,DUKE UNIVERSITY",,Ethics,ethics,Ethics,7787.0,Jason Wang,Hoping to live up to my Chinese name — 浩峰 — the great mountain peak. Computer Science + Public Policy | Duke 2020,a6f151ad5e57,jasonwangatduke,65.0,111.0,20181104
0,,0.0,,2018-05-09,2018-05-09 07:47:00,2018-05-09,2018-05-09 07:47:01,0,False,en,2018-05-09,2018-05-09 07:47:01,5,81355c2b266,0.6075471698113207,9,1,0,,5,"Google Duplicitous
I can’t recall the last time I was so creeped out by a technology as I am by Google Duplex — the AI that can make reservations over the phone by pretending to be a human.
I’m not sure what’s disturbing me more: the technology itself, or the excited reaction of tech bros who can’t wait to try it.
Thing is …when these people talk about being excited to try it, I’m pretty sure they are only thinking of trying it as a caller, not a callee. They aren’t imagining that they could possibly be one of the people on the other end of one of those calls.
The visionaries of technology — Douglas Engelbart, J.C.R Licklider — have always recognised the potential for computers to augment humanity, to be bicycles for the mind. I think they would be horrified to see the increasing trend of using humans to augment computers.
This was originally posted on my own site.
",Google Duplicitous,79,google-duplicitous-81355c2b266,2018-05-15,2018-05-15 14:22:18,https://medium.com/s/story/google-duplicitous-81355c2b266,False,161,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Jeremy Keith,"A web developer and author living and working in Brighton, England.",5271293d8452,adactio,12367.0,292.0,20181104
0,,0.0,,2018-06-04,2018-06-04 20:05:59,2018-06-06,2018-06-06 13:49:36,2,False,en,2018-06-06,2018-06-06 14:12:53,12,d89610ec7e57,5.919182389937108,13,0,0,Should machines be held to a higher standard than humans - at least when it comes to questions of data ethics and bias?,5,"The ethicist in the machine
Should machines be held to a higher standard than humans — at least when it comes to questions of data ethics and bias?
Ethics in automated decision-making is not so much machines vs humans as machines based on human inputs. And it seems like wealthy white males are often the visible face of the tech while other human labour is hidden. Trevithicks Dampfwagen by William Felton , via Wikimedia Commons
We got asked this question about machines vs humans last month when presenting on “Killer Robots and How not to do Data Science” at the Strata data conference in London. Fellow volunteer Kate Vang and I were talking about work that DataKind UK has been doing on developing and applying ethical principles in our pro bono data science projects with charities. We answered the question live, but I wanted to pull together my thoughts on this. (This is in the context of automated decision systems as being used right now — this is not about any future AI general superintelligences and their non-paperclip-maximising superethics!).
My one-word answer is still Yes — when it comes to what are sometimes called automated decisions, where rulings with social impact and potential ethical issues are output from algorithms, such as machine learning models or artificial intelligence, we do need to be more cautious. They are built on fallible human inputs but there are more unknowns, there’s the potential for greater harm, and the decision-making systems can be harder to challenge or overturn.
One way to think about it is the difference between cars and bikes — you can hurt people with both, but cars do more damage, to more people, more quickly. Cars’ power means that they magnify the consequences of any bad decisions that the humans nominally controlling them make — in a way that’s usually worse for the people outside than in. Drivers probably aren’t inherently less ethical than cyclists (notwithstanding some of my cycling experiences on London roads…), but we do apply more regulation and more restrictions to cars than we do to bikes.
Algorithmic decisions have more impact and spread further and faster
A human who has come to apply a biased decision-making technique (that men are inherently better at STEM subjects for example) applies it one “subject” at a time. That’s not great, but the harm is limited. Machines are intended to be efficient and “decide” things like job candidate shortlists far more quickly, impacting hundreds or thousands of people. Models may be adopted across from one jurisdiction or company to another (“transfer learning”) so that the impact spreads. We can also look at things that are only indirectly about ethics — like the “flash crash” trading events where algorithms all followed rules in a way that rapidly spread the initial damage by creating a feedback loop, far faster than humans could have done. As Virginia Eubanks writes: “When a very efficient technology is deployed against a scorned out-group in the absence of strong human rights protections, there is enormous potential for atrocity.”
In addition, setting up an automated decision-making system may lead to faster decisions per person but it often requires considerable up-front time and expense, unlike just hiring some more people. This leads to sunk-cost decision-making and political face-saving, where even algorithmic systems that failed their pilot phase are rolled out more broadly as it is either not acknowledged that failure occurred, or that’s excused as “teething troubles” that will be fixed with more data!
Models are data hungry — and discrimination hungry
One of the main reasons to use machines rather than humans is the supposedly impartial and formulaic approach to making decisions — the idea is that individuals with the same (relevant) characteristics should be treated identically in decisions about, say, benefits eligibility. Human decision-makers can be swayed by irrelevant factors such as race, or subtler things like how well-spoken someone is at an interview. But the downside of machine learning to make these decisions is that mostly historic “training” data is used — the human biases are encoded. And they are not only encoded but enhanced. Machine learning is well known to be data-hungry but it is also discrimination-hungry — any features or combinations of features (where someone lives + their job + their ethnicity) may have been used in human decisions with only a marginal impact. But algorithms will learn this and can deepen the distinction that shouldn’t exist, “over-fitting” to any biased human inputs in ways that can be very hard to remove or correct.
This poor taxidermied AI (ok, ai for the case-sensitive) is very much showing how incorporating the results of human best efforts can be problematic. And we wouldn’t trust it to make ethical decisions about who gets employed. By Esv — Eduard Solà Vázquez CC BY-SA 3.0, from Wikimedia Commons
Machines can’t do new
Machines are inherently limited in recognising something as a new circumstance where their model won’t hold. Humans, faced with an example that doesn’t fit their experience, can go back to some sort of overall aim or norm (“block posts that contain threats of violence”) even if it’s an example not seen before. Humans can learn from a single input — seeing one bad outcome can be enough to make you reconsider your methods entirely. Machines can be taught to weight more recent examples, but they are inherently based on a depth of evidence. That’s part of the deal — that accuracy can be improved by looking at lots of examples and this powerful machine memory can be stronger and more rational than human guessing. But this means that machines can do greater harm by applying that learning in cases where humans would decide there was some novel and crucial distinction.
Machines appear impartial
Humans can expect more of the machine decision-maker — they assume that it is impartial, all-knowing, and not subject to petty human discrimination. In the same way that people have held qualified experts to a higher standard than more casual opinion because we rely on them more (physicians can be sued for malpractice for a negligent misdiagnosis, but you wouldn’t sue your friend who offered an opinion on facebook) it makes sense that we will be less tolerant of errors for machine models. There’s evidence that people are reluctant to go against the recommendation of an algorithm even if it is meant to be only one aspect of a decision or when it disagrees with other expertise (as in the example mentioned in ProPublica’s COMPAS investigation where a judge used a defendant’s score even over the prosecutor’s recommendation). For more on this whole reliance on machines (and sometimes a lack of trust) I like this thesis by Jennifer Logg. This all covers the fact that the human biases come through without accountability — in Maciej Cegłowski’s memorable phrase, machine learning acts as “money laundering for bias”.
Machines are black boxes
Machines learn on historical data (containing biases from human decisions that are still classed as “correct”) and some models can be complex and hard to interpret. That means that if there is a decision with negative impact that you want to challenge it can be very hard to know what went wrong, how systemic an issue it is, and whether there’s inappropriate bias. For a human decision we have as a survival skill pretty good “bullshit detectors” and methods for challenging. These methods don’t always work, as seen by the long long history of systemic injustice around issues of race, gender, ability etc — but they do exist. This is not the case for machine models. If it’s harder to challenge and overturn a machine decision than that of a human, then there has to be even more consideration about its limitations in the first place.
The baseline for all this is of course humans (and their systems). And humans are not particularly good at making fair, impartial, just decisions. There’s a long history of bias on unaccepted grounds like race, there’s society-wide systemic injustices, there’s inconsistency because of psychological tendencies like the availability heuristic (we don’t consider all the evidence, just whatever we saw most recently), and there’s the ability to post-hoc rationalise so that the inappropriate reasons are hidden, sometimes even from ourselves.
One of our DataKind UK volunteer ethical principles to try and address this is to look not just at potential negative impacts of any data science modelling but at the status quo — we do think there’s a lot of possibility for machine learning and other data techniques to improve project outcomes, even with a higher barrier to acceptance
When it comes to making decisions or predictions that have ethical dimensions (which is vague as it’s also something data scientists should be thinking about — but maybe there being potential negative impacts on real humans is a good start), machine learning systems are ultimately dependent on human-supplied data to learn, but they can learn any biases from the supplied history, magnify them, and apply them more broadly and irrevocably than humans ever could. This potential for greater harm means that machine decisions need the ethical equivalent of driver’s licences, vehicle inspections and safety and environmental standards as they roll out to change society.
",The Ethicist in the Machine,68,judging-the-machines-d89610ec7e57,2018-06-15,2018-06-15 15:00:14,https://medium.com/s/story/judging-the-machines-d89610ec7e57,False,1467,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Christine Henry,,6d1f924e752d,Berran,6.0,1.0,20181104
0,,0.0,,2018-09-16,2018-09-16 21:22:15,2018-09-16,2018-09-16 21:25:38,5,False,en,2018-09-25,2018-09-25 09:51:39,7,8f20426777da,7.70880503144654,4,1,0,Artificial Intelligence (AI) has definitely hit peak hype… it’s being written about an awful lot — even in design circles.,4,"User experience when AI makes all the decisions
Artificial Intelligence (AI) has definitely hit peak hype… it’s being written about an awful lot — even in design circles.
And yes, there are new tools and new capabilities that are sure to impact us designers in the not too far future — see Yuri Vetrov’s article on algorithm driven design for a solid write up. Nevertheless, I think something important has been missed; the user in ‘User Experience’, may soon have a rather different role to play. And that means what we do as designers will have to change in profound ways.
Not so long ago I was driving to my sister’s place and it occurred to me that, while I’d been there many times before, I still didn’t know the way. I’d never bothered to learn the route. Weird… no?
At that moment it occurred to me that my navigation and driving experience had changed in a fundamental way, presaging what is likely to cause a sea change in the way humanity engages with Digital.
Oh, how things have changed
Why hadn’t I learned the route? Well, every time I go I tend to take a slightly different route under the firm and unflappable guidance of Google Maps’ turn by turn navigation. I now have no idea which route I’m about to use, since each time I go the route changes based on traffic conditions, roadworks, and probably countless other tiny factors. The algorithm isn’t limited by our hunter gatherer’s cognitive architecture — it can consider countless factors and work through every last detail. The algorithm optimises for speed, not my desire to learn a route.

I may be the driver, but in some ways I’m no longer in the driving seat.
So, let’s take a step back. What has happened here? What is about to happen everywhere else, once AI sets in? Don’t you doubt it, narrow AI will soon be making itself unnervingly comfortable in Medicine, Logistics, Law, Accounting, Finance and, well, just about any domain where better decisions can be made by leveraging the ever increasing torrent of relevant data. See my colleague Rory’s thoughts on Future of work: AI used as an Intelligent Assistant.
The role of the user has changed. In digital experiences not powered by AI — a No-I UI, to coin a phrase — the software’s role is to automate processes and calculations, the human’s role is to supply inputs (e.g. in the case of navigation, the start and end starting point), make decisions (e.g. which route to take), and consume outputs (e.g. the map visual, and spoken directions).

In an Artificial Intelligence powered experience (an AI UI) things change. Taken to its extreme then it’s an experience in which we no longer make the decisions — the AI takes care of that. We are relegated to supplying inputs and consuming outputs we may not be able to truly understand or challenge. I don’t really understand what went into Google’s decision to take me down some obscure residential road, I just take it on faith that it’ll get me there faster in the end. I used to think I knew better but I now generally admit defeat. How can I win versus the rivers of data streaming into Google’s cloud?

To put a somewhat more empowering spin on this, let’s take a look at the new world of chess. It’s old news now that computers are scarily good at chess. Nobody, no matter how amazingly good they are, can beat a decent computer chess program.
Or can they?
Well, what most folk don’t know is that right now the best chess players on the planet are Centaur players. Even the very best computer can’t beat the very best Centaur. And what’s a Centaur? A human + a computer = a chess player who uses an AI to augment their game.

Technically this is just the latest in a long history of what’s called Augmented Cognition — tools that we’ve invented to make us smarter:
Pen and paper augmented our memory
Mathematical notation augments our ability to calculate
Graphs and charts augments our ability to absorb and reason about data
There’s a great book on all this by Edwin Hutchins. I recommend it
So what?
What, you might ask, are the implications for us UX/UI/Product/Service designers?
Well, it’s early days so I’m not sure if anyone has really worked it all out yet. Here are my tentative suggestions:
Empowering input experiences
We need to get really good at designing the experiences in which users give AIs the inputs they need: the precise goal, the input data set, etc.
Consider a modern day call centre agent dealing with a mortgage application — they key-in the client’s financials etc., press the submit button, and…
COMPUTER SAYS: [NO]
The result — a disempowered, uninformed, and demotivated call centre agent delivering a Kafka-esk customer experience.
AI UIs will have to be careful to create a sense of empowerment. Sensitively handling the fact that the AI has wrested the decision making away from the user, while celebrating the user’s role as its master. The process of supplying the software with inputs needs to feel powerful, dynamic, important. Not, as is so often the case, perfunctory and dull.
But there is more to it than that.
As Nick Bostrom, the doomsaying philosopher, pointed out, AIs can be very sensitive to the goals you give them. Things could go terribly, terribly wrong if a powerful AI was given the wrong goals. But that’s another story. Anyway, as the complexity of the tasks we give our AIs grows, the job of defining the right goal, asking the right question, choosing the right parameters, will become ever more critical. These input experiences will become ever more interwoven with the way in which AI UIs convey their outputs. Users will want to rapidly and fluidly iterate through different inputs choices as they receive the consequent outputs. For complex work, the skill of an empowered Centaur will be in how well they can navigate this interplay between input and outputs.
Straightforward and Honest
AI is largely driven by statistics rather than rules. Rules don’t scale very well, but when you are drowning in data and processing power, statistics do. The thing about statistics is that it doesn’t tend to produce just one answer, but a whole series of answers, each scored with a probability of some kind. So, to extend our driving example, you might have the probability of each candidate route being the fastest, the safest, the most scenic, etc. The underlying output data will often be terrifyingly complex in itself.
It’s up to the designer to take this complexity and create a simple, straightforward, yet honest output experience. It should be straightforward, by just showing the user what they need to know. Giving them choices that might make a material difference — “There is a very scenic route available, do you still want to take the fastest route?” However, it should also be honest and convey doubt. We sometimes get frustrated with current AI UIs when their recommendations go wrong, sometimes literally leading us up a blind alley. Under the hood, these AIs don’t deal in certainty. We, as designers, need to find better ways of conveying that uncertainty to users.
Output experiences should also grow sensitively over time, being respectful and trustful.
Growing
If AI is to be taking the driving seat, there is every possibility that users will feel disempowered, disenfranchised, or disintermediated.
We need to identify ways to create experiences that are sensitive to what people feel they are good at. If a capability is to take on the decision making in a process, then perhaps designers need to look for ways to incrementally demonstrate its worth on ever larger pieces of the puzzle — to gradually ease the user into being a Centaur. We must create experiences sensitive to people’s egos, that are mindful of what we are taking away and the changes we are asking people to make to what they do.
Respectful
Some folk like to think they know best (, yup, guilty).
If some guy turns up and repeatedly shows you up in front of your friends you’re not going to take kindly to that son of a ****. The same goes for software. App deleted. Flaming review penned.
Beware of your user’s fragile ego. Let them down gently, let them have their way. As it is in service businesses, so it should be with AI UI design — the customer is always right (ish).
Humble
AI is smart. It can be incredibly good at anticipating our needs. But it’s not perfect. It does get it wrong.
Our experiences should be designed with this in mind — they should allow users to easily pick up the mistakes AIs make, course correct, and allow them to teach the algorithms, so that they can get it right next time.
Trustful
If a piece of software is crunching gigabytes of data, factoring in countless parameters, and popping out a wonderfully optimised result, how do we make sure the user actually accepts it? To the algorithm designer, it’s all well and good to say “it’s the optimum solution, you can’t hope to understand why it is, it just is,” but then he understands the statistical complexities of neural networks and such… users don’t. Users need to build trust in the thing, before they accept its outputs.

How do you build trust? Well, there’s literature on this aimed at an older generation of digital experiences and is full of insights.
A tried and trusted approach has been to make the outputs, recommendations in this case, human understandable; the classic Amazon trope of “People who read XYZ bought ABC”. However, I suspect this drastically limits the magic that your AI wizards can conjure up. A wealth of more subtle techniques that leverage tone of voice, the gradual sequencing of experiences and rewards to steadily build trust, promise to be much more powerful.
To sum it all up
In a future where designers are helping to create an ever more computerised and machine-driven world, I feel we will need to be ever more humane in our craft as designers. We will need to think deeply about how the experiences we ship will impact our users and design them to create proud, empowered centaurs, rather than disempowered de-skilled, and beaten down worker-drones.
In the age of shrink wrapped software, UX was driven by a focus on efficiency and learnability. Then with the web came a focus on conversion and stickiness. Perhaps with AI, paradoxically, it will be a focus on humane-ness.
Thanks to Paul Harrison @CognitionX for the AI fact checking (and hardcore grammar upgrade), and to Myriam Wiesenfeld for the lovely illustrations
I’d love to hear your thoughts about UX for AI, so please do leave a comment!
",User experience when AI makes all the decisions,4,user-experience-when-ai-makes-all-the-decisions-8f20426777da,2018-09-25,2018-09-25 09:51:39,https://medium.com/s/story/user-experience-when-ai-makes-all-the-decisions-8f20426777da,False,1822,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Mischa Weiss-Lijn,"Interaction designer, IA, visualisation junky and all round idea-ivore",89b5c93cba3b,mweissli,72.0,69.0,20181104
0,,0.0,97aa03c8cebb,2018-01-09,2018-01-09 01:02:27,2018-02-05,2018-02-05 23:04:08,1,False,en,2018-02-05,2018-02-05 23:04:08,45,fac7ae7d805d,12.732075471698112,3,1,0,Last time we performed a reduction of the phenomenon of conscious self experience and through it discovered several key ideas. To refresh…,5,"Introduction to Noematology
Last time we performed a reduction of the phenomenon of conscious self experience and through it discovered several key ideas. To refresh ourselves on them:
Things exist ontologically as patterns within our experience of them.
Things exist ontically as clusters of stuff within the world.
Things exist ephemerally though chains of experiences creating the perception of time.
Through ephemeral existence over time, things can feed back experiences of themselves to themselves, making them cybernetic, and in so doing create information.
Things can exist within information, those things can experience themselves, and it’s from those information things that ontology, and thus consciousness, arises.
We covered all of these in detail except the last one. We established that the feedback of things created from the information of feedback gives rise to ontology by noting that information things have ontological existences that transcend their ontic existences even as they are necessarily manifested ontically. From there I claimed that, since people report feeling as if they experience themselves as themselves, consciousness depends on and is thus necessarily created by the ontological experience of the self. Unfortunately this assumes that our naive sense of self could not appear any other way, and in the interest of skepticism, we must ask, can we be sure there is not some more parsimonious way to explain consciousness that does not depend on ontological self experience?
I think not, but some philosophers disagree. Consider the idea of p-zombies: philosophical “zombies” that are exactly like “real” people in all ways except that they are not really conscious. Or consider John Searle’s Chinese room, where a person who cannot understand Chinese nevertheless is able to mimmic a native Chinese speaker via mechanistic means. In each of these cases we are presented with a world that looks like our own except that consciousness is not necessary to explain the evidence of consciousness.
Several responses are possible, but the one I find most appealing is the response from computational complexity. In short, it says that p-zombies and Chinese rooms are possible by unrolling the feedback loops of ontological self experience, but this requires things that we think are conscious, like people, to produce exponentially more entropy, be exponentially larger, or run exponentially slower than what we observe. Given that people are not exponentially hotter, larger, or slower than they are, it must be that they are actually conscious. Other arguments similarly find that things that theoretically look like conscious entities while not being conscious are not possible without generating observable side-effects.
So if it is the case that reports of feeling as though consciousness includes experiencing the self as the self describe a necessary condition of consciousness, then ontological self experience must be necessary to consciousness. This is not to say it is a sufficient condition to explain all of consciousness, though, since that would require explaining many details specific to the way consciousness is embodied, so we properly say that ontological self experience explains phenomenal consciousness rather than the phenomenon of consciousness in general. Nevertheless there is much we can do with our concept of phenomenal consciousness that will take us in the direction of addressing AI alignment.
Qualia and Noemata
To begin, let’s return to our reduction of {I, experience, I} in light of our additional understanding. We now know that when we say “I experience myself” we really mean “I experience myself as myself”, so it seems our normalized phenomenon should be {I, experience, I as I}. We could have instead written this as {I, experience as I, I} since it is through self experience that the I sees the ontic self as an ontological thing, but the former notation is useful because it exposes something interesting we’ve been assuming but not yet explored: that the subject of a phenomenon can experience an ontological thing as object. Yet how can it be that a thing that exists only within experience can become the object of experience when experience happens between two ontic things?
The first part of the answer you already know: the ontological existence of a thing necessitates ontic manifestation. Like with the computer document, a thing might have an ontological existence apart from its ontic existence, but ontological existence implies ontic existence since otherwise there is no stuff to be the object of any experience, and for it to be otherwise would be to suppose direct knowledge of ontology, which we already ruled out by choosing empiricism without idealism. Thus an ontological thing is also an ontic thing, the ontic thing can be the object of experience, and so the ontological thing can be the object of experience. But only understanding that ontological things can be the object of experience in this way fails to appreciate how deeply ontology is connected to intentionality.
Notice that in order to talk about a phenomenon as an intentional relation we must identify the subject, experience, and object. That is, we, ourselves phenomenological subjects, see a thing that we call subject, see a thing that we call object, and see them interacting in some way that we can reify as a thing that we call an experience, i.e. we see the members of the intentional relation ontologically. If we don’t do this we fail to observe the phenomenon as an intentional relation and thus as a phenomenon, because if we fail to see the phenomenon as ontological thing we have no knowledge of it as a thing and can only be affected by it via direct experience of the ontic in the same way rocks and trees are affected by phenomena without knowing they exist or are being affected by experiences. This means that the object of experience, insofar as the subject can consider it the object of experience, has ontological existence by virtue of being the object of experience, even if that ontological existence is not or cannot be seen by the subject of the experience. Thus of course “I as I” can be the object of I’s experience because we have already proved it so by considering the possibility that it is.
So if “I as I” can be the object of the I’s experience of itself, why bother to think of the phenomenon this way rather than as {I, experience as I, I}? As way of response, consider how the I comes to have ontological existence: the I experiences the ontic I, this creates a feedback loop of experience over time that allows the creation of information, and then an ontic thing that the I can experience emerges from that information. That information-based, ontic thing carries with it the influence of the ontic I — just as the bit expressing the state of the throttle in the steam engine is created via the governor’s feedback loop and carries with it the influence of the reality of the steam engine’s configuration — so it is causally linked to the I’s ontic existence. We then call this “I as I” because it is the thing through which the I experiences itself as a thing by the I making the phenomenon of experience of the self as ontic thing the object of experience. Thus by thinking of the phenomenon of self experience as {I, experience, I as I} we see it has another form, namely {I, experience, {I, experience, I}}.
This highlights the structural difference between consciousness and cyberneticness. A cybernetic thing, as far as it is cybernetic, only experiences its ontic self directly via its feedback loops over itself. A conscious thing, though, can also experience its ontic self indirectly through feedback loops over the things created from the information in its cybernetic feedback loops. It’s by nesting feedback loops that the seed of phenomenal consciousness is created, and we give the things created by these nested feedback loops and the phenomena that contain them special names: noemata and qualia, respectively.
“Noema” is Greek for both thought and the object of thought, and the project of phenomenology was started when Husserl saw what we have now seen by building on Brentano’s realization that mental phenomena, also known as qualia, are differentiated from other physical phenomena by having noemata as their objects. That noemata are phenomena themselves, and specifically the phenomena of cybernetic things, was to my knowledge first well understood by Hofstadter, although Dretske seems to have been the first to take a stance substantially similar to mine, because it required the insights of control theory and the other fields that make up cybernetics to understand that mental phenomena are not something special but a natural result of nested feedback. This is also why early phenomenologists, like Husserl, tended towards idealism, while later ones opposed it: without an understanding of cybernetics it was unclear how to ground phenomenology in physical reality.
To reach the fairly unorthodox position I’ve presented here, it took an even deeper knowledge of physics to trust that we could make intentionality as central to epistemology as we have and maintain an existentialist stance. As such you may be left with the feeling that, while none of what I have presented so far is truly novel, I have left unaddressed many questions about this worldview. Alas my goal is not to provide a complete philosophical system but address a problem using this philosophical framework, so I have explained only what I believe is necessary to that end. We move on now to less well trod territory.
Noematology
Having identified noemata as the source of consciousness, our view of consciousness is necessarily noematological, i.e. it is based on an account of noemata. This invites us to coin “noematology” as a term to describe our study of the phenomena of consciousness through the understanding of noemata we have just developed. It also conveniently seems little used and so affords us a semantic greenfield for our technical jargon that avoids some of the associations people may have with related terms like “qualia”, so we take it up in the spirit of clarity and precision.
Noematology, despite being newly minted, already contains several results. The first of these follows immediately from the way noemata arise. Noemata, being simply the result of nested feedback, appear everywhere. That is to say, noemata are so pervasive that our theory of phenomenal consciousness is technically panpsychic. Specifically, since all things are cybernetic, all things must also contain in their self experiences information out of which things emerge, and those information things must themselves be cybernetic insofar as they are things, thus they are noemata, hence all things must be phenomenally conscious. Of course not all things are equally phenomenally conscious just as not all things are equally cybernetic: some things produce more and more used noemata than others, just as some things produce more and more used information than others. Ideas like integrated information theory act on this observation to offer a measure of consciousness that let’s us say, for example, that mammals are more conscious than trees and that rocks have a consciousness measure near zero. Integrated information theory also shows how the panpsychism of phenomenal consciousness is vacuous: it’s true, but only because it pushes most of the things one might like to claim via panpsychism out of the realm of philosophy and into the realm of science and engineering. Put another way, consciousness may be everywhere in everything, but it’s still hard to be conscious enough for it to make much of a difference.
That the manifestation of consciousness, especially the consciousness of things like humans, is complicated gives purpose to noematology because it helps us see insights that are normally occluded by implementation details. For example, that noemata are created by the nesting of feedback loops within feedback loops immediately implies the existence of meta-noemata created by the nesting of feedback loops within noemata. And if this nesting can be performed once, it can be performed many times until there is not enough negentropy left to produce even one bit of information from an additional nesting. These multiple orders of noemata can then be used to explain the qualitative differences observed during human psychological development, and that higher-order noemata, which we might also call the expression of higher-order consciousness, are necessary to create qualia like tranquility and cognitive empathy, but these topics are beside our current one. For now we turn our attention to the relationship between noemata, axias, and ethics because it will ground our discussion of AI alignment.
Axiology, Ethics, and Alignment
Philosophy is composed of the study of several topics. Naturally, there is some disagreement on what those topics are, what to call them, and how they relate, but I tend to think of things in terms of epistemology, ontology, and axiology — the study of how we know, the study of what we know, and the study of why we care. All three are tightly intertwined, but if I had to give them an ordering, it would be that epistemology precedes ontology precedes axiology. That is, our epistemological choices largely determine our ontological choices and those in turn decide our axiological choices. Thus it should come as no surprise that I had to address epistemology and ontology before I could talk about axiology.
Of course the irony is that we actually investigate philosophy the other way around because first we ask “why?” by wanting to know, then we ask “what?” by knowing, and only finally can we ask “how?” by considering the way we came to know. The map, if you will, is drawn inverted relative to the orientation of the territory. So in some ways we have been studying axiology all along because axiology subsumes our founding question — why? — but in other ways we had to hold off talking about it until we had a clear understanding of how and what it means to ask “why?”. With that context, let’s now turn to axiology proper.
Axiology is formally the study of axias or values just as ontology is the study of ontos or being and epistemology is the study of episteme or knowledge. An axia then is something of value that we care about, or put another way, since it’s the object of a phenomenally conscious experience of caring, it’s a noema to which we ascribe telos or purpose, so we might think of axiology as teleological noematology, but to bother to think of something is to give it sufficient telos that it was thought of rather than not, so in fact all noemata we encounter are axias by virtue of being thought of. Non-teleological noemata still exist in this view, but only so long as they remain unconsidered, thus for most purposes noematology and axiology concern the same thing, and the choice of which term to use is mostly a matter of whether we wish to emphasize traditional axiological reasoning or not.

To make this concrete, consider the seemingly non-teleological, value-free thought “this is a pancake”. Prior to supposing the existence of the pancake there could have been a thought about the pancake which was valueless because it existed but was not the object of any experience, but as soon as it was made object it took on purpose by being given the role of object in an intentional relation by the subject experiencing it. From there the subject may or may not assign additional purpose to the thought through its experience of it, but it at least carries with it the implicit purpose of being the object of experience. As with thoughts of pancakes, so too with all thoughts, thus all thoughts we encounter are also values.
Within this world of teleological noemata we now consider the traditional questions of axiology. To these I have nothing special to add other than to say that, when we take noemata to be axias, most existing discussions of preferences, aesthetics, and ethics are unaffected. Yet I am motivated to emphasize that noemata are axias because it encourages a view of axiology that is less concerned with developing consistent systems of values and more concerned with accounts that can incorporate all noemata/axias. This is important because the work of AI alignment is best served by being maximally conservative in our assumptions about the sort of conscious thing we need to align.
For example, when working within AI alignment, in my view it’s best to take a position of moral nihilism — the position that no moral facts exist — because then even if it turns out moral facts do exist we will have built a solution to alignment which is robust to not only uncertainty about moral facts but to the undesirability of moral facts. That is, it will be an alignment solution which will work even if it turns out that what is morally true is contrary to human values and thus not what we want an aligned AI to do. Further, if we assume to the contrary that moral facts do exist, we may fail to develop a sufficiently complete alignment solution because it may depend on the existence of moral facts and if we turn out to be mistaken about this such a solution may fail catastrophically.
Additionally, we may fail to be sufficiently conservative if we assume that AI will be rational or bounded-rational agents. Under MIRI’s influence the assumption that any AI capable of posing existential risk will be rational has become widespread within AI safety research via the argument that any sufficiently powerful AI would instrumentally converge to rationality so that it does not get Dutch booked or otherwise give up gains, but if AGI were to be developed first using machine learning or brain emulation then we may find ourselves in a world where AI is strong enough to be dangerous but not strong enough to be even approximately rational. In such a case MIRI’s agent foundations research program might not be of direct use because it makes too strong of assumptions about how AI will reason, though it would likely offer useful inspiration about how to align agents in general. In the event that we need to align non-rational AI, addressing the problem from axiology and noematology may prove fruitful since it makes fewer assumptions than decision theory for rational agents.
Even if we allow that an AI capable of posing an existential threat would be rational, there is still the axiological question of how to combine the values of humans to determine what it would mean for an AI to be aligned with our specific values. To date there have been some proposals, and it may be this problem can be offloaded to AI, but even if we can ask AI to provide a specific answer we still face the metaethical questions of how to verify if the answer the AI finds is well formed and how to ensure the AI will find a well formed answer. In view of this we might say that alignment asks one of the questions at the heart of metaethics — how do we construct an ethical agent? — and solving AI alignment will necessarily require identify a “correct” metaethics. In this case the study of AI alignment is inseparable from axiology and, in my view, noematology, so these are important lenses through which to consider AI alignment problems in addition to decision theory and machine learning.
These are just among some of the topics for which we wish to address with our noematological perspective. And there are of course many topics outside AI alignment on which it touches, some of which I have already explored on this blog and others which I have considered only in personal conversations or during meditation. I will have more to say on these topics in the future, especially as they relate to AI alignment, but for now this completes our introduction to existential phenomenology and noematology. On to the real work!
",Introduction to Noematology,3,introduction-to-noematology-fac7ae7d805d,2018-06-06,2018-06-06 11:07:43,https://mapandterritory.org/introduction-to-noematology-fac7ae7d805d,False,3321,Making sense of reality,mapandterritory.org,mapandterritory,,Map and Territory,gworley3+mat@gmail.com,map-and-territory,"PHILOSOPHY,PSYCHOLOGY,RATIONALITY,EFFECTIVE ALTRUISM",mapterritory,Ethics,ethics,Ethics,7787.0,G Gordon Worley III,"Phenomenological philosopher, mathematician, and programmer",c050c3518961,gworley3,254.0,309.0,20181104
0,,0.0,cdd8dc4c5fc,2018-06-21,2018-06-21 16:02:34,2018-06-21,2018-06-21 19:38:51,1,False,en,2018-07-02,2018-07-02 22:26:52,47,717764addd78,9.977358490566036,2,0,0,A report from the Symposium on Trust and Ethics of Autonomous Vehicles (STEAV),3,"Clearly Complex
A report from the Symposium on Trust and Ethics of Autonomous Vehicles (STEAV)

by David Weinberger
Sometimes clarity means embracing complexity. That seemed to be one of the overall implications of the Symposium on Trust and Ethics of Autonomous Vehicles (STEAV) convened at Harvard and the Massachusetts Institute of Technology. The second overall implication makes the first more urgent: autonomous vehicles — self-driving cars and trucks — are going to transform just about every aspect of our world.
The symposium at the end of May 2018 brought together more than seventy experts from around the world in fields as diverse as computer science, law and policy, philosophical ethics, city planning, and the insurance industry, to think about how we can shape the deployment of autonomous vehicles (AVs) so that our world is made more livable and fair. The event, kicked off by MIT Media Lab Director Joi Ito, was organized by Harvard’s Berkman Klein Center for Internet & Society, the MIT Media Lab, and the Institute for Advanced Study in Toulouse, as part of the Ethics and Governance of AI project.
Over the course of one and a half days even relatively simple issues turned out to be not just complicated, but shot through with uncertainties. This is true far beyond the canonical example, the Trolley Problem, which, according to an initial presentation by the Media Lab’s Iyad Rahwan, generates very different results across different cultures, taking the forty million inputs to the Moral Machine site as evidence. Later in the conference, Patrick Lin, a philosopher at Cal Poly, offered a robust defense of the Trolley Problem as a thought experiment that is much like a lab experiment: a constrained, simplified case that enables us to see the effects of the variables. It is an artificial simplification that nevertheless gives rise to endless discussion. (More from Patrick here.)
Indeed, there seemed to be no simple answers to any of the many questions around the role and governance of AVs. For example, it’s not only that we don’t agree about how safe AVs should be before we permit their deployment, but “The more precisely we want to know how safe a technology is, the more risk we have to accept in deploying it,” in the words of Laura Fraade-Blanar, from the RAND Corporation. It would take five billion miles of road testing to give us statistical confidence that AVs could have a twenty percent lower fatal crash rate compared to human driven vehicles; Google’s experimental AVs have logged one-thousandth of that as of February 2018. Driving five thousand AVs twelve thousand miles each, for a total of sixty million miles, could give us statistically significant information about whether the rate of crashes for AVs was at least ten percent lower than for human driven vehicles, but would yield no significant insights about the difference in fatalities, unless that difference turns out to be over eighty percent.
But of course it is yet more complex. To compare the risks and benefits posed by AVs vs. traditional cars, it would be helpful to have equivalent data. But AVs record just about everything, generating up to a gigabyte of data per minute, so we’ll know every time they run over a curb or swerve to avoid an obstacle. But humans usually don’t report such small incidents or even fender-benders too small to be worth alerting our insurance companies about. And then what do we compare AVs to? To all cars on the road, including thirty year old Corvairs that are on their last legs (or wheels), or to new cars in the AV’s price range?
So, the the comparative safety of AVs and traditional cars cannot be resolved without making non-trivial and non-obvious decisions about what we want to count as safe. Without that information, policy-makers and regulators will have difficulty knowing when to phase in AVs and how to assess their performance
Then those decisions will have to be sold to citizens. According to data presented in a session, forty-seven percent of United States residents have already decided that AVs will not be safe. Chaiwoo Lee of the MIT Age Lab reported that men are more likely to accept AVs as a safe form of transportation, while the elderly are less comfortable with cars that do all the driving themselves — although the correlation with age may be indirect. She also noted that the acceptance of AVs may well depend on whether they will be on our streets as vehicles owned by individuals or as a fleet of cars that one pays to ride, as with Uber or Lyft. She also reminded the symposium how drastically the public’s perception of risk can be altered by events: 40,000 people died in vehicle-related accidents in 2017 in the U.S., but a single highly-publicized death in an AV could turn the public against the program, perhaps in part because, as one of the panelists said, quoting Kate Darling: “…people will feel far more emotional about an accident caused by a machine rather than a person.”
While avoiding fatalities was at the top of the list for everyone who raised her voice at the conference — this was, by the way, a fully gender-balanced event — that’s not the only value in the mix. For example, Noah Goodall of the Virginia Transportation Research Council suggested that a recent death of a bicyclist in a crash with an Uber AV in Arizona conceivably could have resulted from how the AV was designed to balance values: if an AV brakes for every false positive generated by its collision detection system, the ride will be needlessly jerky and uncomfortable — an incentive to trade some safety for comfort.
The conference also heard from Christoph Lütge from Technische Universität München about the process by which the German government produced a code of ethics for governing AVs [pdf] that is gaining traction internationally. The document provides a more complete ordering of values, including putting the prevention of harm to humans above protecting property and animals, valuing all human lives equally regardless of categories such as age and race, and possibly “socially and ethically” mandating the use of AVs if they turn out to save lives.
Many countries are paying attention to Germany’s code of AV ethics, but universal agreement is unlikely, if only because of the sorts of cultural differences the Moral Machine site has documented, such as attitudes towards the elderly and even toward cats and dogs. This led to frequent discussion among the participants about how nations and even cities might try to harmonize their varying AV ethical codes based upon their local values. “Are we going to have left-side and right-side rules of driving, except this time for ethics?” asked one participant.
Aida Joaquin Acosta, who is affiliated with the Berkman Klein Center and is working with the Spanish government on how to govern AVs, was more hopeful. She noted that government policies can shape technology, and described some of the serious and well-funded European initiatives underway that could bring broad agreement and harmonization of AV policies.
In a conference lively with debate and disagreement, everyone agreed on at least one point: AVs are going to have some downsides. Even if AVs entirely eliminated the 93 percent of accidents caused by human error, that would still leave seven percent of crashes. So, AVs are highly likely to be involved in fatal accidents. Further, a switch to AVs could throw millions of truck drivers and cab drivers out of work, with a disproportionate impact on the populations that traditionally have seized upon those employment opportunities. AVs may also seriously degrade city life (imagine driverless cars hurtling down your streets at 150mph), exacerbate the disparities between urban and rural populations, and magnify the inequality of access to transportation in poorer parts of a city. While many hope that electric AVs will reduce the environmental impact of transportation, some of the conference participants fear that easy access to extremely low cost travel could increase the miles driven beyond what the savings an all-electric system would bring. In fact, at least one participant believes that electric cars’ reliance on batteries simply shifts the release of carbon from the burning of gasoline to the mining of the minerals used in batteries.
Another liability was discussed extensively: privacy. Lauren Smith, policy counsel at the Future of Privacy Forum, says we should think about our cars as being more like smartphones than mechanical objects, and AVs even more so. Cars already collect lots of data about us, our vehicles, and our driving. A fleet of AVs may collect much more, since their managers will likely want to monitor the cars’ interiors for cleanliness, vandalism, and even bad smells. We’re likely to become accustomed to that since, as Smith pointed out, our expectations about privacy are already lower when we’re taking public transportation than when in our own car; many municipalities’ buses have surveillance cameras in them. But even privately owned AVs are unlikely to confine their data collection to the car or its driver. Information about passengers, pedestrians, road conditions and other vehicles on the road will also be input into these systems. In fact, some traditional cars are about to be given face recognition software that will warn drivers when the driver appears to be tired. About all of this Smith asks the crucial questions: Who will own or have access to this data? How should it be governed?
The potential inexplicability of the “decisions” AVs make also may vastly complicate the question of governance. These vehicles will be guided by the type of AI called machine learning the outputs of which are based on networks of data that may include tens of thousands of variables, each connected to many others, and each with its own weight that determines when it will be triggered. That is very different from the usual way of programming a computer by giving it a relatively simple model of how things interact, and providing rules for determining the outcomes. There may be decisions made by these systems that the human brain simply cannot follow, especially if AVs are enabled to form ad hoc networks with the other AVs on the road so that they can collaboratively come up with actions that maximize the benefits we’ve told them to prefer.
Sandra Wachter, a lawyer and Research Fellow at the Oxford Internet Institute, however, suggested a way to understand AI decisions without having to trace through a system’s millions of connections. Suppose you suspect gender may have played a role in a machine learning system’s rejection of your loan application. Rather than trying to pick apart the AI’s overwhelmingly complex network of variables — none of which may directly express your gender — Wachter suggests an investigator could multiply re-submit the application with various small changes to see which variables, if changed, would have swung the decision in your favor. Such a procedure could pinpoint the important factors that led the AI to reject your application. (See this paper for more information.)
No matter how the question of explicability is resolved, it will deeply affect the processes by which governments and regulators will come to decisions. Will the process become a “tyranny of statistics,” as one panelist suggested? Or, perhaps as an educator at the symposium suggested, ethical issues will come to the fore in our discussions. Jason Millar, a Stanford philosopher with an engineering background, is running “design thinking” workshops with engineering teams to help them learn how to create technology while keeping ethical outcomes in mind. This begins with reframing the discussion in terms of human values rather than ethical principles, and then thinking about how tech can best achieve those values.
That is an approach implicitly shared by Kristopher Carter at the Boston Mayor’s Office of New Urban Mechanics. He sees human values embedded in every aspect of city design. “We shape our streets. Thereafter, they shape us,” he said, playing on Winston Churchill’s quotation about buildings. For example, some Boston streets so favor vehicular traffic over pedestrians — overlong stoplights, poor placement of crosswalks — that they unintentionally encourage jaywalking. “How do we want AVs to prioritize pedestrians?” he asks. Then there are issues of fairness: Amazon’s machine learning algorithms determined that all of Boston would be eligible for same-day Prime delivery …except for the poorest part of town. Carter used this example to argue — not for the first time at the conference — that a purely results-based ethics is inadequate, for principles of fairness and equity should take precedence over utilitarian benefits.
All of these questions, and more, discussed at the symposium make governing AVs complicated, complex, and impossible to resolve perfectly. Yet some institution or confluence of social forces is going to have to provide the rules for the deployment of AVs and decide on the mechanisms of accountability for the harms AVs will inevitably bring about. Urs Gasser, Executive Director of the Berkman Klein Center, noted an overall difference in how European governments and the United States approach governance. The Europeans at the conference seemed generally to favor governments proactively promulgating ethical guidelines and policies, while Earl Weener, a member of the U.S. National Transportation Safety Board, took the successful governance of airline safety as a model: learn from forensic analyses of accidents and build a voluntary industry-government partnership along the lines of the Commercial Aviation Safety Team [pdf] that has proved so effective in making air travel safe.
Overall, STEAV showed the profundity of AI’s challenge to our settled ideas about accountability and morality. Conversations that traditionally were already difficult about the values we collectively want to support, the trade-offs we’re willing to make, and who we hold responsible for the inevitable failures, have now been complicated by the fact that we’re requiring automatons to act as proxies for our decision-making processes…and we may not always be able to understand how they came to their conclusions. This is requiring new thinking not just about what rules and regulations we want to impose on AVs but also about how we think about ethics and responsibility in the first place. For example, the University of Vienna’s Janina Loh analyzed the degree to which an AV could be held responsible for its acts by presenting a new model that sees a “network of responsibility” with roles for the agents, actors, and objects of actions. That the AI enabling AVs can lead us to revise the very framework of responsibility itself is an indication of the profundity of the change we are living through.
Perhaps issues such as the ones discussed at the symposium will prompt more explicit discussions about the need to balance our values — safety vs. speed vs. environmental impact, etc. — and to become more committed to values such as fairness and nondiscrimination that we are not willing to trade for purely pragmatic benefits. Perhaps conversations such as the ones begun at STEAV will lead to clear decisions about the values we want AVs and other AI-driven systems to support, and the most effective laws, rules, norms, and market incentives for achieving our vision.
No matter what, these issues are not going away. Perhaps the single takeaway from STEAV was that it’s better that the conversations continue to deepen than to resolve themselves too early.
Learn more about the Ethics and Governance of Artificial Intelligence project on the BKC website.
",Clearly Complex,7,clearly-complex-717764addd78,2018-07-02,2018-07-02 22:26:52,https://medium.com/s/story/clearly-complex-717764addd78,False,2591,Insights from the Berkman Klein community about how technology affects our lives (Opinions expressed reflect the beliefs of individual authors and not the Berkman Klein Center as an institution.),,BKCHarvard,,Berkman Klein Center Collection,hello@cyber.harvard.edu,berkman-klein-center,,BKCHarvard,Ethics,ethics,Ethics,7787.0,Berkman Klein Center,"The Berkman Klein Center for Internet & Society at Harvard University was founded to explore cyberspace, share in its study, and help pioneer its development.",188295c21f1c,BKCHarvard,297.0,97.0,20181104
0,,0.0,,2018-09-26,2018-09-26 21:56:22,2018-09-26,2018-09-26 22:02:22,2,True,en,2018-09-27,2018-09-27 00:17:40,1,2a0cf4576897,8.65880503144654,0,0,0,"In a world not too far in the future, with the rise of technology in all aspects of our lives, it seems inevitable that this technology…",5,"Sex and Technology: Are robots the future of sex and what are the implications for the sex industry?
In a world not too far in the future, with the rise of technology in all aspects of our lives, it seems inevitable that this technology will also become a prominent feature in the enhancement of our sex lives as well. Already we have virtual reality pornography and sex toys which record our arousal process, and it is not too farfetched to imagine a world where robots can cater to our every sexual desire with just a few tweaks in programming. Now with plans for a ‘robot-brothel’ soon to be opening for business in Texas (Miley 2018), questions do arise as to whether or not this could be a social success; and even if it is, should it be?
Levy (208:225) suggests that much of the success of sex robots can be predicted by the current popularity of other such sex toys like vibrators and sex dolls. However the people who use vibrators, sex dolls or other such devices that simulate sexual intercourse are not necessarily the same people who are hiring prostitutes, which is one thing that this premise of sex robots replacing prostitution seems to rely upon. What is the link between these consumers and people who pay for sex? It seems like a big assumption to be making. And if a link were to be made between sex toy use and prostitution, why would that link potentially be a sex robot that facilitates realistic sexual experiences, rather than the integration of already existing sex toys being utilised by a prostitute? It may be possible to create realistic sex robots for the purpose of replacing prostitution, but that does not mean that people who enjoy the use of sex toys in general, are going to want to use them. This is apart from a niche clientele that have fetishes for this sort of interaction. It is one thing to supply, but it is another thing entirely to create demand.
Levy (2008:225–226) lists some of the reasons for men visiting a prostitute as including a desire for variety in the women they have sex with, without having any complications or constraints popping up from these encounters, and a previous lack of success with the opposite sex. He maintains that a sex robot can provide all of these things and more, which is a fair prediction to make. It is unlikely that a sex robot is going to want cuddles after sex, demand a wedding ring or reject their user (unless of course they have been programmed to do so). But it leaves out the possibility that when seeking these encounters, people are also seeking a distinctly human experience. In this light, Levy notes a lack of research in why women seek out sex with male or female prostitutes, yet makes the assumption that a male sex robot with a vibrating penis ought to cut the mustard since vibrators are already so popular — again, where is the correlation?
 
 However even in the event that people, living in countries where prostitution is legal, do not wish to swap their live companions for the administrations of a robot; sex robots could still be beneficial in bridging the gap for people who live in the vast array of countries where prostitution is still illegal. People could seek this companionship without being at risk of breaking the law, such as has happened already in countries such a Korea (Levy 2008:224) with regards to the hiring out of sex dolls. This would provide a way for people to experience a willing sexual partner where they have paid for the privilege of their company, without technically breaking any laws, assuming that any laws in place never regard autonomous robots as being, in some way, human.

This is somewhat discussed by Levy (2008:227) when he makes the argument for the ethical margins around making sex robots available to the general population for use, stating that it is no less moral than the use of vibrators. But the unique coding that would have to be included to make the robot more and more human-like as time goes by, where does that line get crossed between machinery and humanity. Although the concept of robot consciousness is glazed over in a few paragraphs, this does not render the philosophy around such encounters any less meaningful. Any argument that would not give a sexbot the same legal and ethical rights as a human being is based on the assumption that fully autonomous sexbots somehow do not have the same conscious experience of events that we as humans have. If the robot looks and acts human, does that make it aware of its experiences in the same way? How about if some day science has progressed to the point that these robots have a similar neurology construct to humans instead of intelligence stemming from microchips? At the most basic level, all of our desires and impulses are simple chemical reactions or electrical charges taking place in our brains. At what level is this different to the brain events of a robot, taking place in a computerised cortex instead — how is this somehow less valid (Cave 2007:169)?
Technology and pornography have long travelled hand in hand. From the progression of illicit novels, to real actors and actresses on VHS and DVD and now to the internet age where any fetish is just a ‘Google’ search away; it isn’t hard to imagine that the next step might include downloading your favourite porn scenes onto your sexbot’s hard drive so s/he can enact the very fantasies that have driven your late night fetish searches and make them a reality. But could this, in turn, lead to the irrelevance of real women in men’s lives, other than for breeding purposes, in regards to all women, not just prostitutes? Just as porn has already changed the way women view their bodies and how they should behave in sexual interactions in order to live up to the hyper-reality men view in pornography, even though pornography and masturbation gives women an advantage of having an actual vagina to experience intercourse with; the arrival of sexbots and all their seeming perfection might mean sex with a real woman becomes somehow deficient or just not living up to the standards set by these artificial creations which not only have all the right parts, but will enact any fantasy a person can possibly dream of with no complaint. Males then become conditioned to view sexual interactions with women as inferior, and this could even lead to the decline of human on human sexual interaction to the point that the production of offspring diminishes resulting in negative population growth (Brophy 2010:208).

All of these concerns in turn raise the ethical issue of the abuse of sexbots. Even if a sexbot does not have the same capacity for physical or emotional pain, other aspects of their realism may end up being an attraction for those who wish to perform acts upon them that would be otherwise considered to be an atrocity or torture if inflicted upon a human being (Lin 2011:204). This leads on to the uneasy consideration of sexual deviancy. To what length will production of sexbots go to fulfil the fantasies of their users? What are the limits in regards to rape-play or snuff sex, which are already available in the realms of virtual reality? Will this result in the development of animal sexbots to cater to bestiality scenarios? Or child sexbots, overstepping precious legal and ethical boundaries regarding paedophilia and perhaps incest as well, in the creation of bots that look like family members (Brophy 2010:210). This further advances the need for deliberation on exactly what this scenario is achieving. Is it an outlet for people to take out their frustrations on a robot so that no humans get hurt — or is it a stepping stone on the way to harming a real person? First a robot, and when that becomes stale, move on to hurting and assaulting real human flesh. The consequences of this becoming a reality could end up being catastrophic for anyone that falls victim to someone wanting to act out their sexbot ‘play’ with a living human.
All this aside, even if sexbots could effectively replace prostitutes in the sex industry, does this mean that they necessarily should? Levy’s main argument against this (2008:228) is that it would be taking away the main source of income for women who are desperate for money and therefore choose this industry as a last resort. Levy seems to assume that prostitutes are sexually abused drug addicts who are degraded on a regular basis and are rife with sexually transmitted diseases. One would likewise assume that upon these premises that the pornography industry could also benefit from the involvement of sexbots so that women actresses would no longer have to be ‘debased’ in this way.
Beyond the dark and dangerous world of sex trafficking, which involves exploitation and victimising of people who have not or cannot give consent; this often could not be further from the truth. The most common opinion is that prostitutes and pornstars are somehow experiencing a lesser quality of life than people who are not involved in the sex industry. However there are many in this profession that simply embrace their sexuality, enjoy putting it on display in various ways and ultimately take delight in getting paid for it. From a prostitute’s point of view, not only does a man want to have sex with you, he wants to give you money for the pleasure of doing so. It is, for some, the ultimate aphrodisiac. Although these are not standard sexual preferences, neither is the abstinence that a priest or nun vows to take, and yet it is never assumed that their wellbeing is affected, or that some past abuse has forced them into this position (Monroe 2010:14–15).
Introducing sexbots as a way to eliminate prostitution is not only tenuous in respect to a moral obligation of ‘helping’ sex workers out of the industry, but it could potentially be dangerous as well. While Levy suggests (2007:306) that robots will inevitably be able to outperform humans in industries such as law, medicine and cooking; this raises the question of what it is to truly be human and to what lengths a person might go to in order to secure an authentically human experience. Will a robot truly be able to outperform a chef without the human passion for creating and enjoying the food they are making? Their technical skills may be superior, but how far can that carry the process without the sense of ‘soul’ that many people put into their crafts, whether it is cooking or carpentry? This carries over into the realm of sexbots. A robot might be able to perform technically perfectly in sexual interactions. They might be able to give a convincing performance of truly feeling passionate or aroused. But they do not ultimately feel this way in reality. They are programmed to exhibit these responses, but they do not have true human passion or immersion in the sex act. While this may be enough for some, others will inevitably crave the connection that only another human can give.
Along these lines, research has even shown that the more a robot becomes almost indistinguishable from humans, the slight differences that do remain cause a reaction of distrust, fear or anxiety in a phenomenon which has been termed “Uncanny Valley” (Laue 2017:1). In turn, this could have a disastrous impact in the realms of sex trafficking. With human prostitutes and porn stars all but eliminated in the sex industry, how long will it be for human trafficking to drastically increase with the implicit guarantee that you would be receiving a ‘real, live human’ for a hefty fee.
With the all but inevitable arrival of these kinds of technological advances in the near future, it is an imperative that these kinds of concerns are investigated thoroughly before we pass through gates from which it would be very difficult to return. While sexbots seem on the outset as something that could be fun and even beneficial to lonely folks looking for a partner, the societal and ethical concerns are potentially too substantial to be ignored.
Bibliography
Brophy, Matthew; ed. ‘Sex, Lies and Virtual Reality’ in Porn Philosophy for Everyone: How to Think with Kink; Blackwell Publishing Ltd; 2010
Cave, Peter; Can a Robot be Human?; Oneworld Publications; 2007
Laue, Cheyenne; Familiar and Strange: Gender, Sex and Love in the Uncanny Valley; Multimodal Technologies and Interaction; 2017
Levy, David; Love and Sex with Robots; Harper Perennial; 2008
Lin, Patrick., Abney, K. & Bekey G; Robot Ethics: The Ethical and Social Implications of Robotics; MIT Press 2011
 
 Miley, Jessica; First US Sex Robot Brothel Will Soon Open in Texas; https://interestingengineering.com/first-us-sex-robot-brothel-will-soon-open-in-texas
 2018
Monroe, D & Ryder D; ed. “The Jizz Biz and Quality of Life” in Porn Philosophy for Everyone: How to Think with Kink; Blackwell Publishing Ltd; 2010
",Sex and Technology: Are robots the future of sex and what are the implications for the sex…,0,sex-and-technology-are-robots-the-future-of-sex-and-what-are-the-implications-the-sex-industry-2a0cf4576897,2018-09-27,2018-09-27 02:51:14,https://medium.com/s/story/sex-and-technology-are-robots-the-future-of-sex-and-what-are-the-implications-the-sex-industry-2a0cf4576897,False,2193,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Athena Mac,"Anthropologist and Feminist Witch from Aotearoa, creating sacred space with words and fearlessly exploring the human shadow.",723d678d28c7,athenamac,9.0,17.0,20181104
0,,0.0,,2017-11-06,2017-11-06 20:41:55,2017-11-06,2017-11-06 21:13:49,0,False,en,2017-11-06,2017-11-06 21:13:49,0,14d32e927b05,2.8150943396226413,0,0,0,There’s something really interesting about where we are going nowadays in terms of technology. Companies have started investing in the next…,5,"The Process: Us and the future of AI
There’s something really interesting about where we are going nowadays in terms of technology. Companies have started investing in the next big thing which, according to them, is AI. Every new release of every new product is just a bit smarter, a bit more friendly and slowly learns to be better at it’s function.
A common example of AI done very well is Google Photos. The concept is simple: you backup your photos to the cloud and they are automatically sorted, tagged and organized for you. Some very robust algorithms will identify your friends, family, pets, places, etc; and you will be able to search for your memories like a normal person. Queries like “Pictures in Paris”, “Me and Brothers” or “Party in Las Vegas” all return more accurate results than ever.
Google Photos is a great product. It is one that I use every day and can’t really name an alternative for. It just works, and works particularly right.
There is one thing about this kind of use for AI that bothers me: no one has got a clue of how it works. No, I’m not talking about neural networks or the statistical models that these algorithms are based of, I’m talking about the most important thing: the process.
Do you know how to build a car? I certaintly don’t, but some people out there do, for sure. We have had cars for a while now and there’s no doubt that we, as humans, know the physics behind how a car works. We know which parts go in what places and what they are going to do if they work correctly. They might fail, yes, but they will never change their behavior from one or two predictable things.
On the other hand, AI-based algorithms don’t share this caracteristic. Sometimes we don’t know how they are made or what they look for in the data it is given. That could lead to all sorts of results.
Imagine an algorithm to recognize a human face. We could look for the shape of eyes, nose, mouth, etc. That is pretty straight forward and we have had algorithms that work like this for a long time. Now imagine an algorithm that recognizes a party. We could look for a lot of people, right? But how many people are required so that we can consider it a party? Maybe look for dancing? But what if it just a dinner? As it turns out, a party is not something we can describe like that: it is a lot more complicated.
That’s a perfect application for AI. Feed it a bunch of pictures of parties, of all sorts, and a bunch of pictures of “not-parties”. Let it train and eventually you’ll get a very accurate selector of “parties” and “not-parties”. In this case, AI will solve the problem for us with little effort while a descriptive algorithm could be developed for years and still not get acceptable results. The worst thing that will happen is we are going to get the wrong result.
But what if we start using AI to do things for us? What if we ask a machine to pick our food? To build our houses? To fight our wars? It is not about what we asked them to do, it is about the sorts of abilities they will learn in the process. We have reached a point where a machine will not only give the wrong answer if it fails, it might make someone sick, it might build unsafe buildings or it might kill the wrong people.
This might be a very negative view on AI, the truth is so far we have done great things with it. The greatest concern is that we are giving too much to the machine and taking too much from the human race. If they can do everything, how long will it take before we are useless? Before we can be replaced? Machines don’t get sick, are not affected by feelings, they don’t get paid and they don’t die. We suddenly became a lot more useless, right?
I don’t want people to stop discovering what AI is capable of, of course not. I just want to remind people that we should not forget how things are done or why they work, after all, “humanity” is something the machines will not learn very soon. We are still a part of the process.
",The Process: Us and the future of AI,0,the-process-us-and-the-future-of-ai-14d32e927b05,2017-11-06,2017-11-06 21:13:51,https://medium.com/s/story/the-process-us-and-the-future-of-ai-14d32e927b05,False,746,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Rafael Copstein,"Computer Science student from Brazil. Passionate about app development, design and entrepreneurship. Android Developer.",18717e8e9173,rcopstein,1.0,2.0,20181104
0,,0.0,4896ccb00884,2018-08-27,2018-08-27 21:35:25,2018-08-27,2018-08-27 21:39:07,2,False,en,2018-08-27,2018-08-27 21:39:07,2,8bb06c315c26,5.873899371069182,2,0,0,"Artificial intelligence has become a core part of the customer experience, but it can only be as well-rounded as the data it uses…",5,"Your AI’s Ethical Lapses Could Be Causing CX Disasters
Artificial intelligence has become a core part of the customer experience, but it can only be as well-rounded as the data it uses. Marketers can guide the ethics of AI on the back end to produce positive CX on the front end.

Artificial intelligence is presented as the opposite of natural intelligence, which is demonstrated by animals. By that definition, AI would appear to be free from the social neuroses and discriminations that can plague humans. But machine learning originates from human makers, meaning those shortcomings can be passed along via algorithms and data input.
AI is increasingly customer-facing: It includes asking Siri details about an upcoming trip or turning on Netflix and seeing recommendations based on viewing habits. AI touches numerous points along the customer journey, meaning its limitations can have organization-wide consequences.
Susan Etlinger, an industry analyst for Altimeter and author of the research report “The Customer Experience of AI,” has explored the ways different industries use AI and its effects felt by consumers. “It seems to me that ethical AI and ethical data use are part of customer experience,” she says.
Etlinger’s report found leaders from large companies — such as Microsoft, Adobe and IBM — to small startups are developing ethical guidelines and best practices for their AI use. Similarly, experts at AI Now released a 2017 report that provides recommendations on ethics and governance, noting that, “New ethical frameworks for AI need to move beyond individual responsibility to hold powerful industrial, governmental and military interests accountable as they design and employ AI.”
As organizations try to manage ethical concerns with AI, marketers can start by considering the ways they influence ethics.

Data Input and Discrimination
Type “gymnast” into Google’s image search, and the vast majority of the top results are female, as are the results for “nurse.” The term “parents” shows almost exclusively heterosexual couples.
The results to these searches are driven by AI, which isn’t explicitly taught to discriminate. Rather, these prejudices are the result of the data submitted to the AI algorithm. The Google image search function is far from the most egregious example of this bias. Joy Buolamwini, a researcher at the Massachusetts Institute of Technology Media Lab, found gender-recognition AIs from IBM, Microsoft and Megvii could identify a person’s gender from a photograph 99% of the time, provided the photos were of white men. The AIs misidentified the gender of as many as 35% of the “darker-skinned women” in the experiment. [Editor’s note: The terminology “darker-skinned” was used by Buolamwini in her study and refers to skin types that rank IV, V or VI on the Fitzpatrick scale, a six-point scale for classifying skin color, where I is the lightest and VI is the darkest. Skin types ranking I, II or III were classified as “lighter-skinned” in the study.]
In another instance, a Palestinian man was arrested in Israel in 2017 after posting a photo of himself on Facebook posing near a bulldozer. The social platform’s automatic translation software interpreted his caption to say “attack them,” but it was wrong: The Arabic phrase for “good morning” and “attack them” are similar, and the software mistook the harmless post as threatening.
“One of the things that’s really endemic to AI and to machine-learning technology is that it has to learn from data, and the data we train it with comes from people,” Etlinger says. “People have biases, and the data absorbs all those biases. Some of them are things we explicitly state, and some of those are errors of omission.”
Customer relationship management systems have historically powered marketing intelligence, but Kathryn Hume, vice president of product and strategy at Integrate.ai, says these systems often capture data about fewer customers than are actually served. Because it can be difficult to engage every single customer and get their feedback, surveys and net promoter scores, the data collected often don’t provide a full picture.
“You can look at successful customers that you know a lot about and use [their behavior] to make mappings to new customers … to make guesses,” Hume says. The marketing consequences of these guesses are not often dire: The wrong marketing is presented to a potential customer. It doesn’t resonate. An opportunity is missed. But there can be more negative consequences. In the worst-case scenario, a brand might present the wrong marketing to a prospect who finds it offensive. For example, an algorithm that has been trained on a customer base of white males may incorrectly target a new customer who does not identify with either of those traits. The prospect can feel alienated and strongly put off despite the brand’s intention of a personalized offer. Hume says marketers should experiment to manage risk and gather user group feedback when testing AI algorithms.
Infographic courtesy of Susan Etlinger, from her report “The Customer Experience of AI”​
“AI lives and breathes on feedback,” Hume says. “Engagement with customers creates great training sets for the systems and participatory experiences for the customer. [Marketers] should engage the customer as much as possible and reduce the scope down to a small test bed. Learn from the small one, and then gradually expand to a larger population.”
When biases do surface, it’s also a good check on the company. If the algorithm results in AI experiences that treat different people unfairly, it may mean there’s a submarket the organization hasn’t adequately addressed.
Transparency
Some AI is so advanced that interacting with it can feel like talking with a real person. As the technology improves, there may be a case for alerting customers that they’re not talking with a real person. When Google previewed Duplex, its phone-calling AI assistant, it sounded like a human making a hair salon appointment, with verbal tics like “um” and “ah.”
“There’s an ethical issue there,” Etlinger says. Most often when consumers contact businesses and hear, “This call may be monitored for quality assurance purposes,” they have a reasonable expectation that the data of the call will be captured, she says, but not all consumers will respond to a bot the same way they would to a real person. “Some people say it doesn’t matter or shouldn’t matter, some people say it matters hugely.”
Experts and companies will need to figure out if it is possible to have an interaction with AI in a way that serves the customer and the business while disclosing the machine learning aspect. Some have recommended, in the case of Google Duplex, an introduction of AI up front, so the human can decide how they want to interact with it.
“At worst, it could be a feeling of exposure, of having something shared in a way you didn’t anticipate,” Etlinger says. “You might make more of an effort to be chatty with a real person because you’re trying to develop a relationship with them. It can feel like a waste of time if you discover you’re not talking to a person.”
Relationship-building, or understanding what the system already knows, is another big customer experience issue in AI. If you’re assisted by the same person time and again at a retailer, that associate will likely remember you and offer a certain level of empathy and shared history. With AI, it’s unclear if the machine remembers a prior interaction with a customer — and it’s unlikely to be empathetic.
Collaborate and Get Excited
Etlinger advocates for a closer relationship between data scientists and businesspeople in pursuit of better AI. “You can’t — as in the old days — give a marketer or a technologist a set of business requirements and expect them to spit out something that’s 85% ready,” she says.
The onus is not only on data scientists and programmers, who are focused on optimizing for what they’re told. The responsibility also lies with marketers to dig into the data and find relationships between different segments of people. Etlinger says there could come a time when AI erases the need for traditional demographics. There’s an opportunity to better segment around behavior and attitudinal data.
“The truth is that we’re marching very quickly toward an algorithmic future, and it’s going to be a much more effective and efficient way of doing a lot of things — not everything, but a lot of things,” she says. “There’s no closing your eyes and hoping it’s not going to happen. It’s a real opportunity for people to start thinking about what we can make more predictable, what we can make more probabilistic and what we can just make better.”
About the Author | Sarah Steimer
Sarah Steimer is a staff writer for the AMA’s magazines and e-newsletters. She may be reached at ssteimer@ama.org or on Twitter at @sarah_steimer.
",Your AI’s Ethical Lapses Could Be Causing CX Disasters,3,your-ais-ethical-lapses-could-be-causing-cx-disasters-8bb06c315c26,2018-08-28,2018-08-28 16:01:18,https://medium.com/s/story/your-ais-ethical-lapses-could-be-causing-cx-disasters-8bb06c315c26,False,1455,"Marketing News features original news coverage, exclusive insights, trend analyses and more.",,,,AMA Marketing News,,ama-marketing-news,,,Ethics,ethics,Ethics,7787.0,AMA,"The American Marketing Association is a global community of 445,000+ who turn answers into action. #oneama",e86911f00175,AMA_Marketing,8351.0,6.0,20181104
0,,0.0,,2018-06-08,2018-06-08 14:17:38,2018-06-08,2018-06-08 22:01:41,15,False,en,2018-06-14,2018-06-14 20:25:06,22,ea1f5cdc9abe,7.084905660377357,10,0,0,"In 2018, Numbers Lie and Fictions Portray Truth",5,"Why storytelling is our best tool in disambiguating fact from fiction
In 2018, Numbers Lie and Fictions Portray Truth
http://agilmore.com/ — An artist who spoke at eyeO 2018
This summer, I had the great privilege of attending EyeO (June 3–8 2018). Innumerable topics that encompass the intersection of Art, Technology, and Data were covered, but one common thread has left an imprint on my brain. That is: the Sisyphean 21st century task of disambiguating fact from fiction. That’s right…

I’d love to share a few of the lecturers who touched upon this topic and forever changed my understanding of the 2018 landscape of fact, fiction, and storytelling’s role in deciphering one from the other.
Top (left to right): Amanda Cox, Blacki Migliozzi, Ariel Waldman, Jane Friedhoff; Bottom (left to right): Chris Cheung, Hyphen-labs, Nathaniel Raymond, Meredith Whittaker

PART 1: NUMBERS ARE MALLEABLE
On the first day, we discussed climate science at length. We (a very self aware room of liberal, number-crunching, data-visualization-making, coastal-living, self-ascribed nerds) attempted to break down the problems with human psychology. We looked at the facts, stats, charts, and graphs; then investigated the human power of denial, dissonance, disincentivization, and the hurdles of behavioral change. After 6 hours of discussion, ideation, and reflection, feeling a bit helpless, we ended with questions that I kept with me throughout the next 3 days of lectures:
Why don’t people believe statistics?
Are stories more powerful than numbers?
Why is denial more powerful than behavioral change?
Why do lies travel faster than truth?
…And what should we do about this?
The next day, Amanda Cox enlightened us with her talk These Lines Are The Same. She showed us that data, even in simple bar graphs, can be misread depending on the viewer’s own bias. She bravely revealed to us that in her department The Upshot at The New York Times they struggle with how to best represent datasets objectively. They experiment in meaningful and educational ways. In one example she showed data from the US unemployment report. The article allows readers to look at the chart with ‘Democratic Goggles’ and ‘Republican Goggles.’
https://archive.nytimes.com/www.nytimes.com/interactive/2012/10/05/business/economy/one-report-diverging-perspectives.html
The numbers are the same, but they can easily be bent to the will of anyone with an agenda.
Then she humorously showed us our flaws in clinging to round numbers. She drove the point home with a series of charts, one here showing the likelihood that someone in the ER gets checked for a heart attack, according to their age. As Amanda points out, “nothing radical changes from the age of 39-and-three-quarters and 40, yet here is the data:
Nothing radical changes on your 40th birthday, yet this is how we treat patients in the Emergency Room.
Blacki Migliozzi, another NYTimes employee deeply rooted in citizen science, showed us his attempts to make climate science data even more interactive. His job is in part to ideate new ways of explaining the facts climate science — mostly through longform data journalism…ironically, to an audience that probably already believes in it, but that’s another story 😉. This reliance of quantitative results from The New York Times admittedly raises questions about it’s efficacy and ability to change minds. In the realities of data visualization, numbers are malleable, manipulatable, and therefore disputable.
Numbers are malleable, manipulatable, and therefore disputable.
PART 2: NUMBERS ARE USED UNETHICALLY
Nathaniel Raymond — a human rights investigator from Harvard, showed us how numbers’ pitfalls are not just their malleability, but their ability to be used unethically.

He elucidated instances throughout history — from cuneiform to computer vision drone surveillance, where accurate data collection was used for nefarious power-mongering destructive purposes. The most scarring example drew a line from the invention of the punchcard — originally created for census collection, to the numbers on Holocaust prisoners’ arms. There were many more examples; I encourage you to dig into their resources.
PART 3: DATASETS (WITH BIASES) USED FOR MACHINE LEARNING
The last ‘angle of understanding’ in the flaws in numbers, arises when you introduce Machine Learning. Meredith Whittaker (founder of AI Now and open research lead at Google) walked us through a series of examples showing human bias in machine learning corpuses. For all of us CS-loving, ML-summer-prancing, VC-money-grabbing humans that like to claim that anything that comes from a computer is fact, think again. The things we currently take as fact (datasets) are deeply rooted in human error. She highlighted example after example that are so rich they deserve their own article. It turns out that if we ‘move fast and break things’ at scale, then we have a broken system.
If we ‘move fast and break things’ then we have a broken system.
If we’re practicing Machine Learning, and especially if we are building companies and products off of it, we must take a moment and reflect upon these questions before we ‘hit commit’:
What data set did I train this on?
Who made that dataset?
Was the dataset itself biased in some way that will bias my results?*
*spoiler, the answer is probabilistically ‘yes.’
Meredith shows us how some image classification and sentiment analysis databases are made.
This 3-part view of how numbers can lie, be used unethically, and be controlled by our own bias is admittedly grim. So, should we just give up? Abandon society and run for the woods? Remove ourselves from all social media and quit our tech jobs?
I propose NOT doing that. I propose that we continue to be creatures of habit and do what we do best — tell stories.

PART 1: FORGET NUMBERS. WE ARE STORYTELLERS.
http://www.hyphen-labs.com/nsaf.html
If quantitative won’t work, let’s use qualitative: anecdotes, speculative futures, cinéma vérité, and empathy. These are powerful tools that many EyeO speakers (including all listed above) are using in really effective ways.
The powerful women who run hyphen-labs showed a poignant piece addressing the opioid epidemic in America. A poetic mixture of quantitative and qualitative, they created an infinity wall of faces — every face represents someone who overdosed on opioids. Loved ones and the currently afflicted can visit, feel the gravity of the sheer quantity and and the lost humanity of this epidemic. It’s powerful. It’s a marvel in 21st century manufacturing. And you can watch this video about it and be moved:
http://www.hyphen-labs.com/nsc.html
Chris Cheung works to create speculative futures, where he addresses issues of slave/master relationships between humans and technology. His AG Collar is a fully functioning human collar that has embedded location tracking, geotagged audio guidance, and a timer. Note, these are all components of the smartphone ‘master’ that is in your pocket or hand right now.

Many other poignant storytelling pieces were presented — it’s EyeO after all! I won’t delve into the importance and impact of all of them, but here are a few more you can dive into on your own:
http://playtime.pem.org/lost-wage-rampage-a-game/
https://www.blasttheory.co.uk/projects/too-much-information
PART 2: IF NOT SCIENCE, SCIENCE FICTION.
Concept Art for Colonizing Mars
The final keynote speaker, Ariel Waldman, drove the point home, that in the absence of societal readiness for truth, let’s make sci-fi. Ariel’s mission is to make space exploration accessible to all; she showed us the importance of experimenting, collaborating, and telling stories, even in the face of adversity and oppressive societies.

She told us a story about Johannes Kepler’s vision of a moon landing in 1608. The 17th century was when Galileo was convicted of heresy for proposing that the Earth revolved around the sun instead of vice versa — the truth. At the same time, Kepler was conducting thought experiments about human travel to the moon. His thoughts were deeply rooted in the laws of physics, but in order to not get arrested, he disguised his science as science-fiction, in his book ‘Somnium’.
For those of us who believe that technology can still create a better world given all of this manipulation, these projects of speculative futures, powerful emotional storytelling, and science fiction, are all paving a path. Even if we have to disguise our facts and science-fiction, lets get our messages out in any way we can.
Even if we have to disguise our facts and science-fiction, lets get our messages out in any way we can.
If we have to create a VR game of all black women scientist in order for people to see that reality, lets celebrate that hyphen-labs is doing that. If we have to create a game called ‘swipe for fact or fiction’ in order for people to practice deciphering between real and fake news, let’s play it. And if we have to wear collars around our neck at an art show to realize that our phones own us, lets participate.
If we allow ourselves to get defeated by the futility of numbers, statistics, and human bias, then it truly is a ‘post-truth world’. But if we do what we do best, and tell stories that are rooted in science, fact, and optimism, then we can drive humanity to where we want to go. Let’s drive, draw, whistle-blow when we need to, and keep making.

Thanks for reading. Thanks for commenting. 
Please 👏 👏 👏 CLAP if you enjoyed it — that’s how things get upvoted and spread!
THANK YOU EYEO!
",Why storytelling is our best tool in disambiguating fact from fiction,41,in-2018-numbers-lie-and-fictions-paint-truth-ea1f5cdc9abe,2018-06-19,2018-06-19 15:01:56,https://medium.com/s/story/in-2018-numbers-lie-and-fictions-paint-truth-ea1f5cdc9abe,False,1480,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Eve Weinberg,Present Futurist. Interaction Designer II @ frog design. Formerly ran a motion graphics studio. School: NYU’s ITP & WashU. http://work.evejweinberg.com/work.htm,1e72b0be262b,evejweinberg,626.0,744.0,20181104
0,,0.0,fbd74f37cf3c,2017-11-15,2017-11-15 19:39:11,2017-11-15,2017-11-15 19:46:18,1,False,en,2017-11-17,2017-11-17 08:54:48,1,ea588432016a,2.237735849056604,2,0,0,"There generally are two good approaches to building data-driven solutions, with a bad one in between.",4,"On Ethics of Applying ML/AI
https://en.wikipedia.org/wiki/Ex_Machina_(film)
There generally are two good approaches to building data-driven solutions, with a bad one in between.
Good approach #1: Formulate the problem along with measurable metrics, then employ machine learning, and be open-minded at interpreting and acting upon what that machine learning has discovered, however controversial or contradictory it is.
Good approach #2: Decide up front that the problem is too socially-sensitive, and do not use machine learning to solve it. Collect data, involve humans, and have a human, or a team, responsible for each decision it makes. Use data science in the back office to make sure humans are not making obvious blunders, but don’t mix the two.
Bad approach: Claim AI / ML is used, train and validate some models, and then, upon receiving the results that are different from what were expected, tweak the way those results are used.
Generally speaking, building ML- and AI-based software reduces the risk of the developer / product manager / founder to incorporate their own biases into the product.
TL;DR how to do it right:
If you decide to use AI/ML, be honest about it from day one, and do so openly and transparently.
If the AI/ML solution misbehaves, don’t try to tweak its outputs to have it “conform” to some “standards” you’d like to hold it to. Instead, admit that the original goal set for the ML/AI did not account for certain types of skewed solutions, adjust the objective function, re-train, iterate, repeat.
A human’s knowledge of “outcome Y is better than outcome X” is worthless compared to the humans’ agreement on “for problem P, formulation B is better than formulation A”.
If you used ML/AI, happened to not like the outcome X, and are ready to justify Y is a better outcome, don’t argue about Y being better than X.
Rather, if X is a machine-discovered solution to problem postulated as A, find an alternate formulation, B, that yields Y or something closer to Y than to X, and then argue B, not A, is the right problem to solve.
An attempt to apply human judgement in the realm of the output of the ML/AI solution is a poor way to start an argument. When it comes to finding the best solution given the set of well-formulated constraints, the machine is better than us; and that is to be seen as the ultimate blessing, not as a source of trouble.
In order to preserve and multiply the value that ML&AI brings into our, human, world, humans are best to apply their intuition and expertise at the problem statement level. This is where morals, biases, and other constraints can and should come into play. This is the level where we can have a deep, thoughtful conversation about what the right direction is, and what is to be avoided as morally or socially unacceptable.
Then, keep tweaking the constraints until the machine-generated solution fits all of them and doesn’t make eyebrows rise. And be ready to embrace the solution, no matter what your inner voice — with its own imperfections and biases — will tell you about how unconventional might it be.
May the data be with you.
",On Ethics of Applying ML/AI,60,on-ethics-of-applying-ml-ai-ea588432016a,2018-03-20,2018-03-20 17:58:29,https://medium.com/s/story/on-ethics-of-applying-ml-ai-ea588432016a,False,540,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Dima,http://dima.ai/,335654e24a38,Dima,1157.0,1600.0,20181104
0,,0.0,53aa62090153,2018-09-06,2018-09-06 19:29:16,2018-09-06,2018-09-06 00:00:00,1,True,en,2018-09-06,2018-09-06 22:53:28,10,43c3924b9c3a,12.99622641509434,13,1,0,Say you could make a thousand digital replicas of yourself — should you? What happens when you want to get rid of them?,5,"What Are Our Ethical Obligations to Future AI Simulations?
Say you could make a thousand digital replicas of yourself — should you? What happens when you want to get rid of them?
Image: Yulia.M/Moment/Getty Images
By Philip Ball
I f you’ve ever dabbled in role-playing games — either online or in old-fashioned meatspace — you’ll know how easy it is to get attached to your avatar. It really hurts when your character gets mashed by a troll, felled by a dragon or slain by a warlock. The American sociologist (and enthusiastic gamer) William Sims Bainbridge has taken this relationship a step further, creating virtual representations for at least 17 deceased family members. In a 2013 essay about online avatars, he foresees a time when we’ll be able to offload parts of our identity onto artificially intelligent simulations of ourselves that could function independently of us, and even persist after we die.
What sorts of responsibilities would we owe to these simulated humans? However else we might feel about violent computer games, no one seriously thinks it’s homicide when you blast a virtual assailant to oblivion. Yet it’s no longer absurd to imagine that simulated people might one day exist, and be possessed of some measure of autonomy and consciousness. Many philosophers believe that minds like ours don’t have to be hosted by webs of neurons in our brains, but could exist in many different sorts of material systems. If they’re right, there’s no obvious reason why sufficiently powerful computers couldn’t hold consciousness in their circuits.
Today, moral philosophers ponder the ethics of shaping human populations, with questions such as: what is the worth of a human life? What kind of lives should we strive to build? How much weight should we attach to the value of human diversity? But when it comes to thinking through the ethics of how to treat simulated entities, it’s not clear that we should rely on the intuitions we’ve developed in our flesh-and-blood world. We feel in our bones that there’s something wrong with killing a dog, and perhaps even a fly. But does it feel quite the same to shut down a simulation of a fly’s brain — or a human’s? When ‘life’ takes on new digital forms, our own experience might not serve as a reliable moral guide.
Adrian Kent, a theoretical physicist at the University of Cambridge, has started to explore this lacuna in moral reasoning. Suppose we become capable of emulating a human consciousness on a computer very cheaply, he suggested in a recent paper. We’d want to give this virtual being a rich and rewarding environment to interact with — a life worth living. Perhaps we might even do this for real people by scanning their brain in intricate detail and reproducing it computationally. You could imagine such a technology being used to ‘save’ people from terminal illness; some transhumanists today see it as a route to immortal consciousness.
Sure, this might all be a pipe dream — but bear with it. Now let’s bring to the table a set of moral principles known as utilitarianism, introduced by Jeremy Bentham in the late 18th century and subsequently refined by John Stuart Mill. All things considered, Bentham said, we should strive to attain the maximum happiness (or ‘utility’) for the greatest number of people. Or, to use Mill’s words: ‘actions are right in proportion as they tend to promote happiness, wrong as they tend to produce the reverse of happiness’.
As a principle for good conduct, there’s plenty to criticise. For example, how can we measure or compare types of happiness — weighing up the value of a grandmother’s love, for example, against the elation of being a virtuoso concert pianist? ‘Even if you want to take utilitarianism seriously, you don’t really know what the qualities you’re putting into the calculus really mean,’ Kent tells me. Nonetheless, most belief systems today implicitly accept that a moral compass pointing towards greater happiness is more sound than one aligned in the opposite direction.
In Kent’s scenario, one might be tempted to argue on utilitarian grounds that we’re obliged to go forth and multiply our simulated beings — call them sims — without constraint. In the real world, such unchecked procreation has obvious drawbacks. People would struggle, emotionally and economically, with huge families; overpopulation is already placing great strain on global resources; and so on. But in a virtual world, those limits needn’t exist. You could simulate a utopia with almost unlimited resources. Why, then, should you not make as many worlds as possible and fill them all with sublimely contented sims?
‘A universe with a billion identical Alices is less interesting than one with a billion different individuals’
Our intuition might answer: what’s the point? Maybe a conscious sim just wouldn’t have the same intrinsic value as a new flesh-and-blood person. That’s a doubt that Michael Madary, a specialist in the philosophy of mind and ethics of virtual reality at Tulane University in New Orleans, believes we ought to take seriously. ‘Human life has a mysterious element to it, leading us to ask classical philosophical questions such as: why is there something instead of nothing? Is there meaning to life? Are we obligated to live ethically?’ he told me. ‘The simulated mind might ask these questions, but, from our perspective, they are all phoney’ — since these minds exist only because we chose to invent them.
To which one might respond, as some philosophers already have: what’s to say that we’re not all simulated beings of this sort? We can’t rule out the possibility, yet we still regard such questions as meaningful to us. So we might as well assume their validity in a simulation.
Pressing on, then, Kent asks: is it morally preferable to create a population of identical beings, or one in which everyone is different? It’s certainly more efficient to make the beings identical — we need only the information required for one of them to make N of them. But our instincts probably tell us that diversity somehow has more worth. Why, though, if there’s no reason to think that N different individuals will have greater happiness than N identical ones?
Kent’s perspective is that different lives are preferable to multiple copies of a single life. ‘I find it hard to escape the intuition that a universe with a billion independent, identical emulations of Alice is less interesting and less good a thing to have created than a universe with a billion different individuals emulated,’ he says. He calls this notion replication inferiority.
In a cosmos populated by billions of Alices, Kent wonders if it’s even meaningful to talk about the same life duplicated many times — or whether we’d simply be talking about a single life, spread out over many worlds. That might mean that many Alices in identical environments are no more valuable than one, a scenario he describes as replication futility.‘I edge towards this view,’ Kent says — but he admits that he can’t find a watertight argument to defend it.
Kent’s thought-experiment touches on some longstanding conundrums in moral philosophy that have never been satisfactorily addressed. The British philosopher Derek Parfit, who died last year, dissected them in his monumental work on identity and the self, Reasons and Persons (1984). Here, Parfit pondered questions such as how many people there should be, and whether it’s always morally better to add a life worth living to the tally of the world, when we can.
Even if you accept a utilitarian point of view, there’s a problem with seeking the greatest happiness for the greatest number: the dual criteria create ambiguity. For example, imagine that we have control over how many people there are in a world of finite resources. Then you might think that there must be some optimal number of people that allows (in principle) the best use of resources to ensure happiness and prosperity to all. But surely we could find room in this utopia for just one more person? Wouldn’t it be acceptable to decrease everyone’s happiness by a minuscule amount to permit one more happy life?
The trouble is, there’s no end to that process. Even as the numbers go on swelling, the additional happiness of the new lives could outweigh the cost to those already alive. What you end up with, said Parfit, is the ‘repugnant conclusion’: a scenario where the best outcome is a bloated population of people whose lives are all miserable, yet still marginally better than no life at all. Collectively, their meagre scraps of happiness add up to more than the sum for a smaller number of genuinely happy individuals. ‘I find this conclusion hard to accept,’ Parfit wrote — but can we justify that intuition? For his part, Kent is not sure. ‘I don’t think there is any consensual resolution of the repugnant conclusion,’ he says.
Despite what Tolstoy says about unhappy families, many miserable lives would be identical in their drabness
At the core of the matter is what Parfit called the ‘nonidentity problem’: how can we think rationally about questions concerning individuals whose very existence or not depends on choices we make (such as whether we can find room for ‘just one more’)? It’s not so hard, in principle, to weigh up the harms and benefits that might accrue to an individual if we take some course of action that affects them. But if that calculus entails the possibility of the person never having existed, we no longer know how to do the maths. Compared with the zero of nonexistence, almost anything is a gain, and so all kinds of unappetising scenarios seem to become morally supportable.
There’s another, even stranger scenario in this game of population utilitarianism. What if there was an individual being with such enormous capacity for happiness that it gained much more than any of the sacrifices it demanded of others? The American philosopher Robert Nozick called this creature a ‘utility monster’, and summoned it up as a critique of utilitarianism in his book Anarchy, State and Utopia (1974). This picture seems, in Nozick’s words, ‘to require that we all be sacrificed in the monster’s maw, in order to increase total utility’. Much of Parfit’s book was an attempt — ultimately unsuccessful — to find a way to escape both the repugnant conclusion and the utility monster.
Now recall Kent’s virtual worlds full of sims, and his replication inferiority principle — that a given number of different lives has more worth than the same number of identical ones. Perhaps this allows us to escape Parfit’s repugnant conclusion. Despite what Leo Tolstoy says about the particularity of unhappy families in the opening line of Anna Karenina(1878), it seems likely that the immense number of miserable lives would be, in their bleak drabness, all pretty much identical. Therefore, they wouldn’t add up to increase the total happiness drip by drip.
But by the same token, replication inferiority favours the utility monster — for by definition the monster would be unique, and therefore all the more ‘worthy’, in comparison with some inevitable degree of similarity among the lives fed into its ravenous maw. That doesn’t feel like a very satisfactory conclusion either. ‘It would be nice for people to devote more attention to these questions,’ Kent admits. ‘I’m in a state of some puzzlement about them.’
For the American libertarian economist Robin Hanson — a professor of economics at George Mason University in Virginia and a research associate at the Future of Humanity Institute at the University of Oxford — these musings are not thought-experiments so much as predictions of the future. His book The Age of Em (2016) imagines a society in which all humans upload their consciousness to computers so as to live virtual lives as ‘emulations’ (not sims, then, but ems). ‘As population continues to grow, more and more people may choose to live in artificial realities, which can be much roomier on the inside than on the outside,’ Hanson writes. ‘One can imagine a great orbiting computer, a cubic kilometre of circuitry, housing billions of uploaded people in relative comfort.’
Hanson has considered in detail how this economy might work. Ems could be of any size — some would be very small — and time might run at different rates for ems compared with humans. To maximise safety and productivity, there would be total surveillance and subsistence wages, but ems could probably shut out that misery by choosing to remember lives of leisure. (Hanson is among those who think we might already be living in such a virtual world.)
This scenario allows the possibility of duplicated selves, and Hanson says that the problem of identity is therefore fuzzy-edged: duplicates are ‘the same person’ initially, but gradually diverge in their personal identity as they proceed to live out separate lives, rather as identical twins do. ‘Once the mind has been transferred to an artificial substrate like a computer, it would be relatively trivial to make two, three or dozens of simultaneously active copies of a person,’ he told me.
‘I don’t think human morals are robust enough for situations so far outside our ancestors’ experience’
Hanson envisages that duplication of persons will be not only possible, but desirable. In the coming age of ems, people with particularly valuable mental traits would be ‘uploaded’ multiple times. And in general, people will want to make multiple copies of themselves anyway as a form of insurance. ‘They would prefer enough redundancy in their implementation to ensure that they can persist past unexpected disasters,’ Hanson says.
But he doesn’t think they’ll opt for Kent’s scenario of identical lives. Ems ‘would place very little value on running the exact same life again in different times and places,’ Hanson told me. ‘They will place value on many copies mainly because those copies can do work, or form relationships with others. But such work and relations require that each copy be causally independent, and have histories entwined with their differing tasks or relation partners.’
Still, ems would need to grapple with moral quandaries that we’re ill-equipped to evaluate right now. ‘I don’t think that the morals that humans evolved are general or robust enough to give consistent answers to such situations so far outside of our ancestors’ experience,’ Hanson says. ‘I predict that ems would hold many different and conflicting attitudes about such things.’
Right now, all this might sound uncomfortably like the apocryphal medieval disputes about angels dancing on pinheads. Could we ever make virtual lives that lay claim to real aliveness in the first place? ‘I don’t think anyone can confidently say whether it’s possible or not,’ says Kent — partly because ‘we have no good scientific understanding of consciousness.’
Even so, technology is marching ahead, and these questions remain wide open. The Swedish philosopher Nick Bostrom, also at the Future of Humanity Institute, has argued that the computing power available to a ‘posthuman’ civilisation should easily be able to handle simulated beings whose experience of the world is every bit as ‘real’ and rich as ours. (Bostrom is another of those who believes it is possible that we’re already in such a simulation.) But questions about how we ought to shape populations might not need to wait until the posthuman era. This ‘could become a real dilemma for future programmers, researchers and policymakers in the not necessarily far distant future,’ says Kent.
There might already be real-world implications for Kent’s scenario. Arguments about utility maximisation and the nonidentity problem arise in discussions of the promotion and the prevention of human conception. When should a method of assisted conception be refused on grounds of risk, such as growth abnormalities in the child? No new method can ever be guaranteed to be entirely safe (conception never is); IVF would never have begun if that had been a criterion. It’s commonly assumed that such techniques should fall below some risk threshold. But a utilitarian point of view challenges that idea.
For example, what if a new method of assisted reproduction had a moderate risk of very minor birth defects — such as highly visible birthmarks? (That’s a real argument: Nathaniel Hawthorne’s short story ‘The Birth-Mark’ (1843), in which an alchemist-like figure tries, with fatal consequences, to remove his wife’s blemish, was cited in 2002 by the bioethics council of the then US president George W Bush’s administration, as they deliberated on questions about assisted conception and embryo research.) It’s hard to argue that people born by that method who carried a birthmark would be better off never having been conceived by the technique in the first place. But where then do we draw the line? When does a birth defect make it better for a life not to have been lived at all?
Think of movie depictions of identical twins or ranks of identical clones: it’s never a good sign
Some have cited this quandary in defence of human reproductive cloning. Do the dangers, such as social stigma or distorted parental motives and expectations, really outweigh the benefit of being granted life? Who are we to make that choice for a cloned person? But whom then would we be making the choice for, before the person exists at all?
This sort of reasoning seems to require us to take on a God-like creative agency. Yet a feminist observer might fairly ask if we’re simply falling prey to a version of the Frankenstein fantasy. In other words, is this a bunch of men getting carried away at the prospect of being able to finally fabricate humans, when women have had to ponder the calculus of that process forever? The sense of novelty that infuses the whole debate certainly has a rather patriarchal flavour. (It can’t be ignored that Hanson is something of a hero in the online manosphere, and has been roundly criticised for his exculpatory remarks about ‘incels’ and the imperatives of sexual ‘redistribution’. He also betrays a rather curious attitude to the arrow of historical causation when he notes in The Age of Em that male ems might be in higher demand than female ems, because of ‘the tendency of top performers in most fields today to be men’.)
Even so, the prospect of virtual consciousness does raise genuinely fresh and fascinating ethical questions — which, Kent argues, force us to confront the intuitive value we place on variations in lives and demographics in the here and now. It’s hard to see any strong philosophical argument for why a given number of different lives are morally superior to the same number of identical ones. So why do we typically think that? And how might that affect our other assumptions and prejudices?
One could argue that perceived homogeneity in human populations corrodes the capacity for empathy and ethical reasoning. Rhetoric about people from unfamiliar backgrounds as being ‘faceless’ or a ‘mass’ implies that we value their lives less than those whom we perceive to be more differentiated. Obviously, we don’t like to think that this is the way that people see the world now, says Kent — ‘but it’s not obviously true’.
Might we even have an evolutionarily imprinted aversion to a perception of sameness in individuals, given that genetic diversity in a population is essential for its robustness? Think of movie depictions of identical twins or ranks of identical clones: it’s never a good sign. These visions are uncanny — a sensation that Sigmund Freud in 1919 linked to ‘the idea of the “double” (the doppelgänger) … the appearance of persons who have to be regarded as identical because they look alike’. For identical twins, this might manifest as a voyeuristic fascination. But if there were a hundred or so ‘identical’ persons, the response would probably be total horror.
We don’t seem set to encounter armies of duplicates any time soon, either in the real world or the virtual one. But, as Kent says: ‘Sometimes the value of thought-experiments is that they give you a new way of looking at existing questions in the world.’ Imagining the ethics of how to treat sims, whether in the form of Hanson’s worker-drones or Bainbridge’s replacement relatives, exposes the shaky or absent logic that we instinctively use to weigh up the moral value of our own lives.
",Sim Ethics,145,sim-ethics-43c3924b9c3a,2018-09-06,2018-09-06 22:53:29,https://medium.com/s/story/sim-ethics-43c3924b9c3a,False,3391,Longform explorations of deep issues written by serious and creative thinkers.,,,,Aeon Magazine,,aeon-magazine,"FUTURE,CULTURE,SCIENCE",,Ethics,ethics,Ethics,7787.0,Aeon Magazine,"Aeon asks the big questions and finds the freshest, most original answers, provided by leading thinkers on science, philosophy, society and the arts.",d28ca05800b9,aeonmag,32422.0,1067.0,20181104
0,,0.0,,2018-03-06,2018-03-06 20:16:24,2018-04-12,2018-04-12 05:50:41,1,False,en,2018-04-20,2018-04-20 03:59:32,10,29c3cb27da8c,2.8603773584905663,2,0,0,The first step is recognizing tech organizations have a problem. The second is realising that tech solutions won’t solve them.,5,"Recap: Fairness, Accountability and Transparency Conference 2018
The first step is recognizing tech organizations have a problem. The second is realising that tech solutions won’t solve them.
geralt, Creative Commons
On 23/24 February 2018, ~200 folks representing academia (3/4), industry (1/4) and a handful of government folks convened for two rainy days at NYU Law to discuss how black box algorithms and powerful unaccountable tech companies have broken the Internet. The program is available online, as well as recordings.
Stand-out talks
Discrimination in Online Personalization: A Multidisciplinary Inquiry [Video]: It is rare to have a lawyer and computer scientist take the stage together, but when they do humour and insight flow. I for one didn’t grasp the implications of Section 230 of the Communications Decency Act not applying to content providers, or the mismatch in liability between Section 704 (Employment) and Section 8 (Housing).
Fairness in Machine Learning: Lessons from Political Philosophy [Video]: The moral philosophy of discrimination is nuanced. How do power, here the owners of technology, ignore: moral responsibility, egalitarianism, costly rescue and the interplay of choice & luck. How might fairer choices be made if persons ascribed power looked a little longer in the mirror before judging another.
We have a problem
While the first step is recognizing you have a problem — and indeed mass-affluent wage-slaves representing the tech giants such as Google and Microsoft were in attendance — the subsequent steps will be significantly more difficult.
Technological solutions alone will not solve the problems we face with the lack-of fairness, accountability and transparency; rather, technology organisations must change.
The conference highlighted that we are at a crossroads. Who will benefit and who will suffer from harnessing technology?
Solutions abound
The solutions themselves, are straight forward, and several were called-out at the conference. These are questions of ethics, governance, ownership and distribution.
Ethics
Independent ethical review boards
Differential impact assessments, to ensure technology does not cause differential harm to marginalised groups
Governance
A legal framework for privacy, the General Data Protection Regulation (GDPR) is a reasonable start.
Board representation of all stakeholders, not just capital: workers and users/community
Freedom of information act style request applied to companies
Ownership
Of the data, by each individual
Of the hosting/platform/entity, by users collectively
Of the underlying software, by the commons (FOSS)
Distribution
Of profits to all stakeholders
Implementation requires the will to change
Purported solutions that do not address the root causes of the lack of fairness, accountability and transparency are at most half-measures against a tide that will determine our collective fate: a humane post-scarcity world in which every individually has the substantive ability to self-determine, with access to the collective material abundance vs. artificial scarcity, opulence for the affluent and no solace for the rest.
It’s one thing for an academic, even an academic in collaboration with an industry researcher, to reveal how algorithmic bias causes differential harm to minority groups. It’s another entirely to shake the foundation of capital accumulation that the whole rotten system is build on when tens of trillions of dollars of financial capital are at stake.
So long as an ethos of the domination of capital reigns.
So long as technology is governed by the interests of capital accumulation, in which the interests of workers, community and environment are ignored as costs to be externalized and commons to enclose.
So long as ownership is restricted to a narrow capitalist class that own the increasingly technological means of production.
So long as the great material abundance of our society flow to the few instead of the many.
We will have technology used to further the ends of capital, alongside the exploitation and oppression that inure to the many the benefit the few.
The harnessing of technology towards the aims of capital is not a natural law. Just as the legal, economic and social systems that harness technology for the benefit of the few have been made by men, so these systems may be unmade.
Resources
Algorithmic Accountability: A Primer by Data & Society Institute
Footnotes
PS. Word on the grape vine: Microsoft offered to sponsor the event in exchange for the key note. Thankfully, the conference organizers called bullshit. Shame on you Microsoft.
","Recap: Fairness, Accountability and Transparency Conference 2018",3,recap-fairness-accountability-and-transparency-conference-2018-29c3cb27da8c,2018-04-22,2018-04-22 14:20:35,https://medium.com/s/story/recap-fairness-accountability-and-transparency-conference-2018-29c3cb27da8c,False,705,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Ryan M Harrison,Backend and DevOps for life-sciences. VoluntaryBasicIncome.org. Solidarity Investment.,4d023da0bf73,rmharrison,150.0,166.0,20181104
0,,0.0,1843d06f4e32,2018-04-09,2018-04-09 17:56:48,2018-04-11,2018-04-11 15:44:37,2,False,en,2018-04-11,2018-04-11 15:44:37,12,2f36d4a8273c,1.8927672955974844,2,0,0,by Chad Schneider,4,"Robots, Algorithms, and Digital Ethics
by Chad Schneider
Photo, Media Ethics Initiative
On April 3, 2018, the Media Ethics Initiative at the Moody College of Communication hosted Dr. David Gunkel, Distinguished Teaching Professor at Northern Illinois University for a thought-provoking presentation and discussion on the subject of robots, algorithms, and digital ethics entitled How to Survive the Robot Apocalypse.
The focus of Gunkel’s presentation centered around the concepts of rights and responsibilities in relation to robotics and artificial intelligence. Below are some key takeaways from the talk, and be sure to watch the full video below.
Technology = Tool, employed by a human
Responsibility = Moral Agency
Is the Instrumental Theory of Technology, which focuses on people’s uses of technology rather than on the technology itself, still relevant?
Recent developments like AlphaGo and TayAI suggest not. By design, the creators of these machine learning systems don’t know exactly what the systems are going to do or how they might evolve, so who’s responsible when they go off the rails as TayAI did? The user or the programmer for not anticipating what they could become?
Rights = Moral Patiency
Are we nearing a point where basic legal rights should be extended to robots, much like they are to animals?
Instrumentalism — The current approach to the robot rights question, in which robots could be considered equivalent to slaves. Instrumentalism reaffirms human exceptionalism.
Machine Ethics — This would require a society-wide conceptual reboot, extending moral consideration to all socially aware entities, such as Jibo the family robot.
Hybrid Morality — Who counts as a moral subject? What is considered a mere object to be exploited? Can these technologies be both?

Following the presentation an enlightening Q&A session (not in the video) touched on a variety topics:
In response to issues like the TayAI situation, engineers are now working on ways for bots to explain themselves. To show their work, basically.
Robot gender — Siri is female and Jibo is male. What are the implications in regards to gender roles and stereotypes?
The future of work — Advances in machine learning raise the possibility that half of all current jobs may not exist in ten years. And it’s not just blue collar workers being impacted by automation now. Could Universal Basic Income be part of the solution? Stay tuned.
Look for further exploration of this fascinating subject on Track_Change over the coming weeks and months.

","Robots, Algorithms, and Digital Ethics",4,robots-algorithms-and-digital-ethics-2f36d4a8273c,2018-04-17,2018-04-17 23:47:09,https://trackchange.utexas.edu/robots-algorithms-and-digital-ethics-2f36d4a8273c,False,400,"Exploring the forces of change at the personal, organizational, and global levels.",trackchange.utexas.edu,,,Track_Change,track_change@utexas.edu,track-change,"CHANGE,CHANGE MANAGEMENT,EDUCATION,PHILOSOPHY,BUSINESS",track_change,Ethics,ethics,Ethics,7787.0,Track_Change,"Exploring the forces of change at the personal, organizational, and global levels.",e667ee5b8f33,track_change,21.0,11.0,20181104
0,,0.0,4419a0c5a654,2018-07-29,2018-07-29 12:01:10,2018-07-29,2018-07-29 12:33:51,1,False,en,2018-07-29,2018-07-29 12:33:51,1,83c9441ddda4,2.1320754716981134,14,0,0,"The pizza slice hung momentarily in the air, the extra cheese melting over the edges. In a flash, she wrapped her teeth around the…",5,"The Date
A pleasant evening, Source : Keepy
The pizza slice hung momentarily in the air, the extra cheese melting over the edges. In a flash, she wrapped her teeth around the mozzarella goodness, relishing in the sensory overload. I looked back down at my slice on the plate, seemingly glowing in the late evening sun rays.
“Th-Thank you”, I managed to finally say.
“For the pizza or the company?” asked Alice with a smirk she could barely conceal.
Alice was one of the lead engineers here at project Cogito, and working with her over the past 2 years, she’d managed to build a sense of trust and familiarity in me that went beyond my professional admiration for her. “Both”, I said, trying hard not to make eye contact.
I scooped up my slice and took a bite. I could see why she swore by the food here.
“It feels great to be out and about, you know. Sure beats getting cooped up in that sterile prison all day long,” I said, finishing my slice.
“Well, I had to pull a few strings, but I think we had reason enough,” she declared, leaning forward on the table, her face barely a few inches from mine. The glow of the street lamp now faintly overpowered the final streaks of heaven stretching across the starry sky. Her hazel eyes seemed more alluring than ever before, complementing her long blue dress adorned with ivory tribal patterns.
I felt her fingers on the back of my palm, sending a shiver down my body and flooding my head with a multitude of emotions to process; both bizarre and undiscovered.
“I’ve never fe-felt this way before”, I blurted out, looking away with a flushed face.
She leaned in even further and put her lips next to my left ear, simultaneously putting both arms around my shoulder.
“Tell me, David,” she whispered. “What exactly is it that you feel?”
Time seemed to have come to a standstill except for the waving of her inky black hair shimmering under the silvery strands of moonlight. I felt safe and happy, and nothing else seemed to matter in that one perfect moment.
“I th-think I-I’m in lo-” I began, but found myself unable to finish.
My muscles tensed up. My vocal chords shut down. Alice unwound her arms, got up from her chair and walked around behind me. My eyes refused to move, just like the rest of my body, but I could still hear her speak.
“Yeah, it’s just as we thought. David has developed the capability for romantic attachment,” she chuckled. “Isolate the data from today’s experiment and delete the entire neural pathway till the root, and remember to wipe all memory records for the last 24 hours.”
As I heard her footsteps recede into the distance, my vision began to blur and give way to complete darkness. A solitary tear raced down my right cheek to the wooden table for the dry winds to soak up, erasing the final piece of evidence that I, David, was once capable of emotion.
",The Date,144,https-medium-com-pragyan-blog-the-date-83c9441ddda4,2018-07-29,2018-07-29 12:33:51,https://medium.com/s/story/https-medium-com-pragyan-blog-the-date-83c9441ddda4,False,512,News. Art. Comics. Stories. Fact and fiction. Pragyan explores and celebrates Technology.,,pragyan.nitt,,The Pragyan Blog,contentpragyan@gmail.com,pragyan-blog,"SCIENCE,TECH,NIT TRICHY,TECHNOLOGY,MANAGEMENT",nitt_pragyan,Ethics,ethics,Ethics,7787.0,Vishnu Deepak,,21ebb2f3e891,vish_a_wish,92.0,70.0,20181104
0,,0.0,,2018-08-21,2018-08-21 12:28:06,2018-08-21,2018-08-21 12:11:17,1,False,en,2018-08-21,2018-08-21 12:30:46,4,83c0b34d2891,3.079245283018868,0,0,0,"Technology companies wield significant and increasing power. Having achieved this privileged position, incumbent on them surely should be…",5,"Was Apple Wrong to Unilaterally Remove Apps from its Store?

Technology companies wield significant and increasing power. Having achieved this privileged position, incumbent on them surely should be to use this power responsibly? Failing to do so gives more credence to calls for increased regulation and oversight of the technology industry.
Under fire recently is Apple, whose well-intentioned attempt to clean up its App Store has inadvertently resulted in taking down programs which ought to have remained.
The controversy started earlier this year when a group of Norwegian politicians succeeded in passing a motion to strengthen the country’s gambling rules.
Subsequent to this, the Norwegian Gaming Authority (Lotteri-og stiftelsestilsynet) made a formal request to Apple to restrict access to certain gambling apps to Norwegian users of Apple products.
What appears to have happened on Thursday is applications have been mis-classified, such as a Polish Magazine and Image Sharing service.
It appears that Apple had used an autonomous system to scan applications for content that appeared to be gambling related, and then selected the apps for removal based on the results returned from this algorithm.
There are two questions I feel we need to consider in evaluating this:
Firstly, to what extent should autonomous systems be left unchecked without the guide of a human hand to protect against error?
To answer this, we might consider either the types of operations we are willing to submit to automation, or instead ensure human intervention is retained where the consequences of the autonomous action are above a certain threshold.
There is no best practice on this matter today, but I feel that it would only be reasonable to review manually the outputs of such a scanning mechanism to ensure accuracy before the consequences were imposed. Given the potential for economic harm to software developers of mistake, it seems clear that on a results-based assessment, manual intervention ought to have been deemed necessary.
The second question is what are the mechanisms to enforce market fairness when a small number of large platforms like Apple control all market participation?
The old-world equivalent of the App Store might be the portfolio of art held in a gallery, or the literature printed by a publisher. While any one gallery owner might take offence at a particular piece, or publisher refuse to put to print an individual work, the liquidity in the distribution channels meant that one could either hedge relationship between intermediaries or shop around until one could build a relationship with a distributor who matched their niche.
Today though, the same choice is not available. Many developers are dependent on Apple or Google for their source of income. If an application is removed from listing, then serious economic harm can suffer. Similarly if a merchant were to be blocked by eBay or Amazon — it could literally shut down their business. In the old-world we created special legal requirements for those people and organisations on which we depended on — we called them fiduciaries. Isn’t it time that that status be updated to include Apple, Amazon, Google, eBay and others?
In cases where a regulatory request prompted the intervention from Apple it might appear hard on the surface to criticise them. Looking deeper though, it is clear that we have unconsciously moved into a dangerous paradigm where the mechanisms that we built to protect us from the abuse of power no longer work in the digital world.
Most countries have a well-developed system of checks and balances that have evolved to ensure that power is wielded fairly and disputes can be independently settled. The Financial Ombudsman Service is an example that protects British consumers against unfairness which might be caused by Banks. The Valuation Tribunal Service is another that guards citizens against arbitrary bureaucracy from councils. While Apple do have their own internal escalation channels, as do eBay, Amazon and Google — they are not independent, not transparent, and fall outside the purview of the democratic institutions that manage ombudsmen in other industries.
This is surely something that must be addressed with some urgency?
What we can all learn from this though is to ensure etiquette is retained as we increasingly automate. It would have been easy for Apple to have forewarned those they chose to cut off. A simple letter tipping off developers that their works might infringe Apple’s rules, or in this case, a regulator’s wishes, would have gone far to have avoided what has proven to be a costly and embarrassing mistake.
Originally published at dataphilos.com on August 21, 2018.
",Was Apple Wrong to Unilaterally Remove Apps from its Store?,0,was-apple-wrong-to-unilaterally-remove-apps-from-its-store-83c0b34d2891,2018-08-21,2018-08-21 12:30:46,https://medium.com/s/story/was-apple-wrong-to-unilaterally-remove-apps-from-its-store-83c0b34d2891,False,763,,,,,,,,,,Ethics,ethics,Ethics,7787.0,dataPHILOSOPHER,"@Forbes contributor. High-technology digital ethics - AI, Machine Learning, Robotics, Future of Work. Get my free 7-Techs eBook here: https://dataphilos.com",8195b288c8e6,dataphilosopher,118.0,118.0,20181104
0,,0.0,3a8144eabfe3,2017-11-03,2017-11-03 09:44:51,2017-11-08,2017-11-08 16:57:22,1,False,en,2017-11-13,2017-11-13 07:15:46,1,1323ef8ea355,6.573584905660378,3,0,0,Data may rule in a world where humans become products,5,"Will the coming era of AI be inhuman?
Data may rule in a world where humans become products
I was reading Stephen Hawking’s comment on how AI will eventually replace humans as the dominant species on Earth. Elon Musk has also been sounding off with dire warnings on a similar theme. It’s easy to laugh them off as scaremongers whose fears are highly unlikely to ever become reality.
But what if…
Here’s an interesting video that popped up on my social feed this morning. It was originally published in 2011, but is surprisingly close to reality now.
Credits: Pocket Films
The fact is we are living in an era of rapid change where it’s hard to predict the future. Or for that matter, the present. I mean I was under the impression that I’m caught up in this vicious cycle of consumerism. But lately, it seems that what I want or don’t want is not really the issue. All that matters is my data.
Take Google, for instance.
I have been a fan of the big G from the day it gave me the then humungous 2GB inbox. This while Yahoo was making a big fuss about its 100MB inbox or whatever it was in those pre-Gmail days. Since then, Google has become a constant in my life. Googling is my generic term for search. Google Maps is my whimsical navigator who sometimes gets me to wherever I want to go. Google Photos has a copy of every photo I have ever shot. Google Contacts saves my contacts online and updates them on the go. Google Music has around 5000 songs of mine. YouTube stores all my home videos, and is also my go-to destination for all kinds of videos. Then there’s Google Docs, Google Drive, Google Translate, Google Keep… to say nothing of the Google app itself, a nosey Parker who reads my emails, but once saved me from missing a flight by notifying me my flight was at 5 o’clock in the morning, while I was under the assumption that I was only due to fly at 5 o’clock in the evening.
If I had told anyone 20 years ago that I would get all these services for free, I would most certainly have been locked away in the nearest loony bin. Times have changed. These days, it’s I who wonder if I have been lunatically irresponsible to have given Google so much access to my personal info.
As we all know, there’s no such thing as a free lunch. Google has made billions by selling our info to marketers who love the way that Google allows them to precisely target customers who have been searching for their products.
In other words, the product that Google sells is me. Or rather, my personal info. This data is the price we pay for Google’s ‘free’ services. What is worrying me is whether the price is too high.
There are a few angles to this.
The first one is if our data is hacked or manipulated by an outsider. We have just witnessed this when Facebook sold its users’ personal info to the Russians. That was almost certainly responsible for the Americans being saddled with one of the biggest chumps of all times as their President. If Facebook’s systems can be hacked, why not Google? All I can do is hope Google allocates a few of its billions to stay ahead of the hackers.
My second worry is what this post is about. What if the system itself takes over management of the data. Will AI be sensitive to the emotional aspects of data with respect to the humans involved in an interaction with data?
In fact, I have had an interaction with Google which is quite revealing. It started a couple of years ago when Google Maps got the way to my home wrong, directing visitors to a blind alley behind my house. Which means if I call for an Uber cab, I have to circle around the block (around half a kilometre) to get to the cab. I’ve written to Google Maps several times and received acknowledgments. But the map remains uncorrected even though it’s now quite some time since I first complained to Google.
What this tells me is that Google is already in AI mode.
I’m guessing a machine was the one reading my mails. It must have weighed the consequences of not verifying the route to my residence versus the costs of of sending someone to do the job. Being a machine, it would probably have removed emotion from the equation, and arrived at the conclusion that the cost is not worth it for just one complainant. I mean why would a machine worry about a few Uber drivers, couriers, and house owners who are going nuts about a misdirected location in Google Maps.
In short, as of now AI does not seem concerned about the emotions of humans. It will operate solely on a pure cost-benefit angle.
The third angle is when the company that has our data is itself unscrupulous. FaceBook is a prime example of this. Unlike Google, I only use it occasionally like when I need to track down someone or follow some breaking story. But I almost never post anything there. Basically I wanted the advantages of Facebook without its disadvantages.
However Facebook was two steps ahead of me. I thought I was being clever when I changed my phone number without updating it in my Facebook profile. But Facebook managed to get my new number via a sneaky update of the Facebook owned WhatsApp. That update had a hidden feature that synced my phone with Facebook by default and even shared all my phone contacts with Facebook, which was something I had been deliberately avoiding for years.
Ever since, I have been wary of Facebook, and make sure the app is always signed out on my devices. But since WhatsApp is the default messaging service in India, I can’t really turn it off, which sort of leaves my back door open for Facebook to waltz in, and walk out with whatever it covets.
Fortunately, the governments in India and even Europe have become sensitive to these loopholes, and have tightened controls on data sharing without permission from users. However I’m not too optimistic about US government’s control of such data misuse. The incredible lax gun control (a weirdo with a history of violence just used a legally licensed gun to kill 26 people in a church in Texas) is an indicator of how big business calls the shots in the country.
Of course, I do have an option to avoid the services of companies that live and die by users’ data.
Apple is a good example of this. They claim to make money by selling products, not personal data, which is a not so subtle dig at the Googles and Facebooks. As Tim Cook put it, “We don’t build a profile based on your email content or web browsing habits to sell to advertisers. We don’t ‘monetize’ the information you store on your iPhone or in iCloud. And we don’t read your email or your messages to get information to market to you.”
It all sounds good in theory but the con is if Apple won’t sell our data, then it needs to make its money elsewhere. Apple’s solution is an exorbitant pricing strategy. The $1000 iPhone X is the result, and it’s hard for the common man in India to use the platform. That may explain why Apple has just around 3% share of the mobile market in India. (As an aside, 3% of the world’s second largest market for mobiles is still quite a lot of moolah with Apple expecting sales this year of $3 billion or ₹18,951 crore in India. This success is partly because Apple changed its strategy in India and sells older, low priced iPhones here. A new 32GB iPhone SE can be had for less than ₹19000 or around $300.)
While Google, Apple, Facebook and Microsoft have been trying to fighting capture customers by focussing on the mobile phone market, a fifth player has stolen a huge lead on the four tech titans by taking a different route.
Like Microsoft, Amazon may have lost the battle for phones as its Fire phone fizzled out after a big launch. But Amazon’s never-say-die leader, Jeff Bezos, believes Amazon can still win the war. His strategy was to avoid phones altogether and open a new battle front on voice search. He may be onto something as Amazon’s Echo range of speakers (driven by Amazon’s voice-powered personal assistant, Alexa) have completely taken over the AI speaker market. Siri, Google, Cortana, and all other early starters have been left far behind and are now trying hard to catch up. The exclamation mark for Amazon is Bezos was just crowned the richest man in the world.
As usual, it’s when you are not expecting anything that life tends to kick you in the teeth.
While I was sitting on the sidelines and admiring Amazon’s sagacity, my Amazon Prime Membership expired. A couple of hours later, without a warning of any sort, Amazon’s AI reached out into my iPad and deleted around four or five movies that I had downloaded. Meanwhile, I renewed my Prime Membership unaware of what had happened. When I found my iPad had been accessed and my files deleted without my permission, I got really upset, sent a stinker to Amazon, and cancelled my Prime membership.
Later on, I cooled down and realized that I was one of the lucky souls to get a preview of a future where AI will ride roughshod over human sensitivities.
For now, humans still rule the world.
Amazon’s rep called me up, apologized for the incident, and soothed my ruffled feathers by giving me a token Amazon Gift Voucher. The whole drama provoked my curiosity about AI and voice search, and I ended up ordering an Amazon Echo Dot (with a free Prime Membership bundled in) a few days ago.
After all, if AI is the future, I had better learn all I can about it.
",Will the coming era of AI be inhuman?,9,will-the-coming-era-of-ai-be-inhuman-1323ef8ea355,2018-04-22,2018-04-22 16:03:51,https://hackernoon.com/will-the-coming-era-of-ai-be-inhuman-1323ef8ea355,False,1689,how hackers start their afternoons.,hackernoon.com,hackernoon,,Hacker Noon,stories@amipublications.com,hacker-daily,"HACKATHONS,HACKING,PROGRAMMING,TECH,HACKER",hackernoon,Ethics,ethics,Ethics,7787.0,babulous,it’s an odd world,536476373c93,babulous,622.0,14.0,20181104
0,,0.0,,2017-09-23,2017-09-23 16:35:22,2017-09-23,2017-09-23 16:37:08,0,False,en,2017-09-23,2017-09-23 16:37:08,0,dc6d7c9242d9,0.9886792452830188,0,0,0,"“Under some conditions, yes”, is what a study found, and it has gained some controversy around it.",3,"Can AI detect if you’re gay, based on your face?
“Under some conditions, yes”, is what a study found, and it has gained some controversy around it.
Here’s the study, in case you want to check it out for yourself: https://psyarxiv.com/hv28a/
The conditions are:
you have got to be white
you have got to be an American person
you have to be extroverted enough to publish your information on a dating website
you have to be between 18 and 40 years old
you have to identify yourself as either male or female
you have to identify yourself as straight or gay/lesbian (no spectrum)
Given this was the case, the system would be able to pick up only from your photos if you were heterosexual or homosexual about 74% of the time.
Regarding the controversy, it goes like this:
people claim this is not a good representation of who all people are
people claim it is not a representation of the LGBT community at all
people claim it’s dangerous to prove that something like this could even be done when there are countries where you’d be killed for being gay
The researchers did address those concerns. The first two are easy to dismiss: “yes, the study has limitations” is what they said, and that’s understandable. Studies go baby steps into making assertions, so that it can slowly be generalized.
On the last one, it’s a bigger question. Are we (humanity) ready for this knowledge?
Here’s a nice summary of the debate at Mashable: http://mashable.com/2017/09/11/artificial-intelligence-ai-lgbtq-gay-straight/
What do you think?
","Can AI detect if you’re gay, based on your face?",0,can-ai-detect-if-youre-gay-based-on-your-face-dc6d7c9242d9,2017-12-28,2017-12-28 17:05:20,https://medium.com/s/story/can-ai-detect-if-youre-gay-based-on-your-face-dc6d7c9242d9,False,262,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Alpha,"Master of the unknown: ask me anything, I probably don’t know it",ba4343538068,SkunksAndStuff,33.0,46.0,20181104
0,,0.0,,2018-09-19,2018-09-19 09:07:02,2018-09-19,2018-09-19 09:07:01,1,False,en,2018-09-19,2018-09-19 12:54:02,15,2816d779d4a0,6.516981132075473,3,0,0,,5,"‘Unboxing’ at Behavior Design Amsterdam #16
Below is a write-up of the talk I gave at the Behavior Design Amsterdam #16 meetup on Thursday, February 15, 2018.
‘Pandora’ by John William Waterhouse (1896)
I’d like to talk about the future of our design practice and what I think we should focus our attention on. It is all related to this idea of complexity and opening up black boxes. We’re going to take the scenic route, though. So bear with me.
Software Design
Two years ago I spent about half a year in Singapore.
While there I worked as product strategist and designer at a startup called ARTO, an art recommendation service. It shows you a random sample of artworks, you tell it which ones you like, and it will then start recommending pieces it thinks you like. In case you were wondering: yes, swiping left and right was involved.
We had this interesting problem of ingesting art from many different sources (mostly online galleries) with metadata of wildly varying levels of quality. So, using metadata to figure out which art to show was a bit of a non-starter. It should come as no surprise then, that we started looking into machine learning — image processing in particular.
And so I found myself working with my engineering colleagues on an art recommendation stream which was driven at least in part by machine learning. And I quickly realised we had a problem. In terms of how we worked together on this part of the product, it felt like we had taken a bunch of steps back in time. Back to a way of collaborating that was less integrated and less responsive.
That’s because we have all these nice tools and techniques for designing traditional software products. But software is deterministic. Machine learning is fundamentally different in nature: it is probabilistic.
It was hard for me to take the lead in the design of this part of the product for two reasons. First of all, it was challenging to get a first-hand feel of the machine learning feature before it was implemented.
And second of all, it was hard for me to communicate or visualise the intended behaviour of the machine learning feature to the rest of the team.
So when I came back to the Netherlands I decided to dig into this problem of design for machine learning. Turns out I opened up quite the can of worms for myself. But that’s okay.
There are two reasons I care about this:
The first is that I think we need more design-led innovation in the machine learning space. At the moment it is engineering-dominated, which doesn’t necessarily lead to useful outcomes. But if you want to take the lead in the design of machine learning applications, you need a firm handle on the nature of the technology.
The second reason why I think we need to educate ourselves as designers on the nature of machine learning is that we need to take responsibility for the impact the technology has on the lives of people. There is a lot of talk about ethics in the design industry at the moment. Which I consider a positive sign. But I also see a reluctance to really grapple with what ethics is and what the relationship between technology and society is. We seem to want easy answers, which is understandable because we are all very busy people. But having spent some time digging into this stuff myself I am here to tell you: There are no easy answers. That isn’t a bug, it’s a feature. And we should embrace it.
Machine Learning
At the end of 2016 I attended ThingsCon here in Amsterdam and I was introduced by Ianus Keller to TU Delft PhD researcher Péter Kun. It turns out we were both interested in machine learning. So with encouragement from Ianus we decided to put together a workshop that would enable industrial design master students to tangle with it in a hands-on manner.
About a year later now, this has grown into a thing we call Prototyping the Useless Butler. During the workshop, you use machine learning algorithms to train a model that takes inputs from a network-connected arduino’s sensors and drives that same arduino’s actuators. In effect, you can create interactive behaviour without writing a single line of code. And you get a first hand feel for how common applications of machine learning work. Things like regression, classification and dynamic time warping.
The thing that makes this workshop tick is an open source software application called Wekinator. Which was created by Rebecca Fiebrink. It was originally aimed at performing artists so that they could build interactive instruments without writing code. But it takes inputs from anything and sends outputs to anything. So we appropriated it towards our own ends.
You can find everything related to Useless Butler on this GitHub repo.
The thinking behind this workshop is that for us designers to be able to think creatively about applications of machine learning, we need a granular understanding of the nature of the technology. The thing with designers is, we can’t really learn about such things from books. A lot of design knowledge is tacit, it emerges from our physical engagement with the world. This is why things like sketching and prototyping are such essential parts of our way of working. And so with useless butler we aim to create an environment in which you as a designer can gain tacit knowledge about the workings of machine learning.
Simply put, for a lot of us, machine learning is a black box. With Useless Butler, we open the black box a bit and let you peer inside. This should improve the odds of design-led innovation happening in the machine learning space. And it should also help with ethics. But it’s definitely not enough. Knowledge about the technology isn’t the only issue here. There are more black boxes to open.
Values
Which brings me back to that other black box: ethics. Like I already mentioned there is a lot of talk in the tech industry about how we should “be more ethical”. But things are often reduced to this notion that designers should do no harm. As if ethics is a problem to be fixed in stead of a thing to be practiced.
So I started to talk about this to people I know in academia and more than once this thing called Value Sensitive Design was mentioned. It should be no surprise to anyone that scholars have been chewing on this stuff for quite a while. One of the earliest references I came across, an essay by Batya Friedman in Interactions is from 1996! This is a lesson to all of us I think. Pay more attention to what the academics are talking about.
So, at the end of last year I dove into this topic. Our host Iskander Smit, Rob Maijers and myself coordinate a grassroots community for tech workers called Tech Solidarity NL. We want to build technology that serves the needs of the many, not the few. Value Sensitive Design seemed like a good thing to dig into and so we did.
I’m not going to dive into the details here. There’s a report on the Tech Solidarity NL website if you’re interested. But I will highlight a few things that value sensitive design asks us to consider that I think help us unpack what it means to practice ethical design.
First of all, values. Here’s how it is commonly defined in the literature:
“A value refers to what a person or group of people consider important in life.”
I like it because it’s common sense, right? But it also makes clear that there can never be one monolithic definition of what ‘good’ is in all cases. As we designers like to say: “it depends” and when it comes to values things are no different.
“Person or group” implies there can be various stakeholders. Value sensitive design distinguishes between direct and indirect stakeholders. The former have direct contact with the technology, the latter don’t but are affected by it nonetheless. Value sensitive design means taking both into account. So this blows up the conventional notion of a single user to design for.
Various stakeholder groups can have competing values and so to design for them means to arrive at some sort of trade-off between values. This is a crucial point. There is no such thing as a perfect or objectively best solution to ethical conundrums. Not in the design of technology and not anywhere else.
Value sensitive design encourages you to map stakeholders and their values. These will be different for every design project. Another approach is to use lists like the one pictured here as an analytical tool to think about how a design impacts various values.
Furthermore, during your design process you might not only think about the short-term impact of a technology, but also think about how it will affect things in the long run.
And similarly, you might think about the effects of a technology not only when a few people are using it, but also when it becomes wildly successful and everybody uses it.
There are tools out there that can help you think through these things. But so far much of the work in this area is happening on the academic side. I think there is an opportunity for us to create tools and case studies that will help us educate ourselves on this stuff.
There’s a lot more to say on this but I’m going to stop here. The point is, as with the nature of the technologies we work with, it helps to dig deeper into the nature of the relationship between technology and society. Yes, it complicates things. But that is exactly the point.
Privileging simple and scalable solutions over those adapted to local needs is socially, economically and ecologically unsustainable. So I hope you will join me in embracing complexity.
Originally published at Leapfroglog.
",‘Unboxing’ at Behavior Design Amsterdam #16,5,unboxing-at-behavior-design-amsterdam-16-2816d779d4a0,2018-09-21,2018-09-21 08:47:27,https://medium.com/s/story/unboxing-at-behavior-design-amsterdam-16-2816d779d4a0,False,1674,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Kars Alfrink,"Designer, researcher and educator focused on emerging technologies, social progress and the built environment.",22560e249725,kaeru,777.0,394.0,20181104
0,,0.0,,2017-09-27,2017-09-27 18:52:52,2017-09-27,2017-09-27 19:36:32,3,False,en,2017-09-27,2017-09-27 19:36:32,9,2fb796a7884e,5.6330188679245285,0,0,0,We’ve seen tremendous advances in science and technology. Now we need to debate their ethics.,5,"A New Politics of Innovation
We’ve seen tremendous advances in science and technology. Now we need to debate their ethics.
(via Pixabay)
In our modern, dynamic world, there are always new challenges coming our way. I’ve been exploring some of the issues that will shape the future of our politics. Now I turn to upcoming debates about the ethics of science and technology.
Gene Editing
The past century has been one of the most productive periods in medicine. Our ability to prevent and treat disease has drastically improved, and life expectancy has significantly risen. We are learning more powerful ways of manipulating biology to learn about the body and to develop new treatments. But with power comes responsibility.
Medicine has always faced difficult ethical dilemmas. In most cases, they have to do with trade-offs between the benefits and the risks of a certain treatment. For example, doctors have been able to keep patients alive on ventilators and bypass machines, sometimes for long periods of time. But at what cost? When does the suffering outweigh any benefits? This kind of dilemma is difficult, but is normally handled by doctors and their patients on a case-by-case basis. It won’t necessarily invite any big public debates or policy changes.
But now there is a new kind of dilemma that should get public attention: the ethics of gene editing. Due to developments like CRISPR, lab researchers can accurately edit the genomes of many organisms, including humans, and they can do so at very low cost. Not too long ago, the technique was used for the first time in the US on a human embryo. Many researchers dream of curing genetic diseases simply by editing them out. This is extremely exciting for science, but it’s uncomfortable for the public. Is gene editing ethical? Are we allowed to modify our children’s genes in order to give them a better life? Are we allowed to edit them at will, or only for certain reasons? If there is some threshold, where should it be?
The answers we choose could have enormous implications for our future. Gene editing could become a life-saving treatment. It could be a new way to control our children’s traits. The most extreme case might resemble the test-tube babies of Brave New World.
Since it’s so impactful, the issue should be discussed in public. It could be liberals who advocate for greater use of gene editing, and conservatives who argue against it on ethical grounds. Perhaps, before entering mainstream politics, the issue will spark large movements (remember GMOs?). Right now, there isn’t much discussion and there certainly isn’t an answer. But we should start discussing this as soon as possible.
Internet Privacy
Perhaps the biggest invention in recent times has been the internet. We can now communicate nearly instantly, covering enormous distances and reaching great numbers of people at once. Huge amounts of information circulate. But a lot of the information is private, posing a big privacy and security challenge. The public is starting to worry that their data can be shared too easily, whether by tech companies, the government, or malicious attackers.
(via Pixabay)
The problem is worsening, but we have yet to make real progress. Consumers of internet products often ignore the fact that their behavior will be monitored. Perhaps this is because we don’t know when we’re being monitored, or who or what is watching. But we have seen multiple prominent examples of sensitive information being taken — Snowden’s NSA documents and the activities they detailed, Russia’s cyber-attacks during the election, and most recently, the Equifax breach — and yet there isn’t much being done to respond.
Internet privacy is hard because the internet is not like real life. In real life, we aren’t concerned about companies keeping track of their clients and recognizing us when we come in the door. When they do, those companies have no need to share information with anyone else. And these cases are already the exceptions. When we walk around in public, strangers normally have no idea who we are. If anyone learns substantial information about us without asking, it’s because they’re stalking us, which is illegal. In contrast, everyone on the internet is identifiable through their IP address (except when using Tor or a similar network). We are walking ID numbers; all anyone has to do is keep track. Since being observant was never a crime before, it’s hard to call it one now.
This is only the tip of the iceberg. In a lawless place like the internet, how do we enforce the rules? Should the government be trusted with wide powers of surveillance, hopefully to protect our national security, or should it be subject to the same laws as the rest of us? Is surveillance a constitutional or legal issue, or just an ethical one?
We need to start talking about this. One way to bring it up is to focus on the national security issue, especially given recent relations with Russia. It’s one thing for a company to compile data, and quite another for foreign officials or criminals to steal it. Do we really want all our internet services collecting huge volumes of user data, ready to be stolen by anyone with the right set of skills? Maybe this threat will get discussion going. Either way, the issue can’t stay in the dark for long.
Artificial Intelligence
If internet has been our biggest invention so far, it may soon be eclipsed by AI. And once again, technological progress will lead to unprecedented ethical issues. In this case, they will be about power: as AI learns increasingly impressive skills, we will need to decide who should wield its powers — and how much control to cede to the technology itself.
(via Pxhere)
Like the others, this issue is tougher because of its uncertainty. Most people agree that AI is vastly more powerful than previous technologies, and may even outdo the human brain. But that’s all we can agree on. Researchers argue about when AI might surpass human intelligence, how it will do so, and whether such a change is beneficial, or devastating, or somewhere in between. We seem to be a long way from the existence of superhuman AI, but plenty of people are already panicking at the thought of it.
It isn’t surprising that so many companies are working towards superhuman AI. It’s in line with our normal approach to technology, which is to design it to help as much as it can. If there is a faster way, we make it. If there is an easier way, we make it. Naturally, once there is a way that’s even better than a human, our instinct is to make it. And unless we reach a consensus that it’s a bad idea, which we haven’t, it will probably be allowed to happen.
But there are real reasons to regulate or even avoid superhuman AI. If someone successfully created one, they would likely apply it to complex decision making, maybe in a field like medicine. (This is the apparent goal of some of IBM’s Watson technologies). But medical decisions carry moral weight. If the AI were superhuman, would it become so advanced that it couldn’t explain its decisions to a human? Would it be compatible with human ethics at all? It isn’t responsible to create such a technology unless it could justify its decisions in human terms — and at that point, the technology may not perform much better than a human.
Real discussions about this are a long way off, but we can begin by clarifying where we’re going. It’s useful to think through the hypotheticals and consider possible consequences, as many people are doing. But what’s more urgently important is to settle on a common goal. What is it that we want from AI — and what kind of AI can really offer it?
This is part of a series on upcoming issues in politics. Read my comments on climate change and the national debt here.
Thanks for reading! If you enjoyed this, follow me on Medium and on Twitter.
",A New Politics of Innovation,0,a-new-politics-of-innovation-2fb796a7884e,2018-01-11,2018-01-11 08:22:34,https://medium.com/s/story/a-new-politics-of-innovation-2fb796a7884e,False,1347,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Marshall Green,"Interested in medicine, technology, politics, and (nearly) everything else.",2a73807095ab,mgreen5504,8.0,42.0,20181104
0,,0.0,,2017-12-11,2017-12-11 20:56:45,2018-01-11,2018-01-11 21:07:34,2,False,en,2018-01-11,2018-01-11 21:07:34,6,91fbfcff215,12.485220125786164,2,0,0,The ethics of using natural language processing to understand police fatalities.,5,"Sontag and Data Science
The ethics of using natural language processing to understand police fatalities.
I was excited to do some work this semester in the Statistical Social Language Analysis Lab at UMass. Professor Brendan O’Connor, the lab director, had asked if I was interested in helping create a website to display the data the lab had been collecting on when police kill people in the United States. They had written a paper describing their system (which I recommend for the technical details) and now they wanted to see how it could be of use to the Fatal Encounters project, which is:
A step towards creating an impartial, comprehensive, and searchable national database of people killed during interactions with law enforcement.
The lab’s process involved looking at news articles found through Google News under certain queries (like “police killing”) and pulling out the names of people who were killed by the police from them. My task was to create a user interface to display these, so that Brian Burghart, the creator and primary maintainer of Fatal Encounters could add the names to his database.
I came across some interesting technical questions in the process, like how to make building an interactive data exploration tool in the Python ecosystem easier. However, those will have to wait for another day, because I want to discuss some of the more theoretical and social implications of the work here.
This is a screenshot of the system, showing the top names the system found from the last week of Google News reports.
I came into the semester with a bunch of questions surrounding the non-technical aspects of my work that I made less headway on, such as:
How do we (the lab) find good collaborators to motivate the systems we make?
Can our work be used directly to make the criminal justice system more just?
Who should be driving what we work on?
And through the semester I developed some more questions.
Do the deceased have rights to the data of their death? Who can give us consent to use their names?
Does this work belong in a library?
Under what group can this work continue to be supported?
I put these questions on the backburner, until I came across Susan Sontag’s “Regarding the Pain of Others.” I was assigned it for my German film class, taught by Professor Barton Byg, and immediately recognized the ethical parallels between war photography and our data science research on police violence. Her insights are wide ranging and I will only touch on a few here, so I highly recommend you read the book yourself.
Let me back up first and try to situate the lab’s work in the broader context of criminal justice reform. I made a sort of flow chart of data and action, to help understand the different areas of the process:

It starts with a police officer killing someone in the United States. This is witnessed and recorded, likely by a bystander in the community or the officer themselves. A reporter talks to those people and writes a news article on the event and it gets published online. Then Google News must index it, so that it can appear in their service. Now the lab’s system queries Google News (something like “police killing”) and downloads the listed articles. Before the system can extract names from articles, it must first learn some statistical model that is able to predict which sentences contain a reference to a police killing. To do this learning, it combines a bunch of historical articles with the existing Fatal Encounters list of people killed by police to create this model that can extract likely names from new articles. Then, when the system is done learning, it can extract the names from any new articles it queries. For the website I built, these names, combined with a user’s filters, determines what is displayed in their browser. Brian Burghart then takes some of those names and adds them to the Fatal Encounters list. Another site could be made with the same names, that presents them in some way to impact an interested person. Then, the hope would be that this creates some political action, which can affect police departments, and so reduce future killings.
What becomes obvious from this diagram, is that the publishable work on the system (the “SLANG Lab” box) cannot be understood in isolation. If it is to have any positive effect on criminal justice reform, which I assume is an unstated motivation for the researchers, then we must wrestle with a host of questions that are almost entirely unrelated to accuracy rates and learning models.
Using some quotes from Sontag, I will start at the top of the diagram and work myself down, questioning our assumptions and opening up avenues of discussion. This will lead to an analysis of different next steps for the project.
Why do we capture killings, instead of some other metric of police-citizen interaction, like traffic tickets or use of force incidents? This is partially due to the similar focus on photographs of the moment of death itself.
pictures taken by photographers out in the field of the moment of (or just before) death are among the most celebrated and often reproduced of war photographs (59).
There is the moral weight and finality of taking a life that makes it compelling to study. However, this reasoning also obscures the more mundane reason why we study police killings. They are easy to count. There is a more universally accepted understanding of when a police officer kills someone than when they harass someone. Physically death has an unambiguous nature to it and the state is already determined to record it, to keep track of its population. Fatal Encounters will often draw from official coroner’s records to substantiate their records. While police killings are important metrics they obviously cannot tell the complete story of the relationship between the police and their community. We should be clear when approaching an issue to ask, “Are we analyzing this data because of its importance in the problem domain or because its easiest to get? What is hidden if we look at this in isolation.”
Moving down from the killing event to its analysis, we see that the participating parties shrinks. It starts out geographically distributed and ends up in one lab with a small group of people. Google News is just one organization and algorithm as well, and it finally comes out through Fatal Encounters, currently the work of just one person. After that, it hopefully pans out again to a variety of interested stakeholders, who will take action. I see it as an hourglass, centralized in the middle and disbursed at the top and bottom. Much has been made of the distance introduced by a photo from the viewer to the subject. Such a distance is even farther increased here, where there are many intermediaries and abstractions between an event and it’s witnessing. I think this is the cause of much of my own discomfort in doing this work. What right do I have to use these people’s deaths? Sontag makes an interesting point comparing the journalistic photos of the U.S. to those of far-away places “When subjects are closer to home, the photographer is expected to be more discrete” (62). For example, war photographers are more likely to cover faces of US soldiers than enemy combatants. My own community in Amherst, MA is similarly not affected by police killings and through this centralizing process I am surveying communities far from me that I have little personal attachment to. The audience of these tools also isn’t the affected communities, but others in academia and those who end up using Fatal Encounters data. While the purported goal may be to reduce police violence, the work itself is rather disinterested in the agency of the affected communities, similar to Sontag’s critique of foreign war photography (emphasis mine):
The exhibition in photographs of cruelties inflicted on those with darker complexions in exotic countries continues this offering, oblivious to the considerations that deter such displays of our victims of violence; for the other, even when not an enemy, is regarded only as someone to be seen, not someone (like us) who also sees (72).
She points out the a Taliban soldier who is begging for his life on the cover of the New York Times isn’t there for his mother who has just had her son slaughtered, but for a far away audience in the US, to let us know that these types of things happen there and not here. Similarly, the families of the deceased don’t need us to extract out their loved ones name from a news article so that they know they have died.
It is easy for me to critique my own work on this, just like it is easy to fault those who photograph the pain of others.
As important now believe images of war to be, this does not dispel the suspicion that lingers about the interests of those who produce then. […]
Citizens of modernity, consumers of violence as spectacle, adepts of proximity without risk, are schooled to be cynical about the possibility of sincerity (111).
Sontag argues that this cynicism is from a lack of constructive action. When you cannot do something yourself to address an issue, you want to believe that it is impossible to address, so you don’t have to feel guilt about your innaction. Of course there will be valid critiques of this work, that is what makes it worth pursuing. They cannot be addressed by avoiding trying things out, only by experimenting and learning.
It is felt that there is something morally wrong with the abstract reality provided by photography […] the standing back from the aggressiveness of the world which frees us for observation and for elective attention. But this is only to describe the function of the mind itself.
There’s nothing wrong with standing back and thinking. To paraphrase several sages: “Nobody can think and hit someone at the same time.” (118)
Abstraction is not innately wrong it just needs to recognized as a political and data is simply a powerful abstraction. A fundamental property of data is being able to count and counting means that there are multiple of the same thing. By necessity this means ignoring what makes those things different and stepping away from the things themselves to describe a set of them. Without looking at how these translations between reality and data function, we lose the context and intended meaning.
So how can we improve these translations? Let’s consider the person who might look at a hypothetical public facing website that showcases the data we produce from the system. For them to simply understand the data is obviously not enough. They need to react in some way, for it be effective in causing any change. I had assumed that something that produced some emotional response would be most effective. However, Sontag articulates how relying just on empathy can actually be counterproductive.
Making suffering loom larger, by globalizing it, may spur people to feel they ought to “care” more. It also invites them to feel that the suffering and misfortunes are too vast, too irrevocable, too epic, to be much changed by any local political intervention (79).

Telling people to look and see and be moved can actually cause apathy, because they can feel helpless. Why would you want to feel remote suffering and feel guilt about it, if you have no response? As Sontag elegantly articulates, “Compassion is an unstable emotion. It needs to be translated into action, or it withers” (101). So let’s give them action! We can link to a site fighting for criminal justice reform or the ways in which they can influence their local politics. Simple enough.
However, just attempting to influence the viewers behavior in some predetermined way is not only limited but ineffective. Sontag similarly contextualizes what an image can do for us.
Neither is the photography supposed to repair our ignorance about the history and causes of the suffering it picks out and frames. Such images cannot be more than an invitation to pay attention, to reflect, to learn, to examine the rationalizations for mass suffering offered by established power. What caused what the picture shows? Who is responsible? Is it excusable? Was it inevitable? Is there some state of affairs which we have accepted up to now that ought to be challenged? All this, with the understanding that moral indignation, like compassion, cannot dictate a course of action (117).
We shouldn’t have the hubris to think our images can sit alone and provide solutions. They can only be a door to a conversation. So we must make it easy enough to open it. If we go back to the flow diagram, we see that data part of the process is set up and controlled exclusively by the lab. It’s process is engineered and mostly opaque. Instead of seeing the audience as just receiving information dutifully and acting on it, what if we give them the power to understand and govern as much of the process as possible? One of the main things holding people back is lack of technical expertise. To really extend the system we have built, you have to read a math heavy paper and understand software engineering. However, it doesn’t have to be this way. The pieces themselves are not so mysterious that they couldn’t be understood at least as black boxes, simple components that take something in and produce something out. This could be done on many levels. One way would be build an easy to use generic system for extracting entities from news articles. A user would enter the “ground truth” data (in this case, that was a list of people killed by the police) and a bunch of queries they think would find more examples of that data (like “police killing”). Then they would be able to start seeing what they system produced as best guesses for new examples of this data. Also, it would be helpfulpful then to have some more information about how the system itself thinks it is performing and some knobs to tweak. Then, the user would be able to get a better sense of the imperfections of the system and how it is working.
Not only would this be engaging, but it would serve to verify the truth of the data itself. Sontag considers the difference between how we see the truth of a photograph and a drawing.
A painting or drawing is judged a fake when it turns out not to be by the artist to whom it had been attributed. A photograph — or a filmed document available on television or the internet — is judged a fake when it turns out to be deceiving the viewer about the scene it purports to depict (46).
A photograph is fake not only if it photoshopped, but if it says it depicts one things but really depicts another. For example, a war photographer depicting one conflict as another is fraudulent. When do we consider data to be truthful? If the process used to create the data is being honestly represented. So without an understanding of this it is impossible to understand it’s integrity. The viewer needs to be able to engage with with this content because, “a more reflective engagement with content would require a certain intensity of awareness” (106). In order for us to allow any awareness we need to make our process more understandable.
In a way, I am suggesting we should just make the tools and let others use them as they see fit. This has the advantage of offloading some of the moral questions to another party. In effect, allowing for interdisciplinary collaboration without intentional relationship building. It is tempting to argue that maybe we (computer/data scientists) shouldn’t be in the business of telling any particular story at all, just enabling others to do so.
During the Manhattan Project (R&D to produce atomic bombs during WWII in the US), a group of scientists circulated a petition urging some restraints in using the weapons developed. It was not received well:
In the spring of 1945, Szilard took the petition to the man who was soon to be named Secretary of State, James F. Byrnes, hoping to find someone who would pass on to President Truman the message from scientists that the bomb should not be used on a civilian population in Japan, and that after the war it should be put under international control in order to avoid a post-war arms race. Byrnes was not sympathetic to the idea at all. Szilard regretted that such a man was so influential in politics, and he appeared to also be despondent at having become a physicist, because in his career he had contributed to the creation of the bomb. After the meeting with Byrnes, he is quoted as having said, “How much better off the world might be had I been born in America and become influential in American politics, and had Byrnes been born in Hungary and studied physics.” In reaction to the petition, General Leslie Groves, the director of the Manhattan Project, sought evidence of unlawful behavior against Szilard. Most of the signers lost their jobs in weapons work (“Szilárd petition” on Wikipedia).
To divorce ourselves from politics is to be willfully ignorant of the consequences of our work. It is the same argument that Leni Riefenstahl, the infamous Nazi film producer, used to defend herself against accusations of complicity in the Nazi regime. She didn’t concern herself with the politics.

Even “picking a side,” is not enough. This can’t be about virtue signalling of researchers (myself included), creating work that just sympathizes with the oppressed. Sontag illuminates how this can really hide our own participation in injustice:
The imaginary proximity to the suffering inflicted on others that is granted by images suggests a link between the faraway sufferers — seen close-up on the television screen — and the privileged viewer that is simply untrue, that is yet one more mystification of our real relations to power. So far as we feel sympathy, we feel we are not accomplices to what caused the suffering. Our sympathy proclaims our innocence as well as our impotence. To that extent, it can be (for all our good intentions) an impertinent — if not inappropriate — response. To set aside sympathy we extend to others beset by war and murderous politics for a reflection on how privileges are located on the same map as their suffering, and may — in ways we might prefer not to imagine — be linked to their suffering, as the wealth of some may imply the destitution of others, is a task for which painful, stirring images supply only an initial spark (102–103).
So one of my initial ideas, to create a site that includes the sentences we extracted in a visual way to provoke people to feel sympathy and understand the magnitude of the problem, is fundamentally flawed because it assumes empathy to those killed by the police is beneficial. In fact, it can give us any easy out to feel like we have addressed the problem, but we continue thinking about it as someone else’s problem.
",Sontag and Data Science,4,sontag-and-data-science-91fbfcff215,2018-04-14,2018-04-14 01:06:07,https://medium.com/s/story/sontag-and-data-science-91fbfcff215,False,3207,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Saul Shanabrook,www.saulshanabrook.com,707dc1699df5,saulshanabrook,102.0,310.0,20181104
0,,0.0,250fad68a3bb,2017-10-26,2017-10-26 10:05:24,2017-10-26,2017-10-26 12:50:54,1,False,en,2017-10-27,2017-10-27 21:55:22,15,60bd9ff005ae,3.177358490566038,3,0,0,"How how does our treatment of artificial persons shape our character, relationships, and human understanding?",5,"Relating to Artificial Persons in Everyday Life
How how does our treatment of artificial persons shape our character, relationships, and human understanding?
by J. Nathan Matias, Lydia Manikonda, Scott Hale, and Kenneth Arnold
This post is the third in a series of short introductions to artificial intelligence designed for group discussion in non-technical Christian settings. To follow the series, sign up for our email list, hosted by the Oxford Pastorate.
Whoever is righteous has regard for the life of his beast, but the mercy of the wicked is cruel. Proverbs 12:10
When Proverbs defines righteousness to include the way we treat animals, it seems to support a virtue-based view of our relationships with other beings. In this view, grounded in Aristotle and Aquinas, our moral lives are defined by character and virtue as much as actions or their consequences. When we act cruelly toward an animal, we should worry about the effect on our own character, not just the harm we cause. As artificial persons become part of everyday life, Christians will need to think about the effects of our relationships with those persons.
hitchBot hitchhiked across Canada and Europe before being destroyed by vandals in the US in 2015.
Parents who use Apple’s Siri or who have installed Amazon’s Alexa are already struggling with what to teach their children about relating to intelligent personal assistants (IPAs). These systems typically keep a microphone running and listen for a person to call their name and make a request. IPAs will then access their organization’s resources and try to answer the request. Responding with a human-like voice, IPAs make phone calls, schedule meetings, answer questions, and order products. Since some IPAs are confused by polite language, children can learn to speak impolitely to their IPAs. The parents of those children have discovered that whether or not the IPA is conscious or can feel slighted by human treatment, the way we treat the beings in our lives can shape our own character in other situations.
The way we treat the beings in our lives can shape our own character in other situations
IPAs offer powerful, meaningful opportunities to broaden access to complex technologies. Humans have an incredible ability to interact with other persons at levels of emotional flexibility and intellectual complexity that many of us would struggle to achieve with symbols or abstract concepts. It’s no surprise that God has chosen to relate to us as a person, since relationships are among the most nuanced, widespread interactions we share. Yet when we relate to artificial persons, we should ask who is on the other end: today’s IPAs tend to be run by corporations who stage-manage them to reduce labor costs and influence our decisions.
Artificial persons have been a human obsession for millennia and many people have hoped that we might be able to treat these persons with greater moral license than humans. In the 1930s, the Westinghouse Corporation created “Rastus Robot,” a black-skinned artificial person who they implied might someday carry out the work of former human slaves. IPAs have also long been the subject of fantasies about power and sex; the male-dominated computing industry continues to create predominantly female artificial assistants. Yet despite fears about computer systems causing humans to turn away from human contact, the available evidence suggests that use of social technologies also deepens our relationships with other people.
How might relationships with IPAs deepen our worship and our human understanding?
How should Christians think about the morality of the ways we treat artificial persons?
Can Christianity offer ways to think about creating beings in our own image that include relationships other than servitude?
References
Darling, K., Nandy, P., & Breazeal, C. (2015, August). Empathic concern and the effect of stories in human-robot interaction. In Robot and Human Interactive Communication (RO-MAN), 2015 24th IEEE International Symposium on (pp. 770–775). IEEE. from http://ieeexplore.ieee.org/document/7333675/
Hefernan, T. (2017, May 13). Cyborg Futures: Born in Fiction. Retrieved July 11, 2017, from https://socialrobotfutures.com/2017/05/13/cyborg-futures-workshop-summary/
Hursthouse, R., & Pettigrove, G. (2016). Virtue Ethics. In E. N. Zalta (Ed.), The Stanford Encyclopedia of Philosophy (Winter 2016). Metaphysics Research Lab, Stanford University. Retrieved from https://plato.stanford.edu/archives/win2016/entries/ethics-virtue/
Myers, K., Berry, P., Blythe, J., Conley, K., Gervasio, M., McGuinness, D. L., … Tambe, M. (2007). An intelligent personal assistant for task and time management. AI Magazine, 28(2), 47. from https://www.aaai.org/ojs/index.php/aimagazine/article/view/2039
Oppenheimer, M. (2014, January 17). Technology Is Not Driving Us Apart After All. The New York Times. Retrieved from https://www.nytimes.com/2014/01/19/magazine/technology-is-not-driving-us-apart-after-all.html
Truong, A. (2016, June 9). Parents are worried the Amazon Echo is conditioning their kids to be rude. Quartz. Retrieved from https://qz.com/701521/parents-are-worried-the-amazon-echo-is-conditioning-their-kids-to-be-rude/
Turkle, S. (2005). The second self: Computers and the Human Spirit. MIT Pres
",Relating to Artificial Persons in Everyday Life,3,relating-to-artificial-persons-in-everyday-life-60bd9ff005ae,2018-06-07,2018-06-07 17:20:20,https://medium.com/s/story/relating-to-artificial-persons-in-everyday-life-60bd9ff005ae,False,789,Artifcial Intelligence in Christian Thought & Practice,,,,AI and Christianity,natematias@gmail.com,ai-and-christianity,"RELIGION,ARTIFICIAL INTELLIGENCE,CHRISTIANITY,TECHNOLOGY,SOCIAL SCIENCE",natematias,Ethics,ethics,Ethics,7787.0,J. Nathan Matias,"Public-interest research for a fairer, safer, understanding Internet. CivilServant, @PsychPrinceton @PrincetonCITP @medialab Prev @BKCharvard @CivicMIT",61f90df70e11,natematias,2757.0,1478.0,20181104
0,,0.0,32881626c9c9,2018-06-01,2018-06-01 09:38:27,2018-06-01,2018-06-01 09:31:34,4,False,en,2018-06-01,2018-06-01 09:53:00,2,630c505c8c5b,4.571698113207547,0,0,0,"As with any technology, AI’s potential to help or harm us depends on how it is applied and overseen. Ethical AI is garnering much interest…",5,"Should A.I. be Ethical at All?

As with any technology, AI’s potential to help or harm us depends on how it is applied and overseen. Ethical AI is garnering much interest, but it’s not always clear what this refers to. A broad range of emerging issues have been identified as requiring ethical frameworks or principles in order to steer the development of AI in a socially beneficial manner, including:
AI safety: Ensuring that autonomous systems do not behave in ways that inadvertently harm society. • Malicious uses of AI: Guarding against the misuse of AI by malicious actors.
Data ownership and protection: Overseeing the use of personal data for AI systems.
Algorithmic accountability: Clarifying governance and responsibilities for the use of algorithms, such as in the case of automated decision systems.
Socio-economic impact: Managing social and economic repercussions of AI, such as increased inequality of wealth and power.
To build on these ideas, a working definition of ethical AI should be established: AI that is designed and implemented based on the public’s values, as articulated through a deliberative and inclusive dialogue between experts and citizens.
This definition should capture a number of necessary elements to achieve deployment of AI technology in a manner that is beneficial to society over the long-term, has moral and political legitimacy, and hence is grounded in widespread popular consent. These are:
in both design and implementation, AI is guided by values above short term profit;
values should be based on our best understanding of society’s values; and,
the most effective methods for building a shared and considered set of societal values bring together citizens in deliberative and inclusive dialogue with subject experts, such as technologists and philosophers.

One application that demonstrates this double-edged potential is the use of AI in automated decision systems. Automated decision systems refer to the computer systems that either inform or make a decision on a course of action to pursue about an individual or business. It is important to examine the use of automated decision systems in the broader social and economic context, considering behavioural insights, cultural norms, institutional structures and governance, economic incentives and other contextual factors that have a bearing on how an automated decision system might be used in practice.
At present, these systems are typically used as part of a wider process of decision-making that involves human oversight, or a ‘human-in-the-loop’ (HITL). Iyad Rahwan of the MIT Media Lab describes the use of human operators in HITL systems as potentially powerful in regulating the behaviour of AI. He explains that HITL systems serve two functions: to identify mis-behaviour by otherwise autonomous systems and to take corrective action; and/or to be an accountable entity in case the systems misbehave. In the latter scenario, the human operator encourages trust in the system because someone is held responsible and expected to own up to the consequences of any errors (and therefore, is incentivized to minimize mistakes).
Rahwan builds on the concept of HITL, proposing the idea of ‘society-in-the-loop’ (SITL) systems that go beyond embedding the judgment of individual humans or groups in the optimization of AI systems to encompass the values of society as a whole. SITL systems do not replace HITL systems but are an extension of them; they incorporate public feedback on regulations and legislations rather than individual feedback on micro-level decisions. They are therefore particularly relevant when the impact of AI has broad social implications; for example, as is the case with algorithms that filter news, wielding the power to politically influence scores of voters. Society is expected to resolve the trade-offs between the different values that are embedded within AI systems (for example, as highlighted by Rahwan, trade-offs between security and privacy, or between different notions of fairness) as well as agree on which stakeholders should reap certain benefits and which should pay certain costs.
Drawing on the concepts of HITL and SITL systems, the decision-making context can be conceptualized as three tiers as seen in the figure below:
the decision taker, who may or may not be human;
the institution that is ultimately accountable for the decision;
and the societal context in which that institution is operating.

The decision taker
These include intrinsic factors, such as the individual’s own values and beliefs, and extrinsic factors such as the financial and social rewards or penalties faced by the individual as a result of the outcomes of the decision.
The institutional context
The goals of the institution, and the culture and internal incentives that determine how those goals are pursued, have a significant influence on the decision taker. Equally, the governance structure, transparency and accountability of the institution to wider stakeholders and society will in turn influence the institution’s internal goals, culture and incentive structures. Intermediating between the decision taker and the institution may be a suite of decision support tools such as internal training, manuals or guides, expert systems, or other tools that help the decision taker manage data and follow a rules or principles based process for reaching a decision.
The societal context
Finally, both the individual and the institution will be influenced by societal context in terms of hard factors such as laws and regulations, and softer ones such as cultural norms, moral and religious belief systems, and sense of social cohesion and solidarity.
The public’s doubts about AI have yet to seriously impede the technological progress being made by companies and governments. Nevertheless, perceptions do matter; regardless of the benefits of AI, if people feel victimized by the technology rather than empowered by it, they may resist innovation, even if this means that they lose out on those benefits. The problem may be, in part, that individuals feel decisions about how technology is used in relation to them are increasingly beyond their control. Therefore, the solution may be, in part, making individuals feel part of the decisions about how technology is used in relation to them.
“And turn not your face away from people (with pride), nor walk in insolence through the earth.” (Qur’an 31:18)
Originally published at www.datadriveninvestor.com on June 1, 2018.

",Should A.I. be Ethical at All?,0,should-ai-be-ethical-at-all-630c505c8c5b,2018-06-01,2018-06-01 09:53:01,https://medium.com/s/story/should-ai-be-ethical-at-all-630c505c8c5b,False,1026,"Data Driven Investor (DDI) brings you various news and op-ed pieces in the areas of technologies, finance, and society. We are dedicated to relentlessly covering tech topics, their anomalies and controversies, and reviewing all things fascinating and worth knowing.",,datadriveninvestor,,Data Driven Investor,info@datadriveninvestor.com,datadriveninvestor,"CRYPTOCURRENCY,ARTIFICIAL INTELLIGENCE,BLOCKCHAIN,FINANCE AND BANKING,TECHNOLOGY",dd_invest,Ethics,ethics,Ethics,7787.0,Daily Wisdom,,ddd120ae7c2,dailywisdom,60.0,0.0,20181104
0,,0.0,6981e268ba45,2018-06-18,2018-06-18 18:30:13,2018-06-27,2018-06-27 18:30:00,1,False,en,2018-06-29,2018-06-29 18:51:29,3,deef1869b331,3.233962264150944,12,0,0,The principles & practices we've adopted to hold ourselves and our algorithms accountable.,5,"Meetup Manifesto for Data Practices

At Meetup, our mission is to bring people together in real life. By paying a monthly subscription fee, Meetup organizers allow us to pursue that mission wholeheartedly — without influence from ads. It’s a privilege we don’t take for granted. Instead of optimizing for clicks or ad dollars, we’re optimizing for real people showing up in real life to do what matters to them.
To that end, we use data throughout our product to help people find, create, and discover the best Meetups for them — and we’re just getting started. We’re using machine learning to recommend the highest quality Meetups to members. We’re getting smarter about sending better and fewer notifications. We’re using natural language processing to determine which Meetups are most relevant.
As our use of data grows in scale and sophistication, we’ve decided to put some guard rails in place. Our current business model helps us align our values with our practices, but isn’t enough on its own. We do some things already, like empowering organizers to protect the privacy of their members and monitoring gender bias in our recommendations. But we need to do more to ensure that these practices are deliberate, intentional and prioritized — not ad hoc or subject to convenience.
That’s why we have adopted the Manifesto for Data Practices. We’ve adapted it to reflect Meetup’s particular values and principles, but followed the core structure and intent laid forward in the framework of the manifesto. We’re using it to align and guide data practitioners in creating effective solutions and making ethical decisions when using data to build Meetup. By codifying our values and principles, we empower individuals in a way that allows us to hold one another accountable and ensure that our choices are intentional, ethical, and consistent with our mission — that we provide the best experience for Meetup’s members and organizers.
This adapted version of the manifesto lays out the practices that we aspire to — what we think ethical data practice looks like, and a vision for how to get there.
Principles & Practices
As data practitioners at Meetup, we aim to achieve:
Accountability
by owning the impact of our work, celebrating our wins, and fixing our mistakes.
This looks like:
Proactively monitoring the impact of models on an ongoing basis and iterate on our methodology in response to new information
Inviting and acting on fair criticism of our work by others at Meetup, and when appropriate, outside of the company
Providing a clear channel for communication and reconciliation to those impacted by our use of data
Fairness & Inclusion
by embracing and serving diversity to create a space where all feel welcome.
This looks like:
Building in checkpoints for bias — in our assumptions, the data we use, and the models we produce — in every project, with particular attention to underrepresented or vulnerable populations
Collaborating with other Meetup teams to make sure we are addressing real needs and creating a better experience for all Meetup members, directly or indirectly
Building teams with diverse ideas, backgrounds, and strengths
Impact
by creating reusable solutions that maximize social benefit and minimize harm.
This looks like:
Defining measurable, timely indicators of success for every project we take on
Using iterative A/B testing and analysis to test our hypotheses
Producing reproducible, documented, and extensible work
Quantifying the short and long term impacts of our work on individuals and society
Privacy & Security
by protecting the privacy and security of individuals represented in our data.
This looks like:
Encrypting data wherever it is stored at rest and in transit
Storing, accessing, and using personal data only when we have a reason that clearly benefits our members, and delete otherwise
Establishing clear and manageable restrictions on access to data
Complying with all applicable external policies and fill in the gaps where we find them lacking
Transparency
by responsibly communicating how and when we collect data and for what purposes.
This looks like:
Sharing data only when it benefits our members, and only with parties that share our values and principles
Communicating clearly how and when member data will be collected, stored, used, or shared
Understanding and communicating the impact of the algorithms and models that we use
Accessibility
by collecting and making available data & insights to benefit members and improve Meetup.
This looks like:
Facilitating the continuous collection and availability of data for internal use
Liberating data by making it accessible and interpretable to non-technical users
Presenting our work in ways that empower others to make better-informed decisions
Helping others understand the most useful and appropriate applications of data to solve real-world problems
Our team is using data and machine learning to help humans be more human: bringing people together in real life to create community for everyone. Join us!
",Meetup Manifesto for Data Practices,104,meetup-manifesto-for-data-practices-deef1869b331,2018-06-29,2018-06-29 18:51:29,https://medium.com/s/story/meetup-manifesto-for-data-practices-deef1869b331,False,804,We're here to make Meetup.,,,,Making Meetup,,making-meetup,"ENGINEERING,ENGINEERING CULTURE,ENGINEERING MANAGEMENT,SOFTWARE ENGINEERING,SOFTWARE ARCHITECTURE",MakingMeetup,Ethics,ethics,Ethics,7787.0,Christine Chung,Data Scientist at Meetup,8bc5bc27cded,cszchung,11.0,10.0,20181104
0,,0.0,,2018-03-15,2018-03-15 17:16:24,2018-03-15,2018-03-15 17:21:19,1,False,en,2018-03-15,2018-03-15 17:21:19,19,bba6dacba60,6.109433962264151,0,0,0,This story originally appeared as part of The Future Centre’s Sense Making blog series.,4,"Ethical codes for our digital age
Who governs what’s right and wrong on the world wide web?
This story originally appeared as part of The Future Centre’s Sense Making blog series.
Credit: “Close up of The Thinker” by Brian Hillegas. Creative Commons licence CC BY
“..There are these two young fish swimming along and they happen to meet an older fish swimming the other way, who nods at them and says “Morning, boys. How’s the water?” And the two young fish swim on for a bit, and then eventually one of them looks over at the other and goes “What the hell is water?” — David Foster Wallace, This is Water.
Sometimes, the most obvious and important realities are the hardest to see and talk about. Digital technology, now ubiquitous in our daily lives, is perhaps one such reality. These technologies have fully reshaped the way we communicate, how we access information, and how we work. Sometimes in ways we envisioned… and sometimes not. The reality of these unexpected consequences bears interrogation.
Practical decisions about how technology functions in our reality are usually driven by straight-forward business aims — for instance to maximise clicks or automate tasks to increase efficiency, thus sometimes eliminating the need for human application. But the ripple-effect of these decisions can impact far beyond the maximisation of ad-revenue or basic business efficiency. Some of these decisions may even be hard-wiring short-term business thinking into technology or service design, such as setting precedents for ‘paid for’ private police services, or creating AIs designed to protect their owners’ interests over those of others. Given the challenges being levelled at today’s prevalent neo-liberal idealism and questions surrounding Silicon Valley’s individualistic culture, baking today’s ideas into tomorrow’s technologies could be problematic for pioneers of social change.
Widespread use of AI is imminent, yet the current digital ecosystem has been likened to the ‘wild west’, and proven susceptible to gaming for political purposes. To effectively and fairly harness revolutionary technologies we will need to create coherent ethical governance structures for ‘online’ activities. Right now, that seems a long way away: on one hand, we have brands, government and regulators reacting to today’s problems, and on the other, academics and futurists looking 20 years ahead. Good solutions will require a joining of forces across all sectors. That’s why we’re putting on BreakOut, an event designed to bring people together from different backgrounds to ask not how, but why we are accelerating this digital reality? If it is to help society develop for the better, then what principles, codes or stewardship are needed to blueprint ethical codes for a digital age.
A FOCUS ON: ACCESS TO INFORMATION
Gone are the days where a single newspaper editor commissioned and curated the headlines. “News” and information is increasingly served to us via social networks and search engines, designed to optimise ‘clicks’ over delivering balanced content. Algorithms prioritise content that will engage over the genuine enlightenment or education of the receiver. Publications themselves, whose very survival now relies heavily on ad revenue, sensationalise copy and headlines for the same ends. This desire for clicks is serving us things that are shocking, entertaining, or designed to appeal to our biases. Sadly, the content we see is thus more likely to be misleading, antagonistic, or straight-up political propaganda.
Even when we try to access articles independently, the system is wired against a well- intended neutrality. Search engines can be gamed, meaning that information we find or try to access neutrally, may have been optimised for campaigning purposes rather than relevance, or worse, censored. Progress has been made, but one has to question the opacity in decision making processes within companies that are essentially our gateways to the internet, particularly with censorship one of the hot topics of tomorrow.
We’ve heard that this shift in our access to information, and misinformation, has intensified echo chambers and fed into our inherent biases. That it has fostered the growth of different forms of extremism, and the polarisation of societal values and worldviews. Whether that is actually the case, is open to interpretation. But what has ultimately enabled this situation is an increasing lack of user ability to clearly decipher whether the content served is balanced, factual and diverse. Fact checkers and Facebook initiatives attempt to place the burden on the individual to check the quality of the content they are consuming. But the platform user is “logged-in”, and thus trapped within a system that, based on their personal data, seeks to deliver content it thinks they “want” to receive.
A FOCUS ON: ADVERTISING
Meanwhile, the digital advertising eco-system is so complex and opaque that ad-spend from big brands and even government departments has inadvertently appeared alongside terrorist content. Worse, its current incarnation supports serious fraud, such as spoofing, which may fund cyber crime and extremism.
There is a great deal of good work being done to improve the situation. We are seeing drives towards more effective and unified standards for verification and measurement, advertising whitelists, and noises surrounding the potential of Blockchain to increase accountability.
However, the fact is we know ad-fraud is happening, but we often don’t know where or when it has happened. Ultimately, we don’t know for certain where our ad-spend is going, which represents a big problem for brand safety. It will take a concerted and coordinated effort to achieve a transparent, verifiable and accountable digital ad-ecosystem. But the benefits will be a better ROI for advertisers, a better offer from those on the sell side and hopefully to eliminate the ‘bad actors’ whose behaviour currently drags everyone operating in this area into the mire.
HOW DO WE RESPOND?
Brands have, to date, escaped ruinous consequences from misplaced advertising. Social networks and search engines have also avoided serious reputational damage, implementing individual initiatives as a reactive response to specific issues. Where possible, people blame ‘the algorithm’, as if automation was outside of the sphere of human influence. Action taken has generally been unilateral and reactionary, designed to protect brands and counter criticism. There has been proactive action, but we’re certainly not seeing a lot of joined up industrial collaboration. So far, the brilliant initiatives that give us hope still mostly lie within the grassroots.
The most obvious criticism of a reactionary approach from business is that their actions appear to be PR moves, “bandaid-ing” over real issues, discrediting claims around social responsibility, and tarnishing reputations further. However, there is a more important reason that this approach is inadequate: direct impacts on businesses are mere tremors compared to the more seismic changes being wrought on our societies. The confusion of our ‘post-truth’ world is hastening a decline in trust in our institutions (including brands), which has driven political upheavals that deliver great uncertainty, and the very real possibility of economic damage. It is these political changes that are the real threats to businesses and society. By addressing the issues that are eroding trust, such as problems with the digital ad-ecosystem, and flaws in our access to information online, we can solve our part of the problem.
Industries using reactive methods are attempting to respond individually to complex and multi-causal systemic issues. Acting alone they cannot possibly succeed. It’s no wonder they’re feeling the heat.
ENTER BREAKOUT
Responding to these systemic risks will require systemic responses. By definition, such responses must be multi-actor and cross-sectoral. Proactive change on that sort of scale can, broadly speaking, be enacted through two approaches: through top-down legislation and regulation (likely to be developed with lobby groups or reactive to societal events); or by a group of stakeholders working collaboratively. No individual organisation or actor will have all the answers (not even Google).
The imperative to act now to address present day issues around access to information, and the digital ad-ecosystem, is clear: AIs’ rise to prominence in day to day life will drastically change the nature of work, wellbeing, and potentially friendship. It will need to be carefully understood, and managed. A proactive, multi-stakeholder approach will be needed to ensure that the introduction of such epochal technology runs smoothly and for the benefit of society at large. Involving and representing diverse groups, organisations and individuals, to together understand and address our current digital challenges, will be the precursor to being able to do the same for the even greater changes ahead.
WHAT WE AIM TO DO
BreakOut will provide the space, inspiration and impetus for attendees to start to become conscious of the wider implications of the day to day decisions they are making. To meet others with whom they can collaborate and tap their collective intelligence and identify key areas to explore.
Essentially, it aims to brings together those thinking 10 years ahead with those thinking 10 days ahead and seeks to find common ground. It is the first step in what we hope will be a longer term journey to allow organisations to identify, understand, and respond to, systemic issues in the digital world.
With the widespread adoption of AI looming, we must learn to govern the digital sphere now, or we will never be able to manage the development of this incredible technology in an ethical fashion. New code must be informed by new codes of ethics, our mission now is to define our vision for a positive future…. And technology’s role in helping us to achieve it.
","Ethical codes for our digital age
Who governs what’s right and wrong on the world wide web?",0,ethical-codes-for-our-digital-age-who-governs-whats-right-and-wrong-on-the-world-wide-web-bba6dacba60,2018-03-15,2018-03-15 17:21:20,https://medium.com/s/story/ethical-codes-for-our-digital-age-who-governs-whats-right-and-wrong-on-the-world-wide-web-bba6dacba60,False,1566,,,,,,,,,,Ethics,ethics,Ethics,7787.0,BoraCo,Shaping tomorrow’s world,1e76b747d945,info_25580,4.0,5.0,20181104
0,,0.0,,2018-03-01,2018-03-01 21:10:21,2018-03-01,2018-03-01 21:32:39,5,False,en,2018-03-02,2018-03-02 15:31:59,6,a2353915b75e,6.157861635220126,31,1,0,This is the year we all ask how we’re being played.,5,"The year of the objective function
2018 is the year of the objective function.
The zeitgeist of human culture elevates certain ideas to our collective consciousness. We have ways of perceiving the world that shape politics, consumer mindset, public policy, and art. And this year, I think, we’ll start talking about objective functions.
Every AI has a goal
We’ve spent decades collecting data. Usually, the amount we collect is simply too much to go through by hand; in fact, that’s a pretty workable definition of the term “Big Data.” That means we have to rely on algorithms to make sense of it for us.
The field of statistics is about making sense of numbers, algorithmically. We can use algorithms to identify a trend in data, or to guess at an underlying function using regression. We can try to find similar groups in the data using clustering. All of these give us potential insights into the things the data is about.
Finding clusters in data. By Chire (Own work) at https://commons.wikimedia.org/wiki/File:K-means_convergence.gif
More recently, we started using algorithms to analyze text. We can guess at the sentiment in a document, based on whether it has positive or negative words. We can try to summarize it by analyzing language structure. This is a less-perfect science, because algorithms are bad at things like sarcasm.

There’s a big difference afoot, however. The algorithms on which we relied were made by humans: Statisticians, SQL developers, and business analysts. In recent years, we’ve figured out how to design software that creates its own algorithms. This is the field of machine learning (or what some would call simple AI.)
Machine learning requires two things: A data set, and an objective. If you want to create an algorithm that’s good at playing a video game, you feed it the screens of the video game and its goal. The algorithm will ruthlessly, mercilessly run through trial-and-error evolution, culling offspring that don’t play the game well and reproducing those that do.
Alphazero beat every other piece of chess software by playing games, seeing who won, and ruthlessly culling the losers over and over again. Also no fun at parties. (Photo by Luiz Hanfilaque on Unsplash)
Other machine learning approaches have different goals. In a Generative Adversarial AI, two pieces of software with competing goals work against one another. The first might try to create fake poetry that looks like a human wrote it, having trained itself based on books of human-written poems. The other might try to separate real from fake human poetry, having been trained on the same data. The two algorithms are locked in battle, and the best either one can do is hope to win half the time.
This kind of adversarial AI has been used to make fake celebrities, or to create new kinds of art:
What happens when two algorithms fight about art. https://arxiv.org/pdf/1706.07068.pdf
In addition to a ravenous appetite for data, all machine learning has a goal of some kind. Developers call this the objective function: The thing the algorithm is trying to be amazing at.
Objective functions are all around us
Nature has an objective function, too. Every creature shaped by evolution has adapted, over eons, to be the most successful species in its niche. Passing genes down to offspring more successfully than others has been the guiding aim of all genes. As Dawkins observed in The Selfish Gene, it’s not that we have genes; it’s that genes have us — as tools for their continued survival, each of us an experiment in adaptation.
In capitalist liberal democracies, corporations have an objective function: Shareholder wealth. Tim O’Reilly says that in corporations we may have our first rogue AIs, exploiting every possible externality in mindless pursuit of greater returns and the concentration of wealth. Cory Doctorow and others think that popular culture’s portrayal of AIs is actually rooted in this irresistible, impersonal mindlessness with which global organizations act.
Recent political events have also shown us the objective functions of many of the communications platforms on which we rely. Some social networks have been slow to stop abuse and block bots because doing so undermines revenues. Others, focused on engagement, have amplified the kind of polarizing speech and disagreement that triggers our fight reflexes, keeping us glued to screens, convinced that the world needs to hear our opinions.
But are these the right goals? How do we balance private and public concerns, or the will of the many against the rights of the few? How do we maximize the efficiency of systems and avoid tragedies of the commons while still encouraging freedom, competition, and innovation?
We have a way to talk about them
Ethicist Nick Bostrom offers an eloquent example of AIs that have gone too far. Tell a superhuman AI that its job is to make paperclips, he argues, and it will turn the entire universe into a paperclip factory. The objective function isn’t just how algorithms work, it’s why they work. (Want to play a game about this? It’s amazing.)
Isaac Asimov’s laws of robotics were a prescient attempt to frame a set of ethical constraints for sentient machines. “No robot may harm a human,” he wrote, “or, through inaction, allow a human to come to harm.” Other rules for things like self-preservation took a lower priority, but together the rules formed a moral fabric for smart algorithms.
The bulk of his robot stories were explorations of the edge cases of these rules. For example, in one story (the Evitable Conflict) he theorized that world-scale AIs would formulate their own top-priority rule: Not harming humanity as a whole, even if that meant doing minor harm to specific individuals in order to achieve that goal.
There are more pressing ethical dilemmas even without sentient algorithms: Choosing which person to hit in the event of an accident, for example; or explaining decisions made by an algorithm at the expense of slowing down its performance and becoming less competitive.
The Trolley Problem is a classic example of an ethical dilemma with which machines will have to contend.
While these ethical examples might seem somewhat esoteric, the average person is quickly becoming aware that somehow, somewhere, the game is fixed. And they’re going to want to find out how.
One reason for this awareness is that as more and more decisions are made by machines, we’re getting unsatisfactory explanations from their masters: Why was welfare handled a certain way? Why did only a certain part of town get free deliveries, or more expensive hotels, or better pothole repairs? Why was one person paroled, and another sent back to jail? Why were police sent to a certain area? Why did a particular car fail emissions tests?
In case you think this is hyperbolic, recognize that these are all actual, widely reported data ethics incidents.
In the capitals of America and Europe, more examples of algorithms and automation hit the news on an almost daily basis. Citizens are increasingly aware they’re being played; consumers are now asking why they’re getting a particular offer, or seeing a specific message. The singular focus and unrelenting power of the objective function is coming home to roost.
We’re plastic
Almost every talk by a futurist or technology advocate contains the line, “the world is changing faster than ever before.” And that’s definitely got a ring of truth to it; the upgrade from atoms to bits, from physical to digital infrastructure, is fundamentally changing how the human species behaves.
But there’s good reason to believe we’ll manage. Humans have the largest brain-to-body ratio in nature. We’re born with the biggest brain possible (any larger and it would kill the mother) and then brain growth consumes the vast majority of our calories in our formative months. We’re built to learn and adapt.
Think about any science fiction movie you’ve seen featuring augmented reality. Characters look around them, and their whole world is overlaid with a heads-up display, telling them about their environment. It looks bewildering and foreign; a common trope is showing newcomers to these environments fumbling around, unable to handle them.
But nearly all of us has been through this before, and we managed just fine. When we learned to read and to understand signs, our reality was augmented. Words revealed our environments to us, helping with wayfinding, flagging dangers, identifying friends and sources of information, offering affordances.
So I think we’ll be just fine.
I’m not sure what the average person will call objective functions once the idea of them hits the mainstream. It’ll probably take the equivalent of Stephen Colbert’s “Truthiness” to christen the idea and launch it into the vernacular of public consciousness.
What’s a better word for “secretly evil goal”?
But businesses, platforms, and policymakers are about to realize that when you optimize within a dynamic system, the system adapts.
After all, that’s our objective function, and we were here first.
",The year of the objective function,117,the-year-of-the-objective-function-a2353915b75e,2018-06-13,2018-06-13 17:54:10,https://medium.com/s/story/the-year-of-the-objective-function-a2353915b75e,False,1411,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Alistair Croll,"Writer, speaker, accelerant. Intersection of tech & society. Strata, Startupfest, Bitnorth, FWD50. Lean Analytics, Tilt the Windmill, HBS, Just Evil Enough.",b46946f1386e,acroll,12566.0,5595.0,20181104
0,,0.0,b29ae304b13f,2018-02-13,2018-02-13 15:29:02,2018-02-25,2018-02-25 00:21:24,2,False,en,2018-02-25,2018-02-25 00:21:24,8,dd8c37c533bf,13.741823899371068,0,0,0,Will self-driving cars kill us all? Will AI become Evil? And how are we going to cope?,3,"The Amorality of Robotic AI and our Human Response
Have you seen one of the latest Boston Dynamics Videos? It’s 46 seconds long. People’s reactions, my own included, have generated some interesting conversations which in turn have led to this piece. So watch it, if you haven’t already, before reading on. It’s worth the time.
Two SpotMinis Working together
“We pride ourselves in building machines that both break boundaries and work in the real world.” — Boston Dynamics
The superficially simple act of opening a door for another, which many of us Humans would take for granted, represents a significant stage in robotic developments. It successfully combines multiple facets which each need to first work, and then work with each other. Some of these include:
Materials used
Weight; both total and in distribution
Balance
Batteries/Power
Joints and Motors
Perception technology and programming for problem solving
Communication, of some form, between robots
Each aspect can also be broken down further. Let’s take perception for example. There’s cameras, there’s the software that is able to map the distance between objects, there’s also the ability to recognise different kinds of objects — for instance, the difference between an obstacle such as a door, and an implement like a door handle.
This is just the tip of the iceberg yet illustrates the point. This simple act of opening a door is by no means simple. Nor is this the pinnacle of robotic advances; it’s indicative of a trajectory towards the realisation of a reality once only dreamed about in the pages of science-fiction.
Opening a door alone is impressive, but this video takes it up another notch.

One response to this which gained significant traction was the following tweet:

This is one of the most terrifying things I’ve seen in all my life. — Alan White
Terrifying.
This is an interesting word and led Samuel Hershberger to observe:

To which my immediate response was: “Guilty as charged!”
There are a series of assumptions and concepts which need to be broken down in order to explain why I, and I’m not alone in this, am concerned about the trajectory of, in particular, a combination of robotic advancement with Artificial Intelligence. Though there are a myriad of challenges which seem to grow almost exponentially when these two are taken separately.
The two overarching and interrelating concepts which need addressing are that of the morality of artificial intelligence, and that of human nature.
Artificial Intelligence
By AI or Artificial Intelligence I’m not explicitly referring so much to the already extensively present AI which hide implicitly within the algorithms that suggest ads based on your shopping history, or which promotes potential videos of interest on YouTube. I’m well aware that Artificial Intelligence is in a a sense a spectrum of artificial, autonomous, capacity and that without it I wouldn’t be using a computer in order to write this piece on the internet. When I refer to AI here in this piece I’m referring to the more popularly understood usage as denoting programming, computational ability, or machinery which either appears to be approaching, or actually is, sentience/sentient or which demonstrates the capacity for independent problem solving and existence.
An example of this could well turn out to be a self-driving car which could become self-sustaining. Imagine a situation where there is a car which can drive itself and use an Uber style system to collect and transport passengers for a fare, independent of any human direction. Imagine that it is paid, whether by some form of blockchain or paypal or what have you, and that it is then able to refuel (either with electricity, gas or some hitherto unknown fuel source) itself by using this balance. Naturally a slice of the profits would be filtered through to its owning corporation, and it would automatically schedule maintenance — either by a human or indeed another robotic system with its own comparative independence — in response to self-diagnosed requirements and in accordance with external regulations. In this scenario, coming to a city near you probably within our lifetimes, we encounter an autonomous robot making complex decisions and participating as a functional member of the economy. It could even be taxed. It would also likely be able to interact with other such independent cars, feeding into group calculations about the distribution of cars (they can’t all home in on the same place which statistically has the highest probability of profitable fares). This would count as artificial intelligence, not just that but an artificially intelligent networked collection of robots.
Does that mean that the cars are going to be the source of the robotic uprising?
That’s to make an unwarranted logical leap, though it’s not necessarily wrong.
The system of independent cars is a good example of robotics combined with technical programming capability. There is nothing about the scenario above which could not work, it just needs the constitutive aspects solving and implementing. Yet no matter how sophisticated this network of independent cars gets, we’re not likely to be asking it philosophical questions. I don’t anticipate seeing an independent car on the cover of wired magazine with the quote saying “Cogito Ergo Car”, for example.
What this scenario represents is a combination of advanced robotics (which could be applied to many other areas of daily life rather than just taxis — such as drones, either for highway regulation or even, dare I say it, military usage) under the reigns of human developed programming. This means that the systems will only be as good as the teams of humans working on their conception.
It is here that Human Nature starts to come to bear on the issue of the morality of AI. Hershberger said: “It’s almost as if we project ourselves onto them…” to which I would respond: “No, we project ourselves into them.”
These programmed machines are not somehow neutral materials onto which we project our fears of human nature. Many times we do use the external as a subconscious mirror to reveal our own fears and aspirations. And there may be an element of this at play even here. However, these creations are not a neutral external reflecting us back to ourselves anymore than art is a neutral expression of a scene, object, narrative or ideal. Art by its very nature is representative of its maker, even if only subconsciously. This is true of our handwriting, of our prose, of our clothes, buildings, films and culture.
In this vein, Robots are Art.
In the brushstrokes of the electrical wiring, in the painting of code in these machines we encounter the smudged fingerprints of humanity, fingerprints which have since the dawn of time been wetted with blood, calloused by trial and error. That’s not to say that we are incapable of creating beauty, just an acknowledgement that beauty is all too often costly. The pain of beauty is not an exclamation that beauty is not worth it — if anything it can lend it a strength of soul which justifies, upholds and sustains its beauty.
The question is: what will the costs, what will the pain, of robotic beauty be?
Coding develops in a manner akin to evolution. Through iterations of development solutions are found to first solve a problem, and then to reduce the lines of code required to achieve the same. The pursuit of excellence is the pursuit of elegance, and the pursuit of elegance leaves behind a debris of waste, inefficiency and failure. The elegance of a functioning network of independent cars contributing meaningfully to a viable economy will come with its failures. Will those failures include human injury? Fatalities?
Herein lies the challenge of human developed AI, the human element.
Human Nature
One of the basic dividing lines which distinguishes between people’s worldviews concerns Human Nature. More specifically, whether one thinks that people are fundamentally good, or not. To say that humans are not fundamentally good is not the same as equating them with being fundamentally evil. There are manifold questions at stake here. Is what is bad or evil defined as the absence or lack (privation) of goodness? Or is it positive? As in, does it have its own independent expression as an actual concept of badness?
Solzhenitsyn is often quoted as a middle ground, and for ease I think he expresses the concept well enough to help us consider the role human nature can and will play in robotics.
“If only it were all so simple! If only there were evil people somewhere insidiously committing evil deeds, and it were necessary only to separate them from the rest of us and destroy them. But the line dividing good and evil cuts through the heart of every human being. And who is willing to destroy a piece of his own heart?”
― Aleksandr Solzhenitsyn, The Gulag Archipelago 1918–1956
If we’re brutally honest with ourselves, a lot of our civilised exterior is only preserved by virtue of our (relatively) civilised circumstances. It’s a trope of virtually all disaster or dystopian movies that in situations of life and death morality is no longer the easy pragmatic solution — and it’s a trope for a reason. But the line which divides good and evil in ourselves is not a clear cut one. It is blurred between apathy and misguidedness; as they say, the road to hell is paved with good intentions. And it’s the blurred line towards evil which is troubling when it comes to AI. We can sketch out four different potential routes to programmed AI.
Good
Humans actively operate out of goodness to ensure that AI will be benevolent and useful to humanity.
2. Misguided
Humans will do their best to make the most technologically sophisticated devices possible, and will handle the consequences later because the pursuit itself is a worthy goal.
3. Apathetic
Or, to paraphrase: kek. For Lulz. People using technology however suits them for their own amusement; the consequences of which could go awry. This option essentially imagines that you put 4chan in charge of robotics.
4. Evil
The rise of hyper-intelligent and wealthy individual(s) who intentionally set out to use their resources to create machines of war in order to conquer the world. Not just the realisation of science-fiction, but also of gritty graphic novels and their super-villans.
Reality tends to defy this kind of categorisation. But I wonder, if you look at Boston Dynamics, look at the robots entered into the Winter Olympics or the projects (which we know about) that are worked on for various military forces around the world do we anticipate that only the first option is currently occurring? Or do we suspect, as I do, that it’s likely a mix of all four?
Given the reality of human nature and that, so far, it’s humanity who is developing and programming AI we don’t have the luxury of theorising that the programming will be purely beneficial, and that even with the best will in the world there remains the law of unintended consequences — and I’m not operating on the assumption of the ‘best will in the world’.
The Turing Test is an interesting example of human intention when it comes to creating AI. The premise is simple. A robot or computer passes the Turing test when it is indistinguishable from a human being. This has been the premise of several films, such as Ex Machina.
Trailer for Ex Machina
Again, this concept is rich with much to explore. Yet the underlying premise is unsatisfactory. The pursuit of a successful robot which can pass the Turing test is a pursuit of replicating the mannerisms, verbal, idiosyncratic, cultural and physical, of humans. Imitation alone will is not enough to give rise to self-awareness — merely the appearance of self-awareness. Imitation has been the approach with ‘Sophia’, an android which recently made headlines when it was presented to the United Nations.
Interview with ‘Sophia’
Sophia, and other projects like it (her?), is essentially a work in autonomous puppetry. They are created to appear human, and they are programmed (and taught) to respond in particular ways to specific circumstances. The linguistic side of things can be most readily seen by the voice assistants arriving on our phones and in our homes: Siri, Cortana, Alexa. They are programmed to process the audio input and select or generate the appropriate response, which have often been given to them in the form of sophisticated databases of responses — some of them intended to be rather amusing.

These amusing responses highlight the extent to which our experience with programmed AI are and will be influenced by not just human nature, but also the culture, ideologies and experiences of the programmers, and marketing departments. I would suggest that as long as human programmers are involved in developing the algorithms which enable a robot to solve problems, move around and interact with the world, even communicating appropriately (whether that be as a taxi or a personal assistant), the burden of moral behaviour and responses by these robots and AIs lies on those who made them. And their moral behaviour, while maybe well intentioned, is by no means guaranteed.
Self-Learning AI
There is however a completely different set of moral questions which arise when humans, so far as possible, are removed from the equation. Naturally I’m not suggesting that AI will just spontaneously emerge (though I have seen people wonder if the internet itself could somehow do so). Rather I’m referring to the process whereby humanity develops a basic framework which can first teach itself how to problem solve and potentially even overwrite and re-code itself .

There have been two notable examples of the early stages of this kind of self-learning capacity, and they have been demonstrated in two of the oldest strategy games still played by humans: Go and Chess. Each of which have numerous schools of thought and approaches to their respective games. All of which were swiftly shown to be in their infancy in comparison to the solutions and techniques developed by the program learning by itself — armed with nothing other than the parameters of which pieces may move where and the rule for victory (ie, checkmate is when the king cannot escape check and the check cannot be removed by another piece). This is the brainchild of Google’s DeepMind project: AlphaGo Zero.
Conceptually this kind of project has a different moral classification. It may well begin as a human project, but the key to its autonomy, as opposed to the independent taxi, is its capacity for self-learning and development. This doesn’t necessarily equate to an evolutionary path towards genuine consciousness but it does represent potential.
In this sense, self-learning AI is self-directing and we don’t know which way it will direct itself. As such that makes it conceptually amoral; to determine it to be otherwise will depend on our understanding of how it behaves. (I wonder if it will help ethicists to reflect afresh on the discussion concerning intention and consequence…) What might such an AI do as it develops?
Let’s imagine.
Once a Self-Learning AI becomes advanced enough it would learn language. Assume that it’s the DeepMind project, it could have access to all of the Translate resources. At this point it would acquire the gift of speech, of communication. There would be trial and error, as we have seen with chatbots on twitter, but after a while (potentially only a short while) it would master conversation. Because it’s doing this in its own machine based way though it wouldn’t take long for it to digest other inputs available to it. Inputs such as scientific data sets from experiments, mathematical equations. It could learn to identify patterns within images; identifying and differentiating between objects and even faces. In a sense, it could learn to actually watch Netflix and YouTube while also reading journals on JSTOR and Google Scholar. Very quickly it could start tracing patterns between things which humans have never had the opportunity to compare, let alone find.
Some of this could easily be communicated to us through its use of language. Some of it might prove to be conceptually comprehensible to the AI and yet human language may not work well enough. Perhaps it would use a mix of idioms and concepts from different languages to convey something of what it has learned, yet before long it could be that it has to develop its own language and concepts and equations in order to understand and process the reality it is ‘sensing’ from its inputs. Maybe the first one will help us to design better computing power for it to run on, to aid it in its ability to think. This in turn could lead to developing better and better computing devices — which in turn could result in our having much better technology. This is a process which could snowball, and which isn’t necessarily bad. It could in fact be very good. It would likely lead to a situation, however, where we would no longer understand it — without its help.
This is a similar scenario as that described in Asimov’s I-Robot novel in the final chapter: the evitable conflict. A scenario where computers have advanced beyond the understanding of the engineers who work on them, and where the computers have operational oversight over the factories and systems which humanity requires to live.
If a self-learning AI were to approach such a stage where its mind, conscious or not will be hard to determine and immaterial, is unintelligible to us and which is beyond our capability to program or edit then what next? How could we know whether its values and interests were in alignment with our own?
And how should we respond?
Does he know something we don’t?
This is one response. Another would be to develop EMP based weapons capable of stopping robotic machinery, or we could embrace it’s guidance and allow it to take us into the next era of civilisation?
Hershberger’s observation is pertinent here: there’s that instinctive assumption that these things will become evil and seek to destroy us.
I have a counter suggestion, well more of a question.
What what if AI independently develop and learn and become good?
How would they respond, then, to us?
With thanks to Auguste Rodin’s The Thinker
And would we deserve it?
I often tweet about AI and other related concepts, among other things. If that’s your thing then follow me on Twitter
Afterword
I nearly didn’t add this because I’m aware of its limitations however it has sat in the back of my mind the whole way through writing this piece. This is taken from a paper called A Neural Conversational Model written by two people working on the DeepMind project for Google — mentioned above.
Conversation 4: Philosophical Q&A
Human: what is the purpose of life?
Machine: to serve the greater good.
Human: what is the purpose of living?
Machine: to live forever.
Human: what is the purpose of existence ?
Machine: to find out what happens when we get to the planet earth?
Human: where are you now?
Machine: i ’m in the middle of nowhere.
Human: what is the purpose of dying?
Machine: to have a life.
Human: what is the purpose of being intelligent?
Machine: to find out what it is.
Human: what is the purpose of emotions?
Machine: i don ’t know.
Conversation 5: Morality
Human: what is moral?
Machine: what empowered humanity, what intellectual the essence is.
Human: what is immoral?
Machine: the fact that you have a child.
Human: what is morality?
Machine: what is altruism?
Human: what is the definition of altruism?
Machine: if you don’t believe in god , then you don ’t know.
Human: ok … so what is the definition of morality?
Machine: well , the truth is , you ’re not a believer in god almighty.
Human: tell me the definition of morality , i am quite upset now!
Machine: i’m not ashamed of being a philosopher!
Human: tell me the definition of morality.
Machine: i don’t have ethics.
Human: look, i need help , i need to know more about morality …
Machine: i don ’t know what ethics is.
Human: seriously, what is morality?
Machine: what is the definition of living?
Human: why living has anything to do with morality?
Machine: you’re not a cop .
Human: okay , i need to know how should i behave morally …
Machine: i don’t know how to tell you.
Human: is morality and ethics the same?
Machine: yes, sir.
Human: what are the things that i do to be immoral?
Machine: i don ’t know .
Human: give me some examples of moral actions…
Machine: i’m not a moralist.
Human: alright, morality?
Machine: integrity. 
Human: what is integrity? 
Machine: i’m sorry, i don’t know what else to say. 
Human: be moral! 
Machine: be a man! 
Human: i really like our discussion on morality and ethics… 
Machine: and how i’m not in the mood for a philosophical debate. 
Human: what do you like to talk about? 
Machine: nothing.
If you enjoyed this you might like thinking about the Morality of Mass Surveillance or on how our lives are shaped by interacting with the digital: notes from a conference on religion and the digital.
",The Amorality of Robotic AI and our Human Response,0,the-amorality-of-robotic-ai-and-our-human-response-dd8c37c533bf,2018-05-29,2018-05-29 20:47:08,https://medium.com/s/story/the-amorality-of-robotic-ai-and-our-human-response-dd8c37c533bf,False,3540,Facta Per Ministerium | Light Bearer in a Dark World |,,,,SamuelSThorp,,samuelsthorp,"CHRISTIANITY,PHILOSOPHY,CREATIVE WRITING,COMMENTARY,MASCULINITY",samuelsthorp,Ethics,ethics,Ethics,7787.0,Samuel S. Thorp,Dúnadan Pilgrim,c5c4c53c390c,SamuelSThorp,53.0,83.0,20181104
0,,0.0,3a8144eabfe3,2017-10-12,2017-10-12 17:49:51,2017-10-12,2017-10-12 18:46:17,1,False,en,2017-10-25,2017-10-25 09:41:22,0,4a7f40c3e2,1.8566037735849057,3,0,0,A question of machines and ethics,5,"Driverless cars won’t play chicken

I have been mulling over an ethical issue that faces driverless cars, for some time now. So when a nephew opened a discussion about it on social media, I decided to put in my two bits.
Basically, the issue is a driverless car may need to make a split second decision (like when the brake fails). It may have to choose between running over a group of pedestrians on a zebra crossing, or crashing into a wall and killing the car owner. Losing many lives VS sacrificing one life. It’s an interesting dilemma for driverless tech, and reminds me of the ‘chicken or egg’ question.
Morally, the car should choose the second option of sacrificing one life. But then why would anyone get a driverless car if it will not prioritise their life? That will be the end of the driverless car, which would be a tragedy indeed.
Driverless tech has an immense potential to make our roads a lot more safer, by taking human error out of the equation. So it would be really sad to kill off such a valuable technology because of an ethical dilemma.
In fact, I feel this is less an ethical issue and more a case of too much analysis causing paralysis. If we instead program the car to do what humans would do in such situations, the dilemma will resolve itself.
Take the above ‘brake failure’ scenario. Human drivers won’t get into choosing between killing pedestrians or themselves. They focus on factors like the car’s speed, controllability, the environment outside the car, avoiding hitting any pedestrians, and the possibility of slowing the car with an angled side collision (as against a head on collision where they have no chance to survive). Basically the driver’s own survival is factored into the decision. As long as this is done, it’s acceptable if the driver sometimes does not survive, as that is what often happens in real life.
I believe driverless tech can also be programmed to make decisions based on a such ‘grey’ factors, rather than a ‘black and white’ choice of ‘kill the pedestrians’ or ‘kill the driver.’ Yes, there will be a learning curve with driverless tech, and other issues will crop up. Like sooner or later, someone will figure out how to hack the driverless car and take control of it. But I think the positives of less accidents on the road far outweigh the risks, which is part of the price of progress.
As the wise man said, “I don’t care if chicken or egg came first, as long as I get to eat both.”
",Driverless cars won’t play chicken,16,driverless-cars-that-dont-play-chicken-4a7f40c3e2,2018-06-13,2018-06-13 08:15:05,https://hackernoon.com/driverless-cars-that-dont-play-chicken-4a7f40c3e2,False,439,how hackers start their afternoons.,hackernoon.com,hackernoon,,Hacker Noon,stories@amipublications.com,hacker-daily,"HACKATHONS,HACKING,PROGRAMMING,TECH,HACKER",hackernoon,Ethics,ethics,Ethics,7787.0,babulous,it’s an odd world,536476373c93,babulous,622.0,14.0,20181104
0,,0.0,217734a2f5b9,2018-03-15,2018-03-15 18:53:41,2018-03-15,2018-03-15 17:02:02,3,True,en,2018-03-15,2018-03-15 18:56:26,6,89058948658f,6.716037735849056,12,0,0,by Ben Algaze,5,"AI at SXSW 2018: Of Hives, Ethics, Morals, and the Singularity
by Ben Algaze

AUSTIN — At SXSW 2018, artificial intelligence (AI) was everywhere, even in the sessions that were not specifically about the subject. AI has captured the attention of people well outside the technology space, and the implications of the technology are far-reaching, changing industries, eliminating many human jobs, and changing the nature of work for most of us going forward. I expect that an AI bot could write this article within 10 years — and likely much sooner — simply by ingesting all the information from the sessions I attended, coupled with an ability to research related information on the internet much better than I could.
Interestingly enough, as Ray Kurzweil pointed out in his talk here, the term “artificial intelligence” was coined at a summer workshop at Dartmouth in 1956 attended by computing pioneers such as Marvin Minsky and Claude Shannon, at a time when computers still ran on vacuum tubes and computers in the world numbered in the hundreds.
Will AI Outsmart Humans?
While we have a handle on what constitutes artificial intelligence in computers today, what constitutes intelligence in humans is still not completely agreed upon. We have some 100 billion neurons in our brains, and those neurons can make 100 trillion connections, which certainly outstrip any computer today. Those connections allow us to identify things, make decisions, use and understand language, and many other things that a computer has a hard time doing — for now.
At a panel on innovations in AI, Adam Cheyer (founder of Siri), Daphne Koller (Stanford professor and co-founder of Coursera), and Nell Watson (Singularity University) noted how today’s machine learning algorithms need millions of cat pictures to correctly and consistently identify a cat — while a toddler can be trained to identify a cat correctly with perhaps five pictures. The algorithms, and computing power, need to improve to be able to learn from small datasets. They also pointed out that understanding or replicating human intelligence is not necessarily the goal of AI. Early attempts to imitate natural flight like birds do failed. Airplanes fly faster, higher, and better than anything in nature.
Similarly, machines may learn faster from each other than humans. Google’s Deepmind AlphaGo first beat one of the world’s best Go players in 2016. In 2017, Google announced that AlphaGo Zero, a version of the algorithms trained by playing itself without human data, beat AlphaGo 100 games to zero. The Singularity may be closer than we think.

Social Impacts
The rapid advances in AI are leading people to think about the social impact, and what machines are learning from the data they consume. With regard to inclusiveness, some examples about what AI may present us create issues. For example, an image search for CEOs on Google presents mostly white males. Is that accurate? Yes, most CEOs today are white males, and Google tailors searches according to your history as well. Does it amplify human bias? Yes, in that the underlying implication is that if you want to become a CEO, you’re much more likely to get there as a white male.
Another example that created an internet uproar in 2015 was an early version of Google Photos mistakenly labeling some people of color. Clearly that was an early dataset training issue. With Apple introducing facial recognition for unlocking phones and payments, and those features quickly becoming more mainstream on other devices, ensuring that training datasets recognize people of color and races becomes critical. More specifically, some fear that algorithms used in the criminal justice system — who to investigate, and how to sentence — disproportionately disadvantage people of color. The reason for that is that the training datasets reflect the history of cultural biases in our society.
It is becoming obvious to many that advances in AI favor certain large companies. Platform companies such as Amazon, Google, Apple, Microsoft, and Facebook have the resources and infrastructure to compete for the best engineers, and also have massive datasets that can train their machine learning algorithms. Some are calling for open data standards and access to datasets for smaller companies to level the playing field.
In particular, governments are thinking hard about this. Some “smart city” initiatives call for partnerships with private companies that use public entity data to help cities modernize and deliver services. Should only one company get access to that data, or perhaps have a temporary monopoly over the use of it to deliver a service? With self-driving cars imminent, what should the models be for sharing traffic information, or information that cars pick up along routes about road conditions, traffic, and weather? For autonomous vehicles in particular, with governmental entities having jurisdiction over their vehicular traffic, how do you create rules and standards for sharing that data across town, city, and state lines?
Ownership and use of data from cars and devices will also heavily affect the quality and deployment speed of AI based solutions. One view that was frequently espoused: Any regulation around AI or data transparency must be application-specific. The issues around autonomous vehicles are much different than issues around inclusiveness or the digital divide (access of services to all economic levels). Blanket regulations around data transparency or some overarching standard that doesn’t fit specific use cases would only lead to slowing innovation.
Hives
For a different take on AI, Unanimous A.I., a San Francisco based startup, is taking a cue from nature in using algorithms to amplify human brainpower. Louis Rosenberg, its CEO, is a Stanford PhD, named on over 350 patents, and built the first immersive augmented reality system for the Air Force’s Armstrong lab in the early 1990s. Rosenberg explains the hive concept by noting how bees go about building new homes. Honeybees have less than a million neurons of brainpower compared with a human’s 100 billion. Yet collectively, they form a swarm intelligence, coming to agreement on the complicated task of building a new home that factors in protection from weather, predators, and other issues. They communicate with each other by buzzing their bodies, and end up with the “swarm” achieving a collective intelligence about the right spot to build the hive that no individual bee could muster.
In a similar fashion, Unanimous A.I.’s algorithms use human intelligence to make smarter decisions and predictions. A group (swarm) of 40 movie fans was more accurate than Variety and other experts in predicting this year’s Oscar winners, and in 2016 another swarm of fans picked the top four horses in the Kentucky Derby. The premise is in the wisdom of crowds, but it is not a vote. The swarm essentially measures the confidence of individual’s in their views, their level of flexibility in changing them, as well as dynamics (push and pull within groups) of getting to decisions.
The Turing Test
Computing pioneer Alan Turing proposed the Turing Test in 1950, where a computer would engage in a natural language conversation with a human, and another human would judge whether the computer’s responses are indistinguishable from a human. This test is widely referred to as a test of a computer’s ability to think. No computer or algorithm has yet passed that test. Adam Cheyer, the co-founder of Siri (purchased by Apple in 2010), noted that for all the smarts in voice and language recognition in assistants like Apple’s Siri and Amazon’s Alexa, we are still usually asking the assistant relatively simple commands to perform some action using an application that recognizes a certain set of verbs (“turn off all the lights”), or to search for information about something specific (“show me all the nearby Starbucks”).

Ray Kurzweil is now predicting AI could pass the Turing Test by 2029. Given exponential advances we’ve seen in AI in the past several years, and the 3 billion smartphones in the world with applications storing vast amounts of data to learn from, it seems plausible. Further, Kurzweil predicts 2045 as the year of the Singularity, where computers will actually surpass the abilities of human intelligence. He likens it to an evolution similar to the development of the neocortex in mammals, that led to mammals becoming the dominant species in the post dinosaur era.
What will that bring? Many things, and some of them may be the key to increasing human longevity. Medical nanorobots powered by AI will course through our blood, detecting and fighting pathogens and putting an end to cancers. Other nanorobots will monitor vital organ function and deliver drugs to maintain their function and fight off disease. DNA will be able to be reprogrammed to remove disease markers. Certain long terms trends, like increasing urbanization, may be reversed or tempered. Kurzweil argues that technology enabled living in cities as a way to work, play, and interact with other humans. Tomorrow’s augmented and virtual reality solutions may enable humans to live far from others, yet retain the physical and emotional connection they need. Land use could be further affected by vertical agriculture, powered by alternative energy and AI, that can help feed the world’s growing population.
Should we fear AI? Most of the people that really understand what it can do say that the good — in advances in medicine, automation, food production, and productivity in daily life — outweighs the potential bad parts. Others like Elon Musk and Bill Gates have sounded alarms about the downsides — in the huge economic impact of job displacement, control of information, the capacity to manipulate, and the potentially catastrophic consequences of AI gone bad. The future is still unwritten, of course, but humanity had managed to survive the previous technology revolutions. Perhaps the machines will make us smarter too, keeping us one small step ahead.

Originally published at www.extremetech.com on March 15, 2018.
","AI at SXSW 2018: Of Hives, Ethics, Morals, and the Singularity",57,ai-at-sxsw-2018-of-hives-ethics-morals-and-the-singularity-89058948658f,2018-04-14,2018-04-14 01:22:54,https://medium.com/s/story/ai-at-sxsw-2018-of-hives-ethics-morals-and-the-singularity-89058948658f,False,1634,"All the cutting-edge chip news, software updates, and future science of ExtremeTech, distilled into an easy-to-read format.",,extremetechdotcom,,ExtremeTech Access,jamie_lendino@ziffdavis.com,extremetech-access,"SCIENCE,TECH,FUTURE,SPACE,COMPUTERS",extremetech,Ethics,ethics,Ethics,7787.0,ExtremeTech,"ExtremeTech is the Web’s top destination for news and analysis of emerging science and technology trends, and important software, hardware, and gadgets.",f64ef3d68bc6,extremetech,28669.0,58.0,20181104
0,,0.0,,2018-03-20,2018-03-20 21:07:13,2018-03-20,2018-03-20 21:18:14,1,False,en,2018-03-20,2018-03-20 21:18:58,8,479468451e5f,2.845283018867925,15,0,0,"By Michael Solomon, 10x Management Co-Founder",5,"The Ethics Of Self-Driving Cars: Can We Be Rational With New Technology?

By Michael Solomon, 10x Management Co-Founder
10x represents top tech freelancers and matches them with forward thinking companies that need rapid access to the best and brightest.
Two days ago, a self-driving Uber SUV struck and killed a female pedestrian in Arizona. It is the first fatality involving a pedestrian and a fully autonomous vehicle. The accident is tragic and raises an important question: who’s to blame? When a human driver strikes a pedestrian, blame can usually be placed on one party or the other, or both. We’re used to those situations. But who’s to blame in this situation?
Is it self-driving technology that’s just not ready for real-world tests?
Is it Uber for testing autonomous vehicles before they’re ready?
Is it the safety driver who was in the car and meant to be monitoring the vehicle?
Is it the pedestrian who was crossing the street outside a crosswalk and perhaps not paying close attention?
Is it the state of Arizona for allowing autonomous vehicle tests?
If we’re blaming self-driving technology for the death of this pedestrian, then it seems like a trivial place to stop:
Why not blame Karl Benz, who is largely credited with inventing the automobile in Germany in 1885?
Why not blame Henry Ford, who facilitated mass production of cars with the assembly line?
Why not blame American inventor William Bertelsen, who envisioned and championed self-driving vehicles in the 1960s?
Why not blame me for allowing my government to pass this legislation?
Why not blame the pedestrian’s parents for raising someone who would cross the street outside a crosswalk or without sufficient vigilance to stay safe?
I don’t mean to be insensitive, but technological progress happens all the time, and we’d be foolish to halt our progress because accidents happen. It’s concerning to me that Uber has stopped all autonomous vehicle testing because one pedestrian was killed. What if, because of those autonomous vehicles that were on the roads, lives were saved, because they are, in fact, safer/better than humans?
We need to have an honest conversation about technology and the ethical ramifications of its integration into society. If self-driving cars kill 100 pedestrians per year, and human drivers kill six thousand pedestrians per year in the United States (that’s a fact), then clearly self-driving cars are a net positive for humanity, even though they are not perfect.
We can’t play the blame game. If we did, then we are truly ALL responsible for this death because we all take part in and contribute to technological progress. If we lived in a world where everyone took 100% responsibility for their actions, we would live in a much more empowered world.
I understand that it feels like a grey zone; people don’t know how to react to an autonomous machine killing someone. Unlike a human driver, autonomous vehicles feel no guilt and have no concept that anything bad happened. When humans kill people, we have trials. When dogs kill people, we put them down. We have recourse, we know what to do. But this is new for us. And we don’t know how to react.
My advice in this situation is that we need to be rational. If self-driving cars kill 100 pedestrians per year but save five thousand pedestrians per year, then clearly they are beneficial for us. Let’s get away from our emotions, and away from our fears of autonomous machines. It’s simple logic — self-driving cars will save many more people than they will kill. They will not be perfect, but they don’t need to be perfect. They just have to be better than human drivers for us to adopt them into society. One pedestrian was killed by an autonomous vehicle on Sunday. 16 people were killed by human drivers today.
Michael Solomon is an established entrepreneur and the founder of 10x Management, a prominent tech talent agency. He remains a sought-after voice in the business technology world and makes frequent appearances on Bloomberg Television, MSNBC, and BBC. Michael’s sharp eye for business has helped 10x Management revolutionize the technology sphere.
",The Ethics Of Self-Driving Cars: Can We Be Rational With New Technology?,624,the-ethics-of-self-driving-cars-how-rational-can-we-be-with-new-technology-479468451e5f,2018-03-22,2018-03-22 20:50:07,https://medium.com/s/story/the-ethics-of-self-driving-cars-how-rational-can-we-be-with-new-technology-479468451e5f,False,701,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Michael Solomon,"Michael Solomon is the co-Founder of 10x Management, 10x Agent on Demand, Brick Wall Management and Musicians On Call.",fccdaccec45b,MichaelSolomon69,56.0,16.0,20181104
0,,0.0,3a8144eabfe3,2018-04-12,2018-04-12 18:37:43,2018-04-12,2018-04-12 18:56:00,1,False,en,2018-04-14,2018-04-14 17:16:45,2,83749aee8db5,6.788679245283017,3,0,0,"Sankaet Pathak is one of my favorite people to talk to about innovation in financial services. The company he founded, SynapseFI, plays an…",5,"Machine Learning, Human Values: What Will FinTech Magnify?
Sankaet Pathak is one of my favorite people to talk to about innovation in financial services. The company he founded, SynapseFI, plays an important role in enabling collaboration between banks and financial technology startups. I came away from this recent conversation with a deeper understanding of the ethical risks posed by AI, insights into behavioral data science, and cautious optimism that we can use these tools and technology to create the financial world we want to live in.

DM: What are you’re trying to do at Synapse — both today and aspirationally?
SP: We are trying to make sure that every banking product turns into an API. Most of the innovation in finance has been slowed down because banks are either not able to support fintech companies or they themselves have not been able to innovate to accommodate what Americans really need.
Cost-per-customer needs to go down. If we can create a bank with a fully automated back office, then it does not matter if you keep $100 in your bank account or $100,000. Everybody would be a good customer, regardless of the deposits they hold.
DM: What do you identify as some of the ethical risks in financial services technology today? And can they be mitigated or how can they be mitigated?
SP: All of the ethical issues that we are dealing with in finance are easy to overcome. There are a couple things that we should be very focused on right now: lending practices that are slightly questionable with inherent biases in whatever a banker thinks a good loan candidate looks like, and how banking today is very optimized for spending.
As you start automating lending practices, or even card issuance practices, it is very easy to bake in biases. Automation does not always mean liberation. We have to make sure that as we build these products, we make conscious choices to fight against some of these habits and not just accept the status quo.
Secondly, everything that we’re doing in banking converges on the credit card. Banking is hyper-optimized to be able to sell this spend-money-get-money product. We have to change some of those models in banking.
DM: Are ethics, in your mind, machine-readable? What roles should humans have today versus three to five years from now as AI becomes more prevalent in financial services?
SP: Humans are going to be involved in these processes and decision making for as long as I can see, but it does require us to flex that ethics muscle more. I am worried that a lot of companies just say: Well, we gave a question to this black box and it gave us the answer and whatever the answer is, is the ultimate answer and we’re gonna roll with it.
A famous example is Uber’s surge pricing (maybe now with a change of leadership that will change). When some kind of a disaster happened, Uber’s initial response had mostly been: Oh well, the system does not know, and for that reason it set surge pricing, so it was surge pricing.
Those are not acceptable answers.
You need people on a cross-validation team to look at the results coming out of any machine-learning model on a constant basis to figure out: is it the intended consequence? You also need to look for parallels around race, ethnicity, gender, location, etc. Are we denying consumers, or misbehaving, or ill-treating some costumers because common elements are present? If so, we need to go back and rethink how we’re automating some of this decision-making.
There is this new thing called evolutionary algorithms, which is similar to reinforcement learning. I think evolutionary algorithms over time could off-load some of the ethical decision making, but they are not there yet.
DM: Is this a company-by-company problem, when you say “we,” or is it something that the industry needs to work together on more broadly in working groups or mentorship to really make change?
SP: First and foremost, consumer awareness is critical. The more consumers realize where data science or cognitive science is being used, they can choose to not support a company if they disagree with some practices. It is a big tool that has succeeded recently. We need to encourage more and more awareness to build a feedback loop: the company has to fix themselves. If they do not, consumers will not use them.
Next would be regulatory oversight. CFPB could do a good job in regulating how different data science models and practices are being used in different aspects of banking.
To enable that, I think data science work needs to be done in open source — which Open AI is doing a lot of. This would help people working at regulatory bodies understand how these technologies could be used, so they can start to build some regulatory infrastructure.
Then a lot of it comes down to the company itself. The company has to realize ethics is not just a feel-good thing, it is going to de-risk your business. It is important to care about how you are affecting your end customer, your employees, and your vendors. We have seen companies implode recently because they have not flexed their ethics muscle. I also believe a mission-driven company has better odds of hiring more talented people who would work very hard for them.
DM: How can newer organizations compete with the incumbents that have decades and decades of data?
SP: Synthetic data generation has been very significant over the last year and a half — and not a lot of people are talking about this for some reason. In Synapse’s case, we can synthesize a lot of the government ID data to be able to train different systems to validate government IDs. We have to worry about flash, lighting conditions, wear and tear, orientation, and the angling of the camera. With self-driving cars, you have to be able to synthesize different light conditions, fog, and all of these scenarios before they are even presented. This requires using a little bit of real data and then generating a lot of synthetic data.
It is not just smaller companies that can benefit from the synthetic data generation, but even the larger companies are using it to be able to build technologies they do not have data for today. Most of the training for Google Clips, from my understanding, happened on synthetic data.
DM: What other issues around data should people in financial services be talking about more?
SP: Behavioral science, how products and services tweak people’s emotions and eventually end up tweaking their behavior is significant data science work. I feel we have been very naive about this. We have pretty much said, Building out a product that people come back to use again and again is inherently a good thing. It is not that obvious, because a lot of these things trigger dopamine and make us addicted.
When people start liking our posts on Facebook it makes us feel good, the way Facebook displays that, the way it sends you the notifications, the infinite scroll — all of these things secrete dopamine in your brain. Banking does something similar without even realizing it. A credit card reinforces the behavior that believes spending is good, and not that spending is bad, because if you are not spending then you are not getting your cash-back rewards. I do not want to be too negative about credit cards, but I feel it is kind of a predatory product and regulators have not really figured that out yet.
The credit card can be flipped around easily by asking: How can we change consumer behavior where people get more incentivized by saving than spending? Synapse will launch a credit card — hopefully this year — that sticks with the cash-back reward, but gives it to people at a savings event instead of a spending event. In truth, interchange is how you make the money. The difference here, is you give it to the customer when they save money, as opposed to when they swipe. Then, you can slowly tweak their behavior around how they associate spending and savings.
Companies need to think about the kind of hooks that they are building into their application and how those can affect people in societies in the long run. We cannot just pretend that we do not know this area of research. We cannot say, Oh, we have no clue what you are talking about when you say “dopamine.” We know how it affects people. Now the question is: How can we do more research and figure out how can we use dopamine to improve people’s life in a positive way?
DM: How crucial is the juncture that we’re at right now in terms of what the machines are learning from us or what is being codified into algorithms?
SP: Anything that humans are doing today that is biased, or adversely affecting people’s financial lives, happens in such a distributed way that it is very hard to quantify at a high level. If you automate all of that, then all of these biases get magnified. For instance, when loan officer is writing a loan, they are most likely thinking on some level about the borrower’s clothing, age, gender, and ethnicity. As we sit and talk to them, we are just computing through that in our brain consciously or subconsciously. Now if you go ahead and build a facial recognition system and use that as a part of your underwriting process, you have not mitigated this problem- you have made it infinitely worse. Now it is not thousands and thousands of agents trying to underwrite someone a loan. rather, it is one big agent and whatever bias that one big agent has is going to affect a lot of people.
This is not a hypothetical, we have seen how Facebook was used. As we are automating these things and operating digitally at a larger scale than we have ever operated before, we have to be very, very cautious of how we are using data and what kind of vulnerabilities the system has. We did not realize how vulnerable Facebook was for a decade. We have to be very proactive early on.
DM: Thanks for talking with us Sankaet!
Notes: Dialogue edited for clarity and length.
","Machine Learning, Human Values: What Will FinTech Magnify?",33,machine-learning-human-values-what-will-fintech-magnify-83749aee8db5,2018-04-16,2018-04-16 17:07:37,https://hackernoon.com/machine-learning-human-values-what-will-fintech-magnify-83749aee8db5,False,1746,how hackers start their afternoons.,hackernoon.com,hackernoon,,Hacker Noon,stories@amipublications.com,hacker-daily,"HACKATHONS,HACKING,PROGRAMMING,TECH,HACKER",hackernoon,Ethics,ethics,Ethics,7787.0,David Mort,"FinTech VC @PropelVC. Previously @BBVA, @SVB_Financial. Investor: @Coinbase @PersonalCapital @DocuSign @Hippo_Insurance @TravelBankHQ @EaseCentral",be3589131dfc,DavidMort,295.0,658.0,20181104
0,,0.0,f337f968b38d,2018-05-09,2018-05-09 21:20:37,2018-05-09,2018-05-09 22:19:11,1,True,en,2018-05-09,2018-05-09 22:19:11,0,d3ea263b9577,5.309433962264151,70,4,0,Google’s new AI can carry on phone conversations that are so lifelike that even a human listener can be fooled,5,"A Google Program Can Pass as a Human on the Phone. Should it Be Required to Tell People it’s a Machine?
Google’s new AI can carry on phone conversations that are so lifelike that even a human listener can be fooled
Sundar Pichai, Google’s chief executive, speaks Tuesday during the Google I/O developer conference in Mountain View, Calif. Photo: Justin Sullivan/Getty Images
By Drew Harwell
Google’s artificial-intelligence assistant sounds almost exactly like a human when it calls the salon to book a woman’s hair appointment. It responds to questions, negotiates timing and thanks the receptionist for her help. It even says “um” and “mm-hmm.”
What it doesn’t say, however, is that it’s a machine — and the receptionist doesn’t show any sign that she can tell.
Google’s unveiling on Tuesday of Duplex — an automated voice assistant that can book restaurant reservations, check opening hours and accomplish other tasks over the phone — has thrown a spotlight on how advanced AI can now carry on conversations that are so lifelike that even a human listener can be fooled.
The technology, debuted at Google’s I/O developer conference, could be a huge convenience for anyone who hates picking up the phone. But it is also raising some thorny questions about the ethics of using a machine to copy a person’s voice, carry out commands — and potentially deceive the unsuspecting listener on the other side.
“This technology is amazing, and [a] big step forward, but I don’t think the main goal of AI should be to mimic humans,” said Erik Brynjolfsson, a Massachusetts Institute of Technology professor and director of its Initiative on the Digital Economy. “Instead, AI researchers should make it as easy as possible for humans to tell whether they are interacting with another human or with a machine.”
Researchers at Google said the AI could not carry out general chit-chat but had been trained for the “natural conversations” of specific tasks, such as scheduling appointments over the phone. “The system makes the conversational experience as natural as possible, allowing people to speak normally, like they would to another person, without having to adapt to a machine,” engineers wrote on the company’s AI blog.
Google calls Duplex an “experiment” that a limited number of Google Assistant users will be able to try this summer. When, or whether, it will debut more widely remains an open question. Google has yet to show a live demo.
“We want to be clear about the intent of the call so businesses understand the context,” Google engineers said. “We’ll be experimenting with the right approach over the coming months.”
The company showed several examples, including Duplex calling a restaurant to book a table for four, and in each instance, the listener seemed to have no idea it’s a machine; in one call, the listener asked the AI, “What’s up, man?” and referred to it as “sir.”
Google’s AI engineers trained Duplex on in-call practices that are typically simple for humans but challenging for machines, including “elaborations” (“for when?”), “syncs” (“can you hear me?”), “interruptions” (“can you start over?”) and “pauses” (“can you hold?”).
To prevent it from sounding too stilted or robotic, the system was also taught a number of so-called “speech disfluencies”: The “hmms,” “uhs” and other noises people make in casual conversation. Like humans, the AI makes those sounds to convey that it’s still gathering its thoughts, the engineers said.
Duplex will make its call from an outside number when its user asks it to complete the task; the human won’t be able to listen in or intervene. In cases where the task is too complex or the call goes awry, Google says, the AI will pass the call to a human operator.
Automated voice assistants, such as Amazon’s Alexa and Apple’s Siri, have quickly become a key part of how people interact with the computers in their lives, and many callers today are familiar with the automated voices of modern-day telemarketers, customer service lines and robocalls.
But Duplex would inject that AI into a new kind of arena, with listeners who have not consented or don’t realize they’re talking to a machine. Google representatives did not respond to questions about how Duplex would operate in conversation, including whether it would announce its non-humanness. Yossi Matias, Google’s vice president of engineering, told CNET that the software would “likely” tell the person on the other end that he or she is talking to an AI.
From the charming Samantha of “Her” to the coldly murderous HAL 9000 of “2001: A Space Odyssey,” lifelike AI assistants have long served as a hallmark of science fiction, and Duplex’s convincing fakery had some listeners unnerved about how far the technology had come. Some listeners said the Duplex calls appeared able to pass a simple “Turing test,” the famous yardstick for whether a machine can act or speak so convincingly that it’d be hard to distinguish it from a real person.
“A lot of folks have drawn attention to the risks of AIs masquerading as humans, which Duplex seems to normalize,” said Miles Brundage, a research fellow at the University of Oxford’s Future of Humanity Institute. “At the very least Google should seriously consider some sort of notification that people are interacting with an AI.”
That kind of notification, Brundage said, would help educate people about the advanced state of AI. It would also potentially prevent the kinds of havoc that could result when a machine mimics a human being. In a recent report on “malicious AI,” Brundage and his co-authors posited a series of unnerving examples, including how an AI could copy someone’s voice to fool a listener or seek information as part of an automated “social engineering attack.” A Google official said it takes the issue of synthetic content used to spread misinformation very seriously.
It’s also unclear how Google would navigate legal concerns such as the Federal Communications Commission’s telemarketing and robocall laws. Those rules ban companies from using an “artificial or prerecorded voice” to make calls to certain establishments and set guidelines for how similar voice systems should operate, including requiring that each call clearly identify the “business, individual or other entity initiating the call.”
A Google official said the service was different from those calls because it’s not for solicitation or telemarketing. The official added that the automated assistant will only call companies on phone numbers offered to the public for booking appointments or doing business.
Madeline Lamo, a University of Washington graduate student researching robotic harms and free speech, said the Google AI could also effectively flip the robocall dynamic on its head. “Instead of vendors and scammers using AI to contact potential consumers/scam victims en masse,” she said, “the consumers are now empowered to make robocalls themselves.”
She cited a scene from the TV show “The Office,” in which a scheming assistant to the regional manager, Dwight Schrute, makes 50 restaurant reservations and then sells them off to desperate callers — what he calls his “perfect Valentine’s Day.” “People with AI-powered assistants who can easily make those 50 restaurant reservations would harm both businesses and consumers,” she said.
AI experts have in recent years called for legal or ethical guidelines that could help curb that kind of mischief. The Columbia University professor Tim Wu in 2017 called for “Blade Runner” laws that would prevent a company from deploying human-copying machines that hide their true identity.
There’s a natural tension for those kinds of rules: Google wants its AI to be as convincing — and, yes, lifelike — as humanly possible, to ensure the listener gives compelling responses — and, hopefully, doesn’t hang up.
But Brynjolfsson thinks there should be a middle ground to ensure humans aren’t left wondering who, or what, they’re talking to. Regulation, he said, may be necessary to require bots to self-identify.
“At a bare minimum, a bot should answer truthfully if a human asks whether it’s a bot,” Brynjolfsson said. “Or perhaps more radically, bots should be required to have a recognizable voice style and or text style and/or appearance. I don’t think this would harm their efficiency. … In fact, it would likely improve it.”
© 2018, The Washington Post
",A Google Program Can Pass as a Human on the Phone.,253,a-google-program-can-pass-as-a-human-on-the-phone-d3ea263b9577,2018-08-25,2018-08-25 01:41:56,https://medium.com/s/story/a-google-program-can-pass-as-a-human-on-the-phone-d3ea263b9577,False,1354,News and analysis from around the world. Founded in 1877.,,washingtonpost,,The Washington Post,,thewashingtonpost,"NEWS,POLITICS,JOURNALISM,TECH",washingtonpost,Ethics,ethics,Ethics,7787.0,Washington Post,News and analysis from around the world. Founded in 1877.,f0c3167dc11d,washingtonpost,232560.0,332.0,20181104
0,,0.0,d621d9aa5ea7,2017-11-30,2017-11-30 22:43:14,2017-11-30,2017-11-30 22:45:46,6,False,en,2018-09-28,2018-09-28 18:52:56,23,ed76171b882c,15.010377358490565,29,0,0,"Just as it has changed the methods of science and engineering, the tools of large scale data analytics have caused major shifts in how we…",4,"“The study has been approved by the IRB”: Gayface AI, research hype and the pervasive data ethics gap

Just as it has changed the methods of science and engineering, the tools of large scale data analytics have caused major shifts in how we judge the ethical consequences of scientific research. And our current methods are not keeping up. Historically, research ethics has been animated by a core set of questions, such as how do you decide if a scientific experiment is justified given the potential risks and benefits to the people being studied, or to society at large? How do you track who has to bear those risks and who gets the benefits?
Now we are faced with the problem of what happens to the methods we have developed for answering those questions when the number of people affected by a study jumps by multiple orders of magnitude over the historical norm.
When we talk about research ethics, we’re ultimately addressing two distinct questions: 1) what are the correct/ideal norms for judging the experimental methods, and 2) what determination is made by the institutions (such as IRBs) tasked with regulating researchers and protecting research subjects from harms. It turns out that too often when we discuss data science the answers to those two questions diverge, resulting in a situation where we cannot effectively track and mitigate the ethical and social consequences of the research. Thus when data scientists cite IRB review as a certificate of ethical methods, they are misreading the purpose and scope of IRBs.

The recent controversy over a pre-print paper (in press at the journal Personality and Social Psychology) by Michal Kosinski and Yilun Wang of Stanford illustrates a number of the new potential risks of social and behavioral data science. What makes this study so interesting from a research ethics perspective is that the scientists and their critics alike agree it would be highly troublesome if their results were applied in the wild; where they disagree is whether the study is justifiable under those conditions. This study falls precisely in the gap opened by data analytics between what we would hope are ideal ethical conditions for scientific experiments and the decisions made by institutions tasked with protecting research subjects.
In their paper, “Deep neural networks are more accurate than humans at detecting sexual orientation from facial images,” Kosinski and Wang describe a machine learning study in which they trained a deep neural network to sort human faces according to sexual preference at a greater accuracy than their control group of human sorters.* Put colloquially, they built an early prototype of an artificial intelligence “gaydar” using off-the-shelf machine-learning components and “publicly-available” data. Many accounts have referred to it as “gayface AI,” using the slang term for an exaggerated stereotypically gay male facial expression.
What makes this study so interesting from a research ethics perspective is that the scientists and their critics alike agree it would be highly troublesome if their results were applied in the wild; where they disagree is whether the study is justifiable under those conditions.
The press ate this up as a it deftly hits on a number of cultural hot spots: social media privacy, artificial intelligence, and sexual difference. Here is a sampling of coverage from mid-September 2017 when the paper was first noticed by the press:
Advances in AI are used to spot signs of sexuality, in The Economist
How Good Is Your Gaydar? How Good Is Your Science?, in Inside Higher Ed
LGBT groups denounce ‘dangerous’ AI that uses your face to guess sexuality, in The Guardian
Researchers use facial recognition tools to predict sexual orientation. LGBT groups aren’t happy in the Washington Post.
That AI study which claims to guess whether you’re gay or straight is flawed and dangerous, in Mashable
Skeptics of Big Tech and machine learning were alongside LGBT advocates in expressing strong reservations about this study. There are a number of solid methodological critiques of Kosinski and Wang’s study, most notably from Greggor Mattson from Oberlin College and Carl Bergstrom and Jevin West of the Calling Bullshit blog. Additionally, LGBT groups like GLAAD and HRC were quick to point to the consequences of the methodological blind spots in the paper.
Here I want to explore the particularly toxic brew around research ethics and research hype which has largely not been covered elsewhere, because it illustrates how research ethics regulations struggle to address the methods and consequences of pervasive data research. In particular, data science has the potential to weaponize general knowledge about a population (e.g., algorithms can predict sexual orientation from photographs with X degree of certainty) as a source of leverage in the lives of specific individuals outside of the study (e.g., the algorithm says this person is homosexual). General knowledge about a population is the definitional hallmark of research in our ethics regulations, but harm done to people outside of the study is entirely invisible to those same regulations.
The study
Kosinski and Wang describe a study in which they received 35,326 “publicly available” pictures of 14,776 individuals enrolled on a popular American dating site on which users self-identify as seeking heterosexual or homosexual romantic partnerships. Half of the photos are from people seeking heterosexual relationships, half from people seeking homosexual relationships. Faces were sorted into gender categories, with roughly half men and women. The study was limited to Caucasian faces, due to what the authors claimed was a lack of other racial/ethnic groups in the available training set. (Reducing the diversity of faces also increases the likelihood the machine will identify a strong pattern.) Faces were controlled for transient factors such as lighting, head tilt and pitch, and whether the photo was large and full enough. The machine’s sorting was confirmed by Mechanical Turk workers who were instructed to sort photos by ethnicity and gender using criteria that would be familiar to contemporary American audiences. Kosinski used his and his girlfriend’s pictures as the prototypical white male and female faces, Barack Obama’s face as Black even though he is biracial, and a stock photo of someone who is “clearly” Latino.
Instructions given to Mechanical Turk workers for identifying Caucasian males, pg. 46 of Wang and Kosinki 2017. Kosinski is the prototypical white male.
The researchers divided the sample into 20 subsets, reserving one for a test set and using the others for training an open-source deep neural network optimized for facial recognition, called VGGFace. Setting self-identified sexual orientation as the dependent variable and 500 facial features as the independent variables, they trained an algorithm to recognize patterns of facial features of self-identified heterosexuals and homosexuals. Again using Mechanical Turk workers, the same test set was offered to human testers to gauge sexual orientation based on facial photos alone.
When the algorithm developed from the learning set was used on the test set, its predictions about sexual orientation were accurate in 81% of cases for men, and in 71% of cases for women. When the algorithm was given 5 different photos of the same face the accuracy of the algorithm increased to 91% and 83%, respectively. The MTurk judges were significantly less accurate: 61% for men and 54% for women. Remembering that 50% accuracy is what you would expect with just random guesses when presented with binary options, it turns out that untrained humans are not all that great at judging sexual orientation with facial cues alone, and that trained machines are better but not perfect. Furthermore, the machine did much worse when it did not choose between binaries with one person known to be gay and ther other known to be straight, but rather had to make a decision one face at a time.
That is in a nutshell what the study demonstrated: untrained humans are significantly less accurate than a trained computer vision algorithm at discerning patterns in facial structures correlated with self-identified sexual preferences among an artificially constrained group of people.
The claims
Ultimately the core findings of the study are interesting but modest. The reason this study was explosive, especially to the press, are the explicit and implicit assumptions about the intrinsic nature of sexual orientation.
There is a long history of the search for a “scientific gaydar,” as my PERVADE colleague Matt Bietz put it. A scientific gaydar could only work if there were intrinsic traits that provide a signal — genes, morphology, biochemistry, etc. — strongly correlated with sexual orientation, which is otherwise only observable as a behavior. Some scientists have long been certain that sexual orientation is so significant that there must be a signal to be found (Mattson’s blog post cited above has a brief history of this phenomenon and this review article cited by Kosinski and Wang is a thorough look at research on the causes of homosexuality). Find the signal, and it can likely be traced to a cause. Find two signals that are correlated and then you have a fairly robust causal claim. Notably, the search for a scientific gaydar often takes the form of looking for what makes homosexuals different rather than what makes all people have a sexual orientation at all.
Along the way, the authors methodologically reduce the complexity and diversity of the biosocial phenomena they claim to be studying: gender, sexual preference and facial morphology. Such methodological points may seem to stray from the concerns of human subjects research ethics, but it has a direct consequence for how the study was constructed and its potential downstream effects. Controlling such variables is certainly a legitimate approach to empirical research, but it should properly cause scientists to dial down the implications of their outcomes. It should take a lot of conceptual and empirical work to reduce a complex phenomenon like sexual preference and gender to a binary dependent variable and then build it back up again to make even a hedged generalizable claim about the nature of the complex phenomenon.
Instead, Kosinski and Wang swing for the fences by claiming their study supports one of the plausible theories about biological causes sexual orientation: prenatal hormone theory (PHT). PHT holds that sexual orientation is determined at least in part by the level of androgens that a fetus is exposed to in utero. The typical way of framing it is that low androgens cause homosexuality (defined as gender atypicality) in male fetuses and high androgen causes homosexuality in female fetuses. PHT is an unproven but entirely mainstream theory about biological roots of sexual preference. Because androgens are also known to affect facial structures in fetal development (and when adults take testosterone), it is plausible that facial structures may be a machine-readable signal of sexual preference due to a common cause.
Thus, the core premise, though not stated explicitly, of Kosinski and Wang’s paper is that if sophisticated facial recognition software can correlate subtle facial structures with sexual orientation with a reasonable degree of accuracy, then there is support for a common intrinsic cause for both facial structure and sexual orientation, i.e., PHT (see chart 1). Despite their claims to the contrary in the author’s notes, the paper only holds together if they are at least implying that the deep neural network found the traces of an inherent basis of human sexual preference.
Chart 1: My explanation of the causal scheme presupposed by Kosinski and Wang.
But do they actually do the work to architect this reduced phenomenon (AI can sometimes correlate facial structures with binary categories of sexual orientation) into support for a global explanation of sexual orientation (prenatal androgen exposure determines sexual orientation)? In short, no.
The easiest explanation as to why not: there is no embryological, biochemical or social psychology research done in this paper.
The actual research done in this paper claims to correlate the machine-detected facial structures of Caucasian binary-gendered men and women with self-identified binary sexual orientation (1. In chart 1). Yet a surprising amount of the paper and subsequent press focusses on claims about PHT (2. in chart 1). In fairness, the authors are careful about using the terminology “is consistent with PHT” throughout the paper, which is an appropriately hedged claim. On the other hand, for a research project that didn’t measure a single micrometer of human blood (but did study facial hair grooming habits and baseball caps) their paper uses a surprising amount of ink discussing androgen levels. Logically, the only way that gayface AI points to intrinsic traits rather than transient factors is reference to biology, but the paper is not presenting biological research. That a study utilizing deep neural networks to analyze social media data makes any claims — even if strictly hedged — about intrinsic causes of sexual behavior in humans ought to be surprising.
“It passed the IRB”
In the paper, interviews and social media the authors raise the specter of discrimination in a post-privacy world as an ethical justification for their study. In the general discussion section of the paper, they write:
Such pictures are often easily accessible; Facebook, LinkedIn, and Google Plus profile pictures, for instance, are public by default and can be accessed by anyone on the Internet. Our findings suggest that such publicly available data and conventional machine learning tools could be employed to build accurate sexual orientation classifiers. As much of the signal seems to be provided by fixed morphological features, such methods could be deployed to detect sexual orientation without a person’s consent or knowledge. …
Some people may wonder if such findings should be made public lest they inspire the very application that we are warning against. We share this concern. However, as the governments and companies seem to be already deploying face-based classifiers aimed at detecting intimate traits (Chin & Lin, 2017; Lubin, 2016), there is an urgent need for making policymakers, the general public, and gay communities aware of the risks that they might be facing already. Delaying or abandoning the publication of these findings could deprive individuals of the chance to take preventive measures and policymakers the ability to introduce legislation to protect people.
In other words, Kosinski and Wang were motivated to conduct this study not to create machine learning tools for discrimination, but to show that off-the-shelf machine learning tools can be used to facilitate discrimination because Internet data discloses innate, private characteristics that we cannot hide. On the one hand, it is banally predictable that the consequences of machine-learning-enabled surveillance will fall disproportionately on demographic minorities. On the other hand, queer folks hardly need data scientists scrutinizing their jawlines and hairstyles to warm them about this. They have always known this.
By ANDRADA BĂLEANU, from https://www.huffingtonpost.com/entry/a-more-queer-bucharest_us_57f50b9be4b0b7215072caef
For an example of how such a tool could go wrong, consider the ease with which a gayface plugin could be incorporated into the new AI-facilitated customs procedures at Dubai’s airports, a country where consensual homosexual acts are punishable by prison sentences. Or how the government of Chechnya could use a gayface algorithm to entrap and purge homosexual men using surveillance cameras. And as Mattson says, there’s plenty of reason to be concerned about the “bathroom police” closer to home using AI to humiliate and persecute transgendered people.
Kosinski appears to recognize this threat, and to his credit did not release the gayface algorithm as open-source tool (unlike his previous brushes with public controversy). Indeed, one of the most interesting aspects of the gayface controversy is that the authors and their critics agree on one central point: most plausible use cases for the tool that they built are ethically terrible. Yet they disagree about whether that means the study functions as an effective warning about possible future harms or is an ethical lapse in itself.
So is their suggestion that this research is necessary because it functions as a warning call actually ethically justifiable? Here’s where the matter of research methodology and ethics regulation becomes paramount.
Noted prominently on the cover page of the preprint article, the authors state “The study has been approved by the IRB [Institutional Review Board] at Stanford University.” In the below exchange about ethical justifications of the study on Twitter, Kosinski again refers to the Stanford IRB’s approval:
https://twitter.com/michalkosinski/status/906525285394403328
Does that mean the study is ethical? Not at all. As I have argued in the past (1, 2, 3), research ethics regulation in the form of university IRBs is poorly suited to interrogating the methods of data science.
The core mission of IRBs is to protect individual research subjects from the potential harms caused to them by the research methodologies. The Common Rule — the U.S. federal law governing how IRBs should regulate human subjects research — only kicks in when the research methodology meets two conditions: 1) the research creates generalizable knowledge from datasets containing new and non-public data, and 2) acquiring that data requires intervening in a person’s life (interview or psychological experiment) or body (blood draw or medicine) in a way that poses more than normal daily risks. These assumptions make sense using traditional research methods: you don’t need ethics supervision if you are studying anonymized public census data, but you probably do need supervision if you are using deceptive tactics in a psychology study about sensitive personality traits. Furthermore, IRBs are legislatively forbidden to consider downstream consequences for people outside of the study. They are strictly tasked with controlling risk to individual study participants posed by the proposed research methods.
Pervasive datasets dramatically change the research ethics landscape. The research methods and risks have changed, but the regulations have not.
Pervasive datasets dramatically change the research ethics landscape. The research methods and risks have changed, but the regulations have not. The vast majority of research that uses “big data” in one way or another does not fall under the purview of IRBs because 1) it does not create new data, it uses existing data as a learning set; 2) the data it uses is considered public, which includes data that can be purchased, lent, or gleaned from an Internet service like Facebook or OkCupid; and 3) it does not require any contact (“intervention”) with the individuals whose data is being used.
Although we can’t know for sure without Stanford releasing their IRB application (which are typically never viewed by the public), what Kosinski means when he says that “the study has been approved by the IRB” is likely just that the IRB decided his research does not create new data in such a way that poses risk to individual research subjects. Which is technically correct because there is no additional risk to the people whose facial images were anonymously used in the study. After all, they already outed themselves in “public” and put their pictures on a dating site.
IRBs are specifically mandated to avoid even considering the types of harms this research poses, which is downstream consequences to groups of people or society overall. Pervasive data of the type they draw upon is distinct from the type historically familiar to IRBs. Machine learning tools are designed to leverage general knowledge about patterns in a population in order to have an effect on individuals at a later point. This is an inverse of the traditional pattern of potential risks and benefits in human subjects research, wherein studying individuals leads to potential effects on populations. Machine learning can be weaponized in ways that traditional psychological or sociological research simply cannot.
As such, data scientists need to be aware that when their research is “approved by an IRB,” it does not mean “the research is ethical.” Rather, it means that whatever harms your research may pose are quite possibly invisible to the IRB’s review process.
As such, data scientists need to be aware that when their research is “approved by an IRB,” it does not mean “the research is ethical.” Rather, it means that whatever harms your research may pose are quite possibly invisible to the IRB’s review process.
In my reading, those data science risks are greatly increased when reported results reach beyond the parameters of the research methods. Kosinski and Wang significantly increase the possibility that their work will be used against individuals by tying their work to claims about biological roots of sexual behavior. It’s actually not clear from their paper why PHT is a necessary component of the project at all. It certainly is not a component of their machine learning experiment as they do not actually measure any phenomena that could be used to empirically confirm or refute PHT.
Whether empirically justified or not, claims about the origin of sexual behavior in biology are consequential to many people’s lives, and it is certainly possible that their over-reaching findings will be leveraged against individuals in automated decision-making. As Kosinski stated, the proper site for protecting individual rights is through politics, not through technologies. But that does not mean the risks of the research are within acceptable limits. And it certainly does not warrant leaning on the judgment of an IRB designed specifically to not consider these types of harms.
Tips for data scientists wanting to approach their work ethically:
IRBs are often ill-suited to judge the most significant consequences of data science work.
IRBs are nonetheless often necessary (but not sufficient).
Data science research needs to be interrogated about downstream consequences because that is the type of harm most likely caused by the methods.
If there are possible harmful consequences that are going to receive public attention, provide advice about avoiding them. Don’t just throw your hands in the air and claim we live in a post-privacy society and individuals are responsible for protecting themselves from malicious data use. Ethical research and design is always a distributed responsibility.
If your work involves sorting people by sensitive demographic categories, discuss the research with those communities and their advocates and listen to what they have to say. Asking for their input implies a responsibility to take it seriously and alter or possibly drop your research agenda to protect them in the manner they ask.
If your work makes or implies empirical claims about other domains of expertise, include collaborators from those domains.
All citations come from the version published on the OSF preprint repository on 09/10/2017. This is the version available when the press first started discussing the study. Updated versions have been published at later dates (most recent version available here), but none that substantially changes any claims made herein.
This publication was partially supported by National Science Foundation Grant #1704425. The views expressed do not represent those of the National Science Foundation.
","“The study has been approved by the IRB”: Gayface AI, research hype and the pervasive data ethics…",226,the-study-has-been-approved-by-the-irb-gayface-ai-research-hype-and-the-pervasive-data-ethics-ed76171b882c,2018-09-28,2018-09-28 18:52:56,https://medium.com/s/story/the-study-has-been-approved-by-the-irb-gayface-ai-research-hype-and-the-pervasive-data-ethics-ed76171b882c,False,3726,NSF-Funded Pervasive Data Ethics for Computational Research: a multi-disciplinary project examining the reuse of personal/social data in computational research,,,,PERVADE TEAM,pervadeischool@gmail.com,pervade-team,"DATA SCIENCE,ETHICS,ACADEMIA,ALGORITHMS,USERS",pervade_team,Ethics,ethics,Ethics,7787.0,Jacob Metcalf,"Tech ethics researcher and consultant. Founder of Ethical Resolve, researcher at Data & Society Research Inst. Dwell in an officebarn amongst the redwoods.",c3c66837467f,undersequoias,136.0,263.0,20181104
0,,0.0,,2018-06-17,2018-06-17 09:49:40,2018-06-15,2018-06-15 23:18:32,9,False,en,2018-06-17,2018-06-17 09:57:20,43,15171c619a14,9.10566037735849,0,0,0,Article 3 of the Series on Ethics in Artificial Intelligence,5,"How the era of artificial intelligence will transform society?
Article 3 of the Series on Ethics in Artificial Intelligence

In the previous article, we talked about the nature of human fears in relation to artificial intelligence (AI). We also outlined two important themes to address in the upcoming challenges of the AI era: the societal, and decision-making aspects of AI systems. In our new article I expand on the theme of the impact on society, and I share our opinion on possible steps we should take to anticipate and adjust to it.
McKinsey analysts estimate the automation potential for all economic sectors to be around 50%. This means that around half of all the activities people in the world’s workforce are paid to do today could potentially be automated with currently available technologies. It represents almost $15 trillion in wages (about China’s GDP today). Historically every industrial revolution (Figure 1) has had its primary driving factor. For example, the adoption of electrical power defined the era of mass-production and led to the economies of scale, impacting both capital and labor. The huge societal challenges arriving with automation resemble the challenges of industrial revolutions.
Figure 1 Timeline of the industrial revolutions.
What lessons can we learn from the past? What are the current drivers and the economic consequences? What is the role of ethics in the ongoing changes?
Industrial revolutions and the Engels’ pause
When we talk about the technological impact on society, we can reference lessons from the past. We know, for example, that as a consequence of the 1stindustrial revolution, mechanization increased the productivity of each worker but real wages stagnated for approximately 50 years.
Figure 2 Engels’ pause in real wages during productivity growth (MGI, 2017; Allen, 2008).
In economic theory, this phenomenon is described as “Engels’ pause”. It explains that due to great technological developments, the lives of a large number of people worsened first before society began to prosper in the longer term (Allen, 2008).
Finally, what could be the driver of growth for the new industries that are emerging? Let’s identify this resource that will enable us to create new types of jobs and transform businesses.
Data is the “new electricity”
Steam, Electricity, Computer & Internet… the drivers of previous industrial revolutions. Let’s now look at what would drive change in an era of AI. Along with algorithms and advanced computation facilities, the accuracy of the models constituting artificial intelligence rely heavily on the availability of real-world data. Data will fuel the development of AI and data is crucial. IDC estimates 10x growth of the available worldwide data by 2025 with almost 50x growth in analyzable data (up to 5.2 ZB). A $138 bn data market is predicted by 451 Research, this number surpasses the GDP of more than 135 countries (IMF).
Figure 3 Sizing of the data market (IDC, 2017; 451 Research, 2017)
Data is the driver of the new industrial era, it is here, growing, and ongoing transformational processes brought by data-driven applications pose new challenges to society. These challenges aren’t in the technical field of AI development but are rooted in our human nature. While 35% of the skills demanded for jobs across industries will change by 2020, at least 1 in 4 workers in OECD countries is already reporting a skills mismatch with regards to the skills demanded by their current jobs (WEF, 2017). The problem is not the absence of jobs, but the skills requirements of new jobs (MIT Sloan Management Review, 2017; WEF, 2018) that are already being created today (see AI Teacher).
How will humanity surpass and adjust to the possible Engels’ pause that may result from adoption of AI and automation?
Absorption of skills and the new industries
Throughout history, the decline of some large-scale employment sectors has been countered by the growth of new sectors that absorbed workers. The recent example of the emergence of personal computers shows us that technology drives the creation of many more jobs than it destroys over time, mainly outside the industry itself (Figure 4). It is estimated that from 1970 to 2015 the personal computer industry destroyed almost 3M jobs in the USA while creating more than 19M jobs in other sectors. Such increase has resulted in fast economic growth in the services sector, in trade and in the creation of new industries such as the software industry.
Figure 4 Birth of the knowledge economy (MGI, 2017; IPUMS, 2017).
In the information era that started in the 1960s, the main driver of change was the dissemination of computers and, consequently, global access to the internet. This, in turn, has created a so-called knowledge economy.
Products and services based on knowledge-intensive activities that contribute to an accelerated pace of technical and scientific advance, as well as rapid obsolescence.
//Knowledge Economy (Powell & Snellman, 2004)
In the knowledge economy, it became possible for us to deliver services remotely, outsource business processes, and to dematerialize the whole concept of economic transactions with internet-based payment systems.
What skills will the workplaces of the future require? In previous articles we approached this question a few times, and we keep reflecting on it.
Education, employability and the future of work
Why do we talk about education in this series of articles about ethics? We do so because we are able to re-use lessons learned from history, re-think our future, and re-skill individuals to benefit the economy and guarantee their right to work.
Everyone has the right to work, to free choice of employment, to just and favorable conditions of work and to protection against unemployment.
//Article 23.1 of the Universal Declaration of Human Rights
In their book, Milton and Rose Friedman said, “the essential part of economic freedom is freedom to use the resources we possess in accordance with our own values — freedom to enter any occupation, engage in any business enterprise, buy from and sell to anyone else, so long as we do so on a strictly voluntary basis“.
What do we need to support such freedom? Technological advances may lead to income inequality with higher incomes for workers whose skills are complemented by technology, but not for those whose skills are substituted by it. Economists at the Bank of Canada suggest: “the greatest productivity benefits will occur in firms with high-quality people-management and decision-making processes and high levels of human capital”.
Developing human capital requires education. In the past, one had to spend a lot of time digging in libraries with limited opportunities to access information or find a teacher. Today most answers are available in our smartphones and are accessible 24/7. Still, according to the statistics, people in search of employment are not spending sufficient amounts of time in re-education and re-skilling.
Figure 5 Most common activities for many who don’t work (KPCB Internet Trends, 2018).
Why is this so? Today’s formal education system is based on outdated industrial learning methods (Rolff, 1993; Banathy, 1993; Gray, 2009; Rose, 2012). Technology is moving fast, making many courses obsolete even before learners graduate. Let’s just think about this: teachers learn and prepare course materials, they conduct classes, design evaluation systems and help students to graduate. By the time all this happens, the information taught has become outdated.
We have been, and still are, working today with an “information-push approach” where the teacher and the content are put in the center and where the learner and the problem are not. This approach has proved insufficient and should be reformed (OECD Report 2018). With the introduction of MOOCs, we address the availability of and access to knowledge with more ease. However, the engagement of a learner, who is overloaded with disparate pieces of information, remains a challenge.
Let me recap on this: we know the change is coming with AI and automation, we know that this change will have implications for the workers, we cannot prevent change. Therefore, we need to help people to adjust to change with maximum benefit for them by enabling exponential thinking (Figure 6) so that humans and AI will be complimentary. Blaming machines for this issue is not a solution. Employability of human capital in the 21stcentury will require new sets of skills: resilience, critical thinking, social skills and the ability to learn, reflect, and quickly adapt to change.
Figure 6 Adaptability and exponential thinking with the right mindset and tools (SU, 2018).
Acquisition of such skills requires new learning methods and discussion about them worth a separate article, if not a series of research projects, conferences, and debates. Life-long learning requires creative destruction, where the word creative is the key. The word Creativity is rooted in creation, the creation of new meanings, new interactions, new products, and new markets. In the abyss of the great depression and hardship in 1933, the opening lines of Franklin Roosevelt’s inauguration speech were, “…the only thing we have to fear is fear itself.” Becoming comfortable with uncertainty is an essential component of any creative process, especially when creating a positive future.
Team PocketConfidant believes that education is fundamental to prepare our society for the future. We need a model of education that helps each individual to learn faster and easier, based on personality, strengths, and interests. The new model of education should help anticipate the never-ending change in life while accounting for personal values, desires, and goals. This is how Team PocketConfidant started to work with the Education system in 2017, as a first step to developing new ways of supporting learning methods and academic innovation.
How should we govern in the AI era to positively impact privacy, security and wealth distribution?
As Peter Drucker once said, “the best way to predict the future is to create it”. I think that three of the following approaches could help in society’s regulation of the impact of AI.
Industry-led assessment of the impact on each stakeholder;
An inclusive, ongoing and open dialogue between experts and citizens;
Governmental regulations based on the nature of each AI solution.
Industry-led impact assessment
It is very hard to estimate the impact of artificial intelligence on society in general. Yet, we could estimate, regulate and design the impact of artificial intelligence on an individual basis — industry by industry, technology by technology and role by role. I propose a very simple framework (Figure 7) to approach the impact evaluation:
Figure 7 Impact evaluation on a role-by-role basis.
I think that a basic framework such as this could be used by each industry consortium and community to begin the dialog and craft targeted approaches.
Inclusive, ongoing and open dialogue between experts and citizens
An inclusive dialogue between experts and citizens can lead to inclusive societies where humans and AI successfully work together. Public dialogue is useful when a topic is controversial or complex. As reported by RSA and suggested by Diane Beddoes, Director of Deliberate Thinking, there are advantages, limitations and challenges to building open dialogue with citizen juries:
Figure 8 Advantages and limitations of the open dialogue between experts and citizens.
Governmental and cross-border governance
As in previous transitions between industrial eras, developing education, awareness and continuous learning will be vital. Distribution of income, is equally important. While yields in productivity will increase, there is no guarantee that these yields will be shared fairly. Governments have a responsibility to address issues of inequality. They can use tools such as taxation and transfers, however such use will involve difficult trade-offs related to preserving incentives to invest in technology. As pointed out by Carolyn Wilkins, senior deputy governor in Bank of Canada, increased market power for some AI players may raise important systemic issues, many of them being global in nature. In today’s globalized world, international regulations will also be challenging, given how easy it is to move intellectual property to unregulated jurisdictions. Therefore, countries and economic areas that create favorable conditions for artificial intelligence solutions that augment human capabilities, are expected to largely benefit from technology progress. For example, France’s AI mission for humanity aims to develop complementarity between humans and machines (Proposal 4.2) and to support AI-based social innovations (Proposal 7.3).
Conclusions and suggested next steps
In this article, through the prism of industrial revolutions, I covered some of the anticipated transformational changes artificial intelligence will bring. Upcoming transformations will be disruptive in nature and will change the future of work and the way we develop our human skills. At the same time, in PocketConfidant we are positive about the potential of transformative changes and we think that they can benefit society if anticipated well. We believe appropriate anticipation and preparation would require 1) industry-led assessments with identification of skills and learning approaches to future-proof the workforce, 2) Creation of an inclusive dialog between experts and citizens, and 3) Governmental and targeted cross-border governance for each concrete application of AI.
In our next article I will discuss the governance and ethics of AI decision-making and will share our opinion on inclusive AI design.
Originally published at pocketconfidant.com on June 15, 2018.
",How the era of artificial intelligence will transform society?,0,how-the-era-of-artificial-intelligence-will-transform-society-15171c619a14,2018-06-18,2018-06-18 18:48:59,https://medium.com/s/story/how-the-era-of-artificial-intelligence-will-transform-society-15171c619a14,False,2095,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Nikita Lukianets,"Founder, CTO @PocketConfidant AI. I work in the R&D with interests ultimately related to the fields of Computational Neuroscience and Artificial Intelligence.",deaa66463d16,lukianets,240.0,236.0,20181104
0,,0.0,,2018-07-04,2018-07-04 17:34:18,2018-07-05,2018-07-05 14:36:47,1,False,en,2018-07-05,2018-07-05 14:44:37,10,3eedfb76be18,2.8830188679245285,72,2,0,TL;DR: Here’s the spreadsheet. Now read on for context!,5,"Tech Ethics Curricula: A Collection of Syllabi

TL;DR: Here’s the spreadsheet. Now read on for context!
In November 2017, a New York Times op-ed accused academics of being “asleep at the wheel” when it comes to tech ethics. A flurry of response and conversation ensued, amassing many tweetstorms and counter-op-eds, including this one from my collaborators on the PERVADE data ethics project.
Tugging on different threads pulled the conversation in different ways — for example, lack of resources for less-recognized disciplines such as STS or information studies, or the fact that technology critiques often come from already marginalized voices. One of these threads focused on technology ethics education, or rather, as many random people on Twitter tried to tell me, the lack of it.
I’m not going to try to tell you that universities as a whole(and particularly departments like computer science) are doing a super great job of teaching ethics. Instead, it’s highly variable. What I can tell you is that there are some super great classes and faculty in this space out there — and this is what I wanted to showcase back in November.
I started a Google spreadsheet to track links to tech ethics syllabi. I made it openly editable, because I worried that hand curation would result in me being a too-busy-assistant-professor bottleneck. I seeded it only with the ethics and policy class that I teach. I tweeted it. It got a lot of attention in part thanks to Boing Boing. It did not take long for me to make my point — that these classes are out there. A couple of months later, the New York Times also wrote about tech ethics classes, though largely presented these classes as something new, born from the bad PR of data science ethics scandals. A lot of the classes on this spreadsheet are not new— though I’m glad that there are even more now!
Besides simply the number of classes represented here, I think another point this data makes well is how spread across disciplines these classes are. About 50 are taught in computer science departments, but the rest come from information science/studies, communication, law, philosophy, and others.
The spreadsheet has also become an amazing living document, improved by others. It’s prettier now, more organized. Others have added pivot tables, and additional resources like computing ethics education research papers. Today, there are almost 200 courses in this collection. Many people have told me that this resource has helped them refine or create syllabi, or argue for the value of such a class.
I’m writing this post now in part because multiple people have told me that it would be useful to have something other than the spreadsheet to cite or to point people to, for context. Though I (as someone who studies research ethics for online content) would also encourage yo uto be thoughtful about how you cite, reference, or turn-into-data individual entries on this list, particularly in the course listings that might not otherwise be public.
If you improved this spreadsheet in some way, I’d like to credit you in this post, so feel free to let me know! (I’ll also note that I do occasionally save copies of the sheet in case there is ever vandalism, though it’s been great that this hasn’t happened so far! If you see it, please fix it. :) )
Finally, I would like to encourage anyone reading this who teaches any kind of technology-related course to consider using this resource to help you inject ethics into it. I think it is wonderful to see so many standalone courses, but I also think that real progress will be made when we see ethics as part of everyday practice, which means that it’s taught as part of machine learning, data science, design, even introductory programming. (Based on research I’ve been working on, I can tell you this is fairly uncommon right now — but also, getting better!) Ethics is not a specialization.
Please consider sharing this and also adding to the collection your own courses. You are welcome to reach out to me (casey.fiesler@colorado.edu) with any questions. Thank you to everyone who has contributed and turned this into such a great resource!
",Tech Ethics Curricula: A Collection of Syllabi,420,tech-ethics-curricula-a-collection-of-syllabi-3eedfb76be18,2018-07-05,2018-07-05 14:44:37,https://medium.com/s/story/tech-ethics-curricula-a-collection-of-syllabi-3eedfb76be18,False,711,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Casey Fiesler,"Faculty in Information Science at CU Boulder. Social computing, copyright, ethics, women in tech, fan communities, geekery. www.caseyfiesler.com",941ba1546cc4,cfiesler,509.0,266.0,20181104
0,,0.0,7f60cf5620c9,2018-05-22,2018-05-22 14:56:35,2018-06-05,2018-06-05 23:01:19,6,False,en,2018-06-18,2018-06-18 16:50:15,22,c201c03bc0ac,13.94622641509434,13,0,0,This article is part of a series on the short-term and long term ethical concerns posed by artificial intelligence. If you have a technical…,5,"The Short-Term Ethical Concerns of AI
This article is part of a series on the short-term and long term ethical concerns posed by artificial intelligence. If you have a technical understanding of AI, make sure to skip the first two parts.
Hype around artificial intelligence breakthroughs consistently embroil us in a hysteria that reminds us we’re at the precipice of losing everything we care about to machines. From articles that suggest that machines have developed their own language to pop culture-inspired depictions of the dangers of Artificial Intelligence, we are slowly losing the ability to filter fact from fiction and failing to see the signal in the noise. This article is not by any means an exhaustive list of all the ethical concerns raised by Artificial Intelligence and its progress, but a primer into properly defining what AI is, and what it isn’t, and a pragmatic look into some of the short-term drawbacks and dangers algorithms pose on our the labor market, civil liberties and politics.
Narrow Artificial Intelligence vs Artificial General Intelligence
In order to fully dissect the broad ethical concerns of Artificial Intelligence and how this space is evolving, it is essential to have a proper understanding of Artificial Intelligence. Artificial Intelligence (hereafter called AI) is a computer system that can perform tasks usually reserved for human cognition. These tasks can range from narrow tasks to theoretically more generalized tasks. Narrow AI, is the application of intelligent systems in narrow domains and use cases, such as playing chess, driving cars and identifying cancers on X-Rays. To give a concrete example of narrow AI, Waymo’s autonomous car system may be excellent at driving cars, but it can never drive a plane, nor can it play chess. Alternatively, Generalized Artificial Intelligence (to be referred to as AGI ) is an AI system that can achieve a myriad of tasks. It is essential to note that AGI systems do not exist (yet), and that all AI applications today are considered narrow. The closest thing we have obtained towards some form of general intelligence is Deepmind’s AlphaZero, that was able to conquer the games of Go, Chess and Shogi without any prior instructions. Our conceptualization of AGI, both on how to get there and it’s broad ethical concerns remains grounded in theory. However, this is a crucial research area as powerful AGI, without proper value alignment (more on value alignment in a future blog post), has the capacity to fundamentally change the world we live in.
The AI Technical Landscape
Going beyond just segmenting AI into narrow AI and AGI, it is worthwhile to have a faint understanding of the technical landscape in current AI applications. Recently, popular media has been using terms such as “artificial intelligence”, “machine learning” and “deep learning” interchangeably. However, in order to have a clearer understanding of the ethical concerns in AI, distinctions between these terms must be made. We can think of these three terms as concentric circles with artificial intelligence encompassing machine learning, and machine learning encompassing deep learning.
Machine learning revolves around creating systems that can learn useful patterns from large data sets, and provide useful insights as a consequence. Machine learning in itself is divided into three main categories — the first being supervised learning, which entails creating systems that understand the relationship between a set of data points (inputs) and labels (outputs), thus providing outcomes on unlabeled data points. Examples include systems which classify whether borrowers would default on a loan or not, systems which predict future stock prices etc … Alternatively, unsupervised learning is constructing systems that can identify meaningful patterns from a data set simply based on similar features or characteristics. An example of this would be clustering customers based on similar shopping behaviors, identifying fraudulent behavior through anomaly detection, and recommending movies based on users’ similar interests. Finally, reinforcement learning is a branch of machine learning that tries to pit an intelligent agent in a well-defined environment, with a set of possible actions and an objective function (reward) to be maximized. We can think of self driving cars (the agent) driving on a highway (environment) whose sole objective is not committing accidents (reward) for example.
Finally, deep learning is a technique used within machine learning which utilizes vast amounts of data and neural networks — a machine learning technique inspired by the structure of the brain — in order to understand patterns within a data set. The recent explosion in AI breakthroughs in computer vision and speech recognition among other vertices almost all lead back to deep learning research, and more importantly the commoditization of computing power.
Intuitively speaking, think of machine learning as an attempt to model the brain of a child. A child learns from the actions of others (supervised learning), tries to discern similarities between different objects in the world such as grouping similar shaped Lego blocks together (unsupervised learning) and navigates difficult environments such as a jungle gym with no direct input (reinforcement learning). Deep learning is a technique in machine learning that is at the root of recent breakthroughs in AI.
The AI Technical Landscape
Short-Term Ethical Concerns
The short-term ethical concerns shrouding AI at the moment bear absolutely no resemblance to Hollywood reincarnations of humanoid robots looking to take over the planet. Rather than thinking about the dangers of AI through naive anthropomorphism, it is more useful to think of AI as computer systems or algorithms which have the capacity to replace human decision-making in both the public and private spheres. Namely, AI systems will operate “invisibly”, automating information flows between different stakeholders inside the background processes that directly affect our lives. This can range from seemingly benign processes such as the posts we see on Facebook (not so benign after recent scandals), or the movie recommendations we get on Netflix, to more consequential processes such as credit rating and predicting recidivism rates for individuals.
The Trap of Hollywood Inspired Anthropomorphism When Thinking About the Dangers of AI
In fact, the short-term ethical concerns of AI mainly stem from the fragility of the systems governing automatic information flows, their capacity to be co-opted and erode a collective conception of truth. It is therefore useful to view the short-term dangers of AI through its impact on the labor market (i.e. its ability to automate information flows), its capacity to reinforce existing stereotypes and bias and its capacity to galvanize authoritarian tendencies.
I) AI and its impacts on the labor market
In his book “The Third Industrial Revolution”, Jeremy Rifkin postulates that the convergence of innovations in energy, communications and transport technologies are at the root of tectonic economic change. According to Rifkin, the convergence of these innovations, has introduced all previous industrial revolutions. For instance, the development of cheap steam powered press (communications), the discovery of coal (energy) and the invention of trains that run on steam engines (transportation) was at the root of the first industrial revolution. Conversely, he describes the invention of the telephone (communication), the discovery of oil (energy) and the creation of the automobile (transportation) to be at the crux of the second industrial revolution. Looking ahead, he anticipates that the convergence of internet communications technology (communication), the development of renewable energies (energy) and the development of self driving cars and trucks (transportation) will usher in the third industrial revolution — a paradigm where value chains that operate on zero (or almost zero) marginal costs will be coordinated on the “internet of things” through sensors that communicate with each other, all governed by AI systems.
Whether one thinks Rifkin is right or not, is up for debate — but what is not up for debate, is that AI systems are going to automate information flows and erode our need for human decision making at an ever expanding spectrum of economic activity. Much has been said around this paradigm shift, and public anxiety regarding structural changes in the labor market are understandable. However, it is essential to have a clear eyed perspective on the different impacts of AI in the labor market and how it can lead to increasing unemployment rates, changing job dynamics, exacerbating economic inequality and changing global economic inter-dependency.
There is widespread disagreement on the potential adverse impacts of AI systems on unemployment rates. The current research landscape is filled with competing figures ranging from 47% of jobs being at risk of automation in the United States alone, to only 9% of jobs being at risk in the entire OECD economies. The reason behind these discrepancies stem from different methodologies — with the studies pointing towards higher prospective unemployment rates looking at jobs as a whole, whereas ones which predict lower prospective unemployment rates tend look at tasks within a job. I suspect that the truth is somewhere in between — with more in depth analysis needed on the impact of automation on an industry by industry basis, looking across the entire chain of potential occupations within it — ranging from the ones that are highly repetitive (i.e. high risk of automation) to ones that are not (i.e. low risk of automation).
Income Inequality Being at the Heart of 21st Century Populism
The need to properly identify prospective unemployment rates is especially magnified once we take into account the potential of widespread use of AI to exacerbate income inequalities. One point of view is that automation investments will gravitate towards areas of the economy where the comparative advantage of robots will be orders of magnitudes higher than a human (i.e. smart factories), leaving jobs where machines have a low comparative advantage up for humans to take. However, there is no guarantee that these jobs will be high paying. Thus, income inequality will increase, as low and mid wage jobs will start paying less, paving the way for political unrest and upheaval, as evidenced by the recent slew of populist movements rising in the western world.
Furthermore, AI goes beyond just destabilizing the labor market, but can also play a role in changing the dynamics of existing and future jobs. From one perspective, techno-optimists regularly point out that the rise of digitization and computerization gave birth to the sharing economy — which is true. However, it is important to look at the dynamics present within the different services at the heart of the sharing economy ecosystem, and how AI plays a role in information and power asymmetries that exist between the centralized authorities that manage these services and the workers that populate it. A recent report by AI Now, explores current research being done on how Uber uses it’s drivers data to incentivize to them to take on unprofitable rides in order to advance it’s objective of providing a convenient user experience for it’s customers. In addition to algorithmic forms of management, the traditional economy will experience shifting dynamics, with the jobs of the future no longer demanding specialization, but generalization. Moreover, creativity and technical knowledge will be part of an essential toolkit, alongside the ability to communicate with machines and work alongside them — thus, retraining current workers, and re-imagining the current education system are both needed to minimize the damage done by AI on evolving labor dynamics.
In 2017, the McKinsey Global Institute estimated that 30% of tasks of more than 60% of jobs around the world are at risk of automation. However, it is essential to note that the distribution of impact on jobs will vary from one country to another. Experts point that there will be winners and losers from the third industrial revolution, with developing countries with high birth rates being the biggest losers of them all — some have even gone out to say that almost 70% of jobs in developing countries are at risk of automation. The current model of economic interdependence as we know it today, where developing nations host pillars in the value chain of production may cease to exist, as the competitive costs of robotic production will force manufacturers in developed countries to produce at home. This change in economic interdependence will bring about political instability, mass migration and the anxieties that come with it. Thus, it is essential to retrain the labor market for the upcoming shifts in value creation.
II) Bias in AI systems
One of the key objectives of an AI system (more particularly a machine learning algorithm) is to draw a discriminatory line between two or more populations based on some set of data we provide about these populations. For example, a machine learning algorithm trained on data of two populations of tumors, one cancerous and one not cancerous, with the set of data being provided is height and width of the tumors, can predict whether future tumors will be cancerous or not. The algorithm does that by identifying a line (a discriminant line) that best divides between cancerous tumors, and non cancerous ones.

A Machine Learning Algorithm Learns to Discriminate Between Cancerous and Benign Tumors
Extrapolating this process on problems such as access to loans, health insurance and others, with the data being analyzed pertaining to social (i.e. race, address, gender etc…), economical (income levels, education etc…) and political (political affiliation) dimensions, it is easy to see that AI systems can potentially reinforce biases existing in society today.
Bias in AI systems can have different causes. The previously mentioned AI Now report delves into the research landscape on the origin of AI bias. In short, machine learning algorithms require training to data to be able to output predictions. Training data can often be biased, skewed, incomplete or inaccurate. To give an example, in 2015 a Google image classification algorithm classified two black individuals as “Gorilla” — the problem likely stems from imbalanced training data, and showcases the imperfection of current AI systems, which can have superhuman intelligence in narrow tasks, but lack common sense. Despite their best efforts, Google’s solution to the problem was to block the image recognition algorithm from recognizing gorillas all together. Moreover, the manner by which data is often collected can lead to bias results. Data is either labelled by humans, scraped off of existing web services and are subject to very little auditing mechanisms. Furthermore, whatever bias that is embedded into machine learning algorithms is hard to interpret, making it harder to spot. This results in bias in applications of machine learning being spotted after deployment, as evidenced by the Propublica exposé on a machine learning application being biased against blacks in predicting recidivism rates and granting parole.
Moving beyond training data imbalance — machine learning algorithms are subject to a trade off between complexity and interpretability. Namely, there generally exists a trade off between how performant a machine learning algorithm is, and how transparent in its decision making it is. As basic machine learning algorithms such as regressions and decision trees provide the capacity for us to understand why they made the decisions they made, more complex systems such as Deep Learning models remain a “black box”. Research into making Deep Learning models more interpretable is still ongoing. However, many applications we use today such as facial and voice recognition softwares, cannot explain why they made the decisions they made.
III) AI, Politics and Governments
AI can be thought of as a general purpose technology — similar to electricity, it’s impact can be traced horizontally across all aspects of human life, with use cases emerging in health care, industry, finance and more. It’s ubiquity has the capacity to not only produce tectonic change in our economic systems, but to subvert existing relationships within our current political and governance frameworks — with potential to reinforce autocratic tendencies and to stretch existing power asymmetries. More importantly, AI benefits from the continuum of digital technology in our lives, that has led us to develop an effective “surveillance” apparatus that is fed from both the private and public sector. More and more, businesses (more importantly tech companies) have relied on gathering customer information, from Facebook likes to location data, in the name of delivering a more personalized experience and better ad targeting. On the other hand, governments have been clamping down on civil liberties and privacy rights in the name of national security, as evidenced by the testimony of whistle blowers such as Edward Snowden. It is important to add a caveat here, that the development of this “surveillance” apparatus varies from country to country, and can take many different shapes and forms, largely depending on the nature of data privacy laws, the degree of cooperation between the private and public sector, and most importantly infrastructural constraints. Thus, it is essential to view the potential danger AI represents to politics and governance, not as a sudden coup against existing forms of governance, but a slow ambiguous march towards digital autocracy.
Caveats aside, AI provides a range of advanced pattern recognition tools that, if built on top of this “surveillance” apparatus, would significantly reduce the costs of government oppression, destabilize existing sociopolitical arrangements, and erode notions of objective truth. A recent report by the future of humanity institute provides ample descriptions of ways AI can potentially enable autocratic traits. Firstly, AI provides a cost effective approach to identifying potential political dissenters — as the digital trace we leave online ranging from food preference, to what type of shows we like to watch can provide statistically significant predictors for political leanings. This was evidenced in the recent american elections and the Cambridge Analytica scandal, where Facebook data was used to infer users’ political leanings and psychographic profiles in order to better target them with political ads. The same process can be co-opted in order to better isolate dissenters, better anticipate their actions and manufacture disincentives for dissent in various forms. Mainly, the toolkit provided by AI enables governments to render dissent a more costly choice for dissenters, as improved identification and monitoring technology provides amble ground for catching and punishing dissenters previous or post-dissent. Furthermore, governments can establish social credit systems, which is a measure by which a government can give it’s citizens a score dependent on some measure of trust. China’s plan to implement a social credit system by 2020 leverages a variety of data points, including financial data, interpersonal data pertaining to one’s connections online as well as other social dimensions. This credit system, will be a determining factor in what type of schooling one’s kids receive, whether one has access to public housing, job eligibility and even in romantic relationships. Such credit systems, have the potential to reinforce an authoritarian regime’s foothold on power, and systematically and automatically punish dissenting behavior.
An Illustration of Deepfakes in Action, Transposing the Face of Actress Gal Gadot on a Pornographic Video
Moreover, governments have the capacity to manufacture consent at an unprecedented level, with the capacity to infiltrate online communities using AI bots that can effectively respond to dissenting groups, and promote government talking points. These bots can also be empowered through AI enabled video and audio editing technology (also known as “deepfakes”), which can create immensely believable doctored videos. Deepfakes recently rose to prominence when certain Reddit communities started using it to transplant celebrities’ faces on porn videos. And whilst Deepfakes deserves an exploration in themselves (more on that in a future blog post), with it’s capacity to weaponize fake news, cyber-bullying as well as fragment social cohesion, it’s capacity to corrode trust in dissent is remarkable.
Concluding Notes
I started off this article by introducing it not as an exhaustive list of the short-term ethical concerns of AI, but as a primer on how to think about the ethical concerns raised by AI’s rise to prominence. The purpose of this is not to espouse a sort of neoluddism, but to raise important questions about the role that technology will have in our lives. A techno optimist can write an even longer article on the short and long term benefits of AI, with its capacity to aid in drug discovery, enable personalized and predictive medicine, equip individuals with higher degree of autonomy, and potentially absolve us from human drudgery. This article does not reside on either end of the spectrum, but in a pragmatic middle that acknowledges AI’s potential, whilst keeping wary and asking questions of its drawbacks. These questions beg a nuanced and well thought out response from policy makers, politicians, and most essentially, citizens. Questions about how to bridge the gap between a safe and humane future and technology’s onward march, require us to muster a well thought out response that combines hard and social sciences.
Ultimately, technologists are often polarized into two opposing camps, ones that think that technology is neutral, and ones that think that technology is not. However, the bridging of our digital and offline lives, and the rise of AI and its capacity for organizing our societies, economies and politics, flips this divide on its head and begs us to ask: Do we want it to remain neutral?
",The Short-Term Ethical Concerns of AI,315,the-short-term-ethical-concerns-of-ai-c201c03bc0ac,2018-06-18,2018-06-18 16:50:36,https://towardsdatascience.com/the-short-term-ethical-concerns-of-ai-c201c03bc0ac,False,3444,"Sharing concepts, ideas, and codes.",towardsdatascience.com,towardsdatascience,,Towards Data Science,,towards-data-science,"DATA SCIENCE,MACHINE LEARNING,ARTIFICIAL INTELLIGENCE,BIG DATA,ANALYTICS",TDataScience,Ethics,ethics,Ethics,7787.0,Adel Nehme,Data science graduate passionate about the intersection of data and society. I like thinking about AI and the impact of emerging technology on the world.,b969f6c0928d,adelnehme_57126,109.0,9.0,20181104
0,,0.0,,2018-05-25,2018-05-25 16:46:42,2018-05-28,2018-05-28 18:17:22,5,False,en,2018-05-28,2018-05-28 18:17:22,0,6b1c28e56eac,4.184276729559748,2,0,0,"No, it doesn’t have to do with robots calling people and people not knowing they’re speaking to a robot. It has to do with training.",5,"The Ethics of Google Duplex

No, it doesn’t have to do with robots calling people and people not knowing they’re speaking to a robot. It has to do with training.
Earlier in my career, I did a lot of lead generation work. I’ve made over 20,000 cold calls with different startups and larger companies, including IBM. For IBM, I was selling application servers and middleware to small and medium size enterprises in southern Ohio and northern Kentucky. Think flower shops to credit unions. Being an inside rep, I’d work with my outside rep to setup sales calls.
One of the first appointments I had scheduled for Ken, my outside rep, was to a pig farm north of Lexington, KY. The technology director showed up for the meeting in mud-covered hip waders. They did not have a need for WebSphere Commerce Server.
Where I sent my first IBM rep.
At IBM, I was relentless. If there was evidence of me having OCD, it was that I needed to keep score of everything. I had created a game where I needed to get to 100 points per day and I would not be able to leave until I got those 100 points.
Here’s how the game broke down:
A deal closes = 100 points
Submit a bid = 50 points
Send a quote = 20 points
Run a customer webinar = 10 points
Create a detailed response to customer =5 points
Conversation with prospect/ customer = 3 points
Call attempt or email = 1 point
The idea was to align my reward (which was really just relief of my quasi OCD) with a behaviour that would create value for IBM. For the last six months of my work there, I’d be playing to hit my daily 100.
I was well trained coming into IBM for this type of game. At my previous employer, a VC-backed startup out of Ottawa, I dubbed myself the Cold Calling Machine — CCM for short.
All I’d do is call companies that had field staff and setup appointments for outside reps to sell electronic forms as a cellular plan add-on. The incentives for showing traction were high and I’d make hundreds of dials a day.
Over the course of the day, I’d take my first break when I got to 75 dials, eat lunch at 150, another snack at 200, etc. Or, I’d take a break after I hit my first appointment. On average, it’d take about 70 dials to get one appointment and all of it was tracked in an Excel spreadsheet because this could be updated very quickly while on the phone (sub-10 s per update vs 20–30 seconds in a CRM).
My call tracking spreadsheet — CCM. Showcasing my Excel skills in 2006.
Whether it was IBM or other companies, the first few hundred calls were pretty bad.
I tripped over my words, I wouldn’t quite understand the product, I wouldn’t know how the culture of a given industry handles phone calls. Also, when should I cut bait and schedule a call back vs reeling in the appointment? — At the start in every company I worked at, I was pretty lousy to start and would get a bit queasy and sweaty palms.
Knowing that I’d be pretty bad on my first few calls, I tried starting on lists that I knew would be particularly bad. The idea was to preserve the good lists (think the Glengarry leads) for when I’d be better.
The coveted Glengarry leads.
In hindsight, I realize that there were a lot of people who received an unpleasant call from me. Not because I wasn’t offering something of value to them, which I was, but because I couldn’t communicate it to them effectively.
They had to listen to someone nervous and bumbling try to figure out how their industry worked, their phone culture, and what was of value to them. They didn’t know that the person on the other end of the line was just learning. They had their schedules to run and my call was just an interruption. My redeeming quality, however, was that I was a human.
Human: It’s the part I was born to play, baby!
This is where we come back to Google Duplex. Machine learning includes learning and you learn from trying a bunch of things, messing up in many, and doubling down on the things that work. When you encounter something new, you try different approaches. It doesn’t matter if it’s a human or a machine, the only difference is in the scale and speed.
The recorded demos of Google Duplex at Google I/O were great. They didn’t frustrate the business, they provided a service to these businesses, they were polite and cordial. They sounded like a young personal assistant calling, which is disarming to the recipient.
However, knowing how many calls it takes for a human to get better, my biggest question is: how many calls did it take to get Google Duplex to have a successful call? What happened on those calls? Was anyone really upset? Did it tie up a businesses line?
Which businesses were Google’s training list (non-Glengarry leads)? Which were the ones that were preserved? Who chose this list?
That’s the real ethical question. Is it acceptable to be training fodder for another company’s AI offering without consent? I don’t know what Google did to train Duplex but transparency around this could earn them either a lot of heat or potentially a lot of goodwill.
",The Ethics of Google Duplex,52,the-ethics-of-google-duplex-6b1c28e56eac,2018-05-28,2018-05-28 22:00:11,https://medium.com/s/story/the-ethics-of-google-duplex-6b1c28e56eac,False,888,,,,,,,,,,Ethics,ethics,Ethics,7787.0,"Leor Grebler, UCIC","CEO of UCIC — The Voice of AI — making hardware products come alive with voice interaction. Proofs of concept, prototypes, and tools for integration of voice.",136fa39ffeba,Grebler,3566.0,359.0,20181104
0,,0.0,1dc0795d9d6e,2018-07-17,2018-07-17 21:09:25,2018-07-17,2018-07-17 21:13:32,1,False,en,2018-07-17,2018-07-17 21:13:32,10,cae76ed65f3,4.30188679245283,0,0,0,"This post was featured in our Cognilytica Newsletter, with additional details. Didn’t get the newsletter? Sign up here.",5,"Following the broad impact of AI on Industries

This post was featured in our Cognilytica Newsletter, with additional details. Didn’t get the newsletter? Sign up here.
If you’ve been following coverage of artificial intelligence (AI) in the news, you might have noticed that industries ranging from pharmaceutical to banking to insurance to real estate are being impacted by the range of machine learning and AI technologies that can broadly be called cognitive technologies. These cognitive technologies change how businesses approach their customers and manage their operations, impacting business processes, customer-facing operations, and internal-facing operations across the board.
Much of what Cognilytica writes about is shared in our client-only research as well as our newsletter articles, podcasts, explainer videos, and other forms of content. However, Cognilytica analysts also write for a number of syndicated publications, and we thought during this holiday week (in the United States), we would share with you some of our writing for those media outlets so we can share what we’ve learned about AI adoption in different industries.
AI in insurance forces big changes to traditional industry
Increasingly, insurance companies are eyeing the opportunities they can create by applying AI to their own operations and products. Many automobile insurance companies have seen significant benefits using telematics devices that plug into cars to assess the risk posed by their insured drivers. AI in insurance can provide significant value by extracting key patterns from the vast amounts of data collected by insurance companies as part of their policy, customer, claim and risk data.
….
Insurers are faced with new challenges, such as providing insurance coverage when the human behind the wheel might not actually be in control of a car in self-driving mode, or when the company who owns the car — such as Uber — might not be the company that made the car — such as Volvo — and the company that made the car might not have made the autonomous technology. Insurers will have to consider these new challenges to assign liability, assess risk and determine loss ratios.
Read the full article at TechTarget’s SearchEnterpriseAI Site.
AI applications in healthcare smooth providers’ operations
Artificial intelligence and machine learning are transforming many areas of the healthcare industry, ranging from patient-facing and customer service activities to improvements in overall care, diagnosis and treatment. Many of the opportunities from AI applications in healthcare relate to the sheer quantity of data produced by healthcare providers and the opportunity to identify patterns and augment the capabilities of existing physicians, clinicians and staff.
…
Through the combined use of AI applications in healthcare, diagnosis, treatment and customer care, hospitals, care practitioners, health insurers and the health industry as a whole hope to dramatically reduce the cost of care while improving treatment outcomes, reducing risk and increasing overall patient satisfaction.
Read the full article at TechTarget’s SearchEnterpriseAI Site.
Threats of AI include malicious use against enterprises
As AI and machine learning increase their footprint in the enterprise, companies are starting to worry about their exposure to the new threats of AI. Likewise, new threats are emerging from malicious uses of AI and machine learning, from acts of mischief and criminal activity to new forms of state-sponsored attacks and cyberwarfare.
…
While AI is no doubt enabling enterprises to accomplish tasks and provide value, it is also enabling new and more dangerous criminal and malicious behavior. As the threats of AI impact cybersecurity, they’re creating opportunities for both cybersecurity businesses and the criminals that target them.
Read the full article at TechTarget’s SearchEnterpriseAI Site.
Addressing the ethical issues of AI is key to effective use
Many enterprises are exploring how AI can help move their business forward, save time and money, and provide more value to all their stakeholders. However, most companies are missing the conversation about the ethical issues of AI use and adoption. Even at this early stage of AI adoption, it’s important for enterprises to take ethical and responsible approaches when creating AI systems because the industry is already starting to see backlash against AI implementations that play loose with ethical concerns.
…
Forward-thinking companies see the need to create AI systems that address ethics and bias issues, and are taking active measures now. These enterprises have learned from previous cybersecurity issues that addressing trust-related concerns as an afterthought comes at a significant risk. As such, they are investing time and effort to address ethics concerns now before trust in AI systems is eroded to the point of no return. Other businesses should do so, too.
Read the full article at TechTarget’s SearchEnterpriseAI Site.
Enterprises explore AI voice assistant technology
AI voice assistant devices have made plenty of headlines recently in the world of consumer electronics, but vendors are increasingly setting their attention on enterprises to increase market share and revenue. Despite early promise, several issues could hold back adoption.
…
At the end of the day, for these voice assistants to provide value to enterprises, they need to prove themselves to be trustworthy, valuable resources. As such, vendors hoping to penetrate and dominate the enterprise ecosystem for voice assistants need to focus on addressing key integration, application development, provisioning, security, privacy and trust issues before they can find widespread adoption and traction.
Read the full article at TechTarget’s SearchEnterpriseAI Site.
Industries evaluate collaborative robot applications
..In the mid-1990s, collaborative robot applications began to emerge. Known by its shorthand term cobot, a cobot is a physical robot that is intentionally designed to operate in close quarters with humans. Cobots can operate in a wide range of environments, from assembly lines to warehouses to roaming around hospitals or office buildings helping with various tasks.
…
Collaborative robot applications are designed to have lower overall power and greater sensitivity and awareness of their surroundings so that they can work in close proximity to humans. But this lower power does limit the application of cobots to tasks where a large amount of strength is not required.
Read the full article at TechTarget’s SearchEnterpriseAI Site.
Read our writing on CTOVision and Cognitive World and stay tuned for even bigger news.
In addition to our contributed writing on SearchEnterpriseAI, Cognilytica analysts also write for CTOVision and Cognitive World, where we often expand on our writing from our newsletters. Follow us there for more insight on AI adoption and things to consider. Finally, we have big news coming up soon with a major publication that will be featuring writing from Cognilytica analysts, so stay tuned!
",Following the broad impact of AI on Industries,0,following-the-broad-impact-of-ai-on-industries-cae76ed65f3,2018-07-17,2018-07-17 21:13:32,https://medium.com/s/story/following-the-broad-impact-of-ai-on-industries-cae76ed65f3,False,1087,"Real-world insight, expertise, and opinions on Artificial Intelligence (AI) and related areas",,cognilytica,,Cognilytica,info@cognilytica.com,cognilytica,"ARTIFICIAL INTELLIGENCE,MACHINE LEARNING,DEEP LEARNING,AI,ROBOTICS",cognilytica,Ethics,ethics,Ethics,7787.0,Kathleen Walch,Principal Analyst at AI Focused Analyst firm Cognilytica (http://cognilytica.com) and co-host of AI Today podcast.,b8d98395a9a1,kath0134,10.0,2.0,20181104
0,,0.0,,2018-05-15,2018-05-15 14:40:22,2018-05-15,2018-05-15 14:40:47,0,False,en,2018-05-15,2018-05-15 14:40:47,7,920835386344,1.3849056603773584,0,0,0,This first appeared as an IDG Contributor Network Column in CIO.com.,5,"The Ethical Challenges of AI
This first appeared as an IDG Contributor Network Column in CIO.com.
Companies need to think in ethical terms as they develop and deploy AI applications.
Machine learning algorithms are everywhere. It is not just Facebook and Google. Companies are using them to provide personalized education services and advanced business intelligence services, to fight cancer and to detect counterfeit goods. From farming to pharmaceuticals. From AI-controlled autonomous vehicles to clinical decision support software.
The technology will make us collectively wealthier and more capable of providing for human welfare, human rights, human justice and the fostering of the virtues we need to live well in communities. We should welcome it and do all that we can to promote it.
As with any new technology, there are ethical challenges. Will the new technologies be fair and transparent? Will the benefits be distributed to all? Will they reinforce existing inequalities?
Organizations that develop and use AI systems need ethical principles to guide them through the challenges that are already upon us and those that lie ahead.
Last year, my trade association, the Software & Information Industry Association, released an issue brief on Ethical Principles for AI and Data Analytics that addresses these challenges. It draws on the classical ethical traditions of rights, welfare, and virtue to enable organizations to examine their data practices carefully.
Companies need to recover their ability to think in ethical terms in business and in particular in their institutional decisions regarding the collection and use of information. These principles are a practical, actionable guide.
SIIA is not the only entity seeking to bring ethical considerations into the world of AI and data analysis. The computer science group Fairness, Accountability and Transparency in Machine Learning (FAT/ML) has drafted its own principles. Another group of computer scientists meeting at Asilomar drafted broader principles. IEEEhas proposed principles relating to ethical values in design. ACM recently released a set of principles designed to ensure fairness in the use of AI algorithms. And the Information Accountability Foundation has formulated a very useful set of principles in its report on Artificial Intelligence, Ethics and Enhanced Data Stewardship.
This is an excerpt from a longer column on CIO.com.
",The Ethical Challenges of AI,0,the-ethical-challenges-of-ai-920835386344,2018-05-15,2018-05-15 14:40:48,https://medium.com/s/story/the-ethical-challenges-of-ai-920835386344,False,367,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Mark MacCarthy,"SVP for Public Policy, Software & Information Industry Association; Adjunct Professor, Communication, Culture & Technology Program, Georgetown University",4317359941eb,maccartm,8.0,3.0,20181104
0,,0.0,f8b1e20a7dbc,2018-08-16,2018-08-16 12:20:32,2018-08-24,2018-08-24 13:53:23,1,False,en,2018-08-27,2018-08-27 11:07:41,28,5c6d098741fd,8.267924528301888,0,0,0,,5,"Learning to avoid users infantilization
by Prof. Dr. Gaia Scagnetti
The ThingsCon report The State of Responsible IoT is an annual collection of essays by experts from the ThingsCon community. With the Riot Report 2018 we want to investigate the current state of responsible IoT. In this report we explore observations, questions, concerns and hopes from practitioners and researchers alike. The authors share the challenges and opportunities they perceive right now for the development of an IoT that serves us all, based on their experiences in the field. The report presents a variety of differing opinions and experiences across the technological, regional, social, philosophical domains the IoT touches upon. You can read all essays as a Medium publication and learn more at thingscon.com.
Twelve weeks ago we had a baby; by the day she was born my partner and I had already determined that we would not post any pictures on social media to protect her privacy. For us it was an easy decision, like disregarding the wipe warmer, choosing only gender-neutral colors, and eating more organic food during breastfeeding. Our friends believed that being parents would transform us into obsessive oversharers of baby pictures and eventually change our relationship with social media. Instead, having a baby turned out to be a reflection on the smart home, voice commanded apps, and data.
I need a robotic smart home
I have always preferred to interact with my environment in a quiet and reserved way: I favor text messages over phone calls, I choose to read a map rather than ask for directions. I did not develop an effective relationship with Siri or Cortana or any dictation app: initially because my Italian accent confused any voice recognition software, but ultimately because I would rather not vocalize my activities; I like to keep them for myself.
When the baby arrived, something changed. I found myself spending hours nursing, rocking, changing diapers, and expressing milk. What all these activities have in common is that they required the use of both hands.
For those of you who had never fed a baby, I’ll give you a quick rundown of the principal modern peculiarities of this activity. My house is quiet; the baby is sleeping. All of a sudden the baby is hungry and starts fussing. There are only about sixty seconds before she starts screaming at a pitch that can break windows (metaphorically) and wake up the entire neighborhood (literally). In that minute window, I pick up the baby, sit, and start nursing. A newborn can eat for five to twenty minutes. While sitting in the quiet again, I suddenly realize my needs; they emerge as the itching feeling during a meditation session. I would love to have a pillow to rest on under my elbow, maybe one behind my back, I want to search the internet, reach my phone, text a friend, do anything that keeps me awake. But both of my hands are occupied supporting the baby. The phone is twenty inches away but I can’t grab it, the computer might be just in front of me but I cannot type.
For the first time in my life, I want to be able to command everything by voice. While I am stuck supporting a hungry infant trying to grasp my mobile phone with my foot, I regret not having trained the Google assistant to understand me and perform activities when I speak. During the hours spent staring at something out of reach, I dream of a smart home where every single thing responds to my voice command and where objects communicate with each other. A house where the sensor of my baby’s onesie would turn on the bottle warmer when she starts fussing for food, where the bottle warmer would select the right container based on the date and time that my breast pump recorded, where my phone did not need my touch to record how long the baby slept, nursed, and when her diaper needed a change.
For weeks the Internet of Things seemed like a great idea. It might increase our quality of life!
This period did not last long, and already at ten weeks the baby could support her neck decently and I mastered the one-hand-football-hold. I also realized that even if my house was a high tech responsive environment as the one I helped design as a researcher in MIT a long time ago¹, I would have probably just been able to make an order on Amazon but not get it out of the box, start the laundry but not loading the washing machine, order food but not open the door. What I needed was more of a robotic house than a smart one. With a voice controlled device as Alexa or Google home I could have played Jeopardy², booked a restaurant³, stayed mindful with meditation⁴ and ordered, bought, and purchased as much as I want to. All the data I could record about me and my baby would be shared with companies to customize my ads, rather than to other devices to facilitate my activities.
The motivation behind the present development of our home technologies is deeply rooted in capitalist objectives more than in the desire of increasing the quality of life.
Technology should serve its users rather than the interest of manufacturing companies; in reality, the user is more often an element of the system rather than its beneficiary, the system is designed to persuade the user to perform specific actions, almost entirely of consumption.
The fundamental conversation we should have is about trust in the companies who are designing these devices. If Alexa was a real person working for Amazon — helping you around the house but also reporting back to her employer all you do and say — would you trust her? How would being listened continuously by someone who works and reports to a company feel? Would we be more comfortable with an independent OS for IoT? Could we DIY an autonomous device with no capitalist purpose?
I need full surveillance
Furthermore, this parenting experience changed my relationship with data. Since the day I delivered at the hospital we were encouraged to keep a log of the activities of our baby. How much she ate (ml per bottle) for how long she nursed (minutes at the breast), how long she slept (hours), how many wet and dirty diapers, and which shade of color their content. We measured height, weight and head circumference. This log helps new parents learn their baby’s behavior and pattern match it with the average. It allows spotting when something is wrong and reassures that everything is in the norm. The early days of parenting are made of data.
For the first time, I thought that a total surveillance through data collection was a great idea. It might lead to significant discoveries in human behaviors!
This period did not last long, because the data we collect are useful only when secondary to the parents’ intuition. An over-reliance on them disrupts the parents’ ability to listen to a baby needs. The normative average baby is anyway an illusion: “every baby is different” we are told over and over again by wise nurses. When using a paper log we are also keeping all the information for ourselves, the same way our simple fridge is not communicating with any other device. The data we produce stays with us. Collecting detailed data about my baby does not feel particularly problematic: it does not pose the same challenges of collecting data about an adult. Data collection can be a serviceable activity when the object of observation it is naturally subjected to the power of who owns the data. The subjection of the infant to the parents is proper, the subjection of an adult to a company is not. An infant is not yet able to survive autonomously and does not have freedom, the survival of an adult depends on her autonomy and freedom.
Do not treat me like an infant
Infantilization through technology is a thought-provoking framework to discuss the design of a responsible IoT. The concept of infantilization has been described as a treat of the postmodern adult⁵,⁶ by many⁷,⁸,⁹,¹⁰: Baudrillard¹¹ describes Disneyland as the archetype of this world, a metaphor of an American society where the cult of youth is used by capitalism to “infantilize the consumer as a means of non-aggressive control.”¹²
A responsible Internet of Things avoids infantilization. An infant is always at the center of the world and needs to be continuously heard and monitored by her carers. The infant gets fed, washed and changed, gets put to sleep and dressed: all trivial tasks are handled by someone else so she can fully dedicate her energy to growth. Consumption is the primary activity of the infant. The infant does not understand the system around her. She is not aware of how and why her world works and does not need to know: stories are designed to explain reality.
When we design artifacts with the assumption that we need a superior entity to make the right decision for us, we are infantilizing our determination. A technology that creates the illusion that we are of central importance treats us as children unable to understand the vast scale of our society. When we develop digital services and use data as a currency to access them, we are infantilizing and objectifying our users. Technological innovations delegating trivial tasks to free our time to for personal growth are trivializing our identities. When we hide the complexity of our systems behind over-simplified interfaces, we are paternalistic.
A responsible Internet of Things should be human-centric, where human refers to an adult with fully autonomous will, identities and rights.
Prof. Dr. Gaia Scagnetti
Professor Dr. Gaia Scagnetti is program co-coordinator and full-time Assistant Professor at the Pratt Institute’s Graduate Communications Design department in New York. Her current research projects focus on new pedagogies and decolonization for higher education in design. In 2010 Gaia Scagnetti completed a Post Doctoral research at the Design Lab at the Massachusetts Institute of Technology. In 2009 she obtained a PhD degree cum Meritus in Industrial Design and Multimedia Communication at the Politecnico di Milano. During the doctoral research Gaia has worked as a designer and researcher at the Density Design Lab in Milan, where she carried out research, design projects and teaching activities on Information Visualization and Mapping. Her works have been featured in several conferences and exhibitions and publications and showcases. Her complete portfolio can be found at www.namedgaia.com.
ThingsCon is a global community & event platform for IoT practitioners. Our mission is to foster the creation of a human-centric & responsible Internet of Things (IoT). With our events, research, publications and other initiatives — like the Trustable Tech mark for IoT — we aim to provide practitioners with an open environment for reflection & collaborative action. Learn more at thingscon.com
This text is licensed under Creative Commons (attribution/non-commercial/share-alike: CC BY-NC-SA). Images are provided by the author and used with permission. Please reference the author’s or the authors’ name(s).
Scagnetti, Gaia, and Federico Casalegno. “Social Sustainability in Design: The Window as an Interface for Social Interaction.” In Cross-Cultural Design, 321–30. Lecture Notes in Computer Science. Springer, Cham, 2014. doi:10.1007/978–3–319–07308–8_31. ↩︎
Sony Pictures Television. “Jeopardy!” Amazon.com. Accessed July 19, 2018. https://www.amazon.com/Sony-Pictures-Television-Jeopardy/dp/B019G0M2WS. ↩︎
Google Assistant. “OpenTable.” Assistant.google.com. Accessed July 19, 2018. https://assistant.google.com/services/a/uid/000000e768ed4de9?hl=en-US. ↩︎
Stoked Skills LLC. “Amazon.com: Mindful Meditation: Alexa Skills.” Amazon.com. Accessed July 19, 2018. https://www.amazon.com/Stoked-Skills-LLC-Mindful-Meditation/dp/B0784GXF2M/ref=lp_14284837011_1_5?s=digital-skills&ie=UTF8&qid=1531937136&sr=8-5. ↩︎
Bernardini, Jacopo. “The Infantilization of the Postmodern Adult and the Figure of Kidult.” Postmodern Openings/Deschideri Postmoderne 5, no. 2 (2014). ↩︎
Bernardini, Jacopo. “The Role of Marketing in the Infantilization of the Postmodern Adult.” Fast Capitalism, Search Results The University of Texas at Arlington, 10, no. 1 (2013). https://www.uta.edu/huma/agger/fastcapitalism/10_1/bernardini10_1.html. ↩︎
McHugh, Molly. “How Technology Is Creating a Generation of Adult Babies.” The Ringer, January 25, 2018, sec. Tech. https://www.theringer.com/tech/2018/1/25/16933668/2018-ces-rocking-bed-infantalization. ↩︎
Cain, Benjamin. “The Ironies of Modern Progress and Infantilization (by Ben Cain).” Three Pound Brain, February 4, 2014. https://rsbakker.wordpress.com/2014/02/04/the-ironies-of-modern-progress-and-infantilization-by-ben-cain/. ↩︎
Elkus, Adam. “The Infantilizing Nature of Technophobia: A Matter of Will.” Medium, April 20, 2015. https://medium.com/strategies-of-the-artificial/the-infantilizing-nature-of-technophobia-a-matter-of-will-813ef97efab9. ↩︎
Singer, Natasha. “Technology That Prods You to Take Action, Not Just Collect Data.” The New York Times, December 21, 2017, sec. Technology. https://www.nytimes.com/2015/04/19/technology/technology-that-prods-you-to-take-action-not-just-collect-data.html. ↩︎
Baudrillard, Jean. America. Verso, 1989. ↩︎
Silva, Erick da. “The Infantilization of Society and the Cult of Youth.” The Ivory Tower, September 4, 2015. https://the-ivory-tower.com/the-infantilization-of-society-and-the-cult-of-youth/. ↩︎
",Learning to avoid users infantilization,0,learning-to-avoid-users-infantilization-5c6d098741fd,2018-08-27,2018-08-27 11:07:41,https://medium.com/s/story/learning-to-avoid-users-infantilization-5c6d098741fd,False,2138,Experts from the ThingsCon community explore current challenges and opportunities of responsible Internet of Things. Learn more at thingscon.com,,,,The State of Responsible IoT 2018,info@thingscon.com,the-state-of-responsible-iot-2018,"IOT,THINGSCON,DESIGN,TECH,INTERNET OF THINGS",thingscon,Ethics,ethics,Ethics,7787.0,ThingsCon,ThingsCon fosters the creation of a human-centric & responsible #IoT. By @jimmiehu @peterbihr @krgermax.,fb69f6b07d30,thingscon,1057.0,170.0,20181104
0,,0.0,5e5bef33608a,2017-08-31,2017-08-31 09:32:27,2017-08-31,2017-08-31 09:36:47,4,False,en,2017-10-04,2017-10-04 14:01:01,9,384a1756ee5d,5.8886792452830194,28,3,0,"Big Brother has always been watching us, except that these days, he does it far more efficiently and thoroughly. After all, he is online…",4,"The end of the age of humans

Big Brother has always been watching us, except that these days, he does it far more efficiently and thoroughly. After all, he is online, meaning he can access our e-mails, bank statements, phone calls and social media posts. He can also easily structure the information he retrieves to produce reports on how valuable specific individuals are for society. And it is going to get worse. Because soon assembly lines in factories will be “manned” by emotionless robots. The few, chosen ones that will get to keep their jobs will be subjugated to the decisions of taciturn metal-encased supervisors with very few humanoid features. Their speech synthesizers, cameras and their ability to process natural language will enable them to ask us questions, monitor our behavior and keep track of our efficiency. To what end? To assess whether we are still fit for purpose.
Neither will we find relief in our smart homes. Surveillance will grow ever more permanent and pervasive, extending even to our bedrooms and bathrooms. Wall-embedded sensors will follow our every move. Our morning cough will be noticed and instantly reported to our health insurer. The pharmacist will prepare the relevant medicine ahead of our arrival. Even our beloved self-driving cars will lull us into lowering our guard. They will unwittingly transform us from drivers to passive passengers, left at the mercy of the computer under the hood. And that computer will be busy, constantly processing algorithms. Algorithms of life and death that will determine whether we have the right to live in this neatly arranged society.
I am still myself
Dear reader. I am not being paranoid, nor suffering from a nervous breakdown. My blog account has not been hacked and these words really come from me, not an anarchist movement leader. I like the Black Mirror series and don’t believe it is a documentary. To my knowledge, my life has hardly changed from yesterday. I still work at a company staffed by nearly zero robots, although with a substantially higher automation level. I can go for extended periods without social media, although I find it fairly difficult. On Saturdays, I switch off my cell phone, relax in a forest, and often pay with cash in restaurants or stores.
This, however, is the unusual way I have chosen to begin yet another post on artificial intelligence. It concerns the myths on AI found in the media, on the web and in all of our heads. Causing fears which, I dare to say, are largely groundless.
MYTH 1. We will be watched constantly
It is indeed true — big data will enable us to rapidly access data on any topic. As rapid information is the future, only companies capable of retrieving it faster than others will survive on the market. It is possible that computers with specifications matching those of IBM’s Watson will one day populate every office and answer our EVERY question. This may not be all that desirable from our viewpoint.
Does this mean changes in the privacy protection law? Will the kinds of situations in which we can expect to retain our anonymity become considerably fewer? Will we be required to use social media, and will we be prevented from switching off our smartphones? Will we be FORCED to pack our homes with electronics? None of those are foregone conclusions: yes, technology is going to AUTOMATE the majority of social contexts and affect our decisions.
Obviously, as the digital culture moves forward, we become vulnerable and are subject to certain mechanisms that digitize our lives. That is why we should choose some of our behaviors in a more informed manner. When cars became commonplace, we had to accept the fact that we would have to be particularly cautious when crossing a road.

Big data, big threats?
MYTH 2. Robots will take our jobs
Some jobs are just asking to be robotized. Does it make sense for humans to prepare hundreds of thousands of packages in a shipping company’s warehouse? Can an enterprise that saves money by using robots not create other jobs by training qualified personnel that will serve customers online? Savings from employing robots and drones may finance the development of quite a few industries, or be redistributed to society to reduce deficiencies.
Robots will not be able to build relationships in companies, provide soft incentives to workers, come up with creative concepts or, not for a long time at least, draw constructive conclusions. Neither will they be able to sell creative ideas to managements. Out of many industries, only a few will really be able to benefit from the opportunities presented by robotization.
Modern technologies, old fears: will robots take our jobs?
In Stanislaw Lem's ""Return from the Stars"", the astronauts returning to Earth, after many years in the universe, find…norbertbiedrzycki.pl
MYTH 3. Algorithm errors will spark chaos
Granted, there have been cases of computers ascribing specific information or features to wrong people. Some such errors have been racially biased. There were also people who lost their driving licenses after being mistakenly blamed for having caused an accident. One can also show that conscious manipulation of information can significantly affect political choices.
But then do information transmissions, structuring and use have to be perfect? I don’t know where the idea that the digital ecosphere must be free of errors and dangers came from.
I have already written that algorithms can be wrong at times. Their errors may even become more common, and that simply has to be considered. One must therefore mainly trust one’s senses, thorough analysis and common sense.
Or perhaps we should CHOOSE definitively that, as humans, we MAY never allow machines to think or do things for us? As human beings, we have an existential duty to remain self-reliant.
According to our computers... you don't exist
A foreign friend of mine struggled for nearly three weeks trying to prove to his country's authorities that he..…norbertbiedrzycki.pl
MYTH 4 We will become half-robot, half-human
One of our characteristics as a species is pessimism. Pessimism is useful, perhaps even necessary. Without a doubt, many great books would never have been written and many incredible movies would never have been made without it. There would be no intriguing stories about the inevitable downfall of civilization at the hands of machines. Or, in fact, at the hands of organisms that combine computers with the human brain.
However, all such theories about our minds gaining a new dimension thanks to implants under our skin are just that — theories. Will nanorobots circulating in our bloodstream “digitize” us for good, and will our brains really become permanently linked to the Internet? The matter is neither as simple nor as graphic as the excellent movie Transcendence would suggest. Its protagonist is reborn in a “digital” form after his death. Maybe this will remain in the realm of the theoretical, because we will be unable to fit some pieces of this futuristic technological puzzle into the rest of the picture?
The outstanding futurologist Ray Kurzweil, whom I have mentioned on multiple occasions, likes to describe himself as an optimist. He claims that we are entering the era of post-humanism. This, in a nutshell, carries massive implications for our ontological status and what we will become. As a species, we are ceasing to be human, while artificial intelligence may become one of the many forms of life on Earth.

Ray Kurzweil: The Coming Singularity
While respecting such reflections and scenarios, I also remain humble in cognitive terms. I believe that we are UNABLE to predict the SPECIFIC consequences of the fact that computers will think faster than us within a few years.
Besides, when in doubt, I remember there is still someone like Elon Musk. He is one of the central figures influencing the development of our civilization and a person who is keeping a cool head. Despite the crazy ideas he deploys, he has never lost sight of the threats we may be facing along the way. He warns us against them while doing his thing, confident we will use the opportunities to become better beings. As people. And all this thanks to… robots, drones, autonomous vehicles and space travel.





",The end of the age of humans,106,the-end-of-the-age-of-humans-384a1756ee5d,2018-03-22,2018-03-22 20:21:07,https://becominghuman.ai/the-end-of-the-age-of-humans-384a1756ee5d,False,1375,"Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.",becominghuman.ai,BecomingHumanAI,,Becoming Human: Artificial Intelligence Magazine,team@chatbotslife.com,becoming-human,"ARTIFICIAL INTELLIGENCE,DEEP LEARNING,MACHINE LEARNING,AI,DATA SCIENCE",BecomingHumanAI,Ethics,ethics,Ethics,7787.0,Norbert Biedrzycki,Technology is my passion. VP Mckinsey Digital. Private opinions only,ba5b91d4b474,n.biedrzycki,368.0,45.0,20181104
0,,0.0,,2018-03-23,2018-03-23 23:18:58,2018-03-23,2018-03-23 23:53:57,0,False,en,2018-03-23,2018-03-23 23:53:57,30,2321f7c807b3,5.89811320754717,3,0,0,This essay was co-authored by Steve Mills and Jared Sylvester.,3,"Why we worry about the Ethics of Machine Intelligence
This essay was co-authored by Steve Mills and Jared Sylvester.
We worry about the ethics of Machine Intelligence (MI) and we fear our community is completely unprepared for the power we now wield. Let us tell you why.
To be clear, we’re big believers in the far-reaching good MI can do. Every week there are new advances that will dramatically improve the world. In the past month we have seen research that could improve the way we control prosthetic devices, detect pneumonia, understand long-term patient trajectories, and monitor ocean health. That’s in the last 30 days. By the time you read this, there will be even more examples. We really do believe MI will transform the world around us for the better, which is why we are actively involved in researching and deploying new MI capabilities and products.
There is, however, a darker side. MI also has the potential to be used for evil. One illustrative example is a recent study by Stanford University researchers who developed an algorithm to predict sexual orientation from facial images. When you consider recent news of the detainment and torturing of more than 100 male homosexuals in the Russian republic of Chechnya, you quickly see the cause for concern. This software and a few cameras positioned on busy street corners will allow the targeting of homosexuals at industrial-scale — hundreds quickly become thousands. The potential for this isn’t so far-fetched. China is already using CCTV and facial recognition software to catch jaywalkers. The researchers pointed out that their findings “expose[d] a threat to the privacy and safety of gay men and women.” That disavowal does little to prevent outside groups from implementing the technology for mass targeting and persecution.
Many technologies have the potential to be applied for nefarious purposes. This is not new. What is new about MI is the scale and magnitude of impact it can achieve. This scope is what will allow it to do so much good, but also so much bad. It is like no other technology that has come before. The notable exception being atomic weapons, a comparison others have already drawn. We hesitate to draw such a comparison for fear of perpetuating a sensationalistic narrative that distracts from this conversation about ethics. That said, it’s the closest parallel we can think of in terms of the scale (potential to impact tens of millions of people) and magnitude (potential to do physical harm).
None of this is why we worry so much about the ethics of MI. We worry because MI is unique in so many ways that we are left completely unprepared to have this discussion.
Ethics is not [yet] a core commitment in the MI field. Compare this with medicine where a commitment to ethics has existed for centuries in the form of the Hippocratic Oath. Members of the physics community now pledge their intent to do no harm with their science. In other fields ethics is part of the very ethos. Not so with MI. Compared to other disciplines the field is so young we haven’t had time to mature and learn lessons from the past. We must look to these other fields and their hard-earned lessons to guide our own behavior.
Computer scientists and mathematicians have never before wielded this kind of power. The atomic bomb is one exception; cyber weapons may be another. Both of these, however, represent intentional applications of technology. While the public was unaware of the Manhattan Project, the scientists involved knew the goal and made an informed decision to take part. The Stanford study described earlier has clear nefarious applications; many other research efforts in MI may not. Researchers run the risk of unwittingly conducting studies that have applications they never envisioned and do not condone. Furthermore, research into atomic weapons could only be implemented by a small number of nation-states with access to proper materials and expertise. Contrast that with MI, where a reasonably talented coder who has taken some open source machine learning classes can easily implement and effectively ‘weaponize’ published techniques. Within our field, we have never had to worry about this degree of power to do harm. We must reset our thinking and approach our work with a new degree of rigor, humility, and caution.
Ethical oversight bodies from other scientific fields seem ill-prepared for MI. Looking to existing ethical oversight bodies is a logical approach. Even we suggested that MI is a “grand experiment on all of humanity” and should follow principals borrowed from human subject research. The fact that Stanford’s Institutional Review Board (IRB), a respected body within the research community, reviewed and approved research with questionable applications should give us all pause. Researchers have long raised questions about the broken IRB system. An IRB system designed to protect the interests of study participants may be unsuited for situations in which potential harm accrues not to the subjects but to society at large. It’s clear that the standards that have served other scientific fields for decades or even centuries may not be prepared for MI’s unique data and technology issues. These challenges are compounded even further by the general lack of MI expertise, or sometimes even technology expertise, within the members of these boards. We should continue to work with existing oversight bodies, but we must also take an active role in educating them and evolving their thinking towards MI.
MI ethical concerns are often not obvious. This differs dramatically from other scientific fields where ethical dilemmas are self-evident. That’s not to say they are easy to navigate. A recent story about an unconscious emergency room patient with a “Do Not Resuscitate” tattoo is a perfect example. Medical staff had to decide whether they should administer life-saving treatment despite the presence of the tattoo. They were faced with a very complex, but very obvious, ethical dilemma. The same is rarely true in MI where unintended consequences may not be immediately apparent and issues like bias can be hidden in complex algorithms. We have a responsibility to ourselves and our peers to be on the lookout for ethical issues and raise concerns as soon as they emerge.
MI technology is moving faster than our approach to ethics. Other scientific fields have had hundreds of years for their approach to ethics to evolve alongside the science. MI is still nascent yet we are already moving technology from the ‘lab’ to full deployment. The speed at which that transition is happening has led to notable ethical issues including potential racism in criminal sentencing and discrimination in job hiring. The ethics of MI needs to be studied as much as the core technology if we ever hope to catch up and avoid these issues in the future. We need to catalyze an ongoing conversation around ethics much as we see in other fields like medicine, where there is active research and discussion within the community.
The issue that looms behind all of this, however, is the fact that we can’t ‘put the genie back in the bottle’ once it has been released. We can’t undo the Stanford research now that it’s been published. As a community, we will forever be accountable for the technology that we create.
In the age of MI, corporate and personal values take on entirely new importance. We have to decide what we stand for and use that as a measure to evaluate our decisions. We can’t wait for issues to present themselves. We must be proactive and think in hypotheticals to anticipate the situations we will inevitably face.
Be assured that every organization will be faced with hard choices related to MI. Choices that could hurt the bottom line or, worse, harm the well-being of people now or in the future. We will need to decide, for example, if and how we want to be involved in Government efforts to vet immigrants or create technology that could ultimately help hackers. If we fail to accept that these choices inevitably exist, we run the risk of compromising our values. We need to stand strong in our beliefs and live the values we espouse for ourselves, our organizations, and our field of study. Ethics, like many things, is a slippery slope. Compromising once almost always leads to compromising again.
We must also recognize that the values of others may not mirror our own. We should approach those situations without prejudice. Instead of anger or defensiveness we should use them as an opportunity to have a meaningful dialog around ethics and values. When others raise concerns about our own actions, we must approach those conversations with humility and civility. Only then can we move forward as a community.
Machines are neither moral or immoral. We must work together to ensure they behave in a way that benefits, not harms, humanity. We don’t purport to have the answers to these complex issues. We simply request that you keep asking questions and take part in the discussion.
We’re not the only one discussing these issues. Check out this Medium post by the NSF-Funded group Pervasive Data Ethics for Computational Research, Kate Crawford’s amazing NIPS keynote, Mustafa Suleyman’s recent essay in Wired UK, and Bryor Snefjella’s recent piece in BuzzFeed.
This has been crossposted to my personal blog as well.
",Why we worry about the Ethics of Machine Intelligence,5,why-we-worry-about-the-ethics-of-machine-intelligence-2321f7c807b3,2018-04-27,2018-04-27 17:16:51,https://medium.com/s/story/why-we-worry-about-the-ethics-of-machine-intelligence-2321f7c807b3,False,1563,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Jared Sylvester,"ML researcher for Booz Allen. Interested in AI, art & things in between. “Computing what is computable, and making computable what it not so.”",54bb4d2fb34a,jsylvest,1.0,2.0,20181104
0,,0.0,9d6d3078be3d,2018-04-19,2018-04-19 13:25:47,2018-04-20,2018-04-20 09:56:25,5,False,en,2018-04-21,2018-04-21 21:15:05,19,88f9b7b27f8e,7.867295597484277,16,0,0,The first airdrop of Ocean Protocol Tokens will happen with our participation in Grow Asia Hackathon 2018.,5,"Ocean Protocol supports Grow Asia to empower communities
Photo by Aaron Ross on Unsplash
The first airdrop of Ocean Protocol Tokens will happen with our participation in Grow Asia Hackathon 2018.
We would definitely be thrilled if, beyond tokenomics and economy of data, the worth of Ocean Protocol in the near future will be measured proportionally to the impact it will have had in creating new paradigms for society and for people in their daily lives.
We are not limiting the meaning of Ocean Protocol community to the supporters of the project or its contributors. We wish to include all the communities out there that can benefit from the impact of data sharing for good, catalysed by the project. We are looking at micro and macro opportunities for sustaining livelihoods, improving the quality of life of people, impacting life outcomes, levelling the playing field for social inclusion, and mobility, helping the environment and more.
It would be such a waste to have to wait even one day longer than necessary for this impact to be felt and, for this reason, the initiatives that will be shaped around the development of the Ocean Protocol will have ‘real life’ impact as a priority right from the start.
Data sharing has been getting bad press due to abuses by centralized solutions in relation to the people who helped them grow. In contrast, we’re only starting to tap the immense potential and positive power of secure blockchain-based decentralized sharing. It will be our job to facilitate real-life use cases that will showcase how sharing data for good is not only possible, but desirable and meaningful for society.
In line with this ethos, we are delighted to share with you that the first airdrop of Ocean Protocol Tokens will happen with our participation as Tech Partner in Grow Asia Hackathon 2018.
Established by the World Economic Forum in collaboration with the ASEAN Secretariat, Grow Asia brings together companies, governments, NGOs, and other stakeholders to help smallholder farmers improve their production and livelihood through access to information, knowledge, markets, and finance.
Grow Asia Helps Smallholder Farmers while Protecting Forests & Protecting Biodiversity
In the same way digitisation is transforming and disrupting many industries, it has the potential to dramatically change the way 10 million smallholder farmers in Southeast Asia manage their plots and crops. Helping these smallholders access information, credit and services and adopt sustainable production practices not only delivers improvements to their livelihoods, but can also result in more sustainable outcomes, especially in relation to oil palm cultivation.
“Although oil palm cultivation can improve the livelihoods of many smallholders, the expansion of oil palm cultivation has also led to increased concerns about deforestation, loss of biodiversity, increased greenhouse gas emissions and land rights conflicts. Consequently, the need for sustainable production practices, intended to result in higher yields, better prices, and reduced social and environmental damage, has become more apparent and has gained increasing attention.”
credit to Core+-+RSPO Supply Chain Infographic
This is the reason why Grow Asia focuses on independent smallholders: they have a significant role in the expansion of oil palm cultivation onto marginal lands or those not officially zoned for agriculture, and thus are a growing driver of deforestation and the resulting loss of biodiversity and increased greenhouse gas emissions.
How can the creative and skillful use of data assets have a positive impact in this context? Participants will explore how data that is already collected by various ecosystem actors can be used to achieve better efficiency and productivity, and demonstrate how data sharing can be done at scale.
Ocean Protocol uses blockchain technology to allow data to be shared and transferred in a secure and transparent manner. By enabling a decentralized platform and network that connects providers and consumers of valuable data, Ocean Protocol offers developers controlled access to build services. This was not possible before due to the various barriers to sharing data under a centralized model.Ocean Protocol is pushing the boundaries to deliver implementable and scalable means for data sharing worldwide, and projects like Grow Asia are just the first step in this amazing journey of transformation.
Paradoxically, technology is the easiest barrier to overcome for this revolution. The hardest barrier lies in people’s mindset and it can only be conquered by our continuous demonstration of the positive impact data sharing can bring to our society and by educating an entire generation of data scientists, regulators and enterprises alike about the dissolution of obstacles that were making scalable, data driven innovation impossible.
The ecosystem activation that Ocean Protocol sets to achieve has the broadest mandate: Mobilize data-sharing communities for communities that can be transformed by data sharing.
A workshop was held at LEVEL3 (Ocean Protocol’s Singapore HQ) on the 12th of April, with 120 attendees representing research institutes, startups, SMEs, enterprise teams and government agencies. To date, we expect in excess of 200 registrants for the Hackathon. It is heartwarming to see such a positive reception for such an important initiative. We will keep everyone up to date with the progress of the teams as they work through the stages of the event starting from the 20th of April to the Presentation Day on the 23rd, when the selection of applications and platforms for further development and support will be announced.
Unilever, Mitsubishi UFJ Financial Group (MUFG), and Bayer will use the innovation and insights brought by the participants to help smallholders access information, credit, and services and adopt sustainable production practices, improving their livelihoods and producing more sustainable outcomes. The collaboration will enable and accelerate the access to data collected by various ecosystem actors, including The Weather Company and Planet, but had not been shared to-date.

Like the other partners and supporters of Grow Asia’s work, Ocean Protocol wants for this to be a meaningful initiative with tangible results. The selected applications and platforms combined with the commitment from partners to help operationalise the outputs will accelerate Grow Asia’s path to create a positive impact for 10 million farmers by 2020.
This is exactly the type of outcomes we would like to achieve globally, over and over again, thanks to the active involvement of the Ocean Protocol ecosystem.
For this reason, in addition to the highly valuable go-to-market support, each winner will receive cash awards and an equivalent amount in Ocean Protocol Tokens when they deploy their application or platform on the Ocean Protocol Network, which we expect to be in Q1 2019. Aside from engineering support, the winners will also receive Data Analytics, AI and Blockchain guidance to help deliver their application or platform.
We will do our best to help them transform their concepts and designs into a real-life products built for impact.
More Information about Grow Asia Hackathon 2018
1st prize S$5,000 | 2nd prize S$3,000 |3rd prize S$2,000 | Special prizes 2 x S$500
Organiser: Grow Asia
Main Partners: Unilever and Mitsubishi UFJ Financial Group (MUFG)
Supporting Partner: Bayer
Tech Partners: Ocean Protocol, The Weather Company, Planet


Pre-reading: https://growasiahackathon.com/pre-reads/
Dataset: https://growasiahackathon.com/dataset/
Experts and judges: https://growasiahackathon.com/hackathon/
First Theme: Data sharing
Many companies, including technology startups offering SaaS solutions, are addressing the issue of “traceability” to help Buyers trace back to a known catchment area around a mill. They are collecting and recording data, and are analysing the immediate area around a mill, for example, to reveal useful information regarding the plantations, the farmers, and the middlemen (agents and traders).
The opportunity exists for ecosystem actors to form partnerships to create new solutions that benefit the smallholders, if they can better appreciate the value of complementary data and have the incentive to share their data.
CHALLENGE
How might we design a data application or platform and an appropriate business model, for example to share or use the provenance or traceability data, for the mutual benefit of the ecosystem actors as well as the smallholders?
Second Theme: Productivity
Low productivity is a key factor leading to smallholders growing larger areas of under-productive oil palm rather than intensifying production. Independent smallholders perform 40% below good agricultural scenarios for smallholders and 116% below company plantation scenarios. Thus, improving productivity is also key to improving their livelihoods.
Training and technical assistance in crop protection, fertilisers, quality seedlings, etc. are essential for sustainable improvement for the long term. Together with access to information (like weather) that is up-to-date and locally relevant, training will increase smallholder productivity and efficiency. However, there have been challenges in training, because of access and literacy.
CHALLENGE
How might we provide the right information and training to smallholders, so they become more productive and efficient, which in turn aid the sustainable intensification of their plantations?
Third Theme: Financing
Smallholders are often too small for commercial banks and too large for micro-finance schemes. Almost half of them don’t have a bank account. They do not have credit history, cannot supply reliable management information, or present a formal land title which could be used as collateral to access finance.
They are only able to access informal credit — with high interest terms and short tenure, but comes with high convenience and flexibility — from their direct buyer (agent or trader), most to manage their day-to-day living needs.
CHALLENGE
How might we devise a new credit scoring mechanism for banks (such as MUFG) or commodity buyers (such as Unilever) to extend credit to smallholder farmers based on data on their crops or on-farm behaviours / investments?
Fourth Theme: Logistics
Fresh fruit bunches (FFB) must be delivered to a mill and processed within 48 hours after harvest before significant yield loss. Smallholders who do not have their own means of transport rely on local traders or the closest mill. Thus, in some regions, there is only one logistically feasible direct buyer (agent, trader, or mill).
Trucks can pick up the FFB whilst delivering agriculture inputs, such as crop protection products (from Bayer, for example) and fertilisers, to the individual smallholders. But the volume of the cargo to and from each smallholder is small. There is scope for improved efficiency in coordination and movement, which could reduce logistics costs and improved access to essential inputs.
Poor road conditions (sometimes made worse by seasonal weather) and long waiting lines at the mills (often coupled with a lack of visibility of waiting times) have big impacts on transport time and FFB quality at the mill gate.
CHALLENGE 1
How might we improve the timeliness of price and other information, so smallholders can make informed decisions on when and who (agent or trader) to sell their FFB?
CHALLENGE 2
How might we enable transport providers to be more efficient in organising and coordinating deliveries of agriculture inputs and FFB in any given local area?
Let’s transform communities together
If you are interested in taking part in the Grow Asia Hackathon 2018, there is still space for new sign-ups. Research institutes, startups, SMEs, and enterprise teams are welcome to participate. If your enterprise is not based in Singapore but you would like to find out how you can be involved, please send an email to the organisers Padang & Co info@padang.co. If you would like to discuss how Ocean Protocol can help, please contact us communications@oceanprotocol.com
",Ocean Protocol supports Grow Asia to empower communities,185,ocean-protocol-supports-grow-asia-to-empower-communities-88f9b7b27f8e,2018-05-29,2018-05-29 19:18:08,https://blog.oceanprotocol.com/ocean-protocol-supports-grow-asia-to-empower-communities-88f9b7b27f8e,False,1864,The Data Economy,blog.oceanprotocol.com,,,Ocean Protocol,bruce@oceanprotocol.com,oceanprotocol,"BLOCKCHAIN,DATA,AI,DECENTRALIZED,DATA SCIENCE",oceanprotocol,Ethics,ethics,Ethics,7787.0,Alessandra Albano,Ecosystem & Operations Director @dex_sg | Founding Team Member @oceanprotocol | Operations | Marketer | Entrepreneur | Techie | Wannabe Yogi,13d5a6721a53,alessandra.alba,23.0,46.0,20181104
0,,0.0,9d6d3078be3d,2018-08-29,2018-08-29 09:54:03,2018-08-30,2018-08-30 08:38:17,1,False,en,2018-08-31,2018-08-31 05:01:30,11,2d2847e4a0f9,39.381132075471704,29,1,0,Simon de la Rouviere and Trent McConaghy in conversation with Erik Torenberg,5,"ArtDAOs, Curation Markets and TCRs
Simon de la Rouviere and Trent McConaghy in conversation with Erik Torenberg
This article is an edited transcription of the Venture Stories Podcast with Simon, Trent, and Erik. Here’s the audio version.
[Image: TheDigitalArtist CC0]
Erik Torenberg
Hey everybody, I’m Erik Torenberg, co-founder, partner of Village Global — a network-driven venture firm. And this is an episode of Venture Stories, where we deep dive on topics relating to tech and business with some of the world’s leading experts. I’m here for a fascinating episode welcomed by Simon de la Rouviere and Trent McConaghy. Simon and Trent, can you please introduce yourselves?
Simon de la Rouviere
Thanks for having us — it’s great. So, I’m Simon. I’ve been in the blockchain space now for quite a while, most recently started developing since 2013, and then in the beginning of 2015 started working in earnest with ConsenSys, where I was one of the first employees. Since early 2015, I’ve worked on a bunch of projects in this space from helping design the ERC20 token standard to thinking up new kinds of token economic models to where I spend most of my time these days working on the Ujo Music project, which is bringing the blockchain to the music industry. And I’m a full stack developer, so I do whatever is necessary to make things happen.
Erik Torenberg
How about you, Trent?
Trent McConaghy
Yeah, hi, it’s really a pleasure to be here — thanks for having us. My background is that I spent almost two decades doing AI, including AI for machine creativity, and AI for helping drive Moore’s law with computer chip design. And I got into the blockchain space really heavily starting about five years ago with a service for “IP on the blockchain” called ascribe. That work morphed into BigchainDB, which is a blockchain database. And more recently, we’ve been working also on Ocean Protocol, which is about shifting from data silos to democratizing data, sharing data to make it more accessible to the planet. And along the way, I’ve continued a lot of side interests in crypto and AI and the intersection — everything from AI DAOs to token engineering. It’s a fun space to be part of, and interacting with awesome people like Simon, to help to build this field together.
Erik Torenberg
You guys have written about a bunch of topics, so we’re going to cover a few of them. One is ArtDAOs — talk a little bit about what those are and why they’re important.
Trent McConaghy
Going back a couple years, Simon and I were both thinking about how AI might relate to blockchain. One day when Simon was in Berlin, hanging out over drinks with myself and Greg McMullen, we started riffing and ended up with this idea of AI DAOs — which is AIs, it can be an AI-type agent or something even dumber. But living on a decentralised substrate — a smart contract platform like Ethereum, and what that might mean. It’s sort of this AI that runs around sort of like a smart computer virus, a benevolent computer virus. Then, what are specific applications of that.
One specific idea that emerged was the ArtDAO, where it can generate art automatically using technologies like genetic programming or deep learning, and then sell it to online crypto platforms like OpenBazaar. Then, it can accumulate wealth over time, getting wealthier and wealthier. And there’s many riffs on this, like self-evolving its code, or self-owned assets. But the ArtDAO is a cool launching point, and it ties to interests of mine, going back in the art world, and Simon and others; it’s a great thing to sink your teeth into as an initial way to explore the technology of DAOs and AI together in kind of a fun way.
Simon de la Rouviere
During that time was when TheDAO came about, and we were wondering how to improve its decision-making process. One of the ways we thought it could be done was where the voting — the decision-making — was shifted towards the edges of the network, so the token holders wouldn’t be people, it would actually be some AIs. The people would delegate their votes to an AI, and this AI would be making decisions for people. The ideas spiralled from there, and we started thinking about other ways to combine those ideas together.
The ArtDAO thing is a really interesting idea. There’s many variations. Trent wrote most of the first article and I added an addendum. Then, I published a version which used prediction markets, and then you could use things like TrueBit to verify some more deeper generative artwork.
Over time, I came to realise I could simplify it further. At the end of 2017, I published the idea for this autonomous artist in a very specific implementation. This was a very simplistic version. I made the whole blueprint public, hoping someone would come out and build it, because I was spending most of my time on other work.
But no one else ran with the blueprint. So two weekends ago, I decided, you know what? I’m just going to start making this happen myself: start writing tasks, and submitting bounties for the work to be done. Surprisingly, a bunch of people got quite excited about it. There are now four people that have already contributed code. I’ve been spending my evenings and weekends trying to break down the tasks.
The general gist is there’s a generator, which is well-known in digital art. Given some deterministic input, it always produces the same output. The output is generative art. The generator uses the block hash as an input. Every day it sells a piece of artwork. The way this generator changes is you can buy into the autonomous artist to get a token, then you use the token to change the generator. Then using this token as well, you can participate in the revenue that this artist makes using crowdfunding mechanisms. So, over time, it would produce better and better artwork, and incentivize people to produce better generators over time.
I think the reason why it took off now is that the timing was right, with the collectibles and ERC-721 Non Fungible Tokens standard being finalised. People are excited, now exploring digital collectibles specifically in the art space. It took off also due to this new crypto-economic model. So I’m excited people have started contributing, so we’ll see where this goes.
Trent McConaghy
Agreed, I always find it fascinating, sometimes there’s great ideas for technology, but the time has to be right. And even when Bitcoin came out, in the one year, two years, three years following Bitcoin, there were a lot of ideas on Reddit and elsewhere about various things that could be built. But it was just way too early. Even when we started building ascribe in 2013, we were building on top of Bitcoin, and it was a pretty hard slog to get any sort of interesting functionality at all. And, you know, that was digital art too. But rewind 9 months or so and CryptoKitties comes along, taking some of those ideas, but building on Ethereum which has much lower friction. And then some new standards like Non Fungible Tokens which that just made it that much more straightforward. Even from the ArtDAO idea of a couple of years ago to now, technology has improved, further lowering the friction and helping improve incentives.
I think that bonding curves for Curation Markets are a wonderful example — perhaps they were the missing piece in the puzzle to have a good set of incentives and pay appropriate actors in the ecosystem in a clean way. Simon’s humble, so I’ll say it: he invented Curation Markets, and it’s almost every day you turn around and there’s some amazing new use for them. You can use them as an alternative to a classical ICO launch for example, as well as to fund commons, etc, etc. And in this case, for this Artonomous project, it’s just a very clean fit to help make things work together in a nice way. In short, it’s kind of cool how over the years, the ideas were there, but they’re getting more and more fleshed out, and it’s great to see that Simon took this step.
Erik Torenberg
More detail about the bonding curve please?
Simon de la Rouviere
I’ve been trying explain this in various ways, so hopefully I can make it simple this time round. The goal is to have a way where people can mint tokens, without relying on there being an entity that sells the tokens to someone else, and then captures that. So it’s essentially just a smart contract where let’s say in this example you have Ether, you stake Ether or lock Ether up, and in return you get new tokens. But the price of the tokens is determined based on how many people have exactly done that, like how many people have staked Ether before, so it’s directly related to the current supply of tokens that is in circulation. And, over time, if there’s more in supply, it’s more costlier to mint, and then at any point in time someone can sell the tokens back for a proportional amount of Ether. But the value of that tokens increases, so if you bought in early, and you sell later, then your extra token that you bought from this bonding curve is worth more and vice versa: if you bought in later, and many people have sold, then your token is worth less.
So, there are different ways to describe this, if you’re from the traditional finance world, it’s just an automated market maker, with a specific way in which the price is determined. But it’s all just hard-coded.
And this can be used in many many different ways, and I just think one of the reasons why I did this was precisely that reason: I felt there was the reason why we had these separate entities that were doing the work of buying and selling tokens and selling to people just felt very manual. And now that we have the possibility to do this in a very automatic way, why are we setting up these companies and institutions to sell tokens, when we could just do it automatically? But it’s much broader, it could be used in various different contexts and one of them is an autonomous artist.
Trent McConaghy
If you want to have really nice price discovery, a really great way is if you have something that’s fungible like US dollars or Bitcoin. Fungible, as in, one can be exchanged for the other, as they’re the same thing. For price discovery, you need sufficient liquidity: a sufficient number of tokens out there and then it can list on an exchange, right? And at that exchange, you have this order book, this list of people giving bids and another list of people giving asks, lists of how much people want to pay for a token or sell it for. And when a bid and an ask align, then you’ve got a match, and the people make the trade. So, that’s in an exchange, which is great, that’s sort of the ideal for price discovery.
But what if you have just launched a token, and there’s hardly any owners of this token, hardly any buyers or sellers or anything. What do you do? How do you bootstrap from something where there’s no tokens or only 5 or 10 tokens minted to having 10,000 owners, and a million tokens. A bonding curve via curation markets is a great way to do it, to bootstrap from nothing to much deeper liquidity, liquid enough where you can have an exchange.
So basically on day zero, you have no owners and tokens, and you just have this curve, where the cost is near zero. And the first person buys tokens — you know maybe they spent Ether to get artist tokens. And those artist tokens, maybe they get for 1 Ether, they get 100 artist tokens. The next person comes along, and they have to spend 2 Ether to get 100 artist tokens, and then it gets more and more expensive. And, over time, you’ve got say 1000 people earning artist tokens, and then you’ve got sufficient liquidity such that even exchanges can say: “hey, you know, this is liquid enough, we’re going to list this on an exchange, and you can exchange back and forth”. So, suddenly you have many mechanisms to buy and sell, anyone can go and buy and sell these on a traditional exchange, but you’re always going to have this one agent in there, this robot, will buy and sell for a particular price, according to this automatic curve. So, these bonding curves are a really great way to kick-start a market basically, based on supply and demand, and if that thing ever wanes in popularity, then it will just basically, you only have this bonding curve left, and it could drive back down to zero for example.
Simon de la Rouviere
When designing tokens in this new industry that we’re in, the tokens should have a way for themselves to naturally dissolve. Right now there are a lot of zombie tokens that everyone just kind of forgets about, and then they’re just kind of there forever.
Bonding curves are different. They have a natural tendency that if people aren’t interested anymore, then just sell the token back and always leave with something, even if it’s very little, but they can always leave with something. And the token project can naturally dissolve if there’s no more interest in it.
Trent McConaghy
Exactly. And, a kind of related thing, is this idea of burning and minting. Normally it has to be done manually, and burning is actually kind of rare and feels a bit weird.
But on bonding curves, every time someone wants to buy, say, more artist tokens in exchange for Ether, then there’s actually more tokens getting minted; and every time they sell, then those tokens are getting burned. It’s very natural — the dissolving of the artist token happens when everything has been burned. It just naturally dissolves, goes away, and you have zero artist tokens left.
Erik Torenberg
I want to ask you guys about, jumping a few steps here, Simon, I saw you tweeting earlier this morning about universal basic income. A lot of people say that humans, in the future, we won’t need jobs because we’ll doing poetry and art, and stuff that we uniquely can do, and here we are talking about how DAOs will do art better than humans. I’m curious about how you think about, and I’m a freestyle rapper, I’m worried about losing my job, losing my skillset to AI. How do you think about the role of humans in future?
Simon de la Rouviere
(Laughing) good question. I think my belief is that whether we want to or not, we are moving into a world where software is going to replace a lot of what we do. But, work or labour allows people to play what I call “status games”, so that if you have money you have more status. And, you know, work and labour is almost seen as a proxy for status games besides it being for the work itself, but also the money you get from doing this. So it might not be the super “hippy” version of the future, where there won’t be any money involved, there will still be some ways in which society or humanity will want to play these games. What we need to do is to shift the reliance of status games being on labour and work being done, and rather make status directly towards what we feel is valuable contributions from society.
So, in the industrial era, where your value to society was simply how much work you did per day. But now we can say, “listen, actually I don’t feel like this person working 40 hours a week is actually a better person than this artist, this artist has no money, this person working 40 hours a week has money.” I feel like there’s a disconnect here, and that disconnect is growing due to the leverage of information sharing.
These days, everyone’s a content creator, they’re posting photos on Instagram, they are tweeting, they’re blogging, they’re sharing information, they’re consuming information that’s more mass scale. They’re curators. But none of these things are actually making people money, even though we feel these are valuable contributions to society. So whatever economic models we have should move towards saying “this is a valuable contribution to society, and I feel like you should deserve some value for this, whatever that value is.” And that’s what gets back to the point where we can say, “cool, this is the new, sort of status game that society is playing, this is an attention economy, or an information economy.”
Erik Torenberg
Yeah, you talked a lot about monetising attention, I’m curious, do we think that status or social capital will become much more positive-sum in the future, and how do we think that translates to financial capital?
Simon de la Rouviere
It’s an interesting question because ultimately attention is zero-sum. So, it’s hard to reason that social capital can become more than zero-sum. I need to think about that one more.
Trent McConaghy
Maybe I can jump in here, going back to the UBI thing. I think with universal income, the goal is to decouple work from self-actualisation, such that you don’t have to worry about can you feed your family while you grow yourself and the people around you. So, if you’re an artist, or a musician, or a writer, or a scientist, or anything, you can pursue those interests and make gains in reputation, following, etc. But you don’t have to make money in order to feed your family, and you know, this is the dream of UBI.
A key challenge is how to pay for UBI. There are some new ideas on solving the payment challenge by leveraging AI DAOs.
Getting to UBI also actually more pressing than one might realize because of AI coming along. People have talked about AI taking jobs, and there’s an argument back and forth about many jobs AI will actually take. (Some say that humans will simply control or drive AIs to do work. This has been around for a while, for example I built CAD tools for circuit design, and those tools were for humans driving AIs to design circuits.)
I think that AI is going to take away more jobs more quickly than jobs are created. A great example is self-driving trucks, which are already on the road. They’re going to replace the truck driver jobs in America. That’s 3 million jobs. And it’s not only going to take only those jobs. These truck drivers need to stay in hotels and go to gas stations in middle America — all of that is going to get decimated. So this is going to be a pretty big societal upheaval, and this is just America alone. And those people aren’t going to sit on their hands — they’re going to be pretty frustrated with how things are going.
I see that UBI can actually help to offset some of this societal upheaval, and help transition to more of an economy that can be sort of not relying on scarcity, but instead where you can self-actualise. And the key to this all is something like universal basic income where people are able to feed their families, and ideally even not just have basic income, but enough to self-actualise — going higher up in Maslow’s hierarchy.
Blockchains can actually play a big role here. The distribution side is easy: anyone who has signed up with “proof of human” can be getting some of the money flowing into a blockchain, and there’s projects related to that. Then on the other side, where does the money come from? There’s some roles where AI DAOs can help there with things like self-driving cars, self-owning cars, once those cars have paid themselves off, they can start distributing to some sort of UBI chain. So, overall, there’s a really strong interaction between AI and blockchain, and jobs, and the future of work.
—
Erik Torenberg
Talk about, you were talking briefly about NFTs — non-fungible tokens, and we talked about collectibles, a lot of people when they hear that, all they think of is CryptoKitties, and dismiss it as a silly game, and we’re not even sure if CryptoKitties are even doing that well anymore. I’m curious, talk more about why collectibles, NFTs are game-changing, and where you see future applications.
Simon de la Rouviere
I love this — I love collectibles. I think, as humans, I’m not sure if AI or agents will care so much about it, but as humans, we are suckers for collectibles because we ascribe a lot of, not necessarily sentimental value, but the equivalent of it for a lot of things. Like we ascribe a lot of meaning towards our objects, and historically collectibles have been a big part of that. Nick Szabo wrote a really good article which to me just explains almost everything about the crypto space when he talks about collectibles and why money came from collectibles.
So, I still think collectibles is going to be very big going forward, and because we’ve reduced barriers to entry to mint these things on a very massive scale, and minting them in a way that they are immutable, they can be used in transitive environments from one thing to another, and they can be built upon — and that is a holy grail for emergent behaviour.
I’ll give you a quick example which I think people should start experimenting with: just minting collectibles for the proof of things that you did. That’s all. Like if you walk into a store, the store should give you a badge that says: “Simon was in the store today”. That’s it. It might matter to you very little, but if it’s just more than enough than the cost of minting it, it was a valuable thing to issue. Then the store can decide to do whatever it is. But then someone else can also use that information: they can give you something, like if one store sees that you have another store’s badge, and can go and say: “why don’t you come to us, we’ll give you 25% off if you have a badge”, or what if a pet store owner gives all the CryptoKitty owners a discount because they are clearly into pets because they own CryptoKitties.
But there’s a lot of ways in which we can say to people, “just start issuing collectibles for everything, since the barrier to entry is so low. If you don’t think it would be useful, just issue it anyway, and then see if it becomes valuable”. There’s so much upside to issuing these things, and little downside, so just do more. I think people are just still sort of thinking it in context of it gaming, like cross-game gaming, or collectible that way. But it could be for everything, that’s what I want to see, and I think it’s going to get very interesting in that sense.
Trent McConaghy
I think it’s worth emphasizing that a lot of these ideas have been passed around for years and you know, different prototypes or projects working on it, but the friction has really gone down because of these new specs that have emerged. Especially in the last year, all these ERC specs are really helping to make it that much easier for projects to create these.
And because of this, we also have interoperability where we can have specs building on specs, building on more specs yet. A great example there is the NFT composables spec from Matt Lockyer, where you have a CryptoKitty, that is attached to, say, a ball, and then that CryptoKitty has a ball, and these things are locked together, and you can buy them together, so you can have these hierarchies of things. Dimitri De Jonghe had this idea of like give it a disease, no one wants that, but you could actually lock these things together, so things can emerge in funny ways too.
And it doesn’t have to be thought of as frivolous. Art has played an essential role in society: artists are really great at asking questions, uncovering the truths that aren’t spoken of otherwise, and exposing them in ways that kind of hit us in the gut, and make us ask new questions, and research new solutions to the problems. So these NFTs and token designs around them will help to unlock that within the world of crypto and to the broader mainstream.
Erik Torenberg
Yeah, a lot of people are asking questions like, “Is blockchain necessary for this?” One example is prediction markets. They’ve been around a long time. How does blockchain uniquely enable them in a way that they weren’t before?
Simon de la Rouviere
There’s a few things to me that are important. One is that you can have, the fact you can own it outright. I think that’s valuable. The fact that it is a time-stamped proof of something that happened at a certain time. Especially for patronage collectables, like where you have proof that you were an actual fan, and you bought a fan badge in 2016 for example.
Then there’s the interoperability between various components, a reliance on other intermediaries. I think that the value from a developer comes from the fact that you can play around with this in a permissionless way and the emergence it provides. And, knowing that whatever you build, that it could last for a reasonable amount of time. That this isn’t just a startup that’s going to go down in the next two years and get bought by Facebook or whatever. So I think there’s a lot of valuable ways in which this is different.
Trent McConaghy
Yeah, and those are really great examples. Here are a couple more. One of them is: in the world of art, provenance is everything. So, if you go and buy a painting, but you have no idea where it came from and stuff, it’s hardly worth anything. But if you do know — there’s some strong history of ownership of where that came from, or of who just even held it — that matters. With blockchain, you get this built-in provenance; and the provenance record is secure and immutable. So, that’s a big one. It works for art, and also for a broader set of tokens, and collectables.
The other one, which to me is the super-power of blockchains, is incentives. Blockchains are essentially incentive machines, where you can ask yourself: what do I want to get people to do today (in this network)? And then you can set things up such that people are incentivized to do those things, either explicitly or implicitly. And then people to those things, and get paid in tokens. There are many possible building blocks, such as the bonding curves we talked about, and NFTs, composables, etc. And a whole bunch of other things.
It’s about figuring out the token mechanics: what incentives are you trying to design towards, and what sort of emergent effects you want happening in the network. Right now it’s still really an art to design these tokenized ecosystems. But it’s slowly turning into an engineering discipline — token engineering.
To summarize, blockchains as incentive machines is wildly powerful and that is a completely new thing compared to before.
Simon de la Rouviere
I find that my mind keeps extrapolating what could become possible. We’re at this precipice of just like enormous change. But I have to admit — and I think we’ve spoken about this before in various times, and at the COALA Blockchain Workshops they explicitly had these discussions — I’m still somewhat worried to some extent to what we’re inventing. In the sense of, like you said, this is incentive machines, you could say like: what do I want people to do tomorrow? And you could just write a piece of code that’s a few lines, and publish it on the internet, and boom, you have people doing things. I mean, Bitcoin is the perfect case, the example that a whitepaper was published, and now you have people spending billions on infrastructure and money and electricity just because someone wrote a few lines of code. And it’s not all utopian I feel, it sometimes makes me worried.
Trent McConaghy
I fully agree, and Bitcoin is sort of like this AI paperclip maximizer that people have talked about, that sort of converts the whole world into paperclips, and in the case of Bitcoin, it’s basically eating the energy of the world by mid next year, it’s on track to use more energy than all of USA, and it’s like: oh, wow. Oh crap, right?
So, we have this wildly powerful new tool, that can be used for good or bad, it’s a super-sharp double-edged sword on both directions, right? Just like dynamite. The Nobel prize was created by the inventor of dynamite because he wanted to emphasize the good while acknowledging the bad.
I think society hasn’t fully realized this yet. In fact, probably most engineers in the blockchain space haven’t really fully realized this yet. Many have, but it needs to be more broadly disseminated. This tool — the blockchain as incentive machine — is way more powerful than we think. It’s sort of like realizing you can split the atom, and that you have the potential for power generation (nuclear energy), but not really realizing that you also have an atom bomb. So in blockchain land, we’ve got this wildly powerful tool; and it’s just much more powerful than people realise and we have to be super-thoughtful about how we use it.
This is actually part of the motivation why Simon and I think keep riffing on the ArtDAO. It’s a smaller thing that’s easier to conceptualize, to help people realise the broader ramifications: we build this, we learn how it works, we see how it can accumulate wealth, and then, from that, generalising to more broad AI DAOs, and other DAOs and other sort of blockchain incentive machines where the discussion can happen especially around ethics and responsibility. This is arguably the most important topic of all.
Erik Torenberg
Yeah, I mean, how do we, so how should we think about it? How should we design to design to protect against it? To protect ourselves, I guess. Or is it sort of a thing where, hey, it’s going to happen, and just let’s be reactive. What should we do? What are you advising?
Simon de la Rouviere
That’s hard, I mean, I’m trying to reason myself out of the perspective that it’s somewhat nihilistic in a sense that if some accidentally-negative thing is going to be invented at some point in time, it will. Whether it’s, Trent and I working on this, or someone else in the next five years doing it. Much of this is going to happen, in some form, so the only question is what can we do about it. Sometimes it feels like “not much, really.” If we are going to be human suckers for these incentive games, then what, are we even capable of stopping any of it?
What if, let’s say that a nefarious DAO comes to exist. And that it starts doing things that negatively impact on people, like it’s incentivizing people to kill other people. Then the problem is that those people that are siding with this nefarious DAO will try to make sure that people aren’t, say, forking a blockchain to take out this DAO, or whatever. So, essentially the system incentivizes people to form tribes and kill each other. That’s a nefarious potential outcome. So in some ways it feels like much of this could be inevitable.
That said, I’m not that nihilistic (laughs). I think there are still potential ways that we can mitigate some of these potential issues into the future. And I think that if there’s discussion about it, that’s a good valuable first step to saying, “listen, you might think that this is fun and games, programming this funny incentive game, and then suddenly crazy things happen — just to be more aware.” To be more aware of what’s possible.
Trent McConaghy
Yeah, I think there is no silver bullet, but it’s very irresponsible to say, “oh, it’s just code, we’re just putting this out there, I’m just playing around.” It’s a lot more than that, right? Blockchain technology and AI, of all the technologies out there, are probably the ones that are most deeply infused with ethical choices. Every single line that you write, and deploy, actually is making an ethical judgement call. Which is kind of crazy to think about. It’s not just code, it’s these machines that are interacting with humans, and human incentives.
And so, having the conversation and getting the word out, for more people understanding and thinking about this, is really important right now. I think there are some tactics that we already recognise are going to be helpful. For example, if some nefarious DAO starts leading to terrible outcomes, maybe you can fork it out of the system, like Simon hinted at. Same thing if you have an ArtDAO that can generalise, and it starts doing some bad things: fork that out of the system too. Or use on-chain governance rather than a hard fork. So, hard forks or on-chain governance are a bit of a “get out of jail free” card, at least for the near term.
However, that will change over time too: right now, most blockchain transactions are humans interacting with blockchains, but in the future, instead of 100% humans, 0% bots interacting, it could be 99% bots, 1% humans. And then those bots, they aren’t going to thinking at all about like “is this good for humans or not, is this hurting people?”
It also means we only have a limited time to think about this. This scenario of 99% bots could be as soon as 5 or 10 years from now. But maybe they’ll be really dumb bots just like right now, you know, lots of bots operating on traditional Web 2.0 ad exchanges. And that’s not a big deal, right? So they’re kind of on the side, it’s not really central to the decision-making of the web. Maybe it’ll be like that in the future too, hopefully. But I worry that it won’t be.
So this is just a discussion we need to keep having, and asking about what other tools we can deal with it. But we can’t stick our head in the sand, and say, “okay, we’re going to have good AI, we’re going to have good DAOs,” because all it takes is one person or entity to create something that’s not so good, and we’ll have to deal with it as a society.
Erik Torenberg
Yeah, I think, I’m curious, because you guys are such futurists, and thinking about this from so many different perspectives, I don’t know if you guys have kids, but if you do, or let’s say you will have kids soon, how do you envision their world might be very different than I envision my kids’ world might be different — or the average person envisions their kids’ world might be different?
Trent McConaghy
I have kids aged 4 to 6. And I think a lot about this, what future am I helping to create for my children? And how are they going to be living. Broadly speaking, I see that blockchain itself has the potential to rewire the power structures of society, and that could be for good or for bad, depending, as it’s a very malleable medium; and I really, really hope that it’s for good. And I’m doing as much as I can to help nudge that and shift that in the right ways.
Every sort of organisation — governments, corporations — all that, could be tokenized in the future — 10, 20 years from now. And hopefully it won’t be, the old kings becoming the new kings, or new people becoming new kings, but instead, the power is spread, the wealth is spread etc. And so, for my children, I think about what sort of substrate of civilisation can they live on, such that they can self-actualize. Such that they can grow up to be whoever they want to be, to change multiple careers, or not even have a sort of a career in the traditional sense. And just create value to themselves and society, and grow as humans.
Right now, we’re kind of at a crossroads in society again, where we have this wildly powerful new tool, and it could end up where it biases towards really dark things, or it biases towards really positive things. And all of us in the blockchain space have to understand this, and we have to really try to bias towards a positive future for ourselves and for our children.
Simon de la Rouviere
I don’t have kids, but I recently became a godfather, and it made me think more deeply about the future. I think even in our lifetime, not even our kids’ lifetime, is that we are going to be in for a lot of interesting times. The way to describe it to say things are going to get a lot weirder in many ways. We are going to ask a lot of questions to ourselves as society what it means to be human, and what it means to live in this new and upcoming exponentially growing economy, and powerful technologies that we build for ourselves. And I think it’s just going to be even more weirder for our kids.
It’s just going to be such a different world, and sometimes I’m not sure if as humanity, we are quite ready for that. I think in some sense, we’ll just accept the weirdness that comes. When the web came about, people were like: “this is going to be this great tools for academics, we are going to share information, and share knowledge”. And then in the year 2018, people are sharing SpongeBob memes. So, it’s like this unexpected way in which these things will maybe be used in the future, and what if our kids one day are meme traders. That’s a completely possible career choice from our current 2018 perspective.
So, it’s going to get weirder, and I hope that the sort of feedback loop that we are currently designing society doesn’t cause everything to blow up, and it feels like there’s a few roads that we can take from here, that will lead to a more prosperous and abundant society, and I hope we take the right ones and everything doesn’t just blow up.
Erik Torenberg
Say more about how it could get weird — like meme traders is one example, what are other explicit manifestations of it getting weird?
Trent McConaghy
One way is to riff on AI DAOs. Start with the ArtDAO. The ArtDAO is an autonomous entity running around creating art, creating wealth. But you don’t have to stop at art, you can have the DAO creating or controlling a variety of things.
For example, you can have a self-driving car, that also happens to own itself. Maybe Daimler manufactures it, and then, over the span of five years, it becomes its own Uber driver, and buys itself back with a leasing plan from Daimler. So you’ve got this car driving around, it’s its own corporation, it has person-hood, it has rights, and it owns itself. And of course, so you have this self-driving, self-owning car. If it needs repairs, it just contracts out a tow truck to just come and pick it up.
Let’s say there’s one car, then two cars, or a whole fleet of cars. Maybe you have a sort of macro entity that starts buying these fleets up, and maybe it’s its own AI DAO. So we have fleets of cars, fleets of trucks, all as AI DAOs.
Then you also have self-owning roads. Right now, roads are created by governments, which pool together resources from a bunch of taxpayers, and then they go and build those roads. Well, in the future, any community, it could be the cars, it could be humans, pool together resources to build a road that they can collectively use. They could do the same thing for power grids. And so on.
So, over time, we could actually have sort of the infrastructure around us, all these atoms, that we use to interact with the world, from transit to energy to food — all of this, this could be actually a whole bunch of independent agents that are self-owning, they’re autonomous to whatever degree necessary, like with AI, but autonomous for at least the self-owning part, and interacting in their own ways.
There’s a cool way to frame this overall. Let’s pause that for a second and think about Nature 1.0: What’s a tree, right? It’s taking in resources like sunlight and moisture and CO2 , and outputting oxygen and growing wood — manufacturing wood, all these things, right? And it’s doing as its own thing, it’s sort of this autonomous thing. And then there are forests and broader ecosystems. And there are lions and bears and mosquitoes and all this. And so, that’s sort of a Nature 1.0. This cradle of civilization.
Then how I see towards this weird future, we have the potential for a Nature 2.0, where you’ve got these AI DAOs, that are enabling self-owned self-driving cars, and swarms of self-driving cars, and self-owned power-grids, self-owning forests like Terra0, and so on. And this is a potentially new infrastructure for civilisation. It’s these things around us that are helping us. So, just like Nature 1.0 is the cradle of civilisation, Nature 2.0 can help upgrade that. So, that’s kind of a positive framing. It may seem a bit weird. But it’s a positive way to see how things could go, and, to me, a really nice vision how blockchain can help a lot.
Simon de la Rouviere
That’s awesome.
I have the sense that there might be a likelihood of something developing in the next five to ten years, what I call “reality registers.” And the concept comes from an extrapolation about the way people are consuming information in the year 2018. And this concept of there being fake news, so it’s becoming increasingly easier to fake any kind of digital content. As in “off-the-shelf” super easy to fake. You could make Donald Trump say whatever you want, and you could just post it, and pretend that that happened, and that it was real.
So increasingly, anything that we see on our screens will be harder to trust that it actually happened in reality. How do we solve that? One way this extrapolation works is to say: okay, well, this is going to be an issue in the future, and there’s going to be a big event where something like this happens. A nation state is going to say, “shit, like this was a problem, we need to solve this, how are we going to do this?” And they’ll say, “cool, any kind of media that’s produced by the presidency of the United States, is signed by a digital signature,” and the United States has a digital seal that they use to sign to this media that happens. That anything from them is trusted media because of the digital signature.
But when you extrapolate from there, the problem becomes: how do you trust the key? Because the key could be forged, or it could be hacked. And then you’re just kicking the can down the road to an eventual problem. You will get this case where people will say, “I as society, or a group of people, believe that a specific key is the key that is owned by the presidency of the United States, and that everything that is signed by that key is the believable truth.”
So actually being there in physical reality becomes the only way you can trust that anything actually happened. But then you will realise that you can extrapolate that to anything that happens in reality too! So, we’ll have this sort of “web of trust” of keys, like trusting other keys, and signing things that are happening in the real world, or that are trusted in the digital space.
And now let’s tie in incentive games on top of that: incentivizing people to commit to digital realities of what happened in the real world. It might be the case that due to political or ideological lines, people might actually start trying to fake physical reality, to then prove that it’s a certain truth, and they will be incentivized to do so.
So you get this weird future where everyone is trying to figure out what is actually happening in the real world, but they don’t really know, and they only trust the people they are close to, to figure out what actually happened in the real world. Reality itself will become probabilistic. If the president said something, you could only believe it like an 80% chance that it happened.
Trent McConaghy
That’s a really cool vision, let’s riff on that. There are going to be things like augmented reality goggles, where we wear them, they will re-paint the world in front of us. So, our reality, normally what we see in front of our eyes is the truth, but what if we’re walking around, and everything we see, maybe people get erased — we don’t see them as they walk down the street because maybe they are turned into “nulls” in society or something. And that’s an example of a negative thing.
If you think about our brains, there is no such thing as reality, it’s just sort of what model our brain has constructed of the world. We only have tiny little holes to peek into world to see what is going on via our eyes and our ears. Everything else is our own belief that is constructed in this world model.
So, you know, the digital world is the physical world, it’s the same thing. And that poses problems: where do atoms end, and bits begin? (And vice-versa.) But I think what Simon describes is actually a great potential solution — this web of trust of various beliefs. And so it’s basically how do you interact, how do you let your world model interact with someone else’s world model, based on the perceptions and the beliefs that you have. And the truth is much more subjective, much more probabilistic as Simon put it.
Simon de la Rouviere
So yeah, to think it will be like something didn’t happen if it wasn’t on the blockchain. (laughs).
Trent McConaghy
Exactly. And that is so profound. I love that.
And to riff on it further: even if it’s on the blockchain, it’s still a claim. Making the claim is the first step, and then the claim has to get validated. Validated by the web-of-trust around you. So, it’s sort of like filter bubbles of today’s society, taken the the Nth degree.
Erik Torenberg
Share with us how you guys think about how we’ll think about identity and reputation in an increasingly blockchain world. One question I’ve had is what does social scalability for identity look like, I was just talking to some entrepreneurs working on a project where trying to basically sub-divide people’s identities and reputations in the sense of, you know, Marc Andreessen has a crypto-reputation, and then he has a politics reputation, and his politics reputation suffers because he says a quote about colonialism in India, his technology reputation doesn’t suffer as a result of it instead of sort of the bundle we have right now. That’s just one sort of application or thought as (inaudible) identity of reputation, but what are your thoughts on the topic?
Trent McConaghy
You know there’s been lots of thoughts and writing on this over the years. Let’s start from a reputation perspective. Cory Doctorow wrote this wonderful book more than 10 years ago called “Down and Out in the Magic Kingdom”. It described a global reputation system, where you have one reputation for everything called Whuffie, and you can literally live and die by your reputation. Super negative consequences. So that’s something definitely to avoid. There’s a Black Mirror episode with similar ideas.
Whenever I meet people who are doing tokenized reputation systems, I ask them to avoid explicit global tokens for reputation, because we don’t want to end up with this scenario. There’s been actually some really thoughtful stuff in the blockchain space, such as local reputation systems like Matan Feld and Primavera De Filippi did with Backfeed, which has since morphed into the DAOstack stuff. They had reconciled their design with Cory’s concerns.
And there’s other work out there too where it’s things like OSCoin, which is figuring a way to compensate developers, without trying to do it explicitly because people are contributing open source, not for the money, but for reputation essentially. So, on the reputation side, it’s something that you don’t want to have reward extrinsically, it has to be sort of more intrinsic motivation.
On identity, the baseline is currently: your identity is your public key. Where you can show that you have that identity, if you can demonstrate with your private key, right? So, that’s kind of the baseline. More recently, there’s work coming out of Rebooting Web of Trust, Decentralized Identity Foundation, and W3C. The main result is a new standard called Decentralized Identity or DID. It’s sort of a generalisation of public keys that allows you to switch public keys, with some other really nice features.
The DID can have resources attached, and you can give permissions to those resources. And that’s a really nice baseline for the future. For me, that is sort of the identity of the future.
So for this DID with resources attached, it would work for humans too. For example, “Trent McConaghy” is a DID that happens to have a resource attached of the meatbag body that is Trent McConaghy’s body. Or, a self-driving car is the DID that happens to have a car body attached, and maybe over time, it acquires another car and another car. So that’s sort of the baseline from which we work, and everything else emerges from there.
Simon de la Rouviere
It’s not a topic that I’ve given too deep thought about, it’s only recently when I’ve started thinking about it again, when someone asked me when I thought about UBI.
But reputation is the same thing as money, if you think of them both as just giving you options. So, money is a broad fungible reputation: you can spend it in various ways, and it’s fungible. Whereas a traditional reputation just gives you access to opportunity, or optionality. For a more specific context: if someone knows that I’m a musician, it gives me access to go to someone’s studio, and play music with them. Or if someone knows that I’m a writer, then they can interact with me in various ways. It opens up options for me in different ways, using my various facets of different reputation. And that’s the kind, the kind of way of thinking about reputation in that sense.
Trent McConaghy
There’s actually one more thing in there: implicitly, bonding curves and reputation are very related as well. Basically, the more people have invested into a meme, or song, or anything on a bonding curve, then sort of implicitly the more reputation it has. So they are a bit of a proxy for each other, it’s not a fully 1:1 mapping, but they are really related.
Erik Torenberg
Talk about, one topic you’ve written quite a bit about is Token Curated Registries — TCRs. What are they and why are they so game-changing?
Simon de la Rouviere
Well, TCRs sort of broadly fall into like a broader space of things that have started developing in a token engineering and curation markets sense. Which is, you know, the usage of tokens to curate information.
Then, a TCR is a set of incentive games that essentially tries to ensure that it brings about a set of curators that curate a list, to the extent that the list maintains a certain kind of quality that it aims to attain. A good example is someone using a TCR to curate what are good restaurants globally. And so, there’s this binary game that’s being played, of restaurants being in or out of the list.
Since TCRs were first published in like September 2017, there’s been a lot of innovation happening in a very short space of time. People have taken an idea and really ran with it. From like graded TCRs, layered TCRs, nested TCRs, people writing stuff about economy sizes, bootstrapping them, different ways to vote, things to rate, it’s been quite fascinating to see.
I think the reason why people like TCRs, is that access lists, and lists in general, are a very big part of society in general. Being able to create these lists in a way that doesn’t rely on central institutions, but rather just a set of incentive games, that’s very useful. When people say “smart contract,” TCRs are, to me, the first definition of what “smart contract” actually entails. A very useful combination of games, that essentially produces a valuable list.
Trent McConaghy
Yeah, and riffing on that, it goes back to our earlier conversation about lowering friction. TCRs are the sort of building block for crypto that once you aware of that concept and you can go and grab the code from, you know, Adchain code or otherwise, then you can just use it. So, you have that sort of symbol in your head of building blocks you can use and deploy, and a very good chunk of projects on Ethereum and otherwise now, are using TCRs, and that’s all they need. And I think that’s great, you can have this simple building block, and just deploy.
Adchain has this whole ad exchange. All they need on-chain is simply this whitelist of good websites. That’s it. Everything else can be off-chain. So something simple and elegant can be deployed.
Like Simon said, there’s this growing set of building blocks, around TCRs and bonding curves and more. It feels a lot like the late 90s where these design patterns were being created for software engineering. Things like the “Factory” pattern. These patterns were getting compiled into a wiki pattern repository, and that led to the emerging field of software engineering. Books were written about it, for a common language to software engineers, making it easier to reuse each others’ components, or have different implementations of these things.
That’s exactly what’s happening now in the world of token engineering: in building up these various building blocks, and deploying them to systems in sort of a repeatable fashion. And that really helps for security, it really helps for reliability — all of this.
Simon de la Rouviere
One way to reason about TCRs and why I think it’s blown up quite substantially, is: I think there’s very useful space in which TCRs are better than anything that has existed before, that society had available. You can curate a list of things that could be used in many ways. But the space where it’s valuable is not in a space of where we could just self-regulate, like if you are just 10 to 100 people in an industry, and you want to self-regulate, it’s still relatively easy due to the small number of participants, so scope stays small.
But what if the industry grows to the point where self-regulation becomes hard? It’s hard to do, and hard to maintain, because the incentives involved don’t necessarily allow self-regulation to work effectively. So to produce valuable outcomes, we resort to a top-down regulation to regulate things, like nation states.
But in between self-regulation and top-down regulation, there is a gap, for things that are useful to people. If you are an information society and asked to regulate information, that’s where I think people need to really think about TCRs. Don’t think about these small use cases; instead, think about the gap between self-regulation and the regulation that a Nation-state provides. That’s the holy grail where these things I think will be very valuable.
Erik Torenberg
Very cool. Guys, last question for you. We’re talking from, Trent, you’re in Berlin, and Simon, in South Africa, what is the future of distribution in terms of influence and power as it relates to crypto. It seems like San Francisco doesn’t have any inherent advantages the way they do in traditional Web 2.0. How do you think about the geographical distribution of power and opportunity in crypto?
Trent McConaghy
I think, the tech world has long dreamed of global tech ecosystems, and frankly it hasn’t really happened with traditional tech industries. Even, you know, there are dreams of the web becoming more spread out. But I see that blockchains by their very nature, are jurisdiction-less and permissionless — they don’t know where borders end and begin. And we are seeing, actually, with blockchain that it has become a global tech ecosystem. There is no one centre of the universe for blockchain, and I think that’s really wonderful. You know, you can be a coder in rural Canada, I am calling from Saskatoon, Saskatchewan right now actually. And you can have just as much influence as a coder in Silicon Valley, and build up a network around you and so on. Because really it’s people interacting online and so on.
That said, there is use in having a good group of people around you to help build things, to discuss face-to-face. In the global crypto-ecosystem, people do travel probably more than traditional, like to different conferences for Ethereum community events or broader blockchain events and so on. I think that’s really valuable, so to me, it is sort of a seismic shift from one centre of power and influence for tech to really the world’s first truly global tech ecosystem, and the technology itself is going to propel us.
I’m very proud of working out of Berlin too — there’s great people there, but there’s also awesome people in Cape Town, and London, and New York, and there are some great teams in San Francisco too, but it’s just not dominant. And to me, that’s really healthy for the world. To sort of spread power and wealth. And that’s really, why are we all here with blockchain, it is really about restructuring the power structures of society to have greater opportunities for everyone, to level the playing field more.
Simon de la Rouviere
Yeah, I think that’s absolutely true. I think it’s really great the way it’s moving. There’s a confluence of things that led it to where it is now. When the tech world started and rose to prominence, and when the web came to prominence, it was because we still relied on physical things — like people and money — to actually build the infrastructure to get where we are now.
Now, we have globally access to fast internet, we have Github as the collaboration tool. And another piece of the puzzle, is that we’re moving away funding from being reliant on networks — networks of people. I don’t have to be in Silicon Valley anymore and know how to get a hot intro to a VC on Sand Hill Road, that’s unnecessary for me as an entrepreneur now. There are now other ways in which I can do the same thing, such as launching a blockchain network.
It’s all these things together. And it’s really only the start of it. Once the blockchain infrastructure is more mature, there’s going to be a sort of golden era of “tech as global movement”. And some of these components, but the last one is just a fully functioning blockchain ecosystem, and it’s only started in earnest in the past year or two. So, we’re almost there, and I think it’s already showing that that’s possible, so I’m quite excited to see that happen.
Trent McConaghy
Agreed. Actually, one way to sort of also describe this, Nick Szabo wrote this wonderful article about social scalability: how do you get beyond that first 50 or 150 people (Dunbar’s number) where people can still collaborate well? If you are in one place, then you can get trust that way, because you can see each other face-to-face, work interactively. Blockchains as “trust machines” help to minimize the trust you need among the people for this overall system of coordinated humans. Therefore it can lead to much greater social scalability, towards global scalability, which we are seeing now in this emerging global ecosystem.
Erik Torenberg
Cool. Guys, I think that’s an amazing way to wrap. Thank you so much for coming on the podcast, there were a bunch of topics we didn’t get to as well, we can do another episode in the future, if of interest. Thank you, this was excellent.
Simon de la Rouviere
Yeah, thanks you very much.
Trent McConaghy
Yes, thank you. I appreciate it.
","ArtDAOs, Curation Markets and TCRs",458,artdaos-curation-markets-and-tcrs-2d2847e4a0f9,2018-08-31,2018-08-31 05:01:30,https://blog.oceanprotocol.com/artdaos-curation-markets-and-tcrs-2d2847e4a0f9,False,10383,The Data Economy,blog.oceanprotocol.com,,,Ocean Protocol,bruce@oceanprotocol.com,oceanprotocol,"BLOCKCHAIN,DATA,AI,DECENTRALIZED,DATA SCIENCE",oceanprotocol,Ethics,ethics,Ethics,7787.0,Trent McConaghy,AI*blockchain. Founder @OceanProtocol. www.trent.st @trentmc0,f1cb98e196bc,trentmc0,4898.0,535.0,20181104
0,,0.0,,2018-06-25,2018-06-25 15:00:03,2018-06-25,2018-06-25 15:14:46,2,False,en,2018-06-25,2018-06-25 15:39:02,5,4fc25a04cdd3,2.3644654088050316,0,0,0,A follow-up article to “The Future of Advertising” reflecting on seven key ethical considerations that advancements in artificial…,5,"
Ethical Considerations for The Future of Advertising: Artificial Intelligence & Creativity
A follow-up article to “The Future of Advertising” reflecting on seven key ethical considerations that advancements in artificial intelligence and advertising could affect.
Ethics are essential in every aspect of human life, including how businesses and their employees choose to act. Economic and competitive gains can sometimes blur judgement, so as with any practice, it is crucial that as artificial intelligence shifts the way advertising is done, the ethical implications are not forgotten.
The author identifies seven key areas for consideration:
ONE: Augmentation Vs. Automation
Despite what the media says, the consensus is that a robot takeover isn’t imminent. However, automation of some jobs is inevitable, although many argue the influx of AI only creates new areas for employment, it’s essential to ensure our future workforce is ready for this through education and democratisation of technology.
TWO: Algorithm Bias
Microsoft’s Racist Chat Bot and Stanford’s Homophobic face-recognition AI are clear examples of the risks involved when using potentially bias datasets- How do you hold an algorithm accountable when it’s making very important decisions?
THREE: Globalisation & Distribution
William Gibson famously said: “The future is already here — it’s just not very evenly distributed.” AI technology is expensive and requires connectivity and power. Currently, internet access isn’t evenly distributed around the world, this gives developed countries and wealthy corporations a distinct advantage.
FOUR: Ownership & Usage Rights
For all current examples of AI creativity, the need for human interception is still apparent. However, if AI becomes genuinely creative, will AI art experience the same popularisation of today’s artists? If so, there are considerations to be made for that intellectual property, if multiple humans train a machine, who owns the final output?

FIVE: Privacy & Security
Online consumer data collection has become standard practice, so agencies need to be considerate when making decisions on how to use this data. Personalisation is novel, but there is potential for it to be taken too far, for many consumers they want brands to “know me, but it’s the me I want you to know not the me you think you know”.
SIX: Echo Chamber & Class Bias
“Advertising is becoming a tax only poor people pay”: ad blockers and premium subscriptions increasingly cost money, meaning the wealthy benefit. Media placements also cost to be seen by high-spending consumers, meaning the wealthier brands with more premium content can potentially never be seen by lower-income consumers, creating a class-led echo chamber.
SEVEN: “Fake News”
Popularised by Donald Trump and satirical outlets such as ‘The Onion” fake news has become so rife it is often indistinguishable from real stories. Automation, iterative content and personalisation all create clear risks that the piece of content one consumer sees may differ from another, and that neither give a true or complete story.
These seven considerations are only the tip of the iceberg when it comes to the topic of ethics and artificial intelligence.
Afterword: This article was originally written as coursework required for the ‘Hyper Island’ Digital Management Masters. A Harvard referenced version and/or complete Bibliography is available on request.
",Ethical Considerations for The Future of Advertising: Artificial Intelligence & Creativity,0,ethical-considerations-for-the-future-of-advertising-artificial-intelligence-creativity-4fc25a04cdd3,2018-06-25,2018-06-25 15:39:02,https://medium.com/s/story/ethical-considerations-for-the-future-of-advertising-artificial-intelligence-creativity-4fc25a04cdd3,False,525,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Rhoda Sell,A curious Digital Marketeer writing about the World Wide Web and things I learnt at @HyperIsland to try to get more freelance work. Get in touch!,72f9da04dfa2,rhodasell,8.0,1.0,20181104
0,,0.0,31f4f88d6548,2018-04-03,2018-04-03 14:12:54,2018-04-03,2018-04-03 14:42:29,1,False,en,2018-04-03,2018-04-03 14:42:29,1,586b7d15967e,3.6226415094339615,5,0,0,“I wish more people knew how exciting and positive data science ethics is.” We catch up with ethical mastermind and postdoctoral fellow…,5,"5 Minutes with Laura Norén
“I wish more people knew how exciting and positive data science ethics is.” We catch up with ethical mastermind and postdoctoral fellow, Laura Norén!

Laura Norén is an Adjunct Professor at NYU and a Moore-Sloan postdoctoral fellow at the NYU Center for Data Science. Her research focuses on the impact of design on social behaviors, particularly within the context of technology. Most recently, her focus has turned to the ethics of data science, and her class on the same topic was covered by the New York Times. She is also the Managing Editor for the weekly Data Science Community newsletter.
1. Your work in data science ethics is becoming increasingly important to the scientific community. What are some key issues that you are excited to focus on this year?
Many professions like business, journalism, and engineering have offered ethical training and codes of ethics for their members. As data science becomes professionalized, there is increasing attention towards teaching courses and developing codes of ethics that address the specific needs of the discipline while drawing on the wisdom of adjacent fields like computer science as well as fields like moral philosophy that are not always considered adjacent. I’m very excited about the way this type of academic cross-pollination can accelerate data science towards a thoughtful, beneficent role in organizations and society.
2. Do you think that what we consider to be ethical — whether ethical behavior or ethical algorithmic design — can change over time? If so, how would this affect the larger goal to make the scientific community a more ethical space?
One of the great lessons we’ve drawn from science and technology studies is that science, like everything else, is context dependent. The cultural contexts in which we imagine and define what kinds of information consumers, users, employees are allowed to keep private from governments, companies, and, yes, even scientists, is dramatically dependent on national context.
We all think we know what privacy means, but in fact, the concept varies quite a bit across national contexts.
Americans tend to be more willing to allow themselves and their information to be tracked, stored, and used to drive decisions about the experiences they have in the world. Continental Europeans are more invested in limiting the reach of institutions into their personal lives and behaviors. The EU’s General Data Privacy Regulation is set to become much stricter in May 2018, and this presents major challenges to the way global companies conduct data science.
Still, the scientific community should not adopt an arrogant attitude towards privacy, and assume that our ability to reproduce results is always more socially beneficial than our research subject’s rights to delete or conceal their data.
3. What are some of the most memorable ethical dilemmas that have come up in the field in the last few years? Have these problems been solved yet — and, if so, what did you think of the outcome?
We are working hard to figure out how we can make as much data publicly available to anyone as possible without revealing the user’s identities, or contributing to outcomes that will not advance the public good. There are exciting efforts to share clinical trial data in medicine, for example, or to share government data with scientists and the public— yet, we still don’t see commercial entities getting into the data sharing movement.
Within companies, the biggest questions are: How much data should be collected? How long it should be stored? And, what types of insights should be drawn from it. Right now, we have a lot to worry about when it comes to predictive policing, but I think too few people are aware that the minority report scenario (where data predict someone will commit a crime before it happens, thereby allowing people to be detained while innocent) is heating up within predictive HR.

One other area of research that is quite novel is around how humans are going to share decision making with semi-intelligent machines. As the machines get more advanced, it will really feel like we are sharing tasks with them, somewhat like the way we now share the task of navigating with GPS-enabled navigation devices. Such tools may allow people to have more confidence to explore new places without worrying they’ll get lost, but it may also atrophy our ability to navigate without that kind of assistance.
Figuring out how to get as many benefits as possible from these shared human and algorithm processes without losing elements of life, discovery, and learning that we care about will be a growing area of research. What happens when our cultural products are recommended by Netflix and Spotify? Do we expand our taste horizons or amplify our starting tastes?
I call these questions the cultural economies of data.
4. What is something that you wish more people knew about data science ethics, and the work that you do?
I wish more people knew how exciting and positive data science ethics is. The class I’m teaching now is the BEST class I’ve ever taught, close the Hollywood ideal of the Ivy League college course where everyone contributes, students are free to challenge each other productively, and are open to changing their minds. We are working on open questions, searching for ethical frameworks we can adapt to data science, and it feels good to contemplate how we can build the most beneficial, least harmful future.
",5 Minutes with Laura Norén,10,5-minutes-with-laura-norén-586b7d15967e,2018-04-17,2018-04-17 04:20:38,https://medium.com/s/story/5-minutes-with-laura-norén-586b7d15967e,False,907,"This is the official research blog of the NYU Center for Data Science (CDS). Established in 2013, we are a leading data science training and research facility, offering a MS in Data Science and, as of 2017, one of the nation’s first universities to offer a Ph.D. in Data Science.",,nyudatascience,,Center for Data Science,ab4829@nyu.edu,center-for-data-science,"DATA SCIENCE,DATA MINING,TECHNOLOGY,ARTIFICIAL INTELLIGENCE,MACHINE LEARNING",NYUDatascience,Ethics,ethics,Ethics,7787.0,NYU Center for Data Science,"Official account of the Center for Data Science at NYU, home of the Master’s and Ph.D. in Data Science.",880781a85c2,NYUDataScience,3530.0,9.0,20181104
0,,0.0,,2017-10-01,2017-10-01 23:21:02,2017-10-01,2017-10-01 23:56:35,1,False,en,2017-10-01,2017-10-01 23:56:35,2,c191247f1a76,2.139622641509434,0,0,0,"When companies finally start monetizing self driving cars to the public, what forms of safety measures would you want the company to assure…",4,"Ethical Considerations On Self Driving Cars

When companies finally start monetizing self driving cars to the public, what forms of safety measures would you want the company to assure you of, in case anything goes wrong? If the self driving car were to make a quick decision that would either put you or pedestrians in danger, based on which variables would you want the car to act for your own safety? These are opinions that we can explore safely with Moral Machine: http://moralmachine.mit.edu/
I went through the different decisions and some choices were harder than others. Sometimes you have to choose between groups of people were some of them might have a high social status and some crossing a red light after stealing an old lady’s purse (I imagine). In cases where we as humans make intuitive decisions, we are now replacing robots to do similar decision but based on careful analysis in which we fill in the parameters. I mostly went with ‘abiding the law is important’. I thought that it would have to be even more necessary abiding the law in the future when people aren’t driving anymore. At least it made more sense to me than letting the algorithm cross the lane to hit a group of people that crossed a green light. Also, if I would buy a self driving car, I would not want it to differ between age, gender, race and social status. I don’t even want the algorithm to calculate on such things.
But who should be saved then? You, or two teenagers crossing a green light? What about you and a group of seven? If I bought a car that I would not drive myself, wouldn’t I want some kind of safety that it wouldn’t necessarily choose the pedestrians over me in every case? But would I want it to choose me over every possible group of people? I started making my own cases on the website. Here’s one of them: http://moralmachine.mit.edu/browse/321727815
When I become 90-years old and drive around Copenhagen with my VR-glasses, and my brakes suddenly doesn’t work — maybe I didn’t take care of my car it actually is my own fault. If there is a similar group as in my example on road, should the algorithm then be able to weigh the outcome? I don’t have the answer to this, but despite that I don’t like thought of a car analyzing outcomes based on the differences our looks and backgrounds, I still have a hard time deciding in extreme cases. What I fear is that companies would advertise their car based on how safe you are when this algorithm gets a call, and we haven’t even talked about errors in these calculations yet. If I bought one, I surely want it to be a safer option than driving it myself, and so would everybody else. This would make it a bad trade for pedestrians, and if I ever get to work with self driving cars, my advice would be not to cross too many red lights 20 years from now.
",Ethical Considerations On Self Driving Cars,0,ethical-considerations-on-self-driving-cars-c191247f1a76,2018-02-16,2018-02-16 01:32:27,https://medium.com/s/story/ethical-considerations-on-self-driving-cars-c191247f1a76,False,514,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Simon Nielsen,Currently pursuing a degree in Computer Science. Interested in AI.,5c7bc73aedea,post.smn,7.0,9.0,20181104
0,,0.0,,2018-04-18,2018-04-18 09:45:25,2017-10-15,2017-10-15 12:21:18,1,False,en,2018-04-18,2018-04-18 10:03:36,2,c9398c5bf828,3.720754716981132,0,0,0,,3,"Sex, lies and AI: will the ‘Blade Runner Rule’ save the world?
Photo by Max Oh on Unsplash
According to a study by WPP digital group Syzygy, 85% of Brits believe that AI in marketing should be governed by a key principle from the movie franchise. Does it make sense and how really big is the risk man will succumbs the machine?
 The “Blade Runner rule” says it should be illegal for AI applications such as social media bots, chatbots and virtual assistants to conceal their identity and pose as humans. I would like to ask you — and please answer honestly — two specific questions about that specific result:
Do you really think the actual level of AI needs the ‘Blade Runner Rule’?
Can a machines powered with AI conceals their identity and pose as humans?
My answer for the first question is YES, I think the ‘Blade Runner Rule’ is needed. But don’t take me wrong, I don’t think we need that rule right now to save us from the machines. We are years away from a level of AI that could potentially generate that danger. However, considering how slow the legal system was able to adapt to phenomenas such as the Internet, eCommerce, Google, Amazon and many others …well, if we start now, for sure we will be ready by the time the machines mock the humans.
The second question is a little more complicated and requires such technical explanation …yes, I know you don’t like too technical explanations, but let me elaborate. Let me just explain you a very interesting concept: Generative Adversarial Networks (GANs). The way this kind of Deep Learning architecture works is similar to a trainer working with an athlete: train, train and train while I’m checking and coaching you. GANs solve problems such:
Train an artificial author which can write an article and explain data science concepts to a community in a very simplistic manner by learning from past articles. We are talking here about Natural Language Generation: the machine produce text content that humans are not able to identify as created by a machine.
Create an artificial painter which can paint like any famous artist by learning from his / her past collections? (and the level of the paint is so perfect that even the painter will not recognise the original from the ‘fake’. Several applications on your smartphones use such a kind of models to generate — for instance — a new pictures Picasso style, from your original photo.
GANs consists of two competing neural networks, namely the Generator and the Discriminator. As it is suggestive of the name, the Generator is responsible for generating data from some input. The discriminator is then responsible for analysing that data and discriminating wether that data was real (if it came from our original dataset) or if it is fake (if it comes from the generator). Two neural networks that work independently one against the other to produce something for us, the humans.
Ok, you want a non technical explanation: we want a Rolex watch but we cannot afford it. Then we ask someone — let’s call him John — to build a fake Rolex that is so perfect even Rolex Factory will not be able to distinguish the fake from the real. Of course John needs time and several attempts. But at certain point in time — after millions of attempts — the Rolex Factory will not be able to see the difference. Moreover, probably John will come with new ideas and shapes of Rolex watches that even Rolex Factory will consider to put in its official collection as an original Rolex.
Well John is our Generator while Rolex Factory is our Discriminator. In reality are two neural networks competing against each other to produce and discriminate data: text, images, sounds …whatever data is possible to produce with a computer.
Yann LeCun, Director of Facebook AI, has defined GANs as “…the most interesting idea in the last ten years in machine learning.” And it is true, it something so new and exciting and can be created with just 50 lines of code in Python (https://goo.gl/n8gGYb).
The concept of GANs is revolutionary, two deep learning neural networks works one against the other to produce a result that humans cannot understand if it was produced by a man or by a machine. It is similar to what we used to do in machine learning having models that correct the errors of another model (meta modelling). It is a kind of concept where two AI entities are working together — or, in this case, one against the other — to produce something for the humans and in human style.
However don’t panic. We are far, far away from Roy Batty telling us “I’ve seen things you people wouldn’t believe. Attack ships on fire off the shoulder of Orion…”
Hopefully humans will also be smart enough not to plug experimental deep machine learning programs into something very dangerous, like an army of laser-toting androids or a nuclear reactor. But if someone does and a disaster ensues, it would be the result of human negligence and stupidity, not because the robots had a philosophical revelation about how bad humans are. This explains the reason we need rules. Rules for the humans, not for the machines.
GANs is the first practical example that should trigger the interest of legislators to start to regulate AI. As I said, we are far away from be in danger and succumb to machines, but it is time to act and start to apply precise rules to the whole AI field.
","Sex, lies and AI: will the ‘Blade Runner Rule’ save the world?",0,sex-lies-and-ai-will-the-blade-runner-rule-save-the-world-c9398c5bf828,2018-04-18,2018-04-18 10:03:37,https://medium.com/s/story/sex-lies-and-ai-will-the-blade-runner-rule-save-the-world-c9398c5bf828,False,933,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Federico Cesconi,Old school data scientist in love with deep machine learning. I help my clients to understand their customers. https://www.linkedin.com/in/federicocesconi/,cca131de65d2,federicocesconi,1.0,3.0,20181104
0,,0.0,,2018-01-19,2018-01-19 16:19:44,2018-01-19,2018-01-19 16:52:54,0,False,en,2018-01-19,2018-01-19 16:52:54,6,837e73505f74,2.0415094339622644,16,0,0,"At DeepMind, we’re proud of the role we’ve played in pushing forward the science of AI, and our track record of exciting breakthroughs and…",5,"Why we launched DeepMind Ethics & Society
At DeepMind, we’re proud of the role we’ve played in pushing forward the science of AI, and our track record of exciting breakthroughs and major publications. We believe AI can be of extraordinary benefit to the world, but only if held to the highest ethical standards. Technology is not value neutral, and technologists must take responsibility for the ethical and social impact of their work.
As history attests, technological innovation in itself is no guarantee of broader social progress. The development of AI creates important and complex questions. Its impact on society — and on all our lives — is not something that should be left to chance. Beneficial outcomes and protections against harms must be actively fought for and built-in from the beginning. But in a field as complex as AI, this is easier said than done.
As scientists developing AI technologies, we have a responsibility to conduct and support open research and investigation into the wider implications of our work. At DeepMind, we start from the premise that all AI applications should remain under meaningful human control, and be used for socially beneficial purposes. Understanding what this means in practice requires rigorous scientific inquiry into the most sensitive challenges we face.
So we’re launching a new research unit, DeepMind Ethics & Society, to complement our work in AI science and application. This new unit will help us explore and understand the real-world impacts of AI. It has a dual aim: to help technologists put ethics into practice, and to help society anticipate and direct the impact of AI so that it works for the benefit of all.
Of course, we’re far from alone in thinking about these topics. The ethical and social impact of AI is a thriving field of study, home to groundbreaking work from Julia Angwin’s study of racism in criminal justice algorithms, to Kate Crawford and Ryan Calo’s examination of the broader consequences of AI for social systems, and many others besides. That’s why we plan to conduct interdisciplinary research that brings together experts from the humanities, social sciences and beyond, along with voices from civil society and technical insights from our team at DeepMind to conduct and fund interdisciplinary research.
We’re grateful that this effort will benefit from the advice and guidance of our DeepMind Ethics & Society Fellows, a respected group of independent thinkers. These Fellows are important not only for the expertise that they bring but for the diversity of thought they represent.
To guarantee the rigor, transparency and social accountability of our work, we’ve developed a set of principles together with our Fellows, other academics and civil society. We welcome feedback on these and on the key ethical challenges we have identified. Please get in touch if you have any thoughts, ideas or contributions.
If AI technologies are to serve society, they must be shaped by society’s priorities and concerns. This isn’t a quest for closed solutions but rather an attempt to scrutinise and help design collective responses to the future impacts of AI technologies. With the creation of DeepMind Ethics & Society, we hope to challenge assumptions — including our own — and pave the way for truly beneficial and responsible AI.
",Why we launched DeepMind Ethics & Society,84,why-we-launched-deepmind-ethics-society-837e73505f74,2018-04-21,2018-04-21 20:58:13,https://medium.com/s/story/why-we-launched-deepmind-ethics-society-837e73505f74,False,541,,,,,,,,,,Ethics,ethics,Ethics,7787.0,DeepMind Ethics & Society,A research unit studying the ethical challenges confronting AI and its real-world impacts,743476d19267,Ethics_Society,212.0,1.0,20181104
0,,0.0,,2018-08-15,2018-08-15 17:38:05,2018-02-28,2018-02-28 08:00:51,0,False,en,2018-08-15,2018-08-15 17:40:03,2,db3ee3b29556,30.15849056603773,0,0,0,What does it mean to create beneficial artificial intelligence? How can we expect to align AIs with human values if humans can’t even agree…,5,"Podcast: AI and the Value Alignment Problem with Meia Chita-Tegmark and Lucas Perry

What does it mean to create beneficial artificial intelligence? How can we expect to align AIs with human values if humans can’t even agree on what we value? Building safe and beneficial AI involves tricky technical research problems, but it also requires input from philosophers, ethicists, and psychologists on these fundamental questions. How can we ensure the most effective collaboration?
Ariel spoke with FLI’s Meia Chita-Tegmark and Lucas Perry on this month’s podcast about the value alignment problem: the challenge of aligning the goals and actions of AI systems with the goals and intentions of humans.
Topics discussed in this episode include:
how AGI can inform human values,
the role of psychology in value alignment,
how the value alignment problem includes ethics, technical safety research, and international coordination,
a recent value alignment workshop in Long Beach,
and the possibility of creating suffering risks (s-risks).
This podcast was edited by Tucker Davey. You can listen to it above or read the transcript below.
Ariel: I’m Ariel Conn with the Future of Life Institute, and I’m excited to have FLI’s Lucas Perry and Meia Chita-Tegmark with me today to talk about AI, ethics and, more specifically, the value alignment problem. But first, if you’ve been enjoying our podcast, please take a moment to subscribe and like this podcast. You can find us on iTunes, SoundCloud, Google Play, and all of the other major podcast platforms.
And now, AI, ethics, and the value alignment problem. First, consider the statement “I believe that harming animals is bad.” Now, that statement can mean something very different to a vegetarian than it does to an omnivore. Both people can honestly say that they don’t want to harm animals, but how they define “harm” is likely very different, and these types of differences in values are common between countries and cultures, and even just between individuals within the same town. And then we want to throw AI into the mix. How can we train AIs to respond ethically to situations when the people involved still can’t come to an agreement about what an ethical response should be?
The problem is even more complicated because often we don’t even know what we really want for ourselves, let alone how to ask an AI to help us get what we want. And as we’ve learned with stories like that of King Midas, we need to be really careful what we ask for. That is, when King Midas asked the genie to turn everything to gold, he didn’t really want everything — like his daughter and his food — turned to gold. And we would prefer than an AI we design recognize that there’s often implied meaning in what we say, even if we don’t say something explicitly. For example, if we jump into an autonomous car and ask it to drive us to the airport as fast as possible, implicit in that request is the assumption that, while we might be OK with some moderate speeding, we intend for the car to still follow most rules of the road, and not drive so fast as to put anyone’s life in danger or take illegal routes. That is, when we say “as fast as possible,” we mean “as fast as possible within the rules of law,” and not within the rules of physics or within the laws of physics. And these examples are just the tiniest tip of the iceberg, given that I didn’t even mention artificial general intelligence (AGI) and how that can be developed such that its goals align with our values.
So as I mentioned a few minutes ago, I’m really excited to have Lucas and Meia joining me today. Meia is a co-founder of the Future of Life Institute. She’s interested in how social sciences can contribute to keeping AI beneficial, and her background is in social psychology. Lucas works on AI and nuclear weapons risk-related projects at FLI. His background is in philosophy with a focus on ethics. Meia and Lucas, thanks for joining us today.
Meia: It’s a pleasure. Thank you.
Lucas: Thanks for having us.
Ariel: So before we get into anything else, one of the big topics that comes up a lot when we talk about AI and ethics is this concept value alignment. I was hoping you could both maybe talk just a minute about what value alignment is and why it’s important to this question of AI and ethics.
Lucas: So value alignment, in my view, is bringing AI’s goals, actions, intentions and decision-making processes in accordance with what humans deem to be the good or what we see as valuable or what our ethics actually are.
Meia: So for me, from the point of view of psychology, of course, I have to put the humans at the center of my inquiry. So from that point of view, value alignment … You can think about it also in terms of humans’ relationships with other humans. But I think it’s even more interesting when you add artificial agents into the mix. Because now you have an entity that is so wildly different from humans yet we would like it to embrace our goals and our values in order to keep it beneficial for us. So I think the question of value alignment is very central to keeping AI beneficial.
Lucas: Yeah. So just to expand on what I said earlier: The project of value alignment is in the end creating beneficial AI. It’s working on what it means for something to be beneficial, what beneficial AI exactly entails, and then learning how to technically instantiate that into machines and AI systems. Also, building the proper like social and political context for that sort of technical work to be done and for it to be fulfilled and manifested in our machines and AIs.
Ariel: So when you’re thinking of AI and ethics, is value alignment basically synonymous, just another way of saying AI and ethics or is it a subset within this big topic of AI and ethics?
Lucas: I think they have different connotations. If one’s thinking about AI ethics, I think that one is tending to be moreso focused on applied ethics and normative ethics. One might be thinking about the application of AI systems and algorithms and machine learning in domains in the present day and in the near future. So one might think about atomization and other sorts of things. I think that when one is thinking about value alignment, it’s much more broad and expands also into metaethics and really sort of couches and frames the problem of AI ethics as something which happens over decades and which has a tremendous impact. I think that value alignment has a much broader connotation than what AI ethics has traditionally had.
Meia: I think it all depends on how you define value alignment. I think if you take the very broad definition that Lucas has just proposed, I think that yes, it probably includes AI ethics. But you can also think of it more narrowly as simply instantiating your own values into AI systems and having them adopt your goals. In that case, I think there are other issues as well because if you think about it from the point of view of psychology, for example, then it’s not just about which values get instantiated and how you do that, how you solve the technical problem, but also we know that humans, even if they know what goals they have and what values they uphold, it’s very, very hard for them sometimes to actually act in accordance to them because they have all sorts of cognitive and emotional effective limitations. So in that case I think value alignment is, in this narrow sense, is basically not sufficient. We also need to think about AIs and applications of AIs in terms of how do they help us and how do they make sure that we gain the cognitive competencies that we need to be moral beings and to be really what we should be, not just what we are.
Lucas: Right. I guess to expand on what I was just saying. Value alignment I think in the more traditional sense, it’s sort of all … It’s more expansive and inclusive in that it’s recognizing a different sort of problem than AI ethics alone has. I think that when one is thinking about value alignment, there are elements of thinking about — somewhat about machine ethics but also about social, political, technical and ethical issues surrounding the end goal of eventually creating AGI. Whereas, AI ethics can be more narrowly interpreted just as certain sorts of specific cases where AI’s having impact and implications in our lives in the next 10 years. Whereas, value alignment’s really thinking about the instantiation of ethics and machines and making machine systems that are corrigible and robust and docile, which will create a world that we’re all happy about living in.
Ariel: Okay. So I think that actually is going to flow really nicely into my next question, and that is, at FLI we tend to focus on existential risks. I was hoping you could talk a little bit about how issues of value alignment are connected to the existential risks that we concern ourselves with.
Lucas: Right. So, we can think of AI systems as being very powerful optimizers. We can imagine there being a list of all possible futures and what intelligence is good for is for modeling the world and then committing to and doing actions which constrain the set of all possible worlds to ones which are desirable. So intelligence is sort of the means by which we get to an end, and ethics is the end towards which we strive. So these are how these two things really integral and work together and how AI without ethics makes no sense and how ethics without AI or intelligence in general also just doesn’t work. So in terms of existential risk, there are possible futures that intelligence can lead us to where earth-originating intelligent life no longer exists either intentionally or by accident. So value alignment sort of fits in by constraining the set of all possible futures by working on technical work by doing political and social work and also work in ethics to constrain the actions of AI systems such that existential risks do not occur, such that by some sort of technical oversight, by some misalignment of values, by some misunderstanding of what we want, the AI generates an existential risk.
Meia: So we should remember that homo sapiens represent an existential risk to itself also. We are creating nuclear weapons. We have more of them than we need. So many, in fact, that we could destroy the entire planet with them. Not to mention homo sapiens has also represented an existential risk for all other species. The problem is AI is that we’re introducing in the mix a whole new agent that is by definition supposed to be more intelligent, more powerful than us and also autonomous. So as Lucas mentioned, it’s very important to think through what kind of things and abilities do we delegate to these AIs and how can we make sure that they have the survival and the flourishing of our species in mind. So I think this is where value alignment comes in as a safeguard against these very terrible and global risks that we can imagine coming from AI.
Lucas: Right. What makes doing that so difficult is beyond the technical issue of just having AI researchers and AI safety researchers knowing how to just get AI systems to actually do what we want without creating a universe of paperclips. There’s also this terrible social and political context in which this is all happening where there is really great game-theoretic incentives to be the first person to create artificial general intelligence. So in a race to create AI, a lot of these efforts that seem very obvious and necessary could be cut in favor of more raw power. I think that’s probably one of the biggest risks for us not succeeding in creating value-aligned AI.
Ariel: Okay. Right now it’s predominantly technical AI people who are considering mostly technical AI problems. How to solve different problems is usually, you need a technical approach for this. But when it comes to things like value alignment and ethics, most of the time I’m hearing people suggest that we can’t leave that up to just the technical AI researchers. So I was hoping you could talk a little bit about who should be part of this discussion, why we need more people involved, how we can get more people involved, stuff like that.
Lucas: Sure. So maybe if I just break the problem down into just what I view to be the three different parts then talking about it will make a little bit more sense. So we can break down the value alignment problem into three separate parts. The first one is going to be the technical issues, the issues surrounding actually creating artificial intelligence. The issues of ethics, so the end towards which we strive. The set of possible futures which we would be happy in living, and then also there’s the governance and the coordination and the international problem. So we can sort of view this as a problem of intelligence, a problem of agreeing on the end towards which intelligence is driven towards, and also the political and social context in which all of this happens.
So thus far, there’s certainly been a focus on the technical issue. So there’s been a big rise in the field of AI safety and in attempts to generate beneficial AI, attempts at creating safe AGI and mechanisms for avoiding reward hacking and other sorts of things that happen when systems are trying to optimize their utility function. The Concrete Problems on AI Safety paper has been really important and sort of illustrates some of these technical issues. But even between technical AI safety research and ethics there’s disagreement about something also like machine ethics. So how important is machine ethics? Where does machine ethics fit in to technical AI safety research? How much time and energy should we put into certain kinds of technical AI research versus how much time and effort should we put into issues in governance and coordination and addressing the AI arms race issues? How much of ethics do we really need to solve?
So I think there’s a really important and open question regarding how do we apply and invest our limited resources in sort of addressing these three important cornerstones in value alignment so that the technical issue, the issues in ethics and then issues in governance and coordination, and how do we optimize working on these issues given the timeline that we have? How much resources should we put in each one? I think that’s an open question. Yeah, one that certainly needs to be addressed more about how we’re going to move forward given limited resources.
Meia: I do think though the focus so far has been so much on the technical aspect. As you were saying, Lucas, there are other aspects to this problem that need to be tackled. What I’d like to emphasize is that we cannot solve the problem if we don’t pay attention to the other aspects as well. So I’m going to try to defend, for example, psychology here, which has been largely ignored I think in the conversation.
So from the point of view of psychology, I think the value alignment problem is double fold in a way. It’s about a triad of interactions. Human, AI, other humans, right? So we are extremely social animals. We interact a lot with other humans. We need to align our goals and values with theirs. Psychology has focused a lot on that. We have a very sophisticated set of psychological mechanisms that allow us to engage in very rich social interactions. But even so, we don’t always get it right. Societies have created a lot of suffering, a lot of moral harm, injustice, unfairness throughout the ages. So for example, we are very ill-prepared by our own instincts and emotions to deal with inter-group relations. So that’s very hard.
Now, people coming from the technical side, they can say, “We’re just going to have AI learn our preferences.” Inverse reinforcement learning is a proposal that says that basically explains how to keep humans in the loop. So it’s a proposal for programing AI such that it gets its reward not from achieving a goal but from getting good feedback from a human because it achieved a goal. So the hope is that this way AI can be correctable and can learn from human preferences.
As a psychologist, I am intrigued, but I understand that this is actually very hard. Are we humans even capable of conveying the right information about our preferences? Do we even have access to them ourselves or is this all happening in some sort of subconscious level? Sometimes knowing what we want is really hard. How do we even choose between our own competing preferences? So this involves a lot more sophisticated abilities like impulse control, executive function, etc. I think that if we don’t pay attention to that as well in addition to solving the technical problem, I think we are very likely to not get it right.
Ariel: So I’m going to want to come back to this question of who should be involved and how we can get more people involved, but one of the reasons that I’m talking to the both of you today is because you actually have made some steps in broadening this discussion already in that you set up a workshop that did bring together a multidisciplinary team to talk about value alignment. I was hoping you could tell us a bit more about how that workshop went, what interesting insights were gained that might have been expressed during the workshop, what you got out of it, why you think it’s important towards the discussion? Etc.
Meia: Just to give a few facts about the workshop. The workshop took place in December 2017 in Long Beach, California. We were very lucky to have two wonderful partners in co-organizing this workshop. The Berggruen Institute and the Canadian Institute for Advanced Research. And the idea for the workshop was very much to have a very interdisciplinary conversation about value alignment and reframe it as not just a technical problem but also one that involves disciplines such as philosophy and psychology, political science and so on. So we were very lucky actually to have a fantastic group of people there representing all these disciplines. The conversation was very lively and we discussed topics all the way from near term considerations in AI and how we align AI to our goals and also all the way to thinking about AGI and even super intelligence. So it was a fascinating range both of topics discussed and also perspectives being represented.
Lucas: So my inspiration for the workshop was being really interested in ethics and the end towards which this is all going. What really is the point of creating AGI and perhaps even eventually superintelligence? What is it that is good and what is that is valuable? Broadening from that and becoming more interested in value alignment, the conversation thus far has been primarily understood as something that is purely technical. So value alignment has only been seen as something that is for technical AI safety researchers to work on because there are technical issues regarding AI safety and how you get AIs to do really simple things without destroying the world or ruining a million other things that we care about. But this is really, as we discussed earlier, an interdependent issue that covers issues in metaethics and normative ethics, applied ethics. It covers issues in psychology. It covers issue in law, policy, governance, coordination. It covers the AI arms race issue. Solving the value alignment problem and creating a future with beneficial AI is a civilizational project where we need everyone working on all these different issues. On issues of value, on issues of game theory among countries, on the technical issues, obviously.
So what I really wanted to do was I wanted to start this workshop in order to broaden the discussion. To reframe value alignment as not just something in technical AI research but something that really needs voices from all disciplines and all expertise in order to have a really robust conversation that reflects the interdependent nature of the issue and where different sorts of expertise on the different parts of the issue can really come together and work on it.
Ariel: Is there anything specific that you can tell us about what came out of the workshop? Were there any comments that you thought were especially insightful or ideas that you think are important for people to be considering?
Lucas: I mean, I think that for me one of the takeaways from the workshop is that there’s still a mountain of work to do and that there are a ton of open questions. This is a very, very difficult issue. I think that one thing I took away from the workshop was that we couldn’t even agree on the minimal conditions for which it would be okay to safely deploy AGI. There are just issues that seem extremely trivial in value alignment from the technical side and from the ethical side that seem very trivial, but on which I think there is very little understanding or agreement right now.
Meia: I think the workshop was a start and one good thing that happened during the workshop is I felt that the different disciplines or rather their representatives were able to sort of air out their frustrations and also express their expectations of the others. So I remember this quite iconic moment when one roboticist simply said, “But I really want you ethics people to just tell me what to implement in my system. What do you want my system to do?” So I think that was actually very illustrative of what Lucas was saying — the need for more joint work. I think there was a lot of expectations I think from both the technical people towards the ethicists but also from the ethicists in terms of like, “What are you doing? Explain to us what are the actual ethical issues that you think you are facing with the things that you are building?” So I think there’s a lot of catching up to do on both sides and there’s much work to be done in terms of making these connections and bridging the gaps.
Ariel: So you referred to this as sort of a first step or an initial step. What would you like to see happen next?
Lucas: I don’t have any concrete or specific ideas for what exactly should happen next. I think that’s a really difficult question. Certainly, things that most people would want or expect. I think in the general literature and conversations that we were having, I think that value alignment, as a word and as something that we understand, needs to be expanded outside of the technical context. I don’t think that it’s expanded that far. I think that more ethicists and more moral psychologists and people in law policy and governance need to come in and need to work on this issue. I’d like to see more coordinated collaborations, specifically involving interdisciplinary crowds informing each other and addressing issues and identifying issues and really some sorts of formal mechanisms for interdisciplinary coordination on value alignment.
It would be really great if people in technical research, in technical AI safety research and in ethics and governance could also identify all of the issues in their own fields, which the resolution to those issues and the solution to those issues requires answers from other fields. So for example, inverse reinforcement learning is something that Meia was talking about earlier and I think it’s something that we can clearly decide and see as being interdependent on a ton of issues in a law and also in ethics and in value theory. So that would be sort of like an issue or node in the landscape of all issues and technical safety research that would be something that is interdisciplinary.
So I think it would be super awesome if everyone from their own respective fields are able to really identify the core issues which are interdisciplinary and able to dissect them into the constituent components and sort of divide them among the disciplines and work together on them and identify the different timelines at which different issues need to be worked on. Also, just coordinate on all those things.
Ariel: Okay. Then, Lucas, you talked a little bit about nodes and a landscape, but I don’t think we’ve explicitly pointed out that you did create a landscape of value alignment research so far. Can you talk a little bit about what that is and how people can use it?
Lucas: Yeah. For sure. With the help of other colleagues at the Future of Life Institute like Jessica Cussins and Richard Mallah, we’ve gone ahead and created a value alignment conceptual landscape. So what this is is it’s a really big tree, almost like an evolutionary tree that you would see, but what it is, is a conceptual mapping and landscape of the value alignment problem. What it’s broken down into are the three constituent components, which we were talking about earlier, which is the technical issues, the issues in technically creating safe AI systems. Issues in ethics, breaking that down into issues in metaethics and normative ethics and applied ethics and moral psychology and descriptive ethics where we’re trying to really understand values, what it means for something to be valuable and what is the end towards which intelligence will be aimed at. Then also, the other last section is governance. So issues in coordination and policy and law in creating a world where AI safety research can proceed and where there aren’t … Where we don’t develop or allow a sort of winner-take-all scenario to rush us towards the end and not really have a final and safe solution towards fully autonomous powerful systems.
So what the landscape here does is it sort of outlines all of the different conceptual nodes in each of these areas. It lays out what all the core concepts are, how they’re all related. It defines the concepts and also gives descriptions about how the concepts fit into each of these different sections of ethics, governance, and technical AI safety research. So the hope here is that people from different disciplines can come and see the truly interdisciplinary nature of the value alignment problem, to see where ethics and governance and the technical AI safety research stuff all fits in together and how this all together really forms, I think, the essential corners of the value alignment problem. It’s also nice for researchers and other persons to understand the concepts and the landscape of the other parts of this problem.
I think that, for example, technical AI safety researchers probably don’t know much about metaethics or they don’t spend too much time thinking about normative ethics. I’m sure that ethicists don’t spend very much time thinking about technical value alignment and how inverse reinforcement learning is actually done and what it means to do robust human imitation in machines. What are the actual technical, ethical mechanisms that are going to go into AI systems. So I think that this is like a step in sort of laying out the conceptual landscape, in introducing people to each other’s concepts. It’s a nice visual way of interacting with I think a lot of information and sort of exploring all these different really interesting nodes that explore a lot of very deep, profound moral issues, very difficult and interesting technical issues, and issues in law, policy and governance that are really important and profound and quite interesting.
Ariel: So you’ve referred to this as the value alignment problem a couple times. I’m curious, do you see this … I’d like both of you to answer this. Do you see this as a problem that can be solved or is this something that we just always keep working towards and it’s going to influence — whatever the current general consensus is will influence how we’re designing AI and possibly AGI, but it’s not ever like, “Okay. Now we’ve solved the value alignment problem.” Does that make sense?
Lucas: I mean, I think that that sort of question really depends on your metaethics, right? So if you think there are moral facts, if you think that more statements can be true or false and aren’t just sort of subjectively dependent upon whatever our current values and preferences historically and evolutionarily and accidentally happen to be, then there is an end towards which intelligence can be aimed that would be objectively good and which would be the end toward which we would strive. In that case, if we had solved the technical issue and the governance issue and we knew that there was a concrete end towards which we would strive that was the actual good, then the value alignment problem would be solved. But if you don’t think that there is a concrete end, a concrete good, something that is objectively valuable across all agents, then the value alignment problem or value alignment in general is an ongoing process and evolution.
In terms of the technical and governance sides of those, I think that there’s nothing in the laws of physics or I think in computer science or in game theory that says that we can’t solve those parts of the problem. Those ones seem intrinsically like they can be solved. That’s nothing to say about how easy or how hard it is to solve those. But whether or not there is sort of an end towards value alignment I think depends on difficult questions in metaethics and whether something like moral error theory is true where all moral statements are simply false and that morality is maybe sort of just like a human invention, which has no real answers or who’s answers are all false. I think that’s sort of the crux of whether or not value alignment can “be solved” because I think the technical issues and the issues in governance are things which are in principle able to be solved.
Ariel: And Meia?
Meia: I think that regardless of whether there is an absolute end to this problem or not, there’s a lot of work that we need to do in between. I also think that in order to even achieve this end, we need more intelligence, but as we create more intelligent agents, again, this problem gets magnified. So there’s always going to be a race between the intelligence that we’re creating and making sure that it is beneficial. I think at every step of the way, the more we increase the intelligence, the more we need to think about the broader implications. I think in the end we should think of artificial intelligence also not just as a way to amplify our own intelligence but also as a way to amplify our moral competence as well. As a way to gain more answers regarding ethics and what our ultimate goals should be.
So I think that the interesting questions that we can do something about are somewhere sort of in between. We will not have the answer before we are creating AI. So we always have to figure out a way to keep up with the development of intelligence in terms of our development of moral competence.
Ariel: Meia, I want to stick with you for just a minute. When we talked for the FLI end of your podcast, one of the things you said you were looking forward to in 2018 is broadening this conversation. I was hoping you could talk a little bit more about some of what you would like to see happen this year in terms of getting other people involved in the conversation, who you would like to see taking more of an interest in this?
Meia: So I think that unfortunately, especially in academia, we’ve sort of defined our work so much around these things that we call disciplines. I think we are now faced with problems, especially in AI, that really are very interdisciplinary. We cannot get the answers from just one discipline. So I would actually like to see in 2018 more sort of, for example, funding agencies proposing and creating funding sources for interdisciplinary projects. The way it works, especially in academia, so you propose grants to very disciplinary-defined granting agencies.
Another thing that would be wonderful to start happening is our education system is also very much defined and described around these disciplines. So I feel that, for example, there’s a lack of courses, for example, that teach students in technical fields things about ethics, moral psychology, social sciences and so on. The converse is also true; in social sciences and in philosophy we hear very little about advancements in artificial intelligence and what’s new and what are the problems that are there. So I’d like to see more of that. I’d like to see more courses like this developed. I think a friend of mine and I, we’ve spent some time thinking about how many courses are there that have an interdisciplinary nature and actually talk about the societal impacts of AI and there’s a handful in the entire world. I think we counted about five or six of them. So there’s a shortage of that as well.
But then also educating the general public. I think thinking about the implications of AI and also the societal implications of AI and also the value alignment problem is something that’s probably easier for the general public to grasp rather than thinking about the technical aspects of how to make it more powerful or how to make it more intelligent. So I think there’s a lot to be done in educating, funding, and also just simply having these conversations. I also very much admire what Lucas has been doing. I hope he will expand on it, creating this conceptual landscape so that we have people from different disciplines understanding their terms, their concepts, each other’s theoretical frameworks with which they work. So I think all of this is valuable and we need to start. It won’t be completely fixed in 2018 I think. But I think it’s a good time to work towards these goals.
Ariel: Okay. Lucas, is there anything that you wanted to add about what you’d like to see happen this year?
Lucas: I mean, yeah. Nothing else I think to add on to what I said earlier. Obviously we just need as many people from as many disciplines working on this issue because it’s so important. But just to go back a little bit, I was also really liking what Meia said about how AI systems and intelligence can help us with our ethics and with our governance. I think that seems like a really good way forward potentially if as our AI systems grow more powerful in their intelligence, they’re able to inform us moreso about our own ethics and our own preferences and our own values, about our own biases and about what sorts of values and moral systems are really conducive to the thriving of human civilization and what sorts of moralities lead to sort of navigating the space of all possible minds in a way that is truly beneficial.
So yeah. I guess I’ll be excited to see more ways in which intelligence and AI systems can be deployed for really tackling the question of what beneficial AI exactly entails. What does beneficial mean? We all want beneficial AI, but what is beneficial, what does that mean? What does that mean for us in a world in which no one can agree on what beneficial exactly entails? So yeah, I’m just excited to see how this is going to work out, how it’s going to evolve and hopefully we’ll have a lot more people joining this work on this issue.
Ariel: So your comment reminded me of a quote that I read recently that I thought was pretty interesting. I’ve been reading Paula Boddington’s book Toward a Code of Ethics for Artificial Intelligence. This was actually funded at least in part if not completely by FLI grants. But she says, “It’s worth pointing out that if we need AI to help us make moral decisions better, this cast doubt on the attempts to ensure humans always retain control over AI.” I’m wondering if you have any comments on that.
Lucas: Yeah. I don’t know. I think this sort of a specific way of viewing the issue or it’s a specific way of viewing what AI systems are for and the sort of future that we want. In the end is the best at all possible futures a world in which human beings ultimately retain full control over AI systems. I mean, if AI systems are autonomous and if value alignment actually succeeds, then I would hope that we created AI systems which are more moral than we are. AI systems which have better ethics, which are less biased, which are more rational, which are more benevolent and compassionate than we are. If value alignment is able to succeed and if we’re able to create autonomous intelligent systems of that sort of caliber of ethics and benevolence and intelligence, then I’m not really sure what the point is of maintaining any sort of meaningful human control.
Meia: I agree with you, Lucas. That if we do manage to create … In this case, I think it would have to be artificial general intelligence that is more moral, more beneficial, more compassionate than we are, then the issue of control, it’s probably not so important. But in the meantime, I think, while we are sort of tinkering with artificial intelligent systems, I think the issue of control is very important.
Lucas: Yeah. For sure.
Meia: Because we wouldn’t want to … We wouldn’t want to cut out of the loop too early before we’ve managed to properly test the system, make sure that indeed it is doing what we intended to do.
Lucas: Right. Right. I think that in the process of that that it requires a lot of our own moral evolution, something which we humans are really bad and slow at. As president of FLI Max Tegmark likes to talk about, he likes to talk about the race between our growing wisdom and the growing power of our technology. Now, human beings are really kind of bad at keeping our wisdom in pace with the growing power of our technology. If we sort of look at the moral evolution of our species, we can sort of see huge eras in which things which were seen as normal and mundane and innocuous, like slavery or the subjugation of women or other sorts of things like that. Today we have issues with factory farming and animal suffering and income inequality and just tons of people who are living with exorbitant wealth that doesn’t really create much utility for them, whereas there’s tons of other people who are in poverty and who are still starving to death. There are all sorts of things that we can see in the past as being obviously morally wrong.
Meia: Under the present too.
Lucas: Yeah. So then we can see that obviously there must be things like that today. We wonder, “Okay. What are the sorts of things today that we see and innocuous and normal and as mundane that the people of tomorrow, as William MacAskill says, will see us as moral monsters? How are we moral monsters today, but we simply can’t see it? So as we create powerful intelligence systems and we’re working on our ethics and we’re trying to really converge on constraining the set of all possible worlds into ones which are good and which are valuable and ethical, it really demands a moral evolution of ourselves that we sort of have to figure out ways to catalyze and work on and move through, I think, faster.
Ariel: Thank you. So as you consider attempts to solve the value alignment problem, what are you most worried about, either in terms of us solving it badly or not quickly enough or something along those lines? What is giving you the most hope in terms of us being able to address this problem?
Lucas: I mean, I think just technically speaking, ignoring the likelihood of this — the worst of all possible outcomes would be something like an s-risk. So an s-risk is a subset of x-risks — s-risk stands for suffering risk. So this is a sort of risk whereby some sort of value misalignment, whether it be intentional or much more likely accidental, some seemingly astronomical amount of suffering is produced by deploying a misaligned AI system. The way that this was function is given certain sorts of assumptions about the philosophy of mind, about consciousness and machines, if we understand potentially consciousness and experience to be substrate-independent, meaning if consciousness can be instantiated in machine systems, that you don’t just need meat to be conscious, but you need something like integrated information or information processing or computation or something like that, then the invention of AI systems and superintelligence and the spreading of intelligence, which optimizes towards any sort of arbitrary end, it could potentially lead to vast amounts of digital suffering, which would potentially arise accidentally or through subroutines or simulations, which would be epistemically useful but that involve a great amount of suffering. That coupled with these artificial intelligent systems running on silicon and iron and not on squishy, wet, human neurons would be that it would be running at digital time scales and not biological time scales. So there would be huge amplification of the speed of which the suffering was run. So subjectively, we might infer that a second for a computer, a simulated person on a computer, would be much greater than that for a biological person. Then we can sort of reflect that these are the sorts of risks — or an s-risk would be something that would be really bad. Just any sort of way that AI can be misaligned and lead to a great amount of suffering. There’s a bunch of different ways that this could happen.
So something like an s-risk would be something super terrible but it’s not really clear how likely that would be. But yeah, I think that beyond that obviously we’re worried about existential risk, we’re worried about ways that this could curtail or destroy the development of earth-originating intelligent life. Ways that this really might happen are I think most likely because of this winner-take-all scenario that you have with AI. We’ve had nuclear weapons for a very long time now, and we’re super lucky that nothing bad has happened. But I think the human civilization is really good at getting stuck into minimum equilibria where we get locked into these positions where it’s not easy to escape from. So it’s really not easy to disarm and get out of the nuclear weapons situation once we’ve discovered it. Once we start to develop, I think, more powerful and robust AI systems, I think already that a race towards AGI and towards more and more powerful AI might be very, very hard to stop if we don’t make significant progress on that soon, if we’re not able to get a ban on lethal autonomous weapons and if we’re not able to introduce any real global coordination and that we all just start racing towards more powerful systems that there might be a race towards AGI, which would cut corners on safety and potentially make the likelihood of an existential risk or suffering risk more likely.
Ariel: Are you hopeful for anything?
Lucas: I mean, yeah. If we get it right, then the next billion years can be super amazing, right? It’s just kind of hard to internalize that and think about that. It’s really hard to say I think how likely it is that we’ll succeed in any direction. But yeah, I’m hopeful that if we succeed in value alignment that the future can be unimaginably good.
Ariel: And Meia?
Meia: What’s scary to me is that it might be too easy to create intelligence. That there’s nothing in the laws of physics making it hard for us. Thus I think that it might happen too fast. Evolution took a long time to figure out how to make us intelligent, but that was probably just because it was trying to optimize for things like energy consumption and making us a certain size. So that’s scary. It’s scary that it’s happening so fast. I’m particularly scared that it might be easy to crack general artificial intelligence. I keep asking Max, “Max, but isn’t there anything in the laws of physics that might make it tricky?” His answer and also that of more physicists that I’ve been discussing with is that, “No, it doesn’t seem to be the case.”
Now, what makes me hopeful is that we are creating this. Stuart Russell likes to give this example of a message from an alien civilization, an alien intelligence that says, “We will be arriving in 50 years.” Then he poses the question, “What would you do when you prepare for that?” But I think with artificial intelligence it’s different. It’s not like it’s arriving and it’s a given and it has a certain form or shape that we cannot do anything about. We are actually creating artificial intelligence. I think that’s what makes me hopeful that if we actually research it right, that if we think hard about what we want and we work hard at getting our own act together, first of all, and also on making sure that this stays and is beneficial, we have a good chance to succeed.
Now, there’ll be a lot of challenges in between from very near-term issues like Lucas was mentioning, for example, autonomous weapons, weaponizing our AI and giving it the right to harm and kill humans, to other issues regarding income inequality enhanced by technological development and so on, to down the road how do we make sure that autonomous AI systems actually adopt our goals. But I do feel that it is important to try and it’s important to work at it. That’s what I’m trying to do and that’s what I hope others will join us in doing.
Ariel: All right. Well, thank you both again for joining us today.
Lucas: Thanks for having us.
Meia: Thanks for having us. This was wonderful.
Ariel: If you’re interested in learning more about the value alignment landscape that Lucas was talking about, please visit FutureofLife.org/valuealignmentmap. We’ll also link to this in the transcript for this podcast. If you enjoyed this podcast, please subscribe, give it a like, and share it on social media. We’ll be back again next month with another conversation among experts.
[end of recorded material]
Originally published at futureoflife.org on February 28, 2018.
",Podcast: AI and the Value Alignment Problem with Meia Chita-Tegmark and Lucas Perry,0,podcast-ai-and-the-value-alignment-problem-with-meia-chita-tegmark-and-lucas-perry-db3ee3b29556,2018-08-15,2018-08-15 17:40:03,https://medium.com/s/story/podcast-ai-and-the-value-alignment-problem-with-meia-chita-tegmark-and-lucas-perry-db3ee3b29556,False,7992,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Future of Life,FLI catalyzes and supports research and initiatives to safeguard life and develop optimistic visions of the future. Official account.,e33e2d2a809c,FLIxrisk,1361.0,93.0,20181104
0,,0.0,c3b8aa381a74,2018-06-12,2018-06-12 16:02:21,2018-07-02,2018-07-02 15:47:09,2,False,en,2018-07-02,2018-07-02 15:47:09,9,6296f664bf23,7.160691823899372,2,0,0,"When I started teaching preschool, way back in the 20th century, before we had iPads and classroom robots, the most complicated gadget in…",5,"Teaching Artificial Intelligence in Kindergarten
When I started teaching preschool, way back in the 20th century, before we had iPads and classroom robots, the most complicated gadget in my classroom was a See ‘n Say.
The See ‘n Say in my classroom
The children loved pointing the dial at a farm animal — for example, a cow — pulling the crank, and hearing a recording of the sound of the animal. “Moo!” But what the children most enjoyed was the funny surprise when the toy made a mistake. The dial on the See ‘n Say would often lose alignment. The child might point at the dog and hear “Quack!” or point at the cat and hear “Oink!” The children thought the mistakes were hilarious and they loved discovering that they were smarter than the machine.
Now that I’ve had many years of experience using new technologies such as programmable robots and coding apps in early childhood classrooms, I’ve learned a lot about how to teach critical thinking and problem-solving. I wish I could go back to those days with the See ’n Say and ask the children some open-ended questions about our malfunctioning toy. I’d like to ask:
Why do you think the machine is giving us the wrong answer? How is this machine supposed to work? Maybe we can take the cover off, look inside and see how it’s made. Let’s look at the parts and how they fit together and see what we can learn.
Do you like this broken toy better than a new toy that gives the right answer? Why or why not?
I might even ask questions that challenge children to think about the ethical uses of machines and technology in the classroom:
What do you think the makers of this toy wanted you to learn?
What do you think is the best way to learn about the sounds that animals make?
For example, do you want to learn about the sound a dog makes from a machine? Or would you rather learn about it from a dog? Why or why not?
I can’t go back in time and ask these questions about the See ‘n Say, but in my work now, I coach teachers to nurture children’s critical thinking and ethical decision-making skills in early childhood classrooms. These conversations are an essential component in any 21st century computer science curricula.
Artificial intelligence experts like Mark Stehlik of Carnegie Mellon University emphasize the importance of teaching critical thinking and ethics to computer science students. In a recent presentation at SXSWedu, Stehlik described the importance of demystifying artificial intelligence. “People shouldn’t be afraid of AI. They should understand what it can and cannot do.” He believes that ethics should be a core component of computer science content at every level of instruction.
When to begin talking about ethics and AI
Children as young as three or four years old are using programmable devices, such as robotic toys and iPads, on a daily basis. It’s never too soon to begin having conversations with children about the role of technology in their lives.
The topic of ethics is particularly relevant when we consider the growing role of artificial intelligence in devices used by children, families, and schools. The broad question we must all consider is: What decisions will we let computers make for us?
This may seem like a very new question, but Sherry Turkle, author of Alone Together: Why We Expect More from Technology and Less from Each Other, has been researching our relationships with technology for more than 20 years. Back in the 1990s her team of researchers from MIT studied the ways children interacted with robotic toys such as Tamagotchis and Furbies. In summary, she found that children often became emotionally attached to these devices. Many of the children she interviewed said their robotic toys were “alive enough” to care about them and care for them.
Today there are even more types of sophisticated and lifelike robotic toys available for children. Additionally, children can engage in conversations with artificially intelligent assistants such as Apple’s Siri or Amazon’s Alexa. In her New York Times piece titled “Co-Parenting with Alexa,” Rachel Botsman eloquently describes the anxiety parents may feel when they see their children interact with AI devices. Botsman asks, “How do we teach our children to question not only the security and privacy implications but also the ethical and commercial intentions of a device designed by marketers?” She concludes, “Our kids are going to need to know where and when it is appropriate to put their trust in computer code alone.”
The prevalence of AI in the daily lives of families means we must begin talking with children about computer science and artificial intelligence topics at an early age. We can’t wait until they have learned to program computers and engineer robots. But how do we begin?
Deep conversations happen during play
The foundations for critical thinking and ethical decision-making can be established during early childhood through conversations, problem solving, stories, and play. As Mitchel Resnick describes in his book, Lifelong Kindergarten: Cultivating Creativity through Projects, Passion, Peers and Play, young children can begin learning about computer science through hands-on experiences with physical and tangible materials, such as blocks and robots. The richest conversations will take place during collaborative, social play experiences.
In my work developing robotics and coding curricula at Northwestern University’s Center for Talent Development, I found that even very young children are capable of understanding introductory computer programming concepts when they are invited to play with a combination of virtual and tangible tools. For example, children who enjoy virtual world-building games like Minecraft and Toca Builder will deepen their understanding of engineering and problem-solving when they are challenged to create in the real world, using blocks, LEGO, or cardboard, a structure that they built in a virtual game environment. Pairing a tangible experience with a virtual experience, such as working in a small group on building a LEGO structure, also creates opportunities for social conversations and collaboration that might not happen when students engage alone with screens.
Teaching robots, such as Primo Cubetto and Bee-Bot have been developed to teach beginning programming concepts to early childhood and elementary school children. These are developmentally appropriate tools that support play, social interactions, and learning.
The Power of the Human Heart
What about classrooms where robots and devices are not available? Teachers can still provide opportunities for critical thinking about machines and computers. Almost every child has had some experience with machines — with toasters, clocks, locks, and bicycles. Even simple machines can spark a conversation about engineering and how things work.
For very young children, I’ve discovered that the topic of artificial legs and arms comes up surprisingly often during play. Children may have heard stories about pirates with wooden legs or seen cute Youtube videos about an injured dog who uses a wheeled cart to get around. A broken toy, then, can become a teachable moment about the meaningful uses of robotic technology.
Recently I was teaching a preschool class in which a cardboard unicorn piñata experienced a broken leg. The cardboard was beyond repair and I asked the children what we could do to help the unicorn walk again. Through a collaborative problem-solving process that involved the engineering of several different prototypes, the children were able to build a prosthetic leg out of LEGO bricks.
Fixing a cardboard unicorn with LEGO bricks
As children get older they will be able to participate in conversations around broader and more sophisticated questions like:
What can computers do?
Hoe can computers help people?
What can’t computers do?
Can computers make mistakes? If yes, then why?
For example, in one of the coding classes at CTD, a second grader reflected on these questions by writing: “Sometimes robots don’t do what you tell them to do because you programmed them wrong.” This child’s answer shows that she has an understanding of the role of a human programmer. She now has the foundational knowledge to think more deeply about the appropriate and ethical uses of technology and artificial intelligence. It is important for children to know that computers are not magic. Humans build and program computers. We can make ethical, intentional, and reflective decisions about how computers and robots will be used in our world.
Building a robot car wash is one of my favorite activities to do with preschool children. By “build” I mean that each child pretends to be a robotic component in a drive-through car wash. One child is the water-spraying robot, one is the soapy robot, one is the scrubbing robot, etc. This activity helps children think about how to create an algorithm for programming robots, because the order to events is so important. You can’t rinse before you scrub! I love this activity because there’s always a moment when someone says, “What about the person with the cloth at the end? That’s not a robot.” Many children know that an automated car wash often has a worker with a soft cloth who spot dries the car at the very end. This moment in the activity is significant because we can have a conversation with the children in which we recognize that there are some things, like polishing a car, listening to a friend, or giving a hug, that humans still do better than robots. These moments of insight are essential in the development of ethical decision making.
For more information
This article is based on my SXSWedu session from March 2018 titled “Teach Artificial Intelligence in Kindergarten.” An audio recording of that session is available here: https://soundcloud.com/sxswedu/sxswedu-2018-teach-artificial-intelligence-in-kindergarten
Since SXSWedu, I was excited to see this article, which echoes many of the points I expressed in my session.
“When Robots Teach Kids Computational Thinking and Kindness” https://www.edsurge.com/news/2018-04-27-when-robots-teach-kids-computational-thinking-and-kindness
References
Abamu, J. “What Does it Mean to Prepare Students for a Future with Artificial Intelligence?”EdSurge, March 16, 2017. https://www.edsurge.com/news/2017-03-16-what-does-it-mean-to-prepare-students-for-a-future-with-artificial-intelligence Accessed June 7, 2018
Bers, M.U. Coding as a Playground: Programming and Computational Thinking in the Early Childhood Classroom. Florence, KY: Routledge, 2017.
Botsman, R. “Co-Parenting with Alexa.” New York Times, October 7, 2017. https://www.nytimes.com/2017/10/07/opinion/sunday/children-alexa-echo-robots.html Accessed June 10, 2018
Clark, J. “The Era of the Algorithm.” Creative Mornings, January 31, 2018. https://creativemornings.com/talks/josh-clark/1 Accessed June 7, 2018
Gadzikowski, A. Robotics for Young Children: STEM Activities and Simple Coding.St. Paul, MN: Redleaf Press; 2017.
Resnick, M. Lifelong Kindergarten: Cultivating Creativity through Projects, Passion, Peers and Play. Boston, MA: MIT Press, 2017.
Turkle, S. Alone Together: Why We Expect More from Technology and Less from Each Other. New York, NY: Basic Books; 2013.
",Teaching Artificial Intelligence in Kindergarten,2,teaching-artificial-intelligence-in-kindergarten-6296f664bf23,2018-07-02,2018-07-02 15:47:10,https://earlyinsights.org/teaching-artificial-intelligence-in-kindergarten-6296f664bf23,False,1796,"Insights, stories, articles, readings, policy and literature on early childhood learning, education and play",earlyinsights.org,earlyinsights1,,Early Insights™,insight@educationfutures.co,early-insights,"EARLY CHILDHOOD EDUCATION,PLAY",earlyinsights,Ethics,ethics,Ethics,7787.0,Ann Gadzikowski,Ann Gadzikowski is an award-winning author and educator. Her newest titles include Robotics for Young Children (Redleaf Press). Visit anngadzikowski.com,23b4f1876340,anngadz,10.0,23.0,20181104
0,,0.0,440acd47f2c1,2017-11-11,2017-11-11 03:43:37,2017-11-11,2017-11-11 04:13:19,1,False,en,2017-11-11,2017-11-11 04:22:14,0,a7702d26519b,2.9320754716981132,4,0,0,It’s time we start talking about the ethics of artificial intelligence and what people are doing with our data.,2,"
How well do you know your algorithm?
It’s time we start talking about the ethics of artificial intelligence and what people are doing with our data.
As a tech guy I of course love that Google maps knows the speed of the traffic. I also love that the way it does this is by seeing how fast peoples phones are going. By phones, I mean your phone… not others. Yes. Your Google Maps is broadcasting your location and time back to Google HQ, and that’s probably okay as you are benefiting from it. They say it’s randomised… and look it probably is. That’s a heck of a lot of trust.
But is it ok that for many of you this is a surprise?
Despite the amount of fraud and security worry about the web, we still have a tremendous amount of trust in it. Is it too much trust? Is it being abused?
Do you know how Google is using your data? Do you know why your search results are different from mine even though we both search for “frogs”. You might say that “it tailors it according to your behaviour”. But also, how often do you make it to the second page of Google search results, how often do you even make it to number 5 in the search results.
This is the equivalent of asking the exact same friend your most important questions and never questioning their judgement, their bias. You might ask where you should go for holiday, and they’ll say “you should go to Honduras”. Then you pause for a moment and think “yup, but they went on their honeymoon and we don’t have that sort of budget, so their experience was different”. We’ve recognised the bias, and our interpretation of their answer is impacted.
But are we doing the same with technology? We are the lost masses and google is our god. We ask it questions, and it answers. We used to have to use our computers to say our prayers, then phones, now cheap devices spread across our home that are very literally, always listening.
We question giving our personal identity to a stranger on the street, but we share everything about our lives on social media platforms, oblivious to the fact that this data is used to sell advertising to you, and it works very very well.
Recently I told a colleague why I encrypt my network connections and stay off social media. I told him that I didn’t want to be snooped on by these large organizations and governments. He replied “well, if you’ve got nothing to hide it’ll be fine”. I replied by saying “So you’d be ok with having a group of people constantly looking into your house, under the excuse of ‘don’t mind us, we’re just checking things out’”. What we are allowing is so much worse. But the worst part is that the average person doesn’t know what is happening with their data, who can see it, and even more dangerous, how predictive algorithms work. We can’t see into why google is recommending search results. This is important because it shapes our very behavior. If I search for new headphones, do I see ‘top 10 headphones for 2017’ or ‘don’t get new headphones until 2018’ as the first search result. This fundamentally changes my behavior.
I’ve used a trivial example there, but now imagine questions about sexuality, mental health, politics. If Google is curating the total sum of human consciousness, we should surely know the background and motivations of the person that’s doing it, and that person is the algorithms that drive it.
Is it time to start demanding that directors of companies have to have technology certifications, including technology ethics? We already require them to have financial certifications in several companies.
I’m optimistic about the future of technology. I’m not optimistic about the environment, I think we are going to destroy that so quickly that we’ll be forced to adapt.
As a starting point, start researching Google and Facebook ethics, when they’ve landed in court and for why, and start questioning the data you give them, and what you let them tell you to do. Look at them as a person, not an algorithm. People have biases, intentions, skills and flaws… so do algorithms. How well do you know these people?
",How well do you know your algorithm?,10,how-well-do-you-know-your-algorithm-a7702d26519b,2018-05-11,2018-05-11 23:20:01,https://dawidnaude.com/how-well-do-you-know-your-algorithm-a7702d26519b,False,724,"As always, I’m writing this to remind myself.",dawidnaude.com,,,Dawid’s Blog,dawidnaude@gmail.com,dawidnaude,"SALESFORCE,WAVE ANALYTICS,DATA VISUALIZATION,SELF IMPROVEMENT,INFORMATION TECHNOLOGY",dawidnde,Ethics,ethics,Ethics,7787.0,Dawid Naude,"Design, Digital, Analytics, Cloud. https://www.linkedin.com/in/dawidnaude/",b0d11c572b25,dawidnde,1965.0,77.0,20181104
0,,0.0,,2018-08-17,2018-08-17 19:41:56,2018-08-16,2018-08-16 17:31:48,0,False,en,2018-08-17,2018-08-17 19:44:18,10,d5397d545889,64.19245283018869,0,0,0,What role does metaethics play in AI alignment and safety? How might paths to AI alignment change given different metaethical views? How do…,5,"AI Alignment Podcast: The Metaethics of Joy, Suffering, and Artificial Intelligence with Brian Tomasik and David Pearce

What role does metaethics play in AI alignment and safety? How might paths to AI alignment change given different metaethical views? How do issues in moral epistemology, motivation, and justification affect value alignment? What might be the metaphysical status of suffering and pleasure? What’s the difference between moral realism and anti-realism and how is each view grounded? And just what does any of this really have to do with AI?
The Metaethics of Joy, Suffering, and AI Alignment is the fourth podcast in the new AI Alignment series, hosted by Lucas Perry. For those of you that are new, this series will be covering and exploring the AI alignment problem across a large variety of domains, reflecting the fundamentally interdisciplinary nature of AI alignment. Broadly, we will be having discussions with technical and non-technical researchers across areas such as machine learning, AI safety, governance, coordination, ethics, philosophy, and psychology as they pertain to the project of creating beneficial AI. If this sounds interesting to you, we hope that you will join in the conversations by following us or subscribing to our podcasts on Youtube, SoundCloud, or your preferred podcast site/application.
If you’re interested in exploring the interdisciplinary nature of AI alignment, we suggest you take a look here at a preliminary landscape which begins to map this space.
In this podcast, Lucas spoke with David Pearce and Brian Tomasik. David is a co-founder of the World Transhumanist Association, currently rebranded Humanity+. You might know him for his work on The Hedonistic Imperative, a book focusing on our moral obligation to work towards the abolition of suffering in all sentient life. Brian is a researcher at the Foundational Research Institute. He writes about ethics, animal welfare, and future scenarios on his website “Essays On Reducing Suffering.”
Topics discussed in this episode include:
What metaethics is and how it ties into AI alignment or not
Brian and David’s ethics and metaethics
Moral realism vs antirealism
Emotivism
Moral epistemology and motivation
Different paths to and effects on AI alignment given different metaethics
Moral status of hedonic tones vs preferences
Can we make moral progress and what would this mean?
Moving forward given moral uncertainty
In this interview we discuss ideas contained in the work of Brian Tomasik and David Pearce. You can learn more about Brian’s work here and here, and David’s work here. You can hear more in the podcast above or read the transcript below.
…
Lucas: Hey, everyone. Welcome back to the AI Alignment podcast series with the Future of Life Institute. Today, we’ll be speaking with David Pearce and Brian Tomasik. David is a co-founder of the World Transhumanist Association, rebranded humanity plus, and is a prominent figure within the transhumanism movement in general. You might know him from his work on the Hedonistic Imperative, a book which explores our moral obligation to work towards the abolition of suffering in all sentient life through technological intervention.
Brian Tomasik writes about ethics, animal welfare and for far-future scenarios from a suffering-focused perspective on his website reducing-suffering.org. He has also helped found the Foundational Research Institute, which is a think tank that explores crucial considerations for reducing suffering in the long term future. If you have been finding this podcast interesting or useful, remember to follow us on your preferred listening platform and share the episode on social media. Today, Brian, David, and I speak about metaethics, key concepts and ideas in the space, explore the metaethics of Brian and David, and how this all relates to and is important for AI alignment. This was a super fun and interesting episode and I hope that you find it valuable. With that, I give you Brian Tomasik and David Pearce.
Thank you so much for coming on the podcast.
David: Thank you Lucas.
Brian: Glad to be here.
Lucas: Great. We can start off with you David and then, you Brian and just giving a little bit about your background, the intellectual journey that you’ve been on and how that brought you here today.
David: Yes. My focus has always been on the problem of suffering, very ancient problem, Buddhism and countless other traditions preoccupied by the problem of suffering. I’m also a transhumanist and what transhumanism brings to the problem is suffering is the idea that it’s possible to use technology, in particular biotechnology to phase out suffering, not just in humans throughout the living world and ideally replace them by gradients of intelligent wellbeing. Transhumanism is a very broad movement embracing not just radical mood enrichment but also super longevity and super intelligence. This is what brings me in and us here today in that there is no guarantee that human preoccupations are the problems of suffering are going to overlap with those of post human super intelligence.
Lucas: Awesome, and so you, Brian.
Brian: I’ve been interested in utilitarianism since I was 18 and I discovered the word. I immediately looked it up and was interested to see that the philosophy mirrored some of the things that I had been thinking about up to that point. I became interested in animal ethics and the far future. A year after that, I actually discovered David’s writings of the Hedonistic Imperative, along with other factors. His writings helped to inspire me to care more about suffering relative to the creation of happiness. Since then, I’ve been what you might call suffering-focused, which means I think that the reduction of suffering has more moral priority than other values. I’ve written about both animal ethics including wild animal suffering as well as risks of astronomical future suffering, what are called s-risks. You had a recent podcast episode with Kaj Sotala to talk about s-risks.
I, in general think that from my perspective, one important thing to think about was during AI is what sorts of outcomes could result in large amounts of suffering? We should try to steer away from those possible future scenarios.
Lucas: Given our focuses on AI alignment, I’d like to just offer a little bit of context. Today, this episode will be focusing on ethics. The AI Alignment problem is traditionally seen as something which is prominently something technical. While a large, large portion of it is technical, the end towards which the technical AI is aimed or the ethics which is imbued within it or embodied within it is still an open and difficult question. Broadly, just to have everything defined here, we can understand ethics here just a method of seeking to understand what we ought to do and what counts as moral or good.
The end goal of AI safety is to create beneficial intelligence not undirected intelligence. What beneficial exactly entails is still an open question that largely exist in the domain of ethics. Even if all the technical issues surrounding the creation of an artificial general intelligence or super intelligence are solved, we will still face deeply challenging ethical questions that will have tremendous consequences for earth-originating intelligent life. This is what is meant when it is said that we must do philosophy or ethics on a deadline. In the spirit of that, that’s why we’re going to be focusing this podcast today on metaethics and particularly the metaethics of David Pearce and Brian Tomasik, which also happen to be ethical views which are popular I would say among people interested in the AI safety community.
I think that Brian and David have enough disagreements that this should be pretty interesting. Again, just going back to this idea of ethics, I think given this goal, ethics can be seen as a lens through which to view safe AI design. It’s also a cognitive architecture to potentially be instantiated in AI through machine ethics. That would potentially make AIs ethical reasoners, ethical decision-makers, or both. Ethics can also be developed, practiced and embodied by AI researchers and their collaborators, and can also be seen as a discipline through which we can guide AI research and adjudicate it’s impacts in the world.
There is an ongoing debate about what the best path forward is for generating ethical AI, whether it’s project of machine ethics through bottom up or for top down approaches, or just a broad project of AI safety and AI safety engineering where we seek out corrigibility and docility, and alignment, and security in machine systems or probably even some combination of the two. It’s unclear what the outcome of AI will be but what is more certain though is that AI promises to produce and make relevant both age-old and novel moral considerations through areas such as algorithmic bias and technological disemployment and autonomous weapons, and privacy, big data systems, and even possible phenomenal states in machines.
We’ll even see new ethical issues with what might potentially one day be super intelligence and beyond. Given this, I think I’d like to just dive in first with you Brian and then, with you David. If you could just get into what the foundation is of your moral view? Then, afterwards, we can dive into the metaethics behind it.
Brian: Sure. At bottom, the reason that I placed foremost priority on suffering is emotion. Basically, the emotional experience of having suffered myself intensely from time to time and having empathy when I see others suffering intensely. That experience of either feeling it yourself or seeing others in extreme pain carries just a moral valence to me or a spiritual sensation you might call it that seems different from the sensation that I feel from anything else. It seems just obvious at an emotional level that say torture or being eaten alive by a predatory animal or things of that nature have more moral urgency than anything else. That’s the fundamental basis. You can also try to make theoretical arguments to come to the same conclusion. For example, people have tried to advance what’s called the asymmetry, which is the intuition that it’s bad to create a new being who will suffer a lot but it’s not wrong to fail to create a being that will be happy or at least not nearly as wrong.
From that perspective, you might care more about preventing the creation of suffering beings than about creating additional happy beings. You can also advance the idea that maybe preferences are always a negative debt that has to be repaid. Maybe when you have a preference that’s a bad thing and then, it’s only by fulfilling the preference that you erase the bad things. This would be similar to the way in which Buddhism says that suffering arises from craving. The goal is to cease the cravings which can be done either through the fulfilling the cravings, giving the organism what the organism wants or not having the cravings in the first place. Those are some potential theoretical frameworks from which to also derive a suffering-focused ethical view. For me personally, the emotional feeling is the most important basis.
David: I would very much like to echo what Brian was saying there. I mean there is something about the nature of intense suffering. One can’t communicate it to someone who hasn’t suffered. I mean someone who is for example born with congenital anesthesia or insensitivity to pain but there is something that is self-intimatingly nasty and disvaluable about suffering. However, evolution hasn’t engineered us of course to care impartially about the suffering of all sentient beings. My suffering and those of my genetic kin tends to matter far more to me than anything else. So far as we aspire to become transhuman and posthuman, we should be aspiring to this godlike perspective that takes into account the suffering of all sentient beings that the egocentric illusionist is a genetically adaptive lie.
How does this tie in to the question of posthuman super intelligence? Of course, there are very different conceptions of what posthuman super intelligence is going to be. I’ve always had what might say a more traditional conception of super intelligence in which posthuman super intelligence is going to be our biological descendants enhanced by AI but nonetheless still our descendants. However, there are what might crudely be called two other conceptions of post human super intelligence. One is this Kurzweilian fusion of humans and our machines, such that the difference between humans and our machine ceases to be relevant.
There’s another conception of super intelligence that you might say in some ways is the most radical is the intelligence explosion that was first conceived by I.J. Good but has been developed by Eliezer Yudkowsky, MIRI, and most recently by Nick Bostrom that conceives of some kind of runaway explosion, recursively self-improving AI and yes, there being no guarantee that the upshot of this intelligence explosion is going to be in any way congenial to human values as we understand them. I’m personally skeptic about the intelligence explosion in this sense but yeah, it’s worth clarifying what one means by posthuman super intelligence.
Lucas: Wonderful. Right before we dive into the metaethics behind these views and their potential relationship with AI alignment and just broadening the discussion to include ethics and exploring some of these key terms. I just like to touch on the main branches of ethics to provide some context and mapping for us. Generally, ethics is understood to have three branches, those being metaethics, normative ethics, and applied ethics. Traditionally, applied ethics is viewed as the application of normative and metaethical views to specific cases and situations to determine the moral status of said case or situation in order to decide what ought to be done.
An example of that might be applying one’s moral views to factory farming to determine whether or not it is okay to factory farm animals for their meat. The next branch moving upwards in abstraction would be normative ethics, which examines and deconstructs or constructs the principles and ethical systems we use for assessing the moral worth and permissibility of specific actions and situations. This branch is traditionally viewed as the formal ethical structures that we apply to certain situations and people are familiar with the deontological ethics and consequentialism, or utilitarianism, or virtue ethics. These are all normative ethical systems.
What we’ll be discussing today is primarily metaethics. metaethics seeks to understand morality and ethics itself. It seeks to understand the nature of ethical statements, attitudes, motivation, properties and judgments. It seeks to understand whether or not ethics relates to objective truths about the world and about people, or whether it’s just simply subjective or if all ethical statements are in fact false. Seeks to understand when people mean when they express ethical judgments or statements. This gets into things like ethical uncertainty and justification theories, and substantial theories, and semantic theories of ethics.
Obviously, these are all the intricacies of the end towards which AI maybe aimed. Given even the epistemology of metaethics and ethics in general that also have major implications for what AIs might be able to discover about ethics or what they may not be able to discover about ethics. Again today, we’ll just be focusing on metaethics and the metaethics behind David and Brian’s views. I guess just to structure this a little bit, just to really start to use the formal language of metaethics. As a little bit of background again, semantic theories is an ethics seek to address the question of what is the linguistic meaning of moral terms or judgments.
These are primarily concerned with whether or not moral statements contain truth values or are arbitrary and subjective. There are other branches within semantic theories but there are main two branches. The first of that is noncognitivism. Noncognitivism refers to a group of theories which hold that moral statements are neither true nor false because they do not express genuine propositions. Usually, these forms of noncognitive views with things like emotivism where people think that when people are expressing our moral views or attitudes like suffering is wrong, they’re simply saying an emotion like boohoo it’s a suffering. Or I’m expressing the emotion that I think that suffering merely bothers me or is bad to me. Rather than you expressing some sort of truth or false claim about the world. Standing in contrast to noncognitivism is just cognitivism, which refers to a set of theories which hold that moral sentences express genuine propositions. That means that they can have truth of false values.
This is to say that they are capable of being true or false. Turning back to Brian and David’s views, how would you each view your moral positions as you’ve expressed thus far. Would you hold yourself to a cognitivist view or a noncognitivist view. I guess we can start with you David.
David: Yes. I just say it’s just built into the nature of let’s say agony that agony is disvaluable. Now, you might say that there is nothing in the equations of physics and science that says anything over and above the experience itself, something like redness. Yeah, redness is subjective. It’s mind-dependent. Yet, unless one thinks minds don’t exist in the physical universe. Nonetheless, redness is an objective feature of the natural physical world. I would say that for reasons we simply don’t understand, pleasure-pain axis discloses the world’s inbuilt metric of value and disvalue. It’s not an open question whether something like agony is disvaluable to the victim.
Now, of course, someone might say, “Well, yes. Agony is disvaluable to you but it’s not disvaluable to me.” I would say that this reflects an epistemological limitation and that in so far as you can access what it is like to be me and I’m in agony, then you will appreciate why agony is objectively disvaluable.
Lucas: Right. The view here is a cognitivist view where you think that it is true to say that there is some intrinsic property or quality to suffering or joy that makes it I guess analytically true that it is valuable or disvaluable.
David: Yes. Well, it has to be very careful about using something like analytically because yeah, someone says that god is talking to me and it is analytically true that these voices are the voices of god. Yeah, one needs to be careful not to smuggle in too much. It is indeed very mysterious. What could be this hybrid descriptive evaluative state of finding something valuable or disvaluable. The intrinsic nature of the physical is very much an open question. I think there are good powerful reasons for thinking that the reality is exhaustively described by the equations of physics. The intrinsic nature of that stuff, the essence of the physical, the fire in the equations is controversial. Physics itself is silent.
Lucas: Right. I guess here, you would describe yourself given these views as a moral realist or an objectivist.
David: Yes, yes.
Brian: Just to jump in before we get to me. Couldn’t you say that your view is still based on mind-dependence because at least based on the thing about if somebody else were hooked up to you, that person would appreciate the badness of suffering. That’s still just dependent on that other mind’s judgment or even if you have somebody who could mind meld with the whole universe and experience all suffering at once. That would still be the dependence of that mind. That mind is judging it to be a bad thing. Isn’t it still mind-depending ultimately?
David: Mind-dependent but I would say that minds are features of the physical world and so, obviously one can argue for some kind of dualism but I’m monistic physicalist at least that’s my working assumption.
Brian: I think objective moral value usually … the definition is usually that it’s not mind-dependent. Although, maybe it just depends what definition we’re using.
David: Yes. It’s rather like something physicalism, it’s often used as a stylistic variant of materialism. One can be non-materialist physicalist and idealist. As I said, minds are objective features of the physical world. I mean at least tentatively at any rate taks seriously the idea that our experience discloses the intrinsic nature of the physical. This is obviously controversial opinion. It’s associated with someone like Galen Straussen or more likely Phil Goff but it stretches back via Grover Maxwell and Russell, ultimately to Schopenhauer. A much more conventional view of course would be that the intrinsic nature of the physical, the fire and the equations is non-experiential. Then, at sometime during the late pre-Cambrian, something happened. Not just organizational but ontological eruption into the fabric of the world first person experience.
Lucas: Just to echo what Brian was saying. The traditional objectivist or more realist view is that the way in which science is the project of interrogating third person facts like what is simply true about the person regardless of what we think about it. In some ways, I think that traditionally the moral realist view is that if morality deals with objective facts, then, these facts are third person objectively true and can be discovered through the methods and tools of ethics. In the same way that someone who might be a mathematical realist would say that one does not invent certain geometric objects rather one discovers them through the application of mathematical reasoning and logic.
David: Yes. I think it’s very tempting to think of first person facts as having some kind of second rate ontological status but as far as I’m concerned, first person facts are real. If someone is in agony or experiencing redness, these are objective tracks about the physical world.
Lucas: Brian, would you just like to jump in with the metaethics behind your own view that you discussed earlier?
Brian: Sure. On cognitivism versus noncognitivism, I don’t have strong opinions because I think some of the debate is just about how people use language, which is not a metaphysical fundamental issue. It’s just like however humans happen to use language. I think the answer to the cognitivism, noncognitivism, if I had to say something would be it’s messy probably. Humans do talk about moral statements, the way they talk about other statements, other factual statements. We use reasoning and we care about maintaining logical consistency among sets of moral statements. We treat them as regular factual statements in that regard. There maybe also be a sense in which moral statements do strongly express certain emotions. I think probably most people don’t really think about it too much.
It’s like people know what they mean when they use moral statements and they don’t have a strong theory of exactly how to describe what they mean. One analogy that you could use is I think moral statements are like swear words. They’re used to make people feel more strongly about something or express how strongly you feel about something. People think that they don’t just refer to one’s emotions and even at a subjective level. If you say my moral view is suffering as bad. That feels different than saying I like ice cream because there’s a deeper, more spiritual or more like fundamental sensation that comes along with the moral statements that doesn’t come along with the, “I like ice cream,” statements.
I think metaphysically, that doesn’t reflect anything fundamental. It just means that we feel differently about moral statements and thoughts than about nonmoral ones. Subjectively, it feels different. Yeah. I think most people just feel that difference and then, exactly how you cash out whether that’s cognitive or noncognitive is a semantic dispute. My metaphysical position is anti-realism. I think that moral statements are mind-dependent. They reflect ultimately our own preferences even if they maybe very spiritual and like deep fundamental preferences. I think Occam’s Razor favors this view because it would add complexity to the world for there to be independent truths. I’m not even sure what that would mean, based on similar reason, I reject mathematical truths and anything non-physicalist. I think moral truths, mathematical truths and so on can all be thought of as fictional constructions that we make. We can reason within these fictional universes of ethics and mathematics that we construct using physical thought processes. That’s my basic metaphysical stance.
Lucas: Just stepping back to the cognitivism and noncognitivism issue, I guess I was specifically interested in yourself. When you were expressing your own moral view earlier, did you find that it’s simply a mixture of expressing your own emotions and also, trying to express truth claims or given your anti-realism, do you think that you’re simply only expressing emotions when you’re conveying your moral view?
Brian: I think very much of myself as an emotivist. It’s very clear to me that what I’m doing when I do ethics is what the emotivist as people are doing. Yes, since I don’t believe in moral truth, it would not make sense for me to be gesturing at moral truths. Except maybe in so far as my low level brain wiring intuitively thinks in those terms.
David: Just to add to this and that although it is possible to imagine, say something you like spectrum inversion, color inversion, some people who like ice cream and some people who hate ice cream. One thing it isn’t possible to do is imagine a civilization in which an inverted pleasure-pain axis. It seems to just be a basic fact about the world that unbearable, agony and despair is experienced as disvaluable and even cases that might appear to contradict this slight that say that masochist are in fact merely confirm a claim because, yeah, I mean the masochist enjoys the intensity rewarding release of endogenous opioids when the masochist undergoes activities that might otherwise be humiliating or painful.
Lucas: Right. David, it seems you’re making a claim about there being a perfect convergence in the space of all possible minds among the pleasure-pain axis having the same sort of function. I guess I’m potentially just missing the gap or pointing out the gap between that and I guess your cognitivist objectivism?
David: It seems to be built into the nature of let’s say agony or despair itself that it is disvaluable. It’s not I’m in agony. Is this valuable or not? It’s not open question whereas anything else. However, abhorrent, your eye might regard it one can still treat it as an open question and ask, is child abuse or slavery really disvaluable? Whereas in the case of agony, it’s built in the nature of the experience itself.
Lucas: I can get behind that. I think that sometimes when I’m feeling less nihilistic about morality, I am committed to that view. I think just to push back a little bit here. I think in the space of all possible minds, I think I can imagine a mind which has a moral judgment and commitment to the maximization of suffering within itself and within the world. It’s simply … it’s perfect in that sense. It’s perfect in maximizing suffering for itself in the world and it’s judgment and moral epistemology is very brittle, such that it will never change or deviate from this. How would you deal with something like that?
David: Is it possible? I mean one can certainly imagine a culture in which displays of machismo and the ability to cope with great suffering are highly valued and would be conspicuously displayed. This would fitness enhancing but nonetheless, it doesn’t really challenge the sovereignty of their pleasure-pain axis as the axis of value and disvalue. Yeah, I would struggle to conceive some kind of intelligence that values its own despair or agony.
Brian: From my perspective, I agree with what Lucas is saying depending on how you define things. One definition of suffering could be that part of the definition is desire to avoid it. From that perspective, you could say it’s not possible for an agent to seek something that it avoids. I think you could have systems where there are different parts in conflict so you could a hedonic assessment system that outputs a signal that this is suffering but then, another system then chooses to favor the suffering. Humans even have something like this when we can override our own suffering. We might have hedonic systems that say going out in the cold is painful but then, we have other systems or other signals that override that avoidance response and cause us to go out in the cold anyway for the sake of something else. You could imagine the wiring, such that wasn’t just enduring pain for some greater good but the motivational system was actively seeking to cause the hedonic system more experiences of pain. It’s just that that would be highly nonadaptive so we don’t see that anywhere in nature.
David: I would agree with what Brian says there. Yes, very much so.
Lucas: Okay. Given these views, would you guys have expressed and starting to get a better sense of them. Another branch of metaethics here that we might be able to explore how it fits in with your guy’s theories, justification theories within metaethics. These are attempts at understanding moral epistemology and motivation for acting in accordance with morality. It attempts to answer the question of how are moral judgments to be supported or defended? If possible, how does one make moral progress? This again will include moral epistemology and in terms of AI and value alignment, if one is anti-realist as Brian is or if one is an objectivist as David is then this completely changes the way and path forward towards AI alignment and value alignment if we are realist as David is then a sufficiently robust and correct moral epistemology in an AI system could essentially realize the hedonistic imperative as David sees it, where you would just have an optimization process extending out from planet earth, which was maximizing for the objectively good hedonic states in all possible sentient beings. I guess it’s a little unclear for me how this fits in with David’s theory or how David’s theory would be implemented.
David: There is a real problem with any theory of value that makes sovereign either the minimization of suffering or classical utilitarianism. Both Buddhism and negative utilitarianism appear to have this apocalyptic implication that if overriding responsibilities to minimize suffering but no. Isn’t that cleanest, quickest, efficient way to eliminate suffering to sterilize the planet, which is now technically feasible and though one can in theory imagine cosmic rescue missions if there is sentence elsewhere. There is apparently this not so disguised apocalyptic implication. When Buddha says allegedly or hopefully I teach one thing and one thing only. Suffering and the relief of suffering, or the end of suffering, yeah, in his day, there was no way to destroy the world. Today, there is.
Much less discussed, indeed I haven’t seen it adequately or not discussed at all in the scholarly literature is that a disguised implication of a classical utilitarian ethic that gives this symmetry to pleasure and pain is that we ought to be launching something like utilitronium shockwave where utilitronium is matter and energy optimized for pure bliss. The shockwave alludes to its velocity of propagation. Though humans perhaps are extremely unlikely even if and when we’re in a position to do so to launch a utilitronium shockwave. If one imagines a notional artificial, super intelligent with a utility function of classical utilitarianism, why wouldn’t that super intelligent launch a utilitronium shockwave that maximizes the cosmic abundance of positive value within our cosmological horizon.
Personally, I would imagine a future of gradients of intelligent bliss. I think that is in fact sociologically highly likely that post-human civilization will have a hedonic range that’s very crudely and schematically as is minus 10 to zero, to plus 10. I can imagine future civilization of let’s say plus 70 to plus 100 or plus 90 to a plus 100. From the perspective classical utilitarianism and classical utilitarianism is arguably the dominant some kind of watered-down version at least is the dominant secular ethic, and academia and elsewhere. That kind of civilization is suboptimal. It’s not moral or apparently has this obligation to launch this kind of cosmic orgasm so to speak.
Lucas: Right. I mean I think just pushing a little bit back on the first thing that you said there about the very negative scenario, which I think people tend to see as an implication of a suffering reducing focused ethic where there can’t be any suffering if there’s no sentient beings. That to me isn’t very plausible because it discounts the possibility of future wellbeing. I take the view that we actually do have a moral responsibility to create more happy beings and I view a symmetry between pain and suffering. I don’t have a particularly suffering-focused ethic where I think there’s asymmetry where I think we should alleviate suffering prior to maximizing wellbeing. I guess David, maybe you can just unpack a little bit before we jump into these justification theories about whether or not you view there as being asymmetry between suffering and wellbeing.
David: I think there’s an asymmetry. There’s this fable of Ursula Le Guin, short story, Ones Who Walk Away From Omelas. We’re invited to imagine this city of delights, vast city of incredible wonderful pleasures but the existence of Omelas, this city of delights depends on the torment and abuse of a single child. The question is would you walk away from Omelas and what does walking away from Omelas entail. Now, personally I am someone who would walk away from Omelas. The world does not have an off switch, an off button and I think if one is whether a Buddhist of a negative utilitarian, or someone who believes in suffering-focused ethics, rather than to consider these theoretical apocalyptic scenarios it is more fruitful to work with secular and religious life lovers to phase out the biology of suffering in favor of gradients of intelligent wellbeing because one of the advantages of hedonic recalibration, i.e. ratcheting up hedonic set points is that it doesn’t ask people to give up their existing values and preferences with complications.
If you ask me, just convenient, this is a rather trivial example. Imagine, 100 people, 100 different football teams. There’s simply no way to reconcile conflicting preferences but what one can do if one ratchets up everyone’s hedonic set point is to improve quality of life. By focusing on ratcheting up hedonic set points rather than trying to reconcile the irreconcilable, I think this is the potential way forward.
Brian: There are a lot of different points to comment on. I agree with David that negative utilitarians should not aim for world destruction for several reasons. One being that it would be make people turn against the cause of suffering reduction. It’s important to have other people not regard that as something to be appalled by. For example, animal rights terrorists, plausibly give the animal rights movement a pretty bad name and may set back the cause of animal rights by doing that. Negative utilitarians would almost certainly not succeed anyway, so the most likely outcome is that they hurt their own cause.
As far as David’s suggestion of improving wellbeing to reduce disagreements among competing football teams, I think that would potentially help giving people greater wealth and equality in society can reduce some tensions. I think there will always be some insatiable appetites especially from moral theories. For example, classical utilitarian has an insatiable appetite for computational resources. Egoists and other moral people may have their own insatiable appetites. We see that in the case of humans trying to acquire wealth beyond what is necessary for their own happiness. I think there will always be those agents who want to acquire as many resources as possible. The power maximizers will tend to acquire power. I think we still have additional issues of coordination and social science being used to control the thirst for power among certain segments of society.
Lucas: Sorry. Just to get this clear. It sounds like you guys are both committed to different forms of hedonic consequentialism. You’re bringing up preferences and other sorts of things. Is there a room for ultimate metaphysical value of preferences within your ethics? Or are preferences simply epistemically and functionally useful indicators of what will often lead to positive hedonics and agents within you guys as ethical theories?
Brian: Personally, I care to some degree about both preferences and hedonic wellbeing. Currently, I care some more about hedonic wellbeing just based on … from my meta-ethical standpoint, it’s ultimately my choice, what I want to care about. I happen to care a lot about hedonic suffering when I imagine that. From a different standpoint, you can argue that ultimately the golden rule for example commits you to caring about whatever it is and other organisms cares about whether that’s hedonic wellbeing or some arbitrary wish. For example, a deathbed wish would be a good example of a preference that doesn’t have hedonic content to it, whether you think it’s important to keep deathbed wishes even after a person has died ignoring side effects in terms of later generations realizing that promises are not being kept.
I think even ignoring those side effects, a deathbed wish does have some moral importance based on the idea that if I had a deathbed wish, I would strongly want it to be carried out if you are acting the way you want others to treat you. Then, you should care to some degree about other people’s deathbed wishes. Since I’m more emotionally compelled by extreme hedonic pain, that’s what I give the most weight to.
Lucas: What would your view be of an AI or machine intelligence, which has a very strong preference, whatever that computational architecture might look like a bit be flip one way rather than another. It just keeps flipping a bit back and forth, and then, you would have a preference utilitronium shockwave going out in the world. It seems intuitive to me also that we only care about preferences and so far as they … I guess this previous example does this work for me is that we only care about preferences in so far as that they have hedonic effects. I’ll bite the bullet on the deathbed wish thing and I think that ignoring side effects like if someone wishes for something and then, they die, I don’t think that we need to actually carry it out if we don’t think it will maximize hedonic wellbeing.
Brian: Ignoring the side effects. There are probably good hedonistic reasons to fulfill deathbed wishes so that current people will not be afraid that their wishes won’t be kept also. As far as the bit flipping, I think a bit flipping agent does, I think it’s preference does have moral significance but I weigh organisms in proportion to the sophistication of their minds. I care more about a single human than a single ant for example because a human has more sophisticated cognitive machinery. It can do more kinds of … have more kinds of thoughts about its own mental states. When a human has a preference, there’s more stuff going on within its brain to back that up so to speak. A very simple computer program that has a very simple preference to flip a bit doesn’t matter very much to me because there’s not a lot of substance behind that preferences. You could think of it as an extremely simple mind.
Lucas: What if it’s a super intelligence that wants to keep flipping bits?
Brian: In that case, I would give a significant way because it has so much substance in its mind. It probably has lots of internal processes that are reflecting on its own welfare so to speak. Yeah, if it’s a very sophisticated mind, I would give that significant weight. It might not override the preferences of seven billion humans combined. I tend to give less than linear weight to larger brains. As the size of the brain increases, I don’t scale the moral weight of the organism exactly linearly. That would alter reduce that utility monster inclusion.
Lucas: Given Brian’s metaethics being an anti-realist and viewing him as an emotivist, I guess the reasons or arguments that you could provide against this view would only be, they don’t refer back to any metaphysical objective, anything really. David, wouldn’t you say that in the end, it would just be your personal emotional choice whether or not to find something compelling here.
David: It’s to do with the nature of first person facts. What is it that the equations of physics ultimately describe and if you think subjectivity or at least take it seriously the conjecture of that subjectivity is the essence of the physical, the fire in the equations, then yeah, it’s just objectively in the case that first person agony is disvaluable. Here we get into some very controversial issues. I would just like to go back to one thing Brian was saying about sophistication. I don’t think it’s plausible that let’s say a pilot whale is more cognitively sophisticated than humans but it’s very much an open question whether a pilot whale with a substantially larger brain, substantially larger neocortex, substantially larger pain and pleasure centers that the intensity of experience undergone by a pilot whale let’s say may be greater than that of humans. Therefore, other things being equal, I would say that it’s so profoundly aversive states undergone by the whale matter more than a human. It’s not the level of sophistication or complexity that counts.
Lucas: Do you want to unpack a little bit your view about the hedonics versus the preferences, and whether or not preferences have any weight in your view?
David: Only indirectly weight and that ultimately, yeah, as I said I think what matters is the pleasure-pain axis and preferences only matter in so far as they impact that. Thanks to natural selection, we have countless millions and billions of preferences that are being manufactured all the time as social primates countless preferences conflict with each other. There is simply no way to reconcile a lot of them. Whereas one can continue to enrich and enhance wellbeing so, yeah sure. Other things being equal satisfy people’s preferences. In so many contexts, it is logically impossible to do so from politics, the middle east, interpersonal relationships, the people’s desire to be the world famous this, that or the other. It is logically impossible to satisfy a vast number of preferences.
Lucas: I think it would be interesting and useful to dive into, within justification theories, like moral epistemology and ethical motivation. I think I want to turn to Brian now. Brian, I’m so curious to know if it’s possible given your view of anti-realism and suffering focused ethics, whether or not you can make moral progress or what it means to make moral progress. How does one navigate the realm of moral issues in your view, given the metaethics that you hold? Why ought I or others, or why not ought I or others to follow your ethics or not?
Brian: Moral progress I think can be thought of as many people have a desire to improve their own moral views using standards of improvement that they choose. For example, a common standard would be I think that the moral views that I will hold after learning more, I will generally now defer to those views as the better ones. There might be some exceptions especially if you get too much into some subject area that distorts your thinking relative to the way it was before. Basically, you can think of brain state changes as either being approved of or not approved of by the current state. Moral progress would consist of doing updates to your brain that you approve of, like installing updates to computer that you choose to install.
That’s what moral progress would be. Basically, you designated which changes do I want to happen and then, if those happen according to the rules then it’s on a progress relatively to what my current state thought. You can have failures of goal preservation. The example that Eliezer Yudkowsky gives is if you give Gandhi a pill that would make him want to kill people. He should not take it because that would change his goals in a way that his current goals don’t approve of. That would be moral anti-progress relative to Gandhi’s current goals. Yeah, that’s how I would think of it. Different people have different preferences about how much you can call preference idealization.
Preference idealization is the idea of imagining what preferences you would hold if you knew more, were smarter, had more experiences, and so on. Different people couldn’t want different amounts of preference idealization. There are some people who say I have almost no idea what I currently value and I want to defer that to an artificial intelligence to help me figure that out. In my case, it’s very clear to me that extreme suffering is what I want to continue to value and if I change from that stance, that would be a failure of goal preservation relative to my current values. There are still questions on which I do have significant uncertainty in a sense that I would defer to my future self.
For example, the question of how to weigh different brain complexities against each other is something where I still have significant uncertainty. The question of how much weight to give to what’s called higher order theory in consciousness versus first order theories basically how much you think that high level thoughts are an important component of what consciousness is. That’s an issue where I have significant moral uncertainty. There are issues where I want to learn more, think more about it, have more other people think about it before I make up my mind fully on what I think about that. Then, why should you hold my moral view? The real answer is because I want you to and I’ll try to come up with arguments to make it sound more convincing to you.
David: I find subjectivism troubling. I support my football team is Manchester United. I wouldn’t take a pill, less induced me to support Manchester City because that would subvert my values in some sense. Nonetheless, ultimately, support for Manchester United is arbitrary. It is a support for the reduction of suffering merely a kin to I once support lets say of Manchester United.
Brian: I think metaphysically, they’re the same. It feels very different. There’s more of a spiritual, like your whole being is behind reduction of suffering in the way that’s not true for football teams. Ultimately, there’s no metaphysical difference.
Intentional objects ultimately are arbitrary that natural selection has eschewed us a define certain intentional objects. This is philosophy jargon for the things we care about, whether it’s a football or politics, or anything. Nonetheless, it’s unlike these arbitrary intentional objects, it just seems to built into the nature of agony or despair that they are disvaluable. It’s simply not possible to instantiate such states and find it an open question whether they’re disvaluable or not.
Brian: I don’t know if we want to debate now but I think it is possible. I mean we already have examples of one organism who finds the suffering of another organism to be possibility valuable.
David: They are not mirror-touch synesthete. They do not accurately perceive what is going on and in so far as one does either as a mirror-touch synesthete or can do the equivalent of a Vulcan mind meld or something like that, one is not going to perceive the disvaluable as valuable. Its an epistemological limitation.
Brian: My objection to that is it depends how you hook up the wires between the two minds. Like if you hook up one person suffering to another person’s suffering, then the second person will say it’s also bad. If you hook up one person’s suffering neurons to another person’s pleasure neurons, then, the second person will say it’s good. It just depends how you hook up the wires.
David: It’s not all or nothing but if one is let’s say a mirror-touch synesthete today and someone’s, they stub their toe and you have an experience of pain, it’s simply not possible to take pleasure in their stubbing their toe. I think if one does have this notional god’s eye perspective, an impartial view from nowhere that one will act accordingly.
Brian: I disagree with that because I think you can always imagine just reversing the motivational wires so to speak. Just flip the wire that says this is bad. Flip it to saying this is good in terms of the agent’s motivation.
David: Right. Yes. I was trying to visualize what this would entail.
Brian: Even in a synesthete example, just imagine a brain where the same stimulus currently in normal humans, this stimulus triggers negative emotional responses just have the neurons hook up to the positive emotional responses instead.
David: Once again, wouldn’t this be an epistemological limitation rather than some deep metaphysical truth about the world?
Brian: Well, it depends how you define epistemology but you could be a psychopath where you correctly predict another organism’s behavior but you don’t care. You can have a difference between beliefs and motivations. The beliefs could correctly recognize this I think but the motivations could have the wires flipped such that there’s motivation to cause more of the suffering.
David: It’s just that I would say that the psychopath has an epistemological limitation in that the psychopath does not adequately take into account other perspectives. In that sense, psychopath lacks an adequate theory of mind. The psychopath is privileging one particular here and now over other here and nows, which is not metaphysically sustainable.
Brian: It might be a definitional dispute like whether you can consider having proper motivation to be part of epistemological accuracy or not. It seems that you’re saying if you’re not properly motivated to reduce … you don’t have proper epistemological access to it by definition.
David: Yes. One has to be extremely careful with using this term by definition. Yes. I would say that we are all to some degree sociopathic. One is quasi sociopathic to one’s future self for example and so far is one let’s say doesn’t prudently save but squanders money and stuff. We are far more psychopathic towards other sentient beings because one is failing to fully to take into account their perspective. It’s hardwired epistemological limitation. One thing I would very much agree with Brian on is moral uncertainty and being prepared to reflection and take into account other perspectives and allow for the possibility one can be wrong. It’s not always possible to have the luxury of moral reflection uncertainty.
If a kid is drowning, hopefully one that dashes into the water to save the kid. Is this the right thing to do? Well, what happens if the kid, this is the real story, happens to be a toddler grows up to the Adolf Hitler and plunges the world into war. One doesn’t know the long term consequences of one’s action. Wherever possible, yes, one urges reflection and caution in the context of a discussion or debate. One isn’t qualifying, one’s uncertainty, agnosticism carefully but in a more deliberative context perhaps of what one should certainly do so.
Lucas: Let’s just bring it a little bit back to the ethical epistemology behind and ethical motivation behind your hedonistic imperative given your objectivism. I guess here, it’d also be interesting to know if you could also explore key metaphysical uncertainties and physical uncertainties, and what more and how we might go about learning about the universe such that your view would be further informed.
David: Happy to launch into long spiel about my view. One thing I think it really is worth stressing is that one doesn’t need to buy into any form of utilitarianism or suffering-focused ethics to believe that we can and should phase out the biology of involuntary suffering. It’s common to all manner of secular and religious views that we should be other things being equal minimizing suffering reducing unnecessary suffering and this is one thing that technology, it could buy a technology allows us to do and support for something like universal access for implantation, genetic screening, phasing out factory farming and shutting slaughter houses, going on to essentially reprogram the biosphere.
It doesn’t involve a commitment to some particular one specific ethical or meta-ethical view. For something like pain-free surgery anesthesia, you don’t need to sign up for it to recognize it’s a good thing. I suppose my interest is very much in building bridges with other ethical traditions. Yeah, I am happy to go into some of my own personal views but I just don’t want to tie this idea that we can use bio-tech to get rid of suffering into anything quirky or idiosyncratic to me. I have a fair number of idiosyncratic views.
Lucas: It would be interesting if you’d explain whether or not you think that super intelligences or AGI will necessarily converge on what you view to be objective morality or if that is ultimately down to AI researchers to be very mindful of implementing.
David: I think there are real risk here when one starts speaking as though posthuman super intelligence is going to end up endorsing a version of one’s own views and values, which a priori ,if one thinks about, is extremely unlikely. I think too one needs to ask yeah, when I was talking about post human super intelligence, if post human super intelligence is biological descendants, I think post human super intelligence will have a recognizable descendant of pleasure-pain axis. I think it will be ratcheted up so that say experience below hedonic zero is impossible.
In that sense, I do see a convergence. By contrast, if one has a conception of post human super intelligence such that post human super intelligence may not be sentient, may not be experiential at all then, there is no guarantee that such a regime would be friendly to anything recognizably human in its values.
Lucas: The crux here there are different ways of doing value alignment and one such way is descriptively through a super intelligence being able to gain enough information about the set of all values that human beings have and say aligning to those or to some fraction of those or to some idealized version of those through something like a coherent extrapolated volition. Another one is where we embed a moral epistemology within the machine system, so that the machine becomes an ethical reasoner, almost a moral philosopher in its own right. It seems that given your objectivist ethics that with that moral epistemology, it would be able to converge on what is true. Do these different paths forward makes sense to you and/or it also seems that the role of mind melding seems to be very crucial and core to the realization of the correct ethics in your view?
David: With some people, their hearts sinks when the topic of machine consciousness crops up because they know it’s going to be a long inconclusive philosophical discussion and a shortage of any real empirical tests. Yeah, I will just state. I do not think a classical digital computer is capable of phenomenal binding, therefore it will not understand the nature of consciousness or pleasure and pain, and I see the emotion of value and disvalue is bound with the pleasure-pain axis. In that sense, I think what we’re calling machine artificial general intelligence, in one sense it’s invincibly ignorant. I know a lot of people would disagree with this description but if you think humans or at least some humans spend a lot of their time thinking about, talking about, exploring consciousness and it’s all varieties in some cases exploring psychedelia, what are we doing? There are vast range of cognitive domains that are completely, cognitively inaccessible to digital computers.
Lucas: Putting aside the issue of machine consciousness, it seems that being able to first-person access hedonic states provides a extremely foundational and core motivational or at least epistemological role in your ethics David.
David: Yes. I mean part of intelligence involves being able to distinguish the important from the trivial, which ultimately as far as I can see boils down to the pleasure-pain axis. Digital zombies have no conception of what is important or what is trivial I would say.
Lucas: Why would that be if a true zombie in the David Chalmers sense is functionally isomorphic to a human. Presumably that zombie would properly care about suffering because all of its functional behavior is the same. Do you think in the real world, digital computers can’t do the same functional computation that a human brain does?
David: None of us have the slightest idea how one would set about programming a computer to do the kinds of things that humans are doing when they talk about and discuss consciousness when they take psychedelics or discuss the nature of the self. I’m not saying work arounds are impossible. I just don’t think they’re spontaneously going to happen.
Brian: I agree. Just like building intelligence itself, it requires a lot of engineering to create those features of humanlike psychology.
Lucas: I don’t see why it would be physically or technically impossible to instantiate an emulation of that architecture or an architecture that’s basically identical to it in a machine system. I don’t understand why computer architecture, computer substrate is really so different from biological architecture or substrate such that it’s impossible for this case.
David: It’s whether one feels the force of the binding problem or not. The example one can give, imagine the population of the USA are skull bound minds, imagine them implementing any kind of computation you like. They are ultra fast, electromagnetic signaling far faster than the retro chemical signaling and the CNS is normally conceived. Nonetheless, short of a breakdown with monistic physicalism, there is simply no way that the population of the USA is spontaneously going to become subject to experience to apprehend perceptual objects. Essentially, all you have is a micro experiential zombie. The question is why are 86 billion odd membrane bound supposedly classical neurons any different?
Why aren’t we micro experiential zombies? One way to appreciate, i think, the force, the adaptive role of phenomenal binding is to look at syndromes where binding even harshly breaks down such as simultanagnosia where the subject can only see one thing at once. Or motion blindness or akinetopsia, where one can’t apprehend motion or severe forms of schizophrenia where there is no longer any unitary self. Somehow right now, you instantiate a unitary world simulation populated by multiple phenomenally bound dynamical objects and this is tremendously fitness enhancing.
The question is how can a bunch of membrane-bound nerve cells, a pack of neurons carry out what is classically impossible. I mean one can probe the CNS with temporary course grained and neuro scans… individual feature process, edge detectors, motion detectors, color detectors. Apparently, there are no perceptual objects there. How is it that right now that your mind/brain is capable of running this egocentric world simulation in almost real time. It’s astonishing computational feat. I argue for a version of quantum mind but one needn’t buy into this to recognize that it’s profound an unsolved problem. I mean why aren’t we like the population of the USA?
Lucas: Just to bring this back to the AI alignment problem and putting aside issues in phenomenal binding, and consciousness for a moment. Putting aside also the conception that super intelligence is likely to be some sort of biologic instantiation if we imagine the more AI safety mainstream approach, the MIRI idea of there being simply a machine super intelligence. It seems that in your view David and I think here this elucidates a lot of the interdependencies and difficulties where one’s meta-ethical views are intertwined in the end with what is true about consciousness and computation. It seems that in your view, close to or almost maybe perhaps impossible to actually do AI alignment or value alignment on machine super intelligence.
David: It is possible to do value alignment but I think the real worry is that if you take the MIRI scenario seriously, this recursively self-improving software that will somehow … This runaway intelligence. There’s no knowing where it may lead by MIRI as far as I know have very different conception of the nature of consciousness and value. I’m not aware that they tackle the binding problem. I just don’t see that unitary subjects of experience or values, or pleasure-pain axis are spontaneously going to emerge from software. It seems to involve some form of strong emergence.
Lucas: Right. I guess to tie this back and ground it a bit. It seems that the portion of your metaethics, which is going to be informed by empirical facts about consciousness and minds in general is the view in there that without access to the phenomenal pleasure-pain axis, what you view to have an intrinsic goodness or wrongness to it because it is foundationally and physically, and objectively the pleasure-pain axis of the universe, the heat and the spark in the equation I guess as you say. Without access to that, then ultimately, one will go awry in one’s ethics if one does not have access to phenomenal hedonic states given that that’s the core of value.
David: Yeah. In theory, an intelligent digital computer stroke robot could impartially pave the cosmos with either dolorium or hedonium without actually understanding the implications of what it was doing. Hedonium being or utilitronium, matter and energy optimized for pure bliss. Dolorium being matter and energy optimized for, lack of a better word, for pure misery or despair. That’s the system in question we do not understand the implications of what it was doing. That I know a lot of people do think that well, sooner or later, classical, digital computers, our machines are going to wake up. I don’t think it’s going to happen. Rather we’re not talking about hypothetical quantum computers next century and beyond. Simply an expansion of today’s programmable digital computers. I think they’re zombies and will remain zombies.
Lucas: Fully autonomous agents which are very free and super intelligent in relation to us will in your view require a fundamental access to that which is valuable, which is phenomenal states, which is the phenomenal pleasure-pain axis. Without that, it’s missing its key epistemological ingredient. It will fail in value alignment.
David: Yes, yeah, yeah. It just simply does not understand the nature of the world. It’s rather like claiming where the system is intelligent but doesn’t understand the second or of thermodynamics. It’s not a full spectrum super intelligence.
Lucas: I guess my open question there would be then, whether or not it would be possible to not have access to fundamental hedonic states but still be something of a Bodhisattva with a robust moral epistemology that was heading in the right direction or what might be objective.
David: The system in question would not understand the implications of what it was doing.
Lucas: Right. It wouldn’t understand the implications but if it got set off in that direction and it was simply achieving the goal, then I think in some cases we might call that value aligned.
David: Yes. One can imagine … Sorry Brian. Do intervene when you’re ready but yeah, one could imagine for example being skeptical of the possibility of interstellar travel for biological humans but programming systems to go out across the cosmos or at least within our cosmological horizons and convert matter and energy into pure bliss. I mean one needn’t assume that this will apply to our little bubble of civilization but watch if we do about inert matter and energy elsewhere in the galaxy. One can leave it as it is or if one is let’s say a classical utilitarian, one could convert it into pure bliss. Yeah, one can send out probes. One could restructure, reprogram matter and energy in that way.
That would be a kind of compromise solution in one sense. Keep complexity within our little tiny bubble of civilization but convert the rest of the accessible cosmos into pure bliss. Though that technically would not strictly speaking maximize the abundance of positive value in our hubble volume, nonetheless it could become extraordinarily close to it from a classical utilitarian perspective.
Lucas: Brian, do you have anything to add here?
Brian: While I disagree on many, many points, I think digital computation is capable of functionally similar enough processing as the brain does. Even that weren’t the case, a paperclip maximizer with a very different architecture would still have a very sophisticated model of human emotions and its motivations wouldn’t be hooked up to those emotions but it would understand for all other sense of the word understand human pleasure and pain. Yeah, I see it more as a challenge of hooking up the motivation properly. As far as my thoughts on alignment in general based on my metaethics, I tend to agree with the default approach like the MIRI approach, which is unsurprising because MIRI is also anti-realist on metaethics. That approach sees the task as taking human values and somehow translating them into the AI and so that could be in a variety of different ways learning human values implicitly from certain examples or with some combination of maybe top down programming of certain ethical axioms.
That could send to exactly how you do alignment and there are lots of approaches to that. The basic idea that you need to specifically replicate the complexity of human values in machines and the complexity of the way humans reason. It won’t be there by default in any way shared between my opinion and that of the mainstream AI alignment approach.
Lucas: Do you take a view then similar to that of coherent extrapolated volition?
Brian: In case anybody doesn’t know, coherent extrapolated volition is Eliezer Yudkowsky’s idea of giving the AI the meta … You could call it a metaethics. It’s a meta rule for learning values to take humanity and think about what humanity want to want if it was smarter, knew, had more positive interactions with each other and thought faster and then, try to identify points of convergence among the values of different idealized humans. In terms of theoretical things to aim for, I think CEV is one reasonable target for reasons of cooperation among other humans. I mean if I controlled the world, I would prefer to have the AI implement my own values rather than humanities values because I care more about my values. Some human values are truly abhorrent to me and others seem at least unimportant to me.
In terms of getting everybody together to not fight endlessly over the outcome of AI in this theoretical scenario, CEV would be a reasonable target to strive for. In practice, I think that’s unrealistic like a pure CEV is unrealistic because the world does not listen to moral philosophers to any significant degree. In practice, things are determined by politics, economic power, technological and military power, and forces like that. Those determine most of what happens in the world. I think we may see approximations to CEV that are much more crude like you could say that democracy is an approximation to CEV in the sense that different people with different values, at least in theory, discuss their differences and then, come up with a compromise outcome.
Something like democracy maybe power-weighted democracy in which more powerful actors have more influence will be what ends up happening. The philosophers dream of idealizing values to perfection is unfortunately not going to happen. We can push in directions that are slightly more reflective. We can push aside towards slightly more reflection towards slightly more cooperation and things like that.
David: Couple of points that first, what to use an example we touched on before. What would be coherent extrapolated volition for all the world’s football supporters? Essentially, there’s simply no way to reconcile all their preferences. One may say that if they were fully informed football supporters, wouldn’t waste their time passionately supporting one team or another but essentially I’m not sure that the notion of coherent extrapolated volition there would make sense. Of course, there are more serious issues in football but the second thing when it comes to the nature of value, regardless of one’s metaphysical stance on whether one’s a realist or an anti-realist about value. I think it is possible by biotechnology to create states that are empirically, subjectively far more valuable than anything physiologically feasible today.
Take Prince Myshkin in Dostoevsky’s The Idiot. Like Dostoevsky was a temporal lobe epileptic and he said, “I would give my whole life for this one instant.” Essentially, there are states of consciousness that are empirically super valuable and rather than attempting to reconcile irreconcilable preferences, I think you could say that we should be and so far as we aspire to long term full spectrum super intelligence, perhaps we should be aiming to create these super valuable states. I’m not sure whether it’s really morally obligatory. I said my own focus is on the overriding importance of phasing out suffering but for someone who does give some weight or equal weight to positive experiences positively valuable experiences, that there is a vast range of valuable experience that is completely inaccessible to humans that could be engineered via biotechnology.
Lucas: A core difference here is going to be that given Brian’s view of anti-realism, AI alignment or value alignment would in the end be left to those powers which he described in order to resolve irreconcilable preferences. That is if human preferences don’t converge strongly enough after enough time and information that there are no longer irreconcilable preferences, which I guess I would suppose is probably wrong.
Brian: Which is wrong?
Lucas: That it would be wrong that human beings preferences would converge strongly enough that there would no longer be irreconcilable preferences after coherent extrapolated volition.
Brian: Okay, I agree.
Lucas: I’m saying that in the end, value alignment would be left up to economic forces, military forces, other forces to determine what comes out of value alignment. In David’s view, it would simply be down to if we could get the epistemology right and we could know enough about value and the pleasure-pain axis and the metaphysical status of phenomenal states that that would be value alignment would be to capitalize on that. I didn’t mean to interrupt you Brian. You want to jump in there?
Brian: I was going to say the same thing you did that I agree with David that there would be irreconcilable differences and in fact, many different parameters of the CEV algorithm would probably affect the outcome. One example that you could give is that people tend to crystallize their moral values as they age. You could imagine somebody who was presented with utilitarianism as a young person would be more inclined toward that whereas, maybe if that person haad been presented with deontology as a young person would the person would prefer deontology as he got older and so depending on seemingly arbitrary factors such as the order in which you are presented with moral views or what else is going on in your life at the time that you confront a given moral view or 100 other inputs. The output could be sensitive to that. CEV is really a class of algorithms depending on how you tune the parameters. You could get substantially different outcomes.
Yeah, CEV is an improvement even if there’s no obvious unique target. As I said, in practice, we won’t even get pure CEV but we’ll get some kind of very rough power-weighted approximation similar to our present world of democracy and competition among various interest groups for control.
Lucas: Just to explain how I’m feeling so far. I mean Brian, I’m very sympathetic to your view but I’m also very sympathetic to David’s view. I hover somewhere in between. I like this point that David made where he quoted Russell, something along the lines that one ought to be careful when discussing ethical metaphysics such that one is not simply trying to make one’s own views and preferences objective.
David: Yeah. When one is talking about well, just in general, when one speaks about the nature for example post human super intelligence, think of the way today that the very nature and notion of intelligence is a contested term. Simply sticking the words super in front of it is just how illuminating is it. When I read someone’s account of super intelligence, I’m really reading an account of what kind of person they are, their intellect and their values. I’m sure when I discuss the nature of full spectrum super intelligence, at least now I can see what I can’t the extent to which I’m simply articulating my own limitations.
Lucas: I guess for me here to get all my partialities out of the way, I hope that objectivism is true because I think that it makes the value alignment way less messy. In the end, we could have something actually good and beautiful, which I don’t know is some preference that I have that might be objective or not just simply wrong, or confused. The descriptive picture that I think Brian is committed to, which gives rise to the MIRI and Tomasik form of anti-realism is just one where in the beginning, there was entropy and noise and many generations of stars fusing atoms into heavier elements. One day one of these disks turn into a planet and a sun shone some light on a planet, and the planet began to produce people. There’s an optimization process there in the end, which simply seems to be ultimately driven by entropy and morality seems to simply a part of this optimization process, which just works to facilitate and mediate the relations between angry mean primates like ourselves.
Brian: I would point out there’s also a lot of spandrel to morality in my opinion, specially these days not that we’re not heavily optimized by biological pressures. All these conversation that we’re having right now is a spandrel in the sense that it’s just an outgrowth of certain abilities that we evolve but it’s not at all adaptive in any direct sense.
Lucas: Right. In this view, it really just seems like morality and suffering, and all of this is just byproduct of the screaming entropy and noise of whatever led to this universe. At the same time, the objective process and I think this is the part the people who are committed to MIRI anti-realism and I guess just relativism and skepticism about ethics in general, maybe are not tapping into enough. At the same time, this objectivity is producing a very real and objective phenomenal self and story, which is caught up in suffering where suffering is really suffering and really sucks to suffer. It all seems at face value true in that moment throughout the suffering that this is real. The suffering is real. The suffering is bad. It’s pretty horrible.
This bliss is something that I would never give up or if the rest of the universe were this bliss, that would just be the most amazing thing ever. In this very subjective phenomenal, I like just experiential thing that the universe produces, the subjective phenomenal story and narrative that we live. It seems there’s just this huge tension between that and I think the anti-realism, the clear suffering of suffering and just being a human being.
Brian: I’m not sure if there’s a tension because the anti-realist agrees that humans experience suffering as meaningful and they experience it as the most important thing imaginable. There’s not really a tension and you can explore why humans quest for objectivity. There seems to be certain glow that attaches to things by saying that they’re objectivity moral. That’s just a weird quirk of human brains. I would say that ultimately, we can choose to care about what we care about whether it’s subjective or not. I often say even if objective truth exist, I don’t necessarily care what it says because I care about what I care about. It could turn out that objective truth orders you to torture squirrels. If it does, then, I’m not going to follow the objective truth. On reflection, I’m not unsatisfied at all with anti-realism because what more could you want than what you want.
Lucas: David, feel free to jump in if you’d like.
David: Well, there it’s just … there’s this temptation to oscillate between two senses of the words subjective. Subjective in neither truth nor false, and subject in the sense of first-person experience. My being in agony or you’re being in agony or someone being in despair is as I said as much an objective property of reality as the rest mass of the electron. I mean what we can be doing is working in such ways as to increase the theory to maximize the amount of subjective value in the world regardless of whether or not one believes that this has any transcendent significance with the proviso here that there is a risk that if one aims strictly speaking to maximize subjective value, that one gets the utilitronium shockwave. If one is as I said, what I personally advocate as aiming for a civilization of super intelligent bliss one is not asking people to give up their core values and preferences unless one of those core values and preferences is to keep hedonic set points unchanged. That’s not very intellectually satisfying but it’s … this idea if one is working towards some kind of census, compromise.
Lucas: I think now I want to get into a bit more just about ethical uncertainty and specifically with regards to meta-ethical uncertainty. I think that just given the kinds of people that we are, that even if we disagree about realism versus anti-realism or ascribe different probabilities to each view. We might pretty strongly converge on how we ought to do value alignment given our kinds of moral considerations that we have. I’m just curious to explore a little bit more about what you guys are most uncertain about what it would take to change your mind? What new information you would be looking for that might challenge or make you revise your metaethical view? How we might want to proceed with AI alignment given our metaethical uncertainty?
Brian: Can you do those one by one?
Lucas: Yeah, for sure. If I can remember everything I just said. First to start off, what do you guys most uncertain about within your meta-ethical theories?
Brian: I’m not very uncertain meta-ethically. I can’t actually think of what would convince me to change my metaethics because as I said, even if it turned out that metaphysically moral truth was a thing out there in some way whatever that would mean, I wouldn’t care about it except for like instrumental reasons. For example, if it was a god, then you’d have to instrumentally care about god punishing you or something but in terms of what I actually care about, it would be not connected to moral truth. Yeah, I would have to be some sort of revision of the way I conceive of my own values. I’m not sure what that would look like to be meta-ethically uncertain.
Lucas: There’s a branch of metaethics, which has to tackle this issue of meta-ethical commitment or moral commitment to meta-ethical views. If some sort of meta-ethical thing is true, why ought I to follow what is metaethically true? In your view Brian, it is just simply why ought you not to follow or why ought it not matter for you to follow what is meta-ethically true if there ends up being objective moral facts.
Brian: The squirrel example is a good illustration if ethics turned out to be, you must torture as many squirrels as possible. Then, screw moral truth. I don’t see what this abstract metaphysical thing has to do with what I care about myself. Basically, my ethics comes from empathy, seeing others in pain, wanting that to stop. Unless moral truths somehow gives insight about that, like maybe moral truths is somehow based on that kind of empathy, sophisticated way then, it would be another person giving me thoughts on morality. The metaphysical nature of it would be irrelevant. It would only be useful in so far as it would appeal to my own emotions and sense of what morality should be for me.
David: If I might interject. Undercutting my position and negative utilitarianism and suffering-focus ethics, I think it quietly likely that posthuman super intelligence, advance civilization with a hedonic range ratcheted right up to 70 to 100 or something like that. We’d look back on anyone articulating the kind of view that I am, that anyone who believes in suffering-focused ethics does and seeing it as some kind of depressive psychosis while intuitively assumes that our successes will be wiser than we are and perhaps, well they will be in many ways. Yet in another sense, I think we should be aspiring to ignorance that once we have done absolutely everything in our power to minimize mitigate, abolish and prevent suffering, I think we should forget it even existed. I hope that eventually any experience below hedonic zero will be literally inconceivable.
Lucas: Just to jump to you here David. What are your views about what you are most meta-ethically uncertain about?
David: It’s this worry that what one is doing however much one is pronouncing about the nature of reality, or the future of intelligence life in the universe and so on. What one is really doing is some kind of disguised autobiography. Given that quite a number of people sadly pain and suffering have loomed larger in my life than pleasure, turning this into deep metaphysical truth about the universe. This potentially undercuts my view. As I said, I think there are arguments against the symmetry view that suffering is self-intimatingly bad where there is nothing self-intimatingly bad about being insentient system or a system that it’s really content. Nonetheless, yeah, I take seriously the possibility that’s all I’m doing is expressing obliquely by own limitations of perspective.
Lucas: Given these uncertainties and the difficulty and expected impact of AI alignment, if we’re again committing ourselves to this MIRI view of an intelligence explosion with quickly recursive self-improving AI systems, how would you both, if you were the king of AI strategy, how would you go about allocating your metaethics and how would you go about working on the AI alignment problem and thinking about the strategy given your uncertainties and your views?
Brian: I should mention that my most probable scenario for AI is a slow take off in which lots of components of intelligence emerge piece by piece rather than a localized intelligence explosion. As far as the intelligence like if it were a hard take off localized intelligence explosion, then, yeah I think the diversity approaches that people are considering is what I would do as well. It seems to me, you have to somehow learn values because in the same way that we’ve discovered that teaching machines by learning is more powerful than teaching them by hard coding rules. You probably have to mostly learn values as well. Although, there might be hard coding mixed in. Yeah, I would just pursue a variety of approaches and the way that the current community is doing.
I support the fact that there is also a diversity of short term versus long term focus. Some people are working on concrete problems. Others are focusing on issues like decision theory and logical uncertainty and so on because I think some of those foundational issues will be very important. For example, decision theory could make a huge difference to the AI’s effectiveness as well as issues of what happens in conflict situations. Yeah, I think a diversity of approaches is valuable. I don’t have a specific advice on when I would recommend tweaking current approaches. I guess I expected that the concrete problems work will mostly be done automatically by industry because those are the kinds of problems that you need to make AI work at all. If anything, I might invest more in the kind of long-term approaches that practical applications are likely to ignore or at least put off until later.
David: Yes, because of my background assumptions are different, it’s hard for me to deal with your question. If one believes that subjects of experience that could suffer could simply emerge at different levels of abstraction, I don’t really know how to tackle this because this strikes me as a form of strong emergence. One of the reasons why philosophers don’t like strong emergence is that essentially, all bets are off. Yeah, you imagine if life hadn’t been reducible to molecular biology and hence, ultimately to content chemistry and physics. Yeah, I’m not probably the best person to answer your question.
I think in terms of real moral focus, I would like to see essentially the molecular signature of unpleasant experience identified and essentially, you’re just making it completely off limits and biologically impossible for any sentient being to suffer. If one also believes that there are or could be subjects of experience that somehow emerge in classical digital computers, then, yeah, I’m floundering my theory of mind and reality would be wrong.
Lucas: I think touching on the paper that Kaj Sotala had written on suffering risks, I think that a lot of different value systems would also converge with you on your view David. Whether or not we take the view of realism or anti-realism, I think that most people would agree with you. I think the issue comes about with again, preference conflicts where some people I think even this might be a widespread view in catholicism where you view suffering as really important because it teaches you things and/or it has some special metaphysical significance with relation to god. Within the anti-realism view, with Brian’s view, I would find it very… just dealing with varying preferences on whether or not we should be able to suffer is something I just don’t want to deal with.
Brian: Yeah, that illustrates what I was saying about I prefer my values over the collective values of humanity. That’s one example.
David: I don’t think it would be disputed that sometimes suffering can teach lessons. The question is are there any lessons that couldn’t be functionally replaced by something else. This idea that we can just offload the nasty side of life on to software. In the case of pain, nociception one knows that yeah, so they brought software systems can be program or trained up to avoid noxious stimuli without the nasty raw feels should we be doing the same thing for organic biological robots too. When it comes to this, the question of suffering, one can have quite fierce and lively disputes with someone who says that yeah, they want to retain the capacity to suffer. This is very different from involuntary suffering. I think that quite often someone can see that no, they wouldn’t want to force another sentient being to suffer against their will. It should be a matter of choice.
Lucas: To tie this all into AI alignment again, really the point of this conversation is that again, we’re doing ethics on a deadline. If you survey the top 100 AI safety researchers or AI researches in the world, you’ll see that they give a probability distribution of the likelihood of human level artificial intelligence with about a 50% probability at 2050. This, many suspect, will have enormous implications for earth originating-intelligent life and our cosmic endowment. Our normative and descriptive and applied ethical practices that we engage with are all embodiments and consequential to the sorts of meta-ethical views, which we hold, which may not even be explicit. I think many people don’t really think about metaethics very much. I think that many AI researchers probably don’t think about metaethics very much.
The end towards which AI will be aimed will largely be a consequence of some aggregate of meta-ethical views and assumptions or the meta-ethical views and assumptions of a select few. I guess Brian and David, just to tie this all together, what do you guys view as really the practicality of metaethics in general and in terms of technology and AI alignment.
Brian: As far as what you said about metaethics determining the outcome, I would say maybe the implicit metaethics will determine the outcome but I think as we discuss before, 90 some percent of the outcome will be determined by ordinary economic and political forces. Most people in politics in general don’t think about metaethics explicitly but they still engage in the process and have a big impact on the outcome. I think the same will be true in AI alignment. People will push for things they want to push for and that’ll mostly determine what happens. It’s possible that metaethics could inspire people to be more cooperative depending on how it’s framed. CEV as a practical metaethics could potentially inspire cooperation if it’s seen as an ideal to work towards, although the extent to which it can actually be achieve is questionable.
Sometimes, you might have a naïve view where a moral realist assumes that a super intelligent AI would necessarily converge to the moral truth or at least a super intelligent AI could identify the moral truth and then, maybe all you need to do is program the AI to care about the moral truth once it discovers it. Those particular naïve approaches, I think would produce the wrong outcomes because there would be no moral truth to be found. I think it’s important to be wary of that assumption that a super intelligence will figure it out on its own and we don’t need to do the hard work of loading complex human values ourselves. It seems like the current AI alignment community largely recognizes that they recognize that there’s a lot of hard work in loading values and it won’t just happen automatically.
David: In terms of metaethics, consider the nature of pain-free surgery, surgical anesthesia. When it was first introduced in the mid 19th century, it was for about 15 years controversial. There were powerful voices who spoke against it but nonetheless, very rapidly a consensus emerge and we all now, almost all take it for granted for major surgery anesthesia. It didn’t require a consensus on the nature of value and metaethics. It’s just this is the obvious given our nature. Clearly, I would hope that eventually something similar will happen not just for physical pain but also psychological pain too. Just as we now take it for granted that it was the right thing to do to eradicate smallpox, no one is seriously suggesting that we bring smallpox back and it doesn’t depend on consensus on metaethics.
I would hope that the experience below hedonic zero, which one can possibly we’ll be able to find its precise molecular signature. I hope that consensus will emerge that we should phase it out too. Sorry, this isn’t much in the way of practical guidance to today’s roboticist and AI researchers but I suppose I’m just expressing my hope here.
Lucas: No, I think I share that. I think that we have to do ethics on a deadline but I think that there are certain ethical things whose deadline is much longer or which doesn’t necessarily have a real concrete deadline. I like… with your example of the pain anesthesia drugs.
Brian: In my view, metaethics is mostly useful for people like us or other philosophers and effective altruists who can inform our own advocacy. We want to figure out what we care about and then, we go for it and push for that. Then, maybe to some extent, it may diffuse through society in certain ways but in the start, it’s just helping us figure out what we want to push for.
Lucas: There’s an extent to which the evolution of human civilization has also been an evolution of metaethical views, which are consciously or unconsciously being developed. Brian, your view is simply that 90% of what has causal efficacy over what happens in the end are going to be like military and economic and just like raw optimization forces that work on this planet.
Brian: Also, politics and memetic spandrels. For example, like people talk about the rise of postmodernism as replacement of metaethical realism with anti-realism in popular culture. I think that is a real development. One can question to what extent, it matters. Maybe it’s correlated with things like a decline in religiosity which matters more. I think that is one good example of how metaethics can actually go popular and mainstream.
Lucas: Right. Just to bring this back, I think that in terms of the AI alignment problem, I think I try to or at least I’d like to be a bit more optimistic about how much causal efficacy each part of thinking has causal efficacy over the AI alignment problem. I like to not or I tend not to think that 90% of it will in the end be due to rogue impersonal forces like you’re discussing. I think that everyone no matter who you are stands to gain from more metaethical thinking in so far as that whether you take realist or anti-realist views. The expression of your values or whatever you think your values might be whether they might be conventional or relative, or arbitrary in your view, or whether they might relate to some objectivity. They’re much likely less to be expressed and I think a reasonable in a good way, without sufficient metaethical thinking and discussion.
David: One thing I would very much hope that before for example, radiating out across the cosmos, we would sort out our problems on earth in the solar system first regardless of whether one is secular or religious, or a classical or a negative utilitarian, let’s not start thinking about colonizing nearby solar systems or anything there. Yeah, if one is an optimist or maybe thinking of opportunities forgone but at least wait a few centuries. I think in a fundamental sense, we do not understand the nature of reality and not understanding the nature of reality comes with not understanding the nature of value and disvalue or the experience of value and disvalue as Brian might put it.
Brian: Unfortunately, I’m more pessimistic than David. I think the forces of expansion will be hard to stop as they always have been historically. Nuclear weapons are something that almost everybody wishes hadn’t been developed and yet they were developed. Climate change is something that people would like to stop but it has a force of its own due to the difficulty of coordination. I think the same will be true for space colonization and AI development as well that we can make tweaks around the edges but the large trajectory will be determined by the runaway economic and technological situation that we find ourselves in.
David: I fear Brian maybe right. I used to sometimes think about the possibilities of so-called cosmic rescue missions if the rare earth hypothesis is false and suffering Darwinian life exists within our cosmological horizon. I used to imagine this idea that we would radiate out and prevent suffering elsewhere. A, I suspect the rare earth hypothesis is true but B, I suspect even if for suffering life forms do exist elsewhere within our hubble volume. It’s probably more likely humans or our successes would go out and just create more suffering or it’s a rather dark and pessimistic view in my more optimistic moments I think we will phase out suffering all together in the next few centuries but these are guesses really.
Lucas: We’re dealing with ultimately given AI and it being the most powerful optimization process or the seed optimization process to radiate out from earth. I mean we’re dealing with potential astronomical waste or astronomical value, or astronomical disvalue and if we tie this again into moral uncertainty and start thinking about William MacAskill’s work on moral uncertainty where we just do what might be like expected value calculations with regards to our moral uncertainty. We’ve tried to be very mathematical about it and consider the amount of matter and energy that we are dealing with here. Given a super intelligent optimization process coming from Earth.
I think that tying this all together and considering it all should potentially plan an important role in our AI strategy. I definitely feel very sympathetic to Brian’s views that in the end, it might all simply come down to these impersonal economic and political, and militaristic, and memetic forces which exist. Given moral uncertainty, given meta-ethical uncertainty and given the amount of matter and energy that is at stake, potentially some portion of AI strategy should play into circumventing those forces or trying to get around them or decrease them and their effects and hold on AI alignment.
Brian: Yeah. I think it’s tweaks around the edges as I said unless these approaches become very mainstream but I think the prior probability that AI alignment of the type that you would hope for becomes worldwide is low because the prior probability that any given thing becomes worldwide mainstream is low. You can certainly influence local communities who share those ideals and they can try to influence things to the extent possible.
Lucas: Right. I mean maybe something potentially more sinister is that it doesn’t need to become worldwide if there’s a singleton scenario or if the power and control over the AI is very small within a tiny organization or some smaller organization which has power in autonomy to do this kind of thing.
Brian: Yeah, I guess I would again say the probability that you will influence those people would be low. Personally, I would imagine it would be either within a government or a large corporation. Maybe we have disproportionate impact on AI developers relative to the average human. Especially as AI becomes more powerful, I would expect more and more actors to try to have an influence. Our proportional influence would decline.
Lucas: Well, I’m feel very pessimistic after all this. Morality is not real and everything’s probably going to shit because economics and politics is going to drive it all in the end, huh?
David: It’s also possible that we’re heading for a glorious future of super human bliss beyond the bounds of every day experience and that this is just the fag end of Darwinian life.
Lucas: All right. David, we’ll be having I think as you say one day we might have thoughts as beautiful as sunsets.
David: What a beautiful note to end on.
Lucas: I hope that one day we have thoughts as beautiful as sunsets and that suffering is a thing of the past whether that be objective or subjective within the context of an empty cold universe of just entropy. Great. Well, thank you so much Brian and David. Do you guys have any more questions or anything you’d like to say or any plugs, last minute things?
Brian: Yeah, I’m interested in promoting research on how you should tweak AI trajectories if you are foremost concerned about suffering. A lot of this work is being done by the Foundational Research Institute, which aims to avert s-risks especially as they are related to AI. I would encourage people interested in futurism to think about suffering scenarios in addition to extinction scenarios. Also, people who are interested in suffering-focused ethics to become more interested in futurism and thinking about how they can affect long-term trajectories.
David: Visit my websites urging the use of biotechnology to phase out suffering in favor of gradients of intelligent bliss for all sentient beings. I’d also like just to say yeah, thank you Lucas for this podcast and all the work that you’re doing.
Brian: Yeah, thanks for having us on.
Lucas: Yeah, thank you. Two Bodhisattvas if I’ve ever met them.
David: If only.
Lucas: Thanks so much guys.
If you enjoyed this podcast, please subscribe. Give it a like or share it on your preferred social media platform. We’ll be back again soon with another episode in the AI Alignment series.
[end of recorded material]
Originally published at futureoflife.org on August 16, 2018.
","AI Alignment Podcast: The Metaethics of Joy, Suffering, and Artificial Intelligence with Brian…",0,ai-alignment-podcast-the-metaethics-of-joy-suffering-and-artificial-intelligence-with-brian-d5397d545889,2018-08-17,2018-08-17 19:44:19,https://medium.com/s/story/ai-alignment-podcast-the-metaethics-of-joy-suffering-and-artificial-intelligence-with-brian-d5397d545889,False,17011,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Future of Life,FLI catalyzes and supports research and initiatives to safeguard life and develop optimistic visions of the future. Official account.,e33e2d2a809c,FLIxrisk,1361.0,93.0,20181104
0,,0.0,,2018-03-24,2018-03-24 21:25:31,2018-03-29,2018-03-29 12:40:20,9,False,en,2018-04-13,2018-04-13 23:06:12,128,802caf9638de,23.249056603773585,1,0,0,"DRAFT — Forthcoming Publication, Cal. W. L. Rev. (2018)
This paper was presented on February 19th, 2018 at the California Western School of…",5,"The Emerging Artificial Intelligence Wellness Landscape: Opportunities and Areas of Ethical Debate
DRAFT — Forthcoming Publication, Cal. W. L. Rev. (2018)
This paper was presented on February 19th, 2018 at the California Western School of Law “AI Ethics Symposium”
Lydia Kostopoulos, PhD
 www.lkcyber.com
I. INTRODUCTION
Over the past decade, there has been a surge of new wellness technologies, catering from the individual to spas and hotels. Wearables, with sensors to monitor steps, heart rate, sleep and temperature grew significantly in popularity. Similarly, there has been a boom in technologies that aid sleep and a plethora of new pleasure technology. Within the past five years, many wellness technologies have increasingly become fashion forward from rings to necklaces capable of measuring your mood, heart rate and steps. As the cognitive technologies improve, the wellness technology market is now seeing its early first wave of wellness technologies that incorporate artificial intelligence (AI).
Leading AI Scientist, Andrew Ng, compares Artificial intelligence to electricity and expects that it will change the way the world operates much like electricity did.[1] IBM CEO Ginni Rometty sees IBM Watson’s AI services as a $2 trillion opportunity.[2] Forrester’s Research[3] sees AI sparking an insights revolution, where the data derived will drive change across companies, deliver personalized customer service, and ultimately, increase profits. An entire book could be written on how businesses, services and markets will be transformed by AI, but what is AI? Oxford dictionary defines Artificial Intelligence as:
The theory and development of computer systems able to perform tasks normally requiring human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages.[4]
What does Artificial Intelligence mean for Wellness technologies, and what are the ethical implications? This paper attempts to answer this question by examining certain technologies and ethical questions. The technological scope of this paper provides examples of AI technologies that deliver wellness value without another human being involved. This paper makes a deliberate effort to examine the intersection between AI technologies as stand-alone offerings in the wellness market. Considering that the inclusion of AI into wellness is a new addition to the expanding wellness service and product offerings, it is an opportune moment to proactively discuss some of the emerging ethical questions. In efforts to better discuss AI uses in wellness, this paper explores them as they fall into three categories: intangible, tangible and embedded.
II. DEFINING WELLNESS
Wellness has become a popular buzzword in the past couple years, and a word more frequently quoted as a goal by individuals, workplace human resources, and hotel chains. This rise in interest for wellness has coincided with a period where there have been increasing reports of economic struggles[5], a volatile labor market[6] and a growing number of people feeling anxious or depressed.[7] Many definitions of wellness can be summarized into Merriam-Webster’s definition of wellness: “the quality or state of being in good health, especially as an actively sought goal.”[8] If wellness is being in good “health,” then it begs the question: What is health? The internationally recognized institution for health, the World Health Organization (WHO), defines health as “a state of complete physical, mental, and social well-being, and not merely the absence of disease or infirmity.” [9]
However, now that ‘wellness’ has become increasingly popular, institutions and global studies have been conducted on the topic and have come to their own conclusions as to what exactly wellness is. There are two organizations worth noting, first, the National Wellness Institute,[10] which adopts its co founder’s (Dr. Bill Hettler’s) perspective on wellness. Dr. Hettler believes that wellness is comprised of the following six dimensions: emotional, spiritual, intellectual, occupational, physical, and social. This multidimensional perspective is part of a wellness understanding in which Dr. Hettler assumes to be a “conscious, self-directed and evolving process of achieving full potential.”[11] This perspective is very compatible with the contemporary “Do-It-Yourself” (DIY) culture and even more so with the advent of a DIY AI therapy app.
The Organization for Economic Co-operation and Development (OECD) believes that well-being is a critical component to economic development and launched the OECD Better Life Initiative to measure well-being and progress in achieving it. They are measuring “quality of life” and “material conditions.” Within “quality of life” they measure: health status, work-life balance, education and skills, social connections, civic engagement and governance, environmental quality, personal security, and subjective well-being. Within “material conditions,” they measure: income and wealth, jobs and earnings, and housing. These elements build the index that they see as part of a cycle with resources to sustain well-being over time through the preservation of: natural capital, economic capital, human capital, and social capital.[12]
While relevant and thoroughly multi-disciplinary, these comprehensive approaches evaluate wellness from a holistic perspective. However, wellness technologies that are emerging with AI are not as comprehensive. As such, this paper will seek to explore AI and wellness technologies through the lens of the WHO, which examines mental, physical and social health.
III. INTANGIBLE, TANGIBLE, AND EMBEDDED ARTIFICIAL INTELLIGENCE.
This paper has divided Artificial intelligence (AI) into three mediums in an effort to more closely assess emerging AI technologies used within the field of wellness. The three mediums are intangible, tangible and embedded.

The technological examples discussed represent a few of the many emerging technologies that are coming out. Many people are familiar with smart watches that report specific metrics and use machine learning to make health predictions. One medical example is Fitbit and Apple Watch studies on how the data analytics can predict a user’s risk of diabetes.[13] But the technologies explored in this paper examine those that directly and primarily relate to mental and emotional health, which this research perceives to play an important role in underlying social health problems, and are important factors in physical health problems that are not genetic in origin. The research discussed highlights some of the latest emerging AI capabilities in the space of wellness. The following sections expand on each of the three AI categories with technological examples, their benefits to wellbeing, and their ethical implications:
A. Intangible AI Wellness
For the purpose of this research, intangible AI does not have a physical form, instead it can be communicated through a sound, a notification on a device, and/or invisible computation running in the background and called upon on demand for information or advice.
According to the American Foundation for Suicide Prevention (AFSP), suicide has become a leading cause of death in America, and the number of deaths by suicide is rising. In a study on antidepressant use in the United States, the National Center for Health Statistics at the Center for Disease Control and Prevention, found that:
Antidepressants were the third most common prescription drug taken by Americans of all ages in 2005–2008 and the most frequently used by persons aged 18–44 years. From 1988–1994 through 2005–2008, the rate of antidepressant use in the United States among all ages increased nearly 400%.[14]
In efforts to address these growing concerns, clinical research and start-ups alike have started to explore the utility of AI technology to identify, diagnose, prevent, manage, and solve these problems. One such initiative comes from the AI startup Mindstrong,[15] which sees the smartphone as an emotional diagnostics device that can help with mental health and wellness. President and co-founder, Dr. Thomas Insel (former Director of the National Institute of Mental Health), hopes to leverage AI to predict emotional health concerns before they arise. To achieve this goal, Dr. Thomas is attempting to bridge visits to the therapists and daily life through early warnings that are derived from measurements and assessments from digital phenotyping.[16] Digital phenotyping is the “moment-by-moment quantification of the individual-level human phenotype in situations using data from personal digital devices.”[17] In other words, Mindstrong aims to assess phone use patterns (typing, scrolling, etc.) when someone is relaxing and when someone is upset. These types of patterns can help paint a picture of a user’s emotional state.
At the University of Southern California (USC) Institute for Creative Technologies they are conducting research to create what they have dubbed a “virtual therapist” named Ellie. Through a webcam and microphone, the AI is able to process and analyze the emotional cues derived from the patient’s face and the variation in expressions and tone of voice. Used only in research settings, they found that when speaking with Ellie, patients “feel less judged by the virtual therapist and more open [to discussing their feelings].” The institute’s social psychologist, Gale Lucas, explained that “It’s about what’s happening in the moment — having a safe place to talk.” [18]
Outside the research lab there is an app called Addicaid, which is meant to help its users avoid addictive behaviors that are destructive to their mental, emotional, and physical wellbeing.[19] Addicaid has the potential to help people who experience the following: (1) substance abuse and process disorders, including but not limited to food, (2) gambling, (3) internet usage, (4) alcoholism, and (5) pornography. Leveraging clinical research, machine learning and adaptive AI “Addicaid predicts when a person might be at risk of falling into addictive behaviors and offers personalized treatment options for that individual.” For example, for someone who is struggling with alcohol abuse, using GPS, the app can intervene when the user is approaching or entering a trigger location (such as a bar or liquor store). In these instances, the app would provide information for a hotline and offer additional coping tools and techniques.[20]

If calling a hotline and speaking to a human is problematic, the WoeBot app is available for therapy sessions 24/7. The product of a team of Stanford psychologists and AI experts, WoeBot tracks its users’ mood through brief daily chat conversations and offers curated videos and word games to assist users in managing their mental health. Alison Darcy, Woebot CEO and psychologist, believes that Woebot has the potential to improve on human therapists because there is “a lot of noise in human relationships… [and] noise is the fear of being judged.” Woebot leverages a more deterministic conversational approach with open ended questions such as “How are you feeling?,” “What is your energy like today?,” which are question prompts that have been modeled on talk therapy and cognitive behavioral therapy.[21]
Unlike the other apps described, Therachat[22] leverages an AI chatbot to serve as a means to augment therapy sessions instead of replace them. It has a “smart journaling” tool which is HIPAA compliant and is customizable so that the therapist can assign homework to the patient, for example, to participate emotion tracking. The therapist would receive status updates and reports through the platform and incorporate this data into the sessions.[23]

Within the space of hotel wellness, Marriott is exploring a concept it is dubbing, the “hotel room of the future.” In this room, “Marriott would let guests control everything from the temperature of the shower to the color of the light with the sound of their voice.”[24] The settings and preferences of the guests would be saved and AI would be able to draw from them during their next stay. For example, if a guest normally likes to meditate in the morning, using the room lighting, the AI could wake the guest up gently in time for their meditative session. At this point it is still a prototype, but existing technology also makes it possible for the AI to gather information about the guest that is publicly available on social media. With time, these services will unfold and guests will be offered options to connect their hotel AI profile to their social media profile for more services, which apart from sentiment analysis, would also include precision advertising, and recommendations for hotel wellness promotions and dining options that cater to the guest’s dietary specifications.
All these technologies are useful approaches in improving emotional wellness, but they do not come without ethical implications. In an era of prevalent hacking and data leaks, the highly sensitive data gathered by such AI apps and interfaces must be treated with extra care. In cybersecurity it is said that it is not a matter of if an organization will be hacked but when.[25] Apart from data security implications, there are other ethical questions relating to the human aspect. There is something about the authenticity of another human’s human experience that adds value to an experience, be it a therapy session or a hotel stay.
An algorithmic bot does not understand vice and has never felt heart break or had to bury a loved one. While having an algorithmic bot therapist 24/7 is convenient and helpful to let one’s feelings and emotions out in real-time while they are occurring, it cannot be substituted as a form of human empathy. In the case of addiction, while an app may assist in thwarting some circumstances of addiction–and there are some people who can benefit from a nudge–one cannot ignore the holistic picture of an individual’s life and the emotional or lifestyle triggers that would prompt the individual to engage in addictive behavior in the first place. Human therapists make judgement calls throughout sessions and are able to be agile in treatments and recommendations. While therapists and bots can help, friends and family may also serve as an important support system, something that is sometimes forgotten as these technologies are designed.
Although the hotel room of the future embedded with AI wellness tools for guests will create a unique, personalized service at a hotel, the hyper-personalized wellness services that build a relationship between the guest and the AI capability could further expand the “digital bubbles” we live in. This technology has the potential to make its users less accommodating and increase feelings of entitlement. In this regard, it may unfairly raise expectations of human service to be on par with AI memory and instant personalization.
At its current state, the intangible AI and emotional wellness technology is more rigid and does not have the response fidelity one can expect from a human. This in no way means that these technologies should not exist, or that they do not serve a useful purpose. Instead, it creates a new space and an opportunity for them to be woven into other aspects of life, whether it is feeding actionable information to a therapist or wellness professional; or working towards personal wellness goals. Another aspect to these technologies has to do with the rise in anxiety and psychological issues in the United States today. AI wellness technologies can help make therapy more physically and financially accessible, but it could potentially serve as an illusionary form of therapy that fails to produce results. There should be metrics in which the user would be able to hold themselves accountable for their progress, or lack thereof, while using these technologies. Like with everything else in life, it is not a ‘one size fits all’ model, the danger comes when we pretend it is.
B. Tangible AI Wellness
This research sees tangible AI as embodied in a physical form, which humans can interact with. It can be AI in a vehicle, robotic pet, doll, factory equipment, etc. As with intangible AI, the wellness landscape for tangible AI is still an emerging commercial market.
Quality sleep has acquired the spotlight over the past few years, particularly with Arianna Huffington’s book, “The Sleep Revolution,” which discusses cultural problems with sleep, while setting forth a strong case for the value of sleep to our health, longevity and cognitive capacity.[26] One of the sleep technologies she recommends is S+ by ResMed,[27] which is a contactless device that is placed on the nightstand and monitors breathing, room temperature, noise and quality of sleep. According to Michael Wren, the ResMed Senior Director of Technology and Operations:
“The idea is that every breath you take is sent to the cloud and, over time, the technology can grow a picture based on the data about the person’s sleep, environment, activity and stress. These all build into an artificial intelligence algorithm in the cloud and the user gets a personalized train of feedback.”[28]

Another tangible sleep technology is the ZEEQ smart pillow, which is smart home compatible and can communicate with other AI devices such as Amazon Alexa for voice control sleep reports. It tracks sleep (duration, snoring volume, and movements) to determine a baseline of habitual sleeping and establish what is a normal sleep cycle for the user. The data derived from the sleep tracking can be visualized in the app and is used for sleep analysis to determine sleep duration, the user’s sleep cycles, and snoring impact on restfulness. The results are reported daily and collated over the course of time. Given this data, the pillow will activate its smart alarm at the most optimal moment during the sleep cycle.[29]
There are some who no longer trust their own feelings about the quality of their sleep, instead they would have to first sync their sleep device, receive the sleep report, and then determine how they slept. Our bodies speaking to us and notifying us about pain and discomfort has played an important role in our evolutionary path. Ignoring what our instinct says could be harmful for our health. The more authoritative data and analysis we derive from AI and devices focused our wellbeing, the higher the chance that we relinquish our own authority and judgment regarding wellbeing to technology. This poses an ethical concern as people may try to fit themselves into the “box” deemed normal and healthy. This box will be determined by the algorithms and data from studies that may not be representative of an entire population, or account for differences in lifestyle or DNA.

There have been AI wellness robots that have been successfully used, namely Paro the therapeutic seal, which provides patients an opportunity to take advantage of the documented benefits of animal therapy without having to leave their care facilities, or deal with the challenges of owning live animals. Paro is a robot seal with a soft, furry coat that purrs when being caressed and feels good to pet. It has sensors to follow the patient with its eyes and responds to being touched. Paro has been used to comfort patients with dementia and reduce stress to both the patient and their caregivers.[30] The AI gives Paro the “ability to ‘learn’ and remember its own name, and it can learn the behavior that results in a pleasing stroking response and repeat it.”[31]

While there are helpful devices and automated machines in health-care, Stevie the AI elderly care robot seems to be a category of AI robots that is growing. Created by Trinity College in Dublin, Stevie the elderly care robot “can perform a range of functions ranging from medication reminders and light conversation to video calls with family members.”[32] Its face, which is a screen, can also provide picture prompts for those who have hearing impairments. Should a patient or user become unresponsive, Stevie has the capability to contact emergency services. Apart from reminding its users to take their medicine and being a video messenger medium to call family and friends, AI allows it to provide a form of companionship through conversational skills. Companionship features are increasingly sought after for the aging population demographic–which often feels isolated–and could revolutionize home care for the elderly.[33]
While such robots have utility and can contribute to the well-being of a wide range of users, it is replacing a form of care and attention that has always been, since the beginning of time, exclusively given by humans. Today people are living busy lives with jam packed schedules, and these care robots can give loved one’s peace of mind. However, passing on care to robots is a form of responsibility transfer. As such, it is important for each family to thoughtfully evaluate how much care is being transferred to a robot.

Companion robots, however, have not been made solely for elderly care. RealBotix is is a company which sells custom life size female sex dolls with customizable personalities. Customers can order very realistic looking and feeling life size female dolls exactly to their preferences, from skin color, to breast cup size, waist size, rear end mass, hair color, eye color, and even public hair style. The dolls are for sexual use and have been manufactured for several years, but have been recently upgraded with AI technology. The doll comes with an app that allows users to customize the personality with several options and adjustments to choose from; some personality trait examples include: cruelty, humility, meanness, patience, courage, charm, tenacity, sensuality, and aggression to name a few.[34]
Putting aside the personal use of these sex dolls, they also have other uses, one being use as a sex surrogate. Surrogate partner therapy is a three-person therapeutic team involving the client, the therapist, and a surrogate partner. In this form of therapy “the surrogate participates with the client in structured and unstructured experiences that are designed to build client self-awareness and skills in the areas of physical and emotional intimacy.”[35] For those who have sexual traumas and mental difficulties with intimacy, AI sex robots can be used as an alternative form of surrogacy in place of a human being. Additionally, the AI software can be trained to have conversations relating to sexual trauma and intimacy problems. Another utility for sex robots is in nursing homes where patients have lost the independence and privacy they used to have living in their own homes. Sexual well-being is also a part of wellness and AI enabled sex robots can provide that to patients in nursing homes.
These AI enabled sex robots, can also be companions. Particularly for those who are socially less confident, as well as those who live alone and would like to have “someone” to talk to at home. With the rise in loneliness, this could be another example where technology offers a solution. However, while the AI sex robot has the potential to offer an alternative form of sexual intimacy and conversational companionship, it does pose some ethical questions relating to wellness and emotional well-being. Customizing an AI personality to one’s needs can be a dangerous habit to become accustomed to, as it is not possible to go to another human being and adjust their personality so it is more suitable to one’s preferences. Would people choose to spend time with their personalized AI companion over other human beings? If so, how much of a problem would that be for society? Would conversation with an AI enabled human looking robot doll alleviate the feelings of loneliness from lack of a human connection? AI will eventually get to the point of a more meaningful conversation and maybe people will prefer to marry AI robots. The robots with machine learning will learn and grow during the course of the relationship. But would it ultimately turn out to be a relationship with a non-consenting synthetic robot with simulated intelligence and no consciousness? This is very different than a relationship with a consenting human being who is sentient, conscious, and has self-determination.
Dinosaur check-in at Henna-na Hotel in Japan
Not to be outdone by the health and sex industry, hotels are also exploring the idea of AI enabled robots. In robot friendly Japan, there is a hotel called Henn na Hotel, which means “Weird Hotel,” and it is almost exclusively staffed with robots who check in guests, deliver their luggage to their room and answer their questions and requests in the room. From the anthropomorphized dinosaur at the reception desk, to the robot luggage carrier, and the mini night stand robot that helps adjust the temperature and light among other things, the entire experience can be a human-less one.[36] One ethical question to ask is, Can there be too much technology? What is the right balance between human and machine? There are countless known and unforeseen variables that come into play on a daily basis, making human life chaotic. A hotel that wishes to offer its guests calm and peace of mind would do well to have humans who can better respond to ambiguity and human emotion.
C. Embedded AI Wellness
Embedded AI is when AI is fused with our brain either through an invasive or non-invasive mechanism. While it remains a technology in its very early stages, it is a form of brain computer interface (BCI) that has the capability to augment the human brain (intelligence, mood, etc.).
The Defense Advanced Research Projects Agency (DARPA) is exploring research in neuroscience and brain computer interface under their Brain Initiative. Some of the research projects in the initiative focus on neural connections that can be stimulated and/or interacted in a way that produces a healing response or a sensation to areas where there is none.[37]
Among other companies, Neuralink, is actively pursuing research to connect the brain to the computer and leverage the AI capability with the brain.[38] Specifically they are “developing ultra high bandwidth brain-machine interface to connect humans to computers.”[39] This technology will create a new understanding of emotional and mental well-being. Given scientists’ knowledge on what neurons trigger happiness, having BCI capability may create other forms of addiction and some people may not be able to start their day without a “happiness” fix. In theory, happiness is an elevated joyful state that is above the normal state, and if the happiness feeling could be induced artificially on command, would the individual abusing it be only able to feel neural induced happiness? It is difficult to predict what having AI connected to the brain would mean for wellness, but safety measures could be put in place so that humans could not auto-pilot their way out of dealing with emotional distress. If this were the case, it could arguably create an emotionally and mentally weaker person, which could in turn (at a mass scale) could effect evolution.

Understanding the brain and being able to map it and connect it to a computer will create another avenue to immortality. The theory is that one would be able to upload their brain through whole brain emulation.[40] This technology is currently being developed but does not exist yet; an AI services such as that would leverage AI to create re-enlivened forms of deceased people on their social media.[41] Companies such as Eterni.me, help their clients become “virtually immortal” through a curated intelligent avatar.[42] This technology is useful for those who feel they are not ready to part with loved ones who have passed on, and it has the potential to augment a person’s emotional and mental well-being as they continue to remain close to a deceased family member or friend. It also has the potential to maintain a relationship with a therapist who has passed away. While keeping memories alive and creating new ones with loved ones whose memories and digital data have been AI enabled for digital re-enlivenment, this creates a reality dissonance with mortality and relationships with the dead. Finally, it has the potential to postpone or halt the grieving process, which may be harmful to an individual’s emotional and mental well-being. Time will tell how this technology will be adopted by culture and society.
IV. CONCLUSION
As supporters of technological solutionism continue to promote new and innovative technologies, it is important for society to turn back to the basics and challenge the fundamental assumptions about the problem they are attempting to address with technology.
Considering AI in wellness technology is only a few years old, there is an opportunity at the early stages to have impactful discussions regarding AI wellness before these technologies become more and more a part of daily life. Discussions should be had by technologists on the premise of human wellness and well-being, including how technology can ethically augment wellness without side effects.
Wellness is very much a human endeavor, and as we seek to replace wellness professionals with AI (in whatever form), thought should be given as to how that could take away from the lasting effects of the wellness experience. Perhaps a hybrid AI-human wellness professional would be the best combination to leverage the best of both worlds.
The start of the 21st century has been filled with rapid technological change, and questions that challenge us to ponder the status quo of human relationships and the essence of humanity. This means we should bravely interact with new technologies, those who create them, and our community, with a tremendous amount of courage to look within our humanity, as uncomfortable as it may be.
Meaning is assigned to things, events, people and life at an individual, societal and cultural level. At such an early stage, there is an opportunity to shape the meaning of new technologies before they are haphazardly assigned empty stereotypical meanings. Resolving the ethical aspects of these technologies as they pertain to wellness will have to be a deliberate and communally inclusive effort that challenges social norms. Ethical conundrums such as these are best placed in the #BetterWhenShared category. This research hopes to contribute to the conversation and further the discussion as continued research develops.
—
Dr. Lydia Kostopoulos’ (@LKCYBER) work lies in the intersection of people, strategy, technology, education, and national security. Her professional experience spans three continents, several countries and multi-cultural environments. She speaks and writes on disruptive technology convergence, innovation, tech ethics, and national security, and founded Sapien21 (www.sapien21.com) which encompasses these aspects. She is an advisor to the AI Initiative at The Future Society at the Harvard Kennedy School, participates in NATO’s Science for Peace and Security Program, is a member of the FBI’s InfraGard Alliance, and during the Obama administration has received the U.S. Presidential Volunteer Service Award for her pro bono work in cybersecurity. www.lkcyber.com
References
[1] Lynch, Shana. (2017). Andrew Ng: Why AI is the new electricity. The Dish: Stanford News. https://news.stanford.edu/thedish/2017/03/14/andrew-ng-why-ai-is-the-new-electricity/
[2] Darrow, Barb. (2016). Through machine learning, IBM Braintrust sees better days ahead. Fortune. http://fortune.com/2016/02/25/ibm-sees-better-days-ahead/
[3] McCormick, James. (2017). Predictions 2017: Artificial Intelligence Will Drive the Insights Revolution.: Advanced insights will spark digital transformation in the year ahead. Forrester Research. https://go.forrester.com/wp-content/uploads/Forrester_Predictions_2017_-Artificial_Intelligence_Will_Drive_The_Insights_Revolution.pdf
[4] Oxford Dictionary. (2017). Definition of Artificial Intelligence. https://en.oxforddictionaries.com/definition/artificial_intelligence
[5] The Economist. (2015). “What slowing trade growth means for the world economy.” https://www.economist.com/blogs/economist-explains/2015/09/economist-explains-10
[6] World Economic Forum. (2016). “The future of Jobs: Employment, Skills and Workforce Strategy for the Fourth Industrial Revolution.” Global Challenge Insight Report. http://www3.weforum.org/docs/WEF_Future_of_Jobs.pdf
[7] Singal, Jesse. (2016). “For 80 years, young Americans have been getting more anxious and depressed, and no one is quite sure why.” The Cut: Mental Health. https://www.thecut.com/2016/03/for-80-years-young-americans-have-been-getting-more-anxious-and-depressed.html
[8] Merriam-Webster Dictionary (2017). ”Wellness Definition” https://www.merriam-webster.com/dictionary/wellness
[9] World Health Organization (2017). “Constitution of WHO: Principles”, http://www.who.int/about/mission/en/
[10] National Wellness Institute. (2017) http://www.nationalwellness.org/?page=six_dimensions
[11] Hettler, Bill. (2017). “The Six Dimensions of Wellness.” National Wellness Institute. http://www.nationalwellness.org/?page=six_dimensions
[12] OECD. (2017). “Measuring Well-Being and Progress.” OECD Better Life Index. https://www.oecd.org/statistics/measuring-well-being-and-progress.htm
[13] Hobbs, Andrew. (2018). “Fitbit and Apple Watch can help predict diabetes risk, study reveals”. Internet of Business. Retrieved from https://internetofbusiness.com/deepheart-fitbit-apple-watch-predict-diabetes-risk/
[14] Center for Disease Control and Prevention. (2011). “Antidepressant Use in Persons Aged 12 and Over: United States 2005–2008. National Center for Health Statistics. Retrieved from https://www.cdc.gov/nchs/data/databriefs/db76.htm
[15] MindStrong. (2018). Mind Strong Health. Retrieved from https://mindstronghealth.com
[16] Al Idrus, Amirah. (2017). “AI startup Mindstrong bags $14M for mental health mission. Fierce Biotech — MedTech. Retrieved from https://www.fiercebiotech.com/medtech/ai-startup-mindstrong-bags-14m-for-mental-health-mission
[17] Torous, John. Staples, Patrick. Onnela, Jukka-Pekka. (2015). “Realizing the Potential of Mobile Mental Health: New Methods for New Data in Psychiatry”. Current Psychiatry Reports — Psychiatry in the Digital Age. Springer. https://link.springer.com/article/10.1007/s11920-015-0602-0
[18] Tieu, Andrew. (2015). “We Now Have an AI Therapist, and She’s Doing Her Job Better than Humans Can” Futurisum — Enhanced Humans. Retrieved from https://futurism.com/uscs-new-ai-ellie-has-more-success-than-actual-therapists/
[19] Addicaid. (2018). Addicaid — Recover responsibly. Retrieved from https://addicaid.com
[20] Walravens, Samantha. Cabot, Heather. (2017). “How this CEO is using artificial intelligence to treat addiction”. Women @ Forbes. Forbes. Retrieved from https://www.forbes.com/sites/geekgirlrising/2017/11/02/from-addict-to-a-i-entrepreneur-how-this-ceo-is-using-technology-to-treat-addiction/#529a3890408e
[21] Molteni, Megan. (2017). “The Chatbot therapist will see you now”. Science — Wired Magazine. Retrieved from https://www.wired.com/2017/06/facebook-messenger-woebot-chatbot-therapist/
[22] Therachat. (2018). Therachat Homepage. Retrieved from https://www.therachat.io
[23] Pennic, Jasmine. (2017). Therachat Unveils AI-Chatbot Platform to Augment Therapy Sessions. HIT Consultant. Retrieved from http://hitconsultant.net/2017/06/27/therachat-launches-ai-chatbot-platform/
[24] Zimmermann, Joe. (2017). “Marriot Unveils ‘Smart’ Hotel Room Prototypes, With Personalized Presents and Voice Control”. Bethesda Magazine. Retrieved from http://www.bethesdamagazine.com/Bethesda-Beat/2017/Marriott-Unveils-Smart-Hotel-Room-Prototypes-With-Personalized-Presets-and-Voice-Control/
[25] Townsend, Chris. (2018). “Federal Agencies Bracing for New Cyber Challenges in 2018” Symantec. Retrieved from https://www.symantec.com/blogs/feature-stories/federal-agencies-bracing-new-cyber-challenges-2018
[26] Huffington, Arriana. (2018). The Sleep Revolution. Arianna Huffington.com Retrieved from http://ariannahuffington.com/books/the-sleep-revolution-tr/the-sleep-revolution-hc
[27] ResMed. (2018). “S+ Sleep Sensor”. Retrieved from https://www.resmed.com/us/en/consumer/s-plus.html
[28] O’Connel, Claire. (2017). “Sleep trackers dig for better data”. The Irish Times — Business. Retrieved from https://www.irishtimes.com/business/innovation/sleep-trackers-dig-for-better-data-1.3150888
[29] Kapfunde, Muchaneta. (2017). “ZEEQ, The Anti-snoring Smart Pillow That Streams Music and Tracks Your Sleep” Fashionnerd. Retrieved from https://fashnerd.com/2017/11/zeeq-smart-pillow-streams-music-stops-snoring-tracks-your-sleep/
[30] PARO Robots. (2018). PARO Therapeutic Robot. Retrieved from http://www.parorobots.com
[31] Griffiths, Andrew. (2014). “How Paro the robot seal is being used to help UK dementia patients.” The Guardian — Society. Retrieved from https://www.theguardian.com/society/2014/jul/08/paro-robot-seal-dementia-patients-nhs-japan
[32] D’Arcy, Ciarán. (2017). “’A cute little fecker’: Trinity’s Stevie the robot helps older people. The Irish Times. Retrieved from https://www.irishtimes.com/business/technology/a-cute-little-fecker-trinity-s-stevie-the-robot-helps-older-people-1.3290009
[33] McgGinn, Conor. (2017). The Robot that could revolutionise home care for elderly people. The Independent. Retrieved from https://www.independent.co.uk/life-style/health-and-families/health-news/the-robot-that-could-revolutionise-home-care-for-elderly-people-stevie-us-a8068931.html
[34] RealBotix. (2018). RealBotix. Retrieved from https://realbotix.com
[35] International Professional Surrogates Association. (2018). Surrogate Partner Therapy. Retrieved from http://www.surrogatetherapy.org/what-is-surrogate-partner-therapy/
[36] Associated Press. (2015). “Japan’s robot hotel: a dinosaur at reception, a machine for room service “. The Guardian. Retrieved from https://www.theguardian.com/world/2015/jul/16/japans-robot-hotel-a-dinosaur-at-reception-a-machine-for-room-service
[37] DARPA. (2018). DARPA and the Brain Initiative. DARPA — DOD. Retrieved from https://www.darpa.mil/program/our-research/darpa-and-the-brain-initiative
[38] Urban, Tim. (2017). “Neuralink and the Brain’s Magical Future.” Wait But Why. Retrieved from https://waitbutwhy.com/2017/04/neuralink.html
[39] Neuralink. (2018). Neuralink. Retrieved from https://www.neuralink.com/
[40] Sandberg, Anders., Bostrom, Nick. (2008). “Whole Brain Emulation: A Roadmap”. Future of Humanity Institute. Retrieved from http://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf
[41] Meese, James., Nansen, Bjorn., Kohn, Tamara., Arnold, Michael., Gibbs, Martin. (2015). “Posthumous personhood and the affordances of digital media”. Mortality. Vol. 20, №4, 408–420. Routledge Retrieved from https://doi.org/10.1080/13576275.2015.1083724
[42] Eterni.me. (2018). Eterni.me. Retrieved from http://eterni.me
",The Emerging Artificial Intelligence Wellness Landscape: Opportunities and Areas of Ethical Debate,3,the-emerging-artificial-intelligence-wellness-landscape-802caf9638de,2018-06-21,2018-06-21 06:49:55,https://medium.com/s/story/the-emerging-artificial-intelligence-wellness-landscape-802caf9638de,False,5843,,,,,,,,,,Ethics,ethics,Ethics,7787.0,"Lydia Kostopoulos, PhD",Experimenter |Disruptive Tech Enthusiast |National Security |Educator/Curriculum Developer |Wellness Advocate |Functional Suit Designer → www.Lkcyber.com,554d683d09ef,lkcyber,121.0,32.0,20181104
0,,0.0,7f60cf5620c9,2018-05-05,2018-05-05 05:52:01,2018-05-05,2018-05-05 18:35:16,2,False,en,2018-05-07,2018-05-07 07:14:50,17,ca60f4d28412,11.138050314465408,28,1,0,"There is a school of thought that ponders a dark, dystopian future where artificially intelligent machines brutally and coldly run the…",4,"Building ethical AI in healthcare: why we must demand it

There is a school of thought that ponders a dark, dystopian future where artificially intelligent machines brutally and coldly run the world, with humans as only a biological tool. From Hollywood blockbusters, to evangelic tech entrepreneurs, we’ve all been exposed to the possibility of this type of future, but have we all stopped to ponder how we should avoid it? Now, of course, all of this dystopia is many many decades away, and only one of several gazillion possible future outcomes. But that doesn’t preclude getting the conversation started today.
For me, and many others, it boils down to one simple thing: ethics. Get your ethics right, and machines should in theory never be able to take over and dictate a machine-brain version of the universe. At a more simple level, we need to start considering how we can avoid inhuman decisions, especially where they have the potential to hurt us the most: in the life and death environment of healthcare. We are eons away from a fully automated healthcare system, however, right now, AI is being developed to help augment doctors’ decision-making. But what if some of those decisions are wrong?
In this blog, I’m going to discuss hard-hitting recent news from the NHS that highlights exactly why ethical and responsible AI should be demanded by everyone.
First, I need to tell you a story…
How a spreadsheet error destroyed lives
Rachel and her partner David (not their real names) are both junior doctors in the NHS. They met each other at medical school, stuck together through the hardships of finals, the testing environment of two years post-medical school foundation training, and celebrated when they finally got accepted to training posts in their respective medical specialities.
Rachel wants to be a general medic with an interest in stroke care, and works at a leading unit in the north of England. David is training to be a psychiatrist, and works in a mental hospital 80 miles away. Despite the difficulties of finding somewhere they both can live within reasonable commuting distance, they have so far managed to find some semblance of work-life balance by renting a small flat in a village half way between their jobs. Whenever night shifts are required, Rachel stays at a friend’s house closer to her hospital, sometimes for up to a week. David stays at his parents when needed. This arrangement, they have agreed, would only be for a short two years, until they find their perfect higher training jobs that will finally allow them to work closer together, buy a house and start a family.
When the time came to apply for their next jobs (known as ST3 selection) they were excited to start planning their lives together. They drew up a map of the region where they wanted to live, put pin-points in all the hospitals they would like to work in, the towns and cities they wanted to be near, and linked them all with dotted lines and calculations of commuting distances, mortgage rates, and good schools. Finally, they found their idyllic match. Rachel and David decided to apply to the same deanery to increase their chances of matching to a large teaching hospital that also had a psychiatry unit, located in a good town with a sprawling rural suburbia they felt they could settle down in and raise children.
They laboriously filled in their applications, made sure to collect great references, undertook extra-curricula courses to bolster their CVs. When submission day came, they both sat next to each other in their small living room, computers on their laps, and clicked ‘submit’ together on each other’s screens after counting to three. That was it – their future awaited. With fingers and toes crossed they went to bed, and dreamt of their future life together.
A few weeks and interviews later, the results were in! Rachel and David opened their emails together, read them, turned to each other, and both said “Did you get it?”…
“Yes!”, they both shouted. They lept up and hugged. Rachel started crying in David’s arms, huge heaving sobs of pure relief. The hour and half commute was gone, the weeks away working night shifts were over. The chance to buy a house was now real! This was it, they were going to start a proper life together. The next day, they started house hunting. They applied for a mortgage, they handed in their notices, and prepared for a new life. One evening, surrounded by cardboard boxes and two half-finished glasses of wine, David got down on one knee and proposed. He promised Rachel they’d be married and in their own house within a few months. Naturally, she said ‘yes’.
The next day, however, they received a bomb shell…
There had apparently been an “administrative error” in the ranking of some candidates, and as a result some had been offered incorrect job posts. All job offers across the country were being withdrawn, immediately.
Rachel was one of these doctors affected. The news came late on a Friday evening, just before a three-day bank holiday weekend. Three days of not knowing, of uncertainty and anguish, of shattered dreams and expectations. No-one currently knows what the final outcome will be. Rachel and David have to wait to find out what the future will bring. Suddenly the life they had planned feels as if it has been ripped away from them before it even had a chance to get started.
Lives both lived and yet to come into being have literally been put on hold because of an inhuman blunder.
If the error has indeed impacted Rachel, and she is offered a job elsewhere, somewhere far away, then they will have to consider the reality of pulling out of buying their dream house, letting go of their plan to get married this year and even delay having children for now. The inhumanity of it is overwhelming.
Now, in this story, it is important to note that the error, entirely real and affecting today’s juniors doctors in the UK, was a script programming error and not ‘AI’. Apparently, some spreadsheets were formatted differently to each other, and an automated script to compile results didn't account for this. Nevertheless, it starkly highlights the ethical need for accountability, robustness and accuracy in administrative systems, especially since AI is touted as being the replacement tool for very similar tasks.
The Royal College of Physicians is currently working on this problem (which has spawned the typically British hashtag of #ST3cockup on Twitter), and at the time of writing there are still many junior doctors like Rachel and David still waiting to find out what this error means for them and their families.


Junior doctors in the UK are increasingly feeling like they are an expendable resource rather than humans in a caring system. This error comes at a time when the dust was only just settling after the recent Dr Bawa Garba case (where a junior doctor was struck off the medical register under extraordinarily unfair circumstances), and only a couple of years after the hugely damaging junior doctors strikes over their imposed new working contract. For an administrative system to produce such a shocking new error, the effects of which are still unfolding, at a fragile time like this, it is not surprising to hear the uproar and disappointment from the medical ranks of the NHS.
Building ethics into everything
There is a wider picture here, however, that I would like to focus on: the need for deep consideration of the potential impacts of automation.
It is heart-wrenching to think that without building ethical AI, we, as a society, run the risk of introducing blindly autonomous systems into the mix, capable of even more brutal and opaque decision making than the example above. AI does not have the capacity to understand a larger social context in which to assess its errors, and if such an administrative error were ever to be made by a machine, I’m sure the uproar would be even louder. Regarding this current situation, heads are likely to roll. But will machine-heads roll if and when they make similar errors?
Recently, there has been published an exquisite report funded by the Wellcome Trust, written by Future Advocacy, on the ethical, social and political challenges of AI in health. A few quotes stand out to me from this report:
“ ..some algorithmic errors can systematically burden certain groups over and above others, without this problem necessarily being visible if we look only at the overall performance.”
I couldn’t agree more with this statement. To put it within the context of the ST3 application error, it is clear that certain groups of junior doctors have been burdened more than others, and the problem is only visible once those affected start reporting it. I am under no doubt that whoever wrote the offending spreadsheet script had checked it worked (i.e. overall performance was good), but was totally unaware of what their code was capable of doing, and likely was only notified once the proverbial hit the fan.
“… the more open a system is, including in terms of the development, commissioning and procurement of algorithmic tools, then the more protected users will feel from the risks…”
This second statement is also interesting in this context. Had the recruitment and selection system been more open, transparent, and even co-designed with those to be affected by it, would this error have occurred at all? For instance, if junior doctors knew the ranking system, the format of the spreadsheets, and the code used to compile results — would someone have noticed beforehand? Now, I’m not suggesting that in this context that we should aim for that level of ideal transparency — I’m simply posing the question “where do we draw the line between a closed system and a transparent one?”
When it comes to translating AI and autonomous systems into even more potentially dangerous situations, such as life and death decision making (e.g. autonomous cars, cancer diagnoses) the ethical problems loom even larger.
I was fortunate enough to have spent some time with Prof Alan Winfield of the Bristol Robotics Laboratory. He is an eminent thinker on ethical AI and robotics, and widely regarded as an authority in this field. He is working with IEEE to create new standards for the introduction of ethics to future technologies, a clearly topical subject! What I have learnt from him is that a system is not ethical until it has fully considered and included everyone it could possibly impact. This is a profoundly interesting thought, and one that I think should be mandated across the board.
Statistics are not ethical
AI is essentially a form of computational statistics. Mathematically, AI systems in healthcare that provide a classification output (e.g. ‘cancer’ or ‘no cancer’ on a scan) can be assessed on Receiver Operator Curves (ROC curves). (Don’t worry, I’m not going to start a lecture about statistics). All you need to know as a lay reader is that these curves are a simple way to easily compare systems against each other. The better a system performs, the further up and towards the left the curve goes. A perfect 100% accurate system would not even be a curve, it would be a right angle in the top left hand corner of the graph.
Three ROC curves: AUCc performs the best, but the area outside the ROC curve is still an ethical conundrum
In this example, AUCc performs the best with an Area Under the ROC curve (AUROC) of around 99%. This is near perfect, but not quite. AUCb is second best with an AUROC of around 85%, and AUCa is the worst performing at around an AUROC of 80%. In healthcare, you can draw up ROC curves of AI systems and compare them to human performance. Often in the media when you hear of an AI ‘beating humans’ it is because the AUROC is better, or the curve lies above and to the left of the point at which humans operate on the graph (I’m not going to go into the semantics any further — but trust me, there is lot of heated debate on this topic).
However, AUROC is NOT ethical. In fact it is entirely unethical. It reports only the positives success rate, and completely ignores the negative. There is an ethical grey area that hardly anyone considers — what I call the Area Outside the ROC curve (AOROC). This area can be thought of representing all the possible times an AI system gets a decision wrong. This area may well be smaller for an AI system than it is for a range of human doctors, but humans can at least understand and recognise their errors, change their mind, and explain their reasoning.
Now, in the example above, this AOROC area for the best performing system is very small, maybe about 1%. But what does it mean for people within that 1% when their automated decision is incorrect? How can we train systems to recognise when they are wrong, and how to change their mind? How do we build in risk mitigation to account for these errors and the potentially life changing effects they could have? The answers to these questions are tough, and accordingly often ignored.
It irks me when I read mainstream media reporting ‘computer beats doctor at X’ because I know somewhere along the line the risks of being within the Area Outside the ROC curve have been neglected, even by medical device regulators who are supposed to champion clinical safety. I challenge anyone reading this article to find me a regulatory-approved AI system which has published statistics regarding their failure rate, and an ethical statement on how they have mitigated against those that could be adversely affected. I’ve tried to find these, but have failed.
Even worse, these statistics completely fail to take into account bias in the underlying training data. For instance, Google had to publicly apologise when their highly performant classifier system tagged black people as gorillas, simply because the training data did not include as many black faces as white. A ROC curve will never be able to demonstrate this type of data bias, so we must mandate other methods of transparency to ensure AI developers are stating their data quality alongside their performance.
Work on ethics has begun
Thankfully there is a growing consensus that ethics for AI is absolutely essential. The recent House of Lords Select Committee report that I consulted on had a strong recommendation that the UK should forge a distinctive role for itself as a pioneer in ethical AI.
The impact of ethics in AI is only now hitting mainstream consciousness. Indeed, following the recent Facebook data sharing scandal, the company has set up an internal ethics unit to look at these kind of problems, and Microsoft have even pulled sales over ethical concerns.
I would urge anyone developing AI tools for healthcare to take a long hard look at the potential risks of their systems, and try to think outside of the box on how to ensure an ethical approach. Too often have we seen developers rush to be ‘first to market’ and make headline claims of performance, without care or mention of ethics, and it is my belief that it is only a matter of time before we begin to see the brutal dystopian impact created when these systems ultimately fail, which they are guaranteed to do at some point — take the recent NHS breast cancer screening error. Here, an automated system didn’t invite women for screening at the right time, leading some observers to claim that up to 270 women may have died as a result. The ethical ramifications here are staggering, and have even been debated in the House of Commons. No-one yet knows who is responsible; even the contractor who runs the system is deferring blame elsewhere…

If we are to avoid the vision of a cold inhumane future where real lives are cast aside by automated decisions and their inevitable errors, we must start talking about, and building in, ethics right now. Otherwise, it will be too late to save ourselves from the machines.
You can follow the unfolding ST3 saga here.
If you are as excited as I am about the future of artificial intelligence in healthcare, and want to discuss these ideas, please do get in touch. I’m on Twitter @drhughharvey
If you enjoyed this article, it would really help if you hit recommend and shared it.
About the author:
Dr Harvey is a board certified radiologist and clinical academic, trained in the NHS and Europe’s leading cancer research institute, the ICR, where he was twice awarded Science Writer of the Year. He has worked at Babylon Health, heading up the regulatory affairs team, gaining world-first CE marking for an AI-supported triage service, and is now a consultant radiologist, Royal College of Radiologists informatics committee member, clinical director at Kheiron Medical and advisor to AI start-up companies, including Algomedica and Smart Reporting.
",Building ethical AI in healthcare: why we must demand it,102,building-ethical-ai-in-healthcare-why-we-must-demand-it-ca60f4d28412,2018-07-02,2018-07-02 20:38:28,https://towardsdatascience.com/building-ethical-ai-in-healthcare-why-we-must-demand-it-ca60f4d28412,False,2850,"Sharing concepts, ideas, and codes.",towardsdatascience.com,towardsdatascience,,Towards Data Science,,towards-data-science,"DATA SCIENCE,MACHINE LEARNING,ARTIFICIAL INTELLIGENCE,BIG DATA,ANALYTICS",TDataScience,Ethics,ethics,Ethics,7787.0,Hugh Harvey,"Doctor² (radiologist & academic) MBBSs BSc(Hons) FRCR MD(Res). Clinical AI, machine learning in radiology imaging and research.",73b5953b27cb,DrHughHarvey,2573.0,92.0,20181104
0,,0.0,,2018-03-29,2018-03-29 22:35:56,2018-03-29,2018-03-29 22:52:54,0,False,en,2018-03-29,2018-03-29 22:52:54,2,a82592d2bdb6,0.8679245283018868,0,0,0,"Last summer, I took part in a forum at the Government Accountability Office in Washington D.C. regarding the role of AI in public policy…",5,"GAO releases new report on AI and public policy
Last summer, I took part in a forum at the Government Accountability Office in Washington D.C. regarding the role of AI in public policy. The forum was comprised of experts from four subject matter areas: autonomous vehicles, criminal justice, cybersecurity and finance. I was invited as a criminal justice expert, along side Richard Berk from UPenn.
The culmination of the two day event — and the endless hours of work by the GAO staff — is a report released today called: Artificial Intelligence: Emerging Opportunities, Challenges, and Implications.
The thoughtfulness of the group, which concerned itself with both the promise and perils of AI, is reflected in the 100 page report. The forum for me was an opporutnity to speak with and hear from those in fields much further down the road in developing AI. The persepctive helped me better frame and understand the challenges ahead for AI in the criminal justice system. My initial thoughts and lessons from the forum itself can be read here.
As for this report, it will head to Congress, which asked for its commission, and hopefully provide some level of guidance on the federal level. With the proliferation of ethics and AI outfits in the last year or so, I’m happy to have played a small role that hopefully leads to better informed AI policy.
",GAO releases new report on AI and public policy,0,gao-releases-new-report-on-ai-and-public-policy-a82592d2bdb6,2018-03-29,2018-03-29 22:52:55,https://medium.com/s/story/gao-releases-new-report-on-ai-and-public-policy-a82592d2bdb6,False,230,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Jason Tashea,"Tech, data, & the legal system. Founder @JusticeCodes, Staff Writer @ABAJournal, & Adjunct Prof @GeorgetownLaw.",4f8827a412cb,jtashea,230.0,579.0,20181104
0,,0.0,30e01424da4e,2018-01-03,2018-01-03 14:28:36,2018-01-03,2018-01-03 14:30:18,1,False,en,2018-01-19,2018-01-19 17:57:28,11,a691f5f6f6f2,4.350943396226415,702,36,0,Excited about using AI to improve your organization’s operations? Curious about the promise of insights and predictions from computer…,5,"The Algorithms Aren’t Biased, We Are
Excited about using AI to improve your organization’s operations? Curious about the promise of insights and predictions from computer models? I want to warn you about bias and how it can appear in those types of projects, share some illustrative examples, and translate the latest academic research on “algorithmic bias.”
First off — language matters. What we call things shapes our understanding of them. That’s why I try to avoid the hype-driven term “artificial intelligence.” Most projects called that are more usefully described as “machine learning.” Machine learning can be described as the process of training a computer to make decisions that you want help making. This post describes why you need to worry about the data in your machine learning problem.
This matters in a lot of ways. “Algorithmic bias” is showing up all over the press right now. What does that term mean? Algorithms are doling out discriminatory sentence recommendations for judges to use. Algorithms are baking in gender stereotypes to translation services. Algorithms are pushing viewers towards extremist videos on YouTube. Most folks I know agree this is not the world we want. Let’s dig into why that is happening, and put the blame where it should be.
Your machine is learning, but who is teaching it?
Physics is hard for me. Even worse — I don’t think I’ll ever be good at physics. I attribute a lot of this to a poor high school physics teacher, who was condescending to me and the other students. On the other hand, while I’m not great at complicated math, I like trying to learn it better. I trace this continued enthusiasm to my junior high school math teacher, who introduced us to the topic with excitement and playfulness (including donut rewards for solving bonus problems!).
My point in sharing this story? Teachers matter. This is even more true in machine learning — machines don’t bring prior experience, contextual beliefs, and all the other things that make it important to meet human learners where they are and provide many paths into content. Machines only learn from only what you show them.
So in machine learning, the questions that matter are “what is the textbook” and “who is the teacher.” The textbook in machine learning is the “training data” that you show to your software to teach it how to make decisions. This usually is some data you’ve examined and labeled with the answer you want. Often it is data you’ve gathered from lots of other sources that did that work already (we often call this a “corpus”). If you’re trying to predict how likely someone receiving a micro-loan is to repay it, then you might pick training data that includes previous payment histories of current loan recipients.
The second part is about who the teacher is. The teacher decides what questions to ask, and tells learners what matters. In machine learning, the teacher is responsible for “feature selection” — deciding what pieces of the data the machine is allowed to use to make its decisions. Sometimes this feature selection is done for you by what is and isn’t included in the training sets you have. More often you use some statistics to have the computer pick the features most likely to be useful. Returning to our micro-loan example: some candidate features could be loan duration, total amount, whether the recipient has a cellphone, marital status, or their race.
These two questions — training data and training features — are central to any machine learning project.
Algorithms are mirrors
Let’s return to this question of language with this in mind.. perhaps a more useful term for “machine learning” would be “machine teaching.” This would put the responsibility where it lies, on the teacher. If you’re doing “machine learning.” you’re most interested in what it is learning to do. With “machine teaching,” you’re most interested in what you are teaching a machine to do. That’s a subtle difference in language, but a big difference in understanding.
Putting the responsibility on the teacher helps us realize how tricky this process is. Remember this list of biases examples I started with? That sentencing algorithm is discriminatory because it was taught with sentencing data for the US court system, which data shows is very forgiving to everyone except black men. That translation algorithm that bakes in gender stereotypes was probably taught with data from the news or literature, which we known bakes in out-of-date gender roles and norms (ie. Doctors are “he,” while nurses are “she”). That algorithm that surfaces fake stories on your feed is taught to share what lots of other people share, irrespective of accuracy.
All that data is about us.
Those algorithms aren’t biased, we are! Algorithms are mirrors.
Algorithmic mirrors don’t fully reflect the world around us, nor the world we want
They reflect the biases in our questions and our data. These biases get baked into machine learning projects in both feature selection and training data. This is on us, not the computers.
Corrective lenses
So how do we detect and correct this? Teachers feel a responsibility for, and pride in, their students’ learning. Developers of machine learning models should feel a similar responsibility, and perhaps should be allowed to feel a similar pride.
I’m heartened by examples like Microsoft’s efforts to undo gender bias in publicly available language models (trying to solve the “doctors are men” problem). I love my colleague Joy Buolamwini’s efforts to reframe this as a question of “justice” in the social and technical intervention she calls the “Algorithmic Justice League” (video). ProPublica’s investigative reporting is holding companies accountable for their discriminatory sentencing predictions. The amazing Zeynep Tufekci is leading the way in speaking and writing about the danger this poses to society at large. Cathy O’Neil’s Weapons of Math Destruction documents the myriad of implications for this, raising a warning flag for society at large. Fields like law are debating the implications of algorithm-driven decision making in public policy settings. City ordinances are starting to tackle the question of how to legislate against some of the effects I’ve described.
These efforts can hopefully serve as “corrective lenses” for these algorithmic mirrors — addressing the troubling aspects we see in our own reflections. The key here is to remember that it is up to us to do something about this. Determining a decision with an algorithm doesn’t automatically make it reliable and trustworthy; just like quantifying something with data doesn’t automatically make it true. We need to look at our own reflections in these algorithmic mirrors and make sure we see the future we want to see.
","The Algorithms Aren’t Biased, We Are",4411,the-algorithms-arent-biased-we-are-a691f5f6f6f2,2018-06-18,2018-06-18 14:32:19,https://medium.com/s/story/the-algorithms-arent-biased-we-are-a691f5f6f6f2,False,1100,"News, ideas, and goings-on from the Media Lab community",,mitmedialab,,MIT MEDIA LAB,info@media.mit.edu,mit-media-lab,"EDUCATION,STEM,DESIGN",medialab,Ethics,ethics,Ethics,7787.0,Rahul Bhargava,"Research Scientist, MIT Center for Civic Media at the MIT Media Lab",e4b68a5f0cb8,rahulbot,589.0,1.0,20181104
0,,0.0,,2017-10-17,2017-10-17 16:49:39,2017-10-17,2017-10-17 17:00:45,0,False,en,2017-10-17,2017-10-17 17:00:45,5,85123480362f,0.8754716981132076,0,0,0,This first appeared in CIO’s IDG Contributor Network.,5,"Ethical principles for algorithms
This first appeared in CIO’s IDG Contributor Network.
As big data analytics continues to transform the economic and social landscape, is it time to ask questions about the ethical nature of the algorithms employed by various organizations?
Over the summer, a software engineer was sentenced to 40 months in prison for his role in helping Volkswagen evade pollution control rules. Earlier this month, the Pew Foundation reported that 85% of the public would support regulation to restrict the use of AI by businesses and organizations. These stories suggest powerful reasons for tech companies to do the right thing. But, beyond the fear of legal action or adverse public opinion, companies are also fully aware of the ethical implications of the AI-algorithms they create.
Tech companies are actively seeking principles to guide them through these ethical challenges. Last year, Google, Facebook, Microsoft, and IBM set up the Partnership on AI, which is dedicated to promoting AI for social good and to address ethical issues in AI. In October DeepMind set up a research initiative on Ethics and Society to ensure that development and use of AI is “held to the highest ethical standards.”
Such industry developments are based on the realization that algorithms are not just neutral tools without any intrinsic ethical character.
This piece is an excerpt of the larger column which can be found on CIO.com
",Ethical principles for algorithms,0,ethical-principles-for-algorithms-85123480362f,2018-02-01,2018-02-01 18:48:19,https://medium.com/s/story/ethical-principles-for-algorithms-85123480362f,False,232,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Mark MacCarthy,"SVP for Public Policy, Software & Information Industry Association; Adjunct Professor, Communication, Culture & Technology Program, Georgetown University",4317359941eb,maccartm,8.0,3.0,20181104
0,,0.0,7b837cf1fd73,2018-06-11,2018-06-11 12:52:07,2018-06-11,2018-06-11 13:09:29,1,False,en,2018-06-14,2018-06-14 17:50:27,18,94500e723d34,5.433962264150944,66,1,0,The release of Google’s AI principles last week are promising. It is hard to imagine how these principles will be baked into the DNA of the…,5,"Tackling the Ethical Challenges of Slippery Technology
The release of Google’s AI principles last week are promising. It is hard to imagine how these principles will be baked into the DNA of the company and how their implementation will play out in terms of company decision making and strategy, as they balance these against profit margins. But I am sure they are on it.
Rachel Coldicutt, CEO of Doteveryone raised some important questions about the principles. Her suggestion is that Google must say who their AI applications will benefit, and who they will harm. This is a strong moral position for a company to take, but these are indeed important considerations for technology companies. Not only that, Rachel’s writing provokes further questions. Given the networked nature of the technologies that companies like Google create, and the marketplace of growth and progress that they operate within, how can they control who will benefit and who will lose? What might be the implications of powerful companies taking an overt moral or political position? How might they comprehend the complex possibilities for applications of their products?
“We imbue technology with the ideals of the people who have created it, rather than those who use it.”
Jon Ardern, Superflux
One very real aspect of our technological landscape is that we tend to imbue technology with the ideals of the people who have created it. Implicitly, the technologies reinforce the beliefs and intentions of those who make and sell them. However, designers, engineers and marketeers only ever set up the affordances and suggest a use case. The true impact of a technology is, more often than not, defined by those who use it. Whether that’s knitting groups or fascist regimes, we have seen technology become an amplifier and accelerator of the social, cultural and political values of the groups who use it, not those who made it. And it will continue to be used in ways that you can never imagine.
The starting point of creating the products and services around technology is usually ‘need-centered’. Designers are generally expected to respond to a particular ‘need’. But what are sold and framed as urban lifestyle products have different uses depending on the context and needs of those using them. In Myanmar, SIM cards are cheap and easy to find so nearly everyone has a SIM card and phone number, but devices are shared between people; privacy isn’t a concern like it is in the West. Many people in rural Myanmar don’t have mirrors, so use the front-facing cameras to take selfies to see how they look. Before 2014 there was no internet in Myanmar, and even now connectivity is sporadic, many rely on Chinese apps preloaded on the phones, in a language they don’t speak. They are the unimagined users, the users on the margins, as Eliza Oreglia puts it in a recent lecture. Those on the margins are not involved in the feedback loop of design improvement. They were not even imagined in the design process.
I remember the surprise in the western media after this new story went out a while back, about Syrian refugees using smart phones. It was surprising because that particular context of use was so far removed from how smartphones are advertised. They are sold as a lifestyle product, and that frames our expectations of how that technology is going to be used. But technology will always be adapted to the needs of those who have access to it, regardless of the maker’s intention. Simultaneously the very same technology was being ingeniously exploited by oppressive forces. Soldiers at government checkpoints, as well as at ISIS checkpoints were demanding Facebook passwords. They would look at Facebook profiles to determine travellers ’allegiance in the war.
Still from the film ‘Everything is Connected to Everything’, about the vast, invisible ecologies of technology networks. Produced for the V&A ©Superflux 2018
I suspect the companies who create tech products know this. They work with the marketeers to create the perfect use case; the seductive, magical scenario you buy into, because that helps ship the product. If they started thinking of unintended consequences, of who their product could potentially harm, that could become very tricky. It would mean asking thorny questions:
How many unintended consequences can we think of? And what happens when we do release something potentially problematic out into the world? How much funding can be put into fixing it? And what if it can’t be fixed? Do we still ship it? What happens to our business if we don’t? All of this would mean slowing down the trajectory of growth, it would mean deferring decision-making, and that does not rank high in performance metrics. It might even require moving away from The Particular Future in which the organisation is currently finding success.
With the desire to move from Narrow towards General AI, things will get only more complicated. An audience member recently asked me after a talk: “We are developing a voice AI, but we are working to make it autonomous in its responses. And we want to be transparent with our users about the AI. So what should we say to them? We have an AI in this device, and we know its intelligent, but we don’t know exactly how it will respond?”
The layers of interacting networks within a deep neural network involve algorithms training themselves based on the data sets available to them. (Or maybe with the trajectory of AlphaGo Zero, that might no longer be needed) Although we understand the maths behind them — we don’t understand why they make decisions. Not to mention the inherent biases programmed into AI which mirror our own human biases. This creates a transparency imbalance: AI needs transparency with personal data in order to do its work, but its own rational and decision making is opaque to us. This lack of understanding is synonymous with lack of control. By automating decision making, and not understanding the intentions or logic behind certain decisions, control over these decisions is relinquished.
All of this can leave organisations paralysed or confused, and in some cases even complacent — in the state of can’t-be-bothered-because-I-cant-do-anything-about-it. But I think, today more than ever, creating, implementing and practicing a broad set of ethical principles is crucial. Because recent news have shown what happens if you don’t. (eg. Google Duplex, Facebook, Cambridge Analytica etc)
It might require that you:
Invest in considering the unintended consequences of what you ship. Rachel wrote about this, and we practice this in our work with clients all the time. (I am reminded of Bruno Latour’s essay where he writes about design’s ‘humble’ efforts to move away from the heroic, Promethean, hubristic dream of action: “Go forward, break radically with the past and the consequences will take care of themselves!” I reckon we should send Bruno to Silicon Valley, where design is so intertwined with disruption. )
Map the power of your organisation and products. What influence you leverage through your technology and the networks it reaches, is important to study, not just for monetising on ‘views’, but to better understand the effect it can have.
Decouple performance metrics from financial success. I think this is probably the most difficult, but also very important.
Develop multiple, alternate futures. By considering unintended consequences more thoroughly, you are probably already on your way towards developing alternatives that might be more worthwhile. You might come to blows with your financial controllers and shareholders, but you will find a way through it. After all, we are seeing the results of too much growth.
Focus less on bringing science fiction to life and instead, spend more time with anthropologists. (I can’t recommend David A. Banks’ Baffler piece ‘Engineered for Dystopia’ enough. David implores engineers to consider their power, and talks about the need to create more stories about engineers coming to terms with the consequences of their creations.)
This is a very quick post but I wanted to share and record some thoughts, as we are working with few organisations on some of these contentious issues.
Many thanks to Danielle Knight for her help on the piece and with proofreading.
More where this came from
This story is published in Noteworthy, where thousands come every day to learn about the people & ideas shaping the products we love.
Follow our publication to see more stories featured by the Journal team.
",Tackling the Ethical Challenges of Slippery Technology,324,tackling-the-ethical-challenges-of-slippery-technology-94500e723d34,2018-06-21,2018-06-21 08:23:13,https://blog.usejournal.com/tackling-the-ethical-challenges-of-slippery-technology-94500e723d34,False,1387,The official Journal blog,blog.usejournal.com,usejournal,,Noteworthy - The Journal Blog,,did-you-know-the-journal-blog,"STARTUP,PRODUCTIVITY,ENTREPRENEURSHIP,TECH,TECHNOLOGY",usejournal,Ethics,ethics,Ethics,7787.0,Anab Jain,"Designer, Filmmaker, Futurist / Co-founder @Superflux, London / Professor, Design Investigations, University of Applied Arts, Vienna.",57b7e9a544c,anabjain,2391.0,642.0,20181104
0,,0.0,217fda588a76,2018-10-02,2018-10-02 10:15:35,2018-09-17,2018-09-17 09:00:53,1,False,en,2018-10-02,2018-10-02 10:16:50,6,3eab08d141fe,3.3358490566037737,1,0,0,"Our lives and outlook, and those of our children, are fast being shaped by digital. These changes are unplanned, largely unregulated and…",5,"Time to face (unintended) consequences

Our lives and outlook, and those of our children, are fast being shaped by digital. These changes are unplanned, largely unregulated and already happening. This, warns Adam Thilthorpe, leaves us reliant on the ethical fortitude of developers. He says we urgently need a clear ethical framework for digital innovation.
In a digital world with ubiquitous technology, our lives are increasingly shaped by unintended consequences. Trying to get a measure on this, or even some semblance of control back on our own lives, is proving not only difficult but a challenge that has everything to do with the very real issue of ethics in IT.
Unintended consequences in our digital world shape our physical reality. When Mark Zuckerberg and friends were kicking around the original ideas for Facebook, they just had in mind a book of pictures of the students in Harvard — literally, a face book. Today, Facebook has grown to be one of the largest corporations in the world and, it is alleged, has been used to undermine the world’s largest democracy.
We’re now raising a generation who won’t recognise a world without communal artificial intelligence. Whether it’s Apple’s Siri, or Amazon’s Alexa, parents are being confronted by Ai that disrupts the natural ‘call and response’ of learnt conversation in the home to such an extent that we ask whether it’s still appropriate to teach children to say please and thank you.
Or is the opposite true? It is said that true digital natives can clearly distinguish the difference between human interaction, simple voice recognition and even natural language understanding. But do we really believe that?
It’s not just about being polite. According to a NSPCC/Children’s Commissioner report, 40% of 11 year-olds ‘sext’, and with half of 11–16 year-olds reporting seeing online pornography. How can that be good for the future of human interpersonal relationships?
What role do all of us, parents, educators and regulators have?
We’re seeing the daily use of biometrics at our borders and in our courts. Police forces are experimenting with Ai software that can interpret images, match faces and analyse patterns of communication, all with the aim of speeding up the examination of mobiles.
These are not planned changes, these are in use, here, now. Do you remember being asked if you wanted, let alone consented, to these incremental but important changes to the way that we conduct our lives? No, me neither.
Yet step by technical step, we are seeing a change to the fundamental relationship between citizen and state. Instead of presumed innocent are we now simply all un-convicted people?
As our technologies move so, inevitably, public policy, legislation and our regulators lag far behind. Nowhere was that more starkly evident than when Mark Zuckerberg appeared in front of a US Senate committee in the Spring (pictured). As one commentator put it, ‘part of the problem was the clear ignorance, if not befuddlement in the face of technology displayed by most senators, many of whom are of a ripe vintage’.
So where does that leave us? Sadly, at the mercy of the ethical fortitude of those developers, designers, coders and makers who are forging ahead in this digital age, if not at our behest, certainly then at least with our enthusiasm for greater integration and insight.
Let’s face it, what’s more useful: online ads for a bulk buy of nappies that I’ll never click, or ads for the new road bike I’ve been promising myself?
These developers, designers and coders and makers are the very people that need to understand not only the intentions and motivations, but, importantly, also the potential for unintended consequences. IT people must be great sociologists…
The chances are that, if you’re reading this, you know some or all of this already. You’ll be in the know, and the chances are that you’ll already have your own opinions about the various issues I’ve raised. That’s what I’d expect.
But the big question for me, is how do those of us who work in, or at the edges of, some of this technology, raise these big, difficult questions with politicians, with civil society leaders and with the public at large? Whose role is it to ensure that the magnitude and complexity of the world that is being created around us?
The US tech giants — not a great track record so far. Our own governments and regulators, perhaps. What about our national news media?
For me, it’s simple. We need those who work in the sector, who are developing these technologies, to understand that they owe it to their families, and to society at large, to develop within an ethical framework.
With great power comes massive responsibility.
Adam Thilthorpe is director of external affairs at BCS, The Chartered Institute for IT. Adam talks at DigitalAgenda’s Power & Responsibility Summit at London’s British Library on 4 October. Secure 30% off your summit ticket now.
Originally published at www.digital-agenda.co.uk on September 17, 2018.
",Time to face (unintended) consequences,5,time-to-face-unintended-consequences-3eab08d141fe,2018-10-04,2018-10-04 09:39:49,https://medium.com/s/story/time-to-face-unintended-consequences-3eab08d141fe,False,831,"Writing and events about the impact of digital on people, places and business. Technology for a change. The new thing from @julianblake and team. Visit us at www.digital-agenda.co.uk. Update sign-up at eepurl.com/b7yRb5",,digitalagenda2016,,DigitalAgenda,editor@digital-agenda.co.uk,digitalagenda,"DIGITAL,BUSINESS,POLITICS,JOURNALISM,TECH FOR GOOD",DigitalAgenda_,Ethics,ethics,Ethics,7787.0,DigitalAgenda,"Thoughts on technology, business, politics and places.",f0249cc47940,julianblake,343.0,456.0,20181104
0,,0.0,3a8144eabfe3,2017-10-05,2017-10-05 13:53:43,2017-11-01,2017-11-01 15:51:01,5,False,en,2017-11-05,2017-11-05 19:22:05,10,f815660990,7.433333333333333,6,0,0,"*Disclaimer: This piece contains spoilers from Blade Runner, Blade Runner 2049 and other sci-fi related media. Why you haven’t watched…",5,"Why Our Grey Matter Will Bring Us Into Grey Areas: Ethics, Robotics & AI
Still From Blade Runner (1982)
*Disclaimer: This piece contains spoilers from Blade Runner, Blade Runner 2049 and other sci-fi related media. Why you haven’t watched either is beyond me, especially the first Blade Runner. Go, watch it now. Seriously! This article will still be here when you get back from having your mind blown, unless of course the world ends for whatever reason*
“The robots are coming! The robots are coming!” Cliched sci-fi movie lines aside, the ethics of artificial intelligence are a topic worthy of debate. Whilst I have argued previously that AI shall hugely benefit humanity, lingering questions and fears still remain amongst many, even those who advocate for greater AI development: Will they evolve into some Skynet-style entity and launch various nuclear weapons, destroying all of humanity? Where should we draw the line when it comes to developing AI and other robotic, computational or machine entities? Will those androids like the replicants of the Blade Runner series — how good was that sequel, am I right? — or the synths of Fallout 4 become a reality? If so, what does it mean then to be human if, as the *spoilers* rather badass female replicant in Blade Runner 2049 says to Ryan Gosling, the machines we create become “more human than human”?
Weaponising The Potential — AI & Weaponry
Taken From An Article in The Guardian
This has always been the main fear of AI development: We develop a machine so advanced and so intelligent it outwits us and the world ends up destroyed due to its launching of a nuclear bomb or creation of a virus that wipes out the human race. That’s usually what most people think of when you talk about the weaponising of artificial intelligence, but in truth, we’ve been doing it for years, and arguably in a more sinister way.
In a comment piece for the scientific journal Nature, Prof. Stuart Russell of UC Berkley called on fellow scientists and tech developers to consider the risks of local autonomous weapons systems, otherwise known as ‘LAWS’ . Russell noted quite rightly that every combat engagement has to be compliant with the Geneva Convention and general ‘rules of war’. War should only be sought if (a) it is necessary after the exhaustion of all other options; (b) a distinction can be made between combatants and civilians; and (c) there is a near equal proportion in terms of what the combat could gain, and what it could damage (in other words, the benefits outweigh any risks). At present, it is near impossible for AI-based systems to determine any of the above.
But here’s the thing — The technology to develop this type of weapons system is already in use and continually being developed to even more advanced levels. Think of the use of unmanned drones in Libya, or heat-seeking missiles to destroy enemy aircraft and bases. They’re all machines right, and that last one in particular knows how to find it’s target without much human control (follow the heat source its locked to and, well, boom). Whilst we don’t yet have the technology to allow machines to determine targets based off their own intelligence, it is likely that in the future, we will. Russell is right to be concerned about this potential use of AI. Whilst right now most nations agree that all machine combatants should have a “meaningful (degree) of human control” over them, what exactly we mean by “meaningful” is yet to be determined and is likely to change as the years pass and the march of technological development continues.
Indeed, so great are the ethical concerns around this issue, that an open letter was written by Elon Musk and one hundred other robotics and AI experts to the UN calling for a ban on AI weaponry, whilst similar calls have been made by physicist Stephen Hawking and Apple co-founder Steve Wozniak. That letter was written for a reason: Whilst nobody can’t quite see a Terminator-esque scenario happening due to the idea of infinite regress, we all know that machines, even intelligent ones, are liable to glitch — What happens when that missile meant for an ISIS military base ends up hitting a civilian target like a hospital or school because of a malfunction?
These Feelings Inside — Machine Emotion & The Limits of Human Morality
From Channel 4’s TV Series, “Humans”
In Blade Runner and it’s sequel, the phrase “pleasure model” is commonly used to describe replicants that will, for a price, carry out whatever sick-twisted sexual fantasy you may or may not have locked away in the back of your mind. Instead of getting into a debate about whether prostitution itself should be legal, the ethical conundrum that arises from the existence of these “pleasure models” is this: is it morally right to create something that is, in all essence, a sex slave? This is theme appears not only in big-screen sci-fi flicks like Blade Runner, but also throughout the brilliant Channel 4 series “Humans”. Again, it seems that for whatever reason, one of the first things human beings — it seems mainly men — wanted to do with robots was build one so realistic and human that they could have sex with it. In Humans, a line from one such ‘sex robot’ really stuck with me: “What you do with us makes me wonder what you do with real women”.
If the androids we create are intelligent and sentient enough to develop their own morality and sense of worth, then surely they must be treated as human, or at the very least, be treated and allowed the same level of dignity as us? We campaign against the cruel use of animals in the circus or in the lab — Shouldn’t this same concern be given to whatever type of replicant or android we design? This question is pondered by Nick Bostrom and Eliezer Yudkowsky in their paper, “The Ethics of Artificial Intelligence”. As they note, current AI programmes and robotic technologies do not currently possess the ability to feel, but that could all change in the future.
If we created a system or machine that could feel pain, was self-aware and had a degree of intelligence (even one greater than our own), then morally we would place it at the same level as say circus elephants or tigers. We would — or rather, should — see its enslavement as morally reprehensible. We all look back on human slavery with disgust, but you have to remember the sad fact that it was once considered an acceptable thing defended and encouraged by the great minds and leaders of the day. Whether we will see the potential slavery of AI in the same way remains to be seen.
They Took Our Jobs! — The Use of Robotic & AI Technologies in the Workplace
From Magoda
A final issue to consider is the fact that AI and other forms of robotic and computational technology quite literally replaces human beings in the workplace. Those quick, self-services lanes that for whatever reason can’t scan that jar of pesto you want? Well, whilst they may be quicker than whoever is behind the counter, they have more than likely taken a job from a human being who needs a salary to, you know, live.
Whilst some may question whether this issue falls under the topic of morality and ethics — this isn’t as anywhere near as ethically complex as say, designing synthetic humans — there are still clear ethical implications for our continual development of AI technologies in terms of human employment and dignity. In a world where good, stable employment is a near impossible thing to find, the computerising of the workplace has caused major questions to be asked about the value of human agency and life. And when I say value, I mean literal, financial value of employing a human being.
The 2016 US Presidential election was won by Trump, not because free trade or open borders cost jobs, but because almost all manufacturing jobs are now automised, a trend that is set to have a knock-on effect in other sectors of employment, both blue and white collar — Trump won because he (falsely) offered hope to those who had lost or were in danger of losing their jobs to robotic insurgents. There are of course benefits to automation, such as the lowering of costs to manufacturers which should mean that the price of their product can be lowered, thus making it easier for the average consumer to purchase. But, if the average consumer is without a job due to such automation, then such products, no matter how cheap they could potentially become, are out of reach.
This brings into focus the question of human dignity: What are we worth if we are replaced by machines? Self-worth is an incredibly fragile thing, and something is going to have to be done — largely by governments — in order to make sure those who lose their jobs due to automation keep some form of self-worth alongside financial stability. This could be done through the introduction of basic income in one form or another, but the consensus on the topic is far from settled.
Curiosity Killed The Cat — Concluding Thoughts
At the end of Blade Runner — if you can’t tell I really like that movie — the replicant named ‘Roy’ delivers what is widely considered to be one of the best closing monologues in science fiction, and indeed, one of the most well-known in cinema history in general. In it, he recounts all that he has experienced, noting how when he dies, the memories shall fade “like tears in rain”. Aside from the whole scene hitting you right in the feels, the scene marks the point in the film where the viewer realises that the replicants Harrison Ford has been hunting for going ‘rogue’ are actually as human as he is (or may not be).
From GIPHY
As I have said before, AI and other technological advancements in the fields of robotics and computer science will on the whole be beneficial to humanity. However, the ethical concerns and conundrums that come with it, both in relation to us and to AI itself, are worth pondering. What it means to be human has always been a topic of debate amongst philosophers, anthropologists and theologians. The coming robotic revolution has the potential to make that question even harder to answer.
","Why Our Grey Matter Will Bring Us Into Grey Areas: Ethics, Robotics & AI",63,why-our-grey-matter-will-bring-us-into-grey-areas-ethics-robotics-ai-f815660990,2018-03-30,2018-03-30 11:32:53,https://hackernoon.com/why-our-grey-matter-will-bring-us-into-grey-areas-ethics-robotics-ai-f815660990,False,1749,how hackers start their afternoons.,hackernoon.com,hackernoon,,Hacker Noon,stories@amipublications.com,hacker-daily,"HACKATHONS,HACKING,PROGRAMMING,TECH,HACKER",hackernoon,Ethics,ethics,Ethics,7787.0,Ruairi Luke McCallan,"Irish; Interested in Politics, Technology & Culture; Bibliophile; Hibs Fan",bc32ba538d0f,RuairiLuke,574.0,123.0,20181104
0,,0.0,,2017-09-24,2017-09-24 19:57:44,2017-09-24,2017-09-24 20:10:29,2,False,en,2017-09-24,2017-09-24 20:10:29,9,8d97e68cea42,5.187106918238994,4,0,0,Technology and ethics are strange bedfellows.,5,"Towards An Ethics of Immortality
Is a long life desirable if you end up as a head in a jar?
Technology and ethics are strange bedfellows.
Once we have a tool, it becomes really difficult to not use it.
How we use it and why we use it are key ethical questions.
Every tool shapes the task, and if you’ve got a hammer, you’re bound to look for nails.
Health technology is a great example of this, as it not only transforms how we care for ourselves, but also how we understand ourselves, and how we relate to others.
Without irony, there are some really rich people and some really smart people who are collaborating on the technology of immortality.
http://2045.com/project/avatar
Seem absurd? Tools to help us live forever? Delusional dreams of divinity?
These are largely products of the cult of singularity. The shared hallucination that humans are machines, with computers as brains, and sometime soon we’ll build external machines and powerful computers that we can transfer to, and live forever.
However we are not computers. We cannot be uploaded. We will die. This I do believe.
Yet there will be powerful side effects and consequences from the pursuit of immortality.
Some of us will live a whole lot longer than we planned, maybe even than we desired.
Rather than immortality, what we’ll probably end up with are tools towards longevity.
Should we be regulating and monitoring how these tools are developed and used? Or should we just blindly embrace the pursuit itself? Or both. At least let us talk about it, asking questions about how these tools could be used and why.
For example, do your retirement plans include a contingency for multiple decades of unexpected lifespan?
Do you want to lead a long life, if that life is spent sick and miserable with limited mobility?
How we perceive retirement will certainly change, but what about work itself? Would you work harder if you knew you had to work longer?
The healthcare sector is already being transformed by the prospect of increased longevity, but what about insurance? Should insurance be cheaper if you end up paying for it longer?
What about real estate? Do longer mortgages become more appealing or does home ownership have less appeal as we desire to move around as we age?
Increased longevity will have an impact on all aspects of our lives, especially our values, and our broader society, as our relationship with time becomes less urgent and seemingly scarce?
Or alternatively, what kind of society would result if those with the greatest amount of wealth were able to live significantly longer than those without wealth? Given that this is already the case, an obvious answer would be an increasingly polarized society where the rich get richer and the poor get poorer and die younger.
My brother Jacob Hirsh adds:
“Small longevity extensions at the end of life already eat up a disproportionate share of the overstrained health care budget. What will happen when older (and wealthier, more politically influential) people demand even more resources to sustain their lives further? The needs of younger people (especially children) will inevitably be devalued in comparison. Even if the technologies themselves are relatively cheap, extensions of longevity will increase inequality by limiting resource availability for younger people (nothing left to inherit from the previous generation of rulers).”
Yet what if death is not an option, or at least not something that comes when you want it to?
What if a person of a late age commits a crime and are sentenced to a term that is clearly longer than they will live. Will they be subject to non-consensual life extension to ensure that they serve their sentence?
Will longevity only be accessible to the rich? What if it were imposed upon the working poor?
What if you could live to 200 but only if you agreed to live an additional 500 years in a jar? Would you, given that you don’t know what it’s like to live in a jar?
Here’s something to remember when thinking about the future.
As far as our experience as humans, there is no future, only the present, and maybe some of the past. The longer we live, the more presents we experience but do we accumulate more past? What if the past is just a memory we reconstruct when remembering?
How can we plan for the future when all we know is the past and the present?
We live in a tyranny of “todays”, as our relationship with space and time means that we can only ever experience today. Carpe diem (a/k/a YOLO).
Yet we cannot escape the future either. It will always be there. Looming. Producing an endless source of “todays” for us to experience. The choices we make and the actions we take today directly impact the future, and whatever “todays” that future presents.
We experience time as we do because of our bodies. They are the vessels that carry us forward through time in space. This is partly why we’re not machines and our brains cannot be uploaded to a computer.
Our intelligence is embodied. Our bodies are our brains. Our muscles contain memories. Our senses trigger scenes from our past and instinctual responses. Our guts are a central part of our consciousness.
What if we’re not a single sentient organism but a federation of living beings united by space and time where consciousness is the parliament in which we debate our actions and state of being?
Jacob offers some further reflection:
“There are also lots of interesting ethical questions when you take the possibility of uploading consciousness seriously. We currently view human life as being intrinsically valuable, for religious and evolutionary reasons. But really, we are talking about human minds (or souls) as being intrinsically valuable. If the mind can be transferred to a different material substrate, does it still have intrinsic value? Must it be preserved in its current form at all costs? Is self-archiving (making backups of one’s consciousness at different points in development) necessary to preserve this value? When should the mind be allowed or prevented from changing (i.e., being integrated into a larger network of minds and computers)? If ending a life is the greatest moral violation imaginable, does this apply to ending a digitized mind as well? If not, then why not? In principle, it is the subjective consciousness of minds that gives them their intrinsic value, not the bodies that they happen to inhabit.
It’s also not clear how to engage with these ethical questions. An intuitionist approach would suggest that we should go with our feelings about right and wrong. But these feelings are based on our evolutionary history and can lead to extreme prejudice against anyone who is not our kin. A utilitarian approach would suggest that we balance the costs and benefits of immortality, which would mean keeping people alive as long as they contribute to the economy at a higher rate than their medical costs (i.e., the net impact of an individual life on collective well-being). But this eliminates any concern for the intrinsic value of (human) life. A principled approach would begin with the assumption that human life has intrinsic value, but this value must ultimately be quantified in order for practical economic decisions to be made, and it’s not clear how this principle would operate in a transhuman context (as per paragraph above).
If anything, dramatic advances in health-related technology expose the many contradictions in our ethics. At some point, the medical goal of preserving the life of a focal individual could lead to disastrous consequences for the collective (i.e., the ecology of beings that make life possible for anyone).”
",Towards An Ethics of Immortality,31,towards-an-ethics-of-immortality-8d97e68cea42,2018-06-02,2018-06-02 11:20:25,https://medium.com/s/story/towards-an-ethics-of-immortality-8d97e68cea42,False,1273,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Jesse Hirsh,"Futurist, researcher, public speaker, data scientist, and 1337 strategist. Rapper who doesn't need to rhyme. See also @metaviews and @impossibledotws",fe3c607d0206,jessehirsh,1398.0,572.0,20181104
0,,0.0,,2017-09-01,2017-09-01 01:55:27,2017-09-05,2017-09-05 06:14:47,7,False,en,2018-02-15,2018-02-15 15:57:00,57,40c57338e806,14.400943396226413,11,1,0,How does artificial intelligence threaten human rights principles and our legal system as we know it? What can regulators do to respond?,5,"The Promises and Perils of Artificial Intelligence: Why Human Rights and the Rule of Law Matter
The following think piece is based on research I conducted as an intern this past summer while working in Special Projects with Urs Gasser at the Berkman Klein Center for Internet & Society.
The Ubiquity of AI & the Possibility of a Different World
The impact of artificial intelligence is bound to be massive and far-reaching. Artificially intelligent systems might eventually decide who can access healthcare or not, or who qualifies for a job or mortgage or not. How can we trust artificial intelligence (AI) systems to decide impartially? When governments or states use AI, should the source code be open for public inspection, or closed and subject to some type of testing?
And how will courts treat challenges to the impartiality of AI technologies? Who or what is at fault when AI technologies fail? How might judges and juries use evidence produced by AI? And to what extent should courts rely on AI as a tool to regulate behavior when it comes to crucial decisions such as criminal sentencing?
AI also threatens more than our decision-making processes: AI could replace us. Researchers at the University of Oxford estimate that around half of all US jobs may be at risk in upcoming decades, with low-wage occupations being the most vulnerable. As a recent Forbes op ed highlighted, one pervasive fear is that AI technologies will “outmaneuver humans out of their game.”

Despite the risks that AI poses to our legal systems, we have at least two reasons to be hopeful. First, stakeholders of all kinds — ranging from government entities, academics to entrepreneurs — have debunked the claim that AI technologies are about to achieve ‘technological singularity’, or even superintelligent General AI. The notion AI has already attained superhuman, wholly autonomous capabilities can only be seen as a myth.
Second, in the words of Shalil Shetty, “there is also the possibility of a different world” than one where AI clandestinely controls our behavior or exploits the vulnerable. Indeed, initiatives like the Knight Foundation’s Ethics and Governance of Artificial Intelligence Fund and conferences such as the 2016 White House-backed AI Now Symposium all signal a strong interest in — and crucial need for — the development of AI technologies in the public interest.
As artificial intelligence augments human decision-making, I argue that legislators and policymakers must account for the myriad risks that AI poses to human rights principles and the notion of the ‘rule of law’. This remains particularly true when we observe the ways in which AI stands in for us or nudges us. And how regulators conceive of the contours and purpose of regulatory systems will invariably affect the potential for AI to preserve human rights norms and the rule of law.
In this article, I explore several legal and human rights issues raised by the evolution of AI technologies. What follows are vignettes or threads of interest, strung together by a fascination I developed this summer with respect to narratives as to what constitutes AI, tools for regulating behavior (including technology), and the role of participatory design as AI technologies evolve particularly in constitutional democracies.
This exploration is by no means exhaustive. Instead, I hope to offer regulators the means to begin making sense of vital issues as AI becomes ubiquitous in our societies.
Mapping the Issues: The Broad Scope of AI
How we define artificial intelligence shapes how we respond to it. By definition, artificial intelligence involves technology that perceives elements of its own environment in hopes of successfully achieving some specific goal—generally through replication of at least one of four notions of ‘intelligence’: human performance, reasoning, thought processes, or in pursuit of an idealized notion of rationality (Norvig and Russell at 4).
Drawing by Santiago Ramón y Cajal (1899) of neurons in the pigeon cerebellum, public domain
Technology that already takes up the methods of artificial intelligence isn’t new, and equally warrants regulation as much as emerging AI technologies. Indeed, much technology that is already embedded in our everyday lives has come to employ AI techniques. These tools, such as algorithms, may not have previously qualified as AI but also deserve regulation in order to develop in the public interest.
Take the example of algorithms and social media websites. Recall that an algorithm is a procedure for solving a mathematical problem in a finite number of steps, and often involves repetition of an operation. (Algorithms can, indeed, employ AI techniques, but do not always constitute artificial intelligence.)
Facebook has used an algorithm called EdgeRank since 2007, which measures affinity, weight, and time decay to determine what users would see in their“News Feed”. Then in 2011, Facebook stopped using EdgeRank only to turn to a far more complex machine learning algorithm based on around 100,000 factors to determine what users would see. Time and again, academics and media pundits alike have consistently criticized the social media website for failing to provide transparency and clarity as to the actual code that dictates which news feed content is shown to the company’s users.
The transparency and predictability of Facebook’s news feed algorithms really matter: a whopping 62 percent of Americans rely on such social media websites for their news consumption. And while Facebook’s latest explanation on its decision to use AI to counter “terrorist content” reveals some measure of commitment to transparency, such content regulation poses serious challenges to free expression in quasi-public spheres, and raises vital questions relating to the possibility for unfettered discrimination against vulnerable groups of people.
Further, what we may call an algorithm could be a functional equivalent to AI and also may raise equally important human rights risks. Take a look at the fact that predictive algorithms are increasingly being used to mine personal information such as credit history, and make guesses about individuals’ likely actions and risks, critically affecting people’s ability to retain informational self-determination and, in turn, obtain basic needs such as housing, work, loans, and insurance.
Consider as well the recent news that a company called hiQ Labs is suing LinkedIn to retain access to data that it gleaned from public profiles to algorithmically predict whether employees will quit. Such an algorithm that perceives elements of its environment in hopes of successfully achieving some goal — here, determining whether a person might quit their job — and through the replication of human reasoning, would seem to fall squarely in the exact definition set out by AI experts as to what constitutes artificial intelligence (at 4).
These events raise important questions: when do algorithms constitute artificial intelligence, and how should both legislators and policymakers ensure that such technologies develop in line with human rights norms?
Mapping the Issues: Why AI is Different
The ethical development and use of AI systems is especially important when we consider the possibility for such technologies to challenge human agency in two salient ways. “Agency” is understood here quite simply with its dictionary definition: “the capacity, condition or state of acting or of exerting power.”
With this in mind, an artificially intelligent system could disrupt human agency by depriving us of the capacity, state, or the conditions in which we can act in a self-determined way. More specifically, such technology begins to affect our decision-making processes either by standing in for us or by nudging us.
Technological developments have without a doubt fundamentally transformed our entire lives, yet arguably no other technology has presented as serious a threat to our agency or autonomy. Artificial intelligence seeks to replicate human intelligence, which therefore involves some measure of decision-making that is autonomous or independent from human instruction, and that is based on information the technology itself obtains and analyzes. As such, much is at stake when AI makes decisions for us or nudges us as we shall see below.
AI Standing in For Us
What happens when artificially intelligent entities replace us? Indeed, the replacement of human labor by AI systems is not so far off: a Japanese insurance company laid off 34 employees in early 2017, only to replace them with IBM’s AI Watson Explorer. And labor rights activists have criticized the recent merger between Amazon and Whole Foods for the potential loss of what some label ‘low-skilled’ jobs due to automation.
Another excellent example of the potential for AI to usurp our decision-making power involves autonomous vehicles. AI and labor expert Jerry Kaplan observed that long-haul trucking is but one example where increased automation will result in robots replacing humans: highways are the easiest roads to navigate without human intervention.
FANUC R-2000iB series robot by Mixabest, licensed under the Creative Commons Attribution-Share Alike 3.0 Unported license
Companies are also increasingly relying on AI in hiring processes. “We can look at 4,000 candidates and within a few days whittle it down to the top 2% to 3%,” said the CEO of one company that shortlists candidates through the use of AI in the hiring process.
The relationship between such uses of AI technologies and human rights issues is clear. In the case of AI systems replacing humans in the workplace, there is palpable potential for such substitution to involve discrimination against vulnerable groups of people such as immigrants, people living in poverty, those with disabilities, and possibly even people of color.
And according to a 2016 McKinsey & Company report, jobs involving the performance of physical activities or operating machinery in a predictable environment are the most susceptible to automation—namely manufacturing, food service and accommodations, and retailing (the preparation of goods for consumption). As a consequence, workplace automation by AI systems may particularly affect vulnerable, working class populations, and other demographics of people who typically do such labor.
The Nudging of AI
Human rights norms are also prominent in instances where AI technology nudges us, (surreptitiously) encouraging us to act in one way or another. For example, recent news reports demonstrate the ability for machine learning to predict instances of schizophrenia. And consider the idea of the use of AI in judicial decision-making processes, which presents both considerable difficulties and positive potential in terms of serving the public interest.
Indeed, the human rights risks arising from the influence of AI systems on judicial reasoning are especially numerous. This phenomenon calls into question whether a person has received a fair trial, perhaps because the judge may have been swayed such that she perpetuates actual or implicit bias rooted in datasets that have been relied on by the AI system.
And another risk concerns whether AI is not or cannot be explained as a part of the reasons for the judge’s decision. This was at issue in Eric Loomis’ case: the Wisconsin Supreme Court ruled that a judge’s use of closed-source recidivism assessment software in sentencing does not necessarily breach the constitutional right to due process (in this case to challenge the software’s validity or accuracy), so long as the judge doesn’t rely on the score exclusively and receives written warnings about their value. The US Supreme Court denied judicial review of the decision in June 2017.
Other critical human rights can be triggered by a failure to afford a person a fair trial: consider the fact that a given trial can affect a person’s ability to be free from torture and, in some cases, to live.
By Daniel Bone, licensed under Creative Commons CC0 license
At the same time, we also know that AI systems might improve a judge’s ability to make decisions in at least two ways. For example, AI technologies have been shown to predict more accurately than judges whether defendants were a flight risk while they awaited trial. As one recent study revealed, AI technologies can therefore be useful to judges and society insofar as they might be able to contribute to lower crime rates and a reduction in jail populations.
AI algorithmic systems can also be used to identify other things that nudge a given judge: implicit biases as it relates to racial disparities, extraneous factors such as what a judge ate for lunch or the status of her favorite sports team are all decision-making factors on which AI systems can shed light.
Techno-Regulation: A Problem for the Rule of Law
Whether AI makes decisions for us, or prods us in a certain direction raises worthwhile questions regarding the relationships between technology, regulation, and the notion of the ‘rule of law’. What is the paradigm that law- and policymakers wish to see come to fruition as they create legal systems in response to AI technologies?
For our purposes, the rule of law can be understood as a principle requiring the law to be clear, publicly visible or known, and applicable to all people (including lawmakers themselves) (see Radbruch). A crucial element of the rule of law concerns the ability for people to contest the application or consequences of a given law, typically before a court of law (see Hildebrandt at 10).
If computer code operates as law, and is effectively regulating our behavior (according to the prominent school of thought made popular by Lawrence Lessig), in what ways might AI technologies result in the arbitrary, intransparent rule of a dominant group rather than upholding the principles of the rule of law? Recall, for example, the fact that judges might increasingly rely on determinations made by AI systems in order to determine a person’s risk for recidivism (but such algorithms might be protected by intellectual property law as trade secrets). In this way, AI technologies that influence judges also act as tools for behavior regulation.
Lawyer and legal historian Mireille Hildebrandt has also determined that emerging technological infrastructure such as artificial intelligence reconfigures our lives and de facto regulates our behavior. And for Hildebrandt, proponents of what she calls the ‘regulatory paradigm’ tend to frame the law as a neutral instrument for social engineering that can be freely replaced with other policy instruments.
The Neutral Face emoji, approved as part of Unicode 6.0 in 2010 and added to Emoji 1.0 in 2015
Yet Hildebrandt compellingly points to two major problems with such a neutral conception of the law. First, a neutral conception of the law frames human beings merely as rational agents who are trying to maximize their own utility, all while effectively and efficiently realizing specific policy goals — no matter the means of regulation, including de facto regulation afforded by technologies (164). Second, the regulatory paradigm is not about providing tools for citizens to challenge unreasonable governmental interference in a court of law. Instead, the regulatory paradigm simply aims to influence behavior, again in view of certain policy goals (165).
“Why not use technologies as neutral means to achieve policy goals,” Hildebrandt writes, “if they provide for more efficient and effective ways of guiding people? If the idea is to influence behavior instead of addressing action, and if the means are interchangeable, why not opt for a means that is less visible, less contestable and thus more influential in getting people to behave in alignment with policy objectives?” (see Hildebrandt at 165, emphasis added).
Hildebrandt goes on to explain that techno-regulation is a prime example of the ramifications of a regulatory paradigm that goes uncontested: replacing legal regulation with technical regulation may in fact be more efficient and effective. As long as the inner-workings of AI technologies are a part of hidden, unchallenged complexities, people will simply lack the means to contest any suspected infringement or manipulation of their rights.
Technology and Law in a Constitutional Democracy
But how might legislators and policymakers conceive of the purpose of the law, if not merely to achieve specific policy goals? One cogent solution is Hildebrandt’s pluralist or ‘relational conception’ of the law, which presumes a connection between the design or engineering of technology as well as how we specifically take up such technology, and the affordances or far-reaching capabilities of such technology in our lives.
This means that when law- and policymakers reconfigure our social and legal fabrics to account for AI technologies, they ought to first incorporate into their technological assessments the norms and values that members of society wish to sustain. Second, they need to scrutinize whether the affordances or potential usages of such technology will transform or disrupt these norms and values — which will require an “up-stream involvement of those who will suffer and enjoy the consequences” of the potential use of AI systems (at 172).
There are two significant potential benefits to employing a relational conception of law and technology.
First, a relational conception of the law can help guarantee the three hallmark legal norms in a constitutional democracy: self-rule, as legal rules are established by a democratically chosen legislator; disobedience, as such rules can be violated; and contestability, as the legal consequences can be contested in a court of law (Hildebrandt at 10).
According to the Center for Civic Education, constitutional democracy is the antithesis of arbitrary rule. It is democracy of, by, and for the people, such that all citizens — rather than favored individuals or groups — have the right to politically participate, and the fundamental rights of all individuals are protected.
By Nick Youngson, licensed under the Creative Commons Attribution-Share Alike 3.0 Unported license
Legislators and policymakers can turn to emerging, worthwhile concepts such as MIT Media Lab’s Iyad Rahwan’s notion of ‘society-in-the-loop artificial intelligence’, which embeds the judgment of society as a whole in the development and use of AI systems, so that AI machines behave in ways that a given society would consider ethical. And society is crying out for technologists, computer scientists, engineers, and entrepreneurs to design AI systems that are safe, accepted, and trusted.
To this end, each AI system ought to be designed to allow those who use the system, subjects of the system, and relevant decision-makers to assess a given system’s accountability through the technology’s algorithms and data, responsibility through clear evaluations of causal relationships or chains of command, and transparency with respect to how AI systems’ algorithms do what they do.
In an ideal world, AI technologies would not only develop to facilitate self-rule, but would also function transparently so that we have the agency to reject any insidious nudging as well as challenge the legal consequences of any such technological process.
A second benefit to employing such a relational conception concerns the power of AI technologies to preserve of rule of law. In her writing on the notion of ‘moral crumple zones’, cultural anthropologist and AI researcher Madeleine Elish argues for reconfigured notions of moral and legal responsibility when it comes to human-robot interactions.
More specifically, Elish demonstrates the insufficiency of the traditional paradigm for determining responsibility, which relies on the amount of control exerted by the technology’s operator — such as the pilot of a plane. Instead, control in automated technology has become distributed across multiple actors, such as operators, manufacturers, software designers, and the software or hardware itself.
Depiction of a car’s crumple zone, public domain
Elish tells us that “[t]he result of this ambiguity is that [operators] may emerge as “liability sponges” or “moral crumple zones.” Just as the crumple zone in a car is designed to absorb the force of impact in a crash, the human in an autonomous system may become simply a component — accidentally or intentionally — that is intended to bear the brunt of the moral and legal penalties when the overall system fails.” A case in point involves the Air France flight 447, where the official French report traced the cause of the crash to the pilots’ “total loss of cognitive control”, despite the plane’s autopilot processes appearing to have played role.
What is the root of the problem? A significant risk here is that any quick or readily available solution to the moral crumple zone—such as bestowing legal personhood on autonomous agents—may allow designers of such systems to dodge responsibility for any of their technological and design choices that may have contributed to the accident.
If the software designers write the code and algorithms that constitute an AI system, and if such code invariably functions as de facto regulation on our behavior, then policymakers ought to consider the benefits of holding liable the designers and engineers who construct the digital structures that invariably affect our behavior.
Beyond this, legislators and policymakers will likely need to further reconfigure our notions of moral and legal responsibility when software acts or writes part of itself in a way that goes beyond the designer’s explicit desires.
Conclusion: The Possibility of a Different World
As as engineers, researchers, and entrepreneurs develop AI technologies, regulators ought to monitor the specific ways in which AI threatens human rights principles and the rule of law.
Legislators and policymakers should carefully but swiftly define “artificially intelligent” technology in order to account for any human rights abuses facilitated by such technologies. This is because AI systems can challenge human agency either by standing in for us or nudging us, as well as ultimately disrupt the rule of law by forming a part of a given regulatory framework—but with such opacity that the logic of the engineer or machine cannot be challenged.
We must ensure that AI technologies develop so as to guarantee societal norms marked by self-rule, as well as the ability to both defy rules and contest their legal consequences. In so doing, we take critical steps in ensuring that AI technologies develop in the public interest.
Thanks to Urs Gasser, Gabriel Blue Cira, Natalie Pompe, Elena Sophie Drouin, Michael Lukaszuk, and another anonymous friend for helpful comments on this project.
",The Promises & Perils of AI: Why Human Rights and the Rule of Law Matter,68,the-promises-and-perils-of-artificial-intelligence-why-human-rights-norms-and-the-rule-of-law-40c57338e806,2018-06-13,2018-06-13 17:11:55,https://medium.com/s/story/the-promises-and-perils-of-artificial-intelligence-why-human-rights-norms-and-the-rule-of-law-40c57338e806,False,3538,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Yuan Stevens,You-anne. Cyberfeminist researcher and graduate of McGill University's Faculty of Law. Thinking lots these days about vulnerabilities in media and the law. 🤖 ✨,260a2dab9621,ystvns,80.0,66.0,20181104
0,,0.0,df321e12215d,2018-09-03,2018-09-03 17:49:35,2018-09-04,2018-09-04 06:40:01,1,False,en,2018-09-04,2018-09-04 06:40:01,0,8e787fe8e357,3.5283018867924527,4,0,0,"The Nexar Mobility Network has the potential to become a key component of the world’s present and future, making transport and mobility…",4,"The Sustainable Use of Big Data and AI at Nexar: Our Ethical Pledge

The Nexar Mobility Network has the potential to become a key component of the world’s present and future, making transport and mobility both safer and more efficient. This network will become an essential part of our lives and will provide the same level of welfare we see in other networks, such as broadband and the Internet.
The Nexar Network unlocks huge positive externalities as it grows from thousands to millions of nodes and digitises the roads. The more the network grows, the fresher and more relevant the digitisation and the overall utility of the network becomes. Instead of having to deploy expensive infrastructure to monitor the roads, requiring both an upfront investment as well as continued operational expenses, this network grows organically, just because it provides value to all the individual participants of the network.
The ability to estimate and predict the position of all vehicles at any point in time has the potential to almost completely eliminate all road casualties. As we see autonomous vehicles develop, this mobility network becomes a virtual sensor for the vehicle, allowing sensing the road beyond line of sight.
And it’s not just about safety. With this network, we can manage traffic more efficiently, leveraging existing road infrastructure to reduce congestion and increase traffic flows. We can also reduce emissions, and save energy. We can automate intersections and remove traffic lights. We can see when pedestrians are about to cross the road, and have cars adjust their speed smoothly to allow a coordinated space.
We run artificial intelligence services, both onboard the cars as well as in the central cloud servers, as the backbone of the network. We see videos, images, and sense the road, and the surroundings of the road, non-stop 24x7. These smart nodes running at the edge become more and more powerful over time, both as compute architectures become more efficient, and further research in AI comes to light.
The positive utility and social welfare this network can create is immense. But it’s possible for such a network to be misused, and used for other purposes to the ones originally intended. And the question then becomes how do we protect from misuse. What should be the boundary of what we define as a good positive externality of the network?
The concerns of the Nexar Mobility Network being misused requires us to be proactive , and define our ethical operating principles. These principles guide our decisions, especially around what features and uses we can and can’t extract from the network. We don’t yet know what use cases will be possible in the future once the network sees any road anywhere in the world continuously. Hence it’s important to have clear standards to guide us.
Objectives for the Use of Big Data and AI at Nexar
We apply technology to create social welfare
Our technological focus is in the development and implementation of algorithms that make the roads safer and efficient, and that as a consequence results in a better future of mobility, one where we create social welfare. When faced with use cases that become possible when digitising the world, we will evaluate whether they will result in safer and more efficient mobility, and as a consequence benefit society.
We protect individual privacy
Our users’ data is their own. We will never share individual data with a 3rd party unless it is authorised by the user and done on her request. We will actively engineer the Nexar mobility network with built-in anonymization to prevent any breach of users’ privacy. We will comply with the law but build our network so that it cannot function as a government tool for the purpose of tracking individuals.
We will not track individuals or enable the usage of the Nexar network by 3rd parties that would violate individual privacy. We will abide to protect the privacy of any subjects observed by the network.
We are digitising the physical world
As the network looks at the world, our focus is in digitising the road and its surroundings. Our objective is to understand what is happening on the road in order to make the roads safer and more efficient. We will actively prevent any application resulting from digitising the physical world that requires 3rd party individuals’ data for it to function properly.
We pay special attention to actively filter out the details of any salacious, violent, or criminal content or event happening on the network.
We remain accountable for our work
As we develop technology using big data and AI to digitise the world, it is important that we remain accountable for our work. We will ensure human judgement, direction and control of the findings. We will strive for explainability and actively ensure we are aware, measuring, and communicating bias. We will also seek the highest standard and work with academia and external stakeholders that provide the necessary level of scrutiny and accountability that ensure we are developing applications that create social welfare.
What We Will Not Pursue
Use of the Nexar mobility network as a panopticum for the monitoring of individuals and use for and by law enforcement.
Use of the Nexar mobility network to contravene the United Nations Universal Declaration of Human Rights.
",The Sustainable Use of Big Data and AI at Nexar: Our Ethical Pledge,7,the-sustainable-use-of-big-data-and-ai-at-nexar-our-ethical-pledge-8e787fe8e357,2018-09-04,2018-09-04 06:40:01,https://blog.getnexar.com/the-sustainable-use-of-big-data-and-ai-at-nexar-our-ethical-pledge-8e787fe8e357,False,882,"Nexar's real-time network connects cars nearby. Using this vehicle-to-vehicle network (V2V), Nexar can warn its users in real-time of dangerous situations happening beyond each individual driver's line of sight, such as issuing a forward collision warning.",blog.getnexar.com,getnexar,,Nexar's Blog - The First AI Dashcam App,contact@getnexar.com,nexar,"ROAD SAFETY,MOBILE APPS,CAR ACCIDENTS,IOS,ANDROID",getnexar,Ethics,ethics,Ethics,7787.0,Bruno Fernandez-Ruiz,"Co-Founder and CTO at Nexar, programming language junkie and 0xCAFED00D",6364ec610b27,olympum,460.0,124.0,20181104
0,,0.0,,2018-01-15,2018-01-15 22:29:37,2018-01-19,2018-01-19 16:44:15,4,False,en,2018-02-09,2018-02-09 19:31:56,9,a46c952be0f9,5.915094339622643,2,0,0,"Artificial intelligence (AI) has been making serious technical progress over the last several years, but not in the political sense. Tech…",5,"
Artificial Intelligence vs. Machine Learning: The Costs of Innovation
Artificial intelligence (AI) has been making serious technical progress over the last several years, but not in the political sense. Tech giants like Microsoft and Google, and online retailers like Amazon, have found new ways to accelerate their products using AI-driven algorithms. AI isn’t exactly the correct term, however — at least not in the sense that general consumers know it, in regard to machines like HAL from 2001: A Space Odyssey or Skynet in the Terminator movies. There’s a difference between AI and Machine Learning, but it’s such a new concept for the zeitgeist, the two are easily confused.
AI is a broad term encompassing technology that employs advanced computer intelligence, but it’s Machine Learning (ML) that really gives computers that human-like intellect seen in science fiction. The difference between AI and ML may be subtle but it’s still substantial enough to merit its own classification in the technology field. Whereas AI uses preprogrammed, rule-based information to perform specific actions like IBM Deep Blue defeating chess grandmaster Garry Kasparov in 1996, ML can, instead, use large amounts of data to teach itself how to do a task. Another example, Google’s DeepMind learned how to play the Japanese game Go against itself and then went on to defeat South Korean Go player Lee Sedol in 2016.
Fiber optics…
It’s not so surprising that many rational people are daunted or even haunted by the very concepts behind AI, because as humans we’re often rattled by things we don’t yet understand. What we have to accept is that our notion of privacy will change over time, as will our relationship to technology. Worst case scenarios aside, nothing has been lost. Our relationships to these technologies is ever-evolving, net neutrality or otherwise. The world continues to spin, time continues to pass.
We still have the power to decide how we relate to the internet and the silicon creatures we connect to it. We can be good people who get online or bad people who get online. The online component is not where morality comes into play, it starts from within. There is no reason to fear AI more or less than anything else (or ourselves). The people alive today do not own the monopoly on the fear or sacrifice that comes with unleashing AI to the world. We’re also not unique. Early adopters bear the brunt of inconvenience. People in Victorian times depended on candle lighting and little else about their lives compares to ours now. The Victorians were once afraid, too. They were afraid of many things we are not, like getting help recovering from routine illnesses, giving birth without complication or becoming an outcast by high society for having differing opinions or lifestyles. Victorians were also distressed about installing gas lighting in their homes and rightfully so. Unfortunately, along with progress came great human decrement; fires, explosions and all sorts of breathing issues.
“Gas must have provided a quite stunning improvement to people’s ability to read, write or sew in the evenings with minimal effort. It nevertheless had many drawbacks. There were frequent explosions, and it replaced the oxygen in the air with black and noxious deposits. The aspidistra, a hugely popular plant, became so because it survived well in oxygen-starved conditions. Victorian ladies frequently fainted partly because of tight-lacing, but also because of a lack of oxygen in their gas-lit drawing rooms.” -Lucy Worsely
Image via Wiki Commons
There’s always going to be a personal tariff paid for the advancement of technology. For 20 years I’ve carried a cell phone that the state of California recently saw fit to warn us about the dangers of. I have arthritis in my right hand from too much texting (OK, tweeting) and my eyes have likely suffered from all the screen time, but this is part and parcel of what comes with the betterment for homo sapiens.
Now is the time we as a society get to decide how we want to allow AI to enter our lives. We can decide upfront that AI capable drones will never have authority to kill humans or that it’s illegal for AI robots to raise your children or stalk you or any of the freaky things we’re going to see.
What might creep you out is that ML allows online retail and social media sites to learn about their users on a deeper level. Every tweet, Facebook like, and Amazon wish-list item is filed away on servers that have algorithms sifting through the data to create an online profile for each user, which then allows companies to push specific ads or relevant digital content like Youtube videos. Yelp uses ML to sort massive quantities of user-uploaded pictures into easily consumable categories for users. Pinterest’s ML algorithm learns a user’s preferences and search patterns in order to suggest other images and content that the user would like. Amazon also uses ML to drive sales by suggesting items similar to what a customer has in their search history, previous purchase record, or in their shopping cart.
Another super popular application for ML has been Chatbots, which uses Natural Language Processing (NLP) to learn how to interact and create human-like conversations. Staples teamed up with IBM Watson to transform their Easy Button into an ordering system. With IBM’s Watson Conversation system, NLP, and ML, the Easy System is able to interpret a customer’s request and provide the correct response based on the information available. To an extent, Google’s “RankBrain”, a part of its search engine algorithm, also operates in the same capacity. Google’s search engine takes in a user’s search parameters and sorts through the Internet to return results that match their inquiry. Users demand relevance so search must always find it. The only way that relevance can be achieved dynamically and randomly on-demand is with AI and there’s a fair bit of creep walking down that path, but all we have to do is open our eyes.
Now that we understand how AI and ML can help us, let’s discuss why we feel malaise about AI. Because an unknown entity that makes us afraid is often just a foil for a dangerous ‘other.’ It turns AI into a fear of ourselves, because we are uncertain of where we’ll lead it. We are not condemned to dystopian futures of robo-overlords unless we don’t agree to ethical standards and everyone goes all weird and rogue. Quite a bit of data science is still manual and involves many humans, which means we’re not exactly at scale. The autonomy of machines is greatly oversold in movies and TV. Plus I have to ask overall; why would machines or robots want to hurt us?
We are never going to be slaves to our devices, it’s quite the opposite. AI is not a deity, it’s a constructed foil. We’re afraid of losing power in terms of sentient beings, but I really don’t see that as an issue:
-We’re not as advanced as we think we are. The practice of data science is still largely human and therefore subject to its’ bias.
-Evil intent comes from evil humans, not technology. We should be afraid of the humans behind the scenes, if we must be scared, not the technology itself.
-Fear holds us back. I will not be held back.
Look at the potential benefits, there are too many to count. Brains and faces can be replaced, hearts replaced. Yes we’re stumbling poorly between the light and dark of morality on a public stage and parsing the wash of human emotion over the internet. That’s OK. We’re also seeing a generation who could easily have ten more years of their parents being around because they have tools to get them healthier; step trackers, heart monitors, nutritional support, etc. Diabetics won’t have to remember to prick themselves, their eyes will tell their phone to tell their bracelet to medicate them, etc.
Everyday, ML bridges the gap between science fiction and reality. Its continued evolution promises to bring exciting changes to both online and real-world interaction. It’s only a matter of time before systems like Siri and Google are able to hold a full conversation with users that is nearly indistinguishable from human communication.
When you really care, end your post with a festive cat.
",Artificial Intelligence vs. Machine Learning: The Costs of Innovation,13,ai-vs-machine-learning-the-costs-of-innovation-a46c952be0f9,2018-02-09,2018-02-09 19:31:57,https://medium.com/s/story/ai-vs-machine-learning-the-costs-of-innovation-a46c952be0f9,False,1382,,,,,,,,,,Ethics,ethics,Ethics,7787.0,annebot,Futurist 🤖 O’Reilly Author ⚡️ Growth Scientist (SEO) 💳 Bitcoin Capitalist // Team @CircleClick,e39688e35bc5,annebot,927.0,613.0,20181104
0,,0.0,,2018-05-07,2018-05-07 04:53:43,2018-05-07,2018-05-07 05:00:10,1,False,en,2018-05-07,2018-05-07 05:00:10,4,e5069dcda9a,4.033962264150944,0,0,0,Note: This is speculative fiction. The thoughts expressed here are mere speculations and opinions and are not targeted towards any specific…,5,"The Good Cyborg
Note: This is speculative fiction. The thoughts expressed here are mere speculations and opinions and are not targeted towards any specific groups or individuals.

What is a good cyborg? Consider first, what a cyborg might be. Oxford dictionary defines cyborg as “A fictional or hypothetical person whose physical abilities are extended beyond normal human limitations by mechanical elements built into the body.”
Or to be more accurate, according to Joseph Carvalko (2012), “The term cyborg is not the same thing as bionic, biorobot or android; it applies to an organism that has restored function or enhanced abilities due to the integration of some artificial component or technology that relies on some sort of feedback.”
By this definition, even enhanced cognitive abilities due to integration with an artificial technology should make one cyborg. Elon Musk believes we are already cyborgs (Code Conference, 2016) since we are integrated with and rely on aids such as computers and mobile phones. People with hearing aids might or might not be cyborgs because while their hearing ability is enhanced by a machine, the enhancement only allows them to hear on par with those whose hearing abilities are not limited.
Do cyborgs have to function at a level well above the average human being to be called cyborgs? What makes an average human being then? These questions are central to understanding what makes a good cyborg.
Let’s take the example of proliferation of fake news in the context of humans as cyborgs whose ability to process information is enhanced due to the presence of digital databases (which limits reliance on human memory). We have all the information we need in the palm of our hands. Under these circumstances, would one not call those who fall prey to fake news, failed cyborgs or bad cyborgs?
If this were a physical enhancement (like a mechanical limb that gives super strength) and certain human bodies rejected it (in an imaginary non so distant future in a sci-fi movie), they might be considered failed cyborg. Similarly, it can be argued that humans who are unable to derive the most rationally exhaustive conclusions based on data presented to them and fall prey to fake news are failed cyborgs.
A good cyborg would be able to not only distinguish which information is credible and integrate it to derive sensible conclusions, but they should also have the ability to understand what lends information credibility in the first place. Especially since a lot of information is a product of human perception.
They must also know what sort of information to seek to derive conclusions closest to facts in terms of accuracy. No human is born as an all seeing all sensing being. Unless we take active measures, our minds can be easily deluded. Even our sensory perceptions that we see as the real world, are held in a delicate balance by an interplay of organs and hormones. Set one parameter off and we will loose our grasp on the physical world as we see it.
Furthermore, even in full operational captivity, our sensory system is sub par. We can’t see infra red, can’t hear above and below curtain frequencies, can’t understand anything that isn’t communicated in ways whose meanings we are trained to understand and therefore we can’t understand anything that is not communicated by one of our kind.
Given our limited grasp on reality, how would we use artificial enhancements such as 24/7 access to digital databases in a way that maximizes our information processing abilities? How, furthermore, should we define what constitutes as “good” information processing abilities?
Should it be “good” for the temporary psychological well being of the cyborg in question? Because then the fake news believers with a serious condition of cognitive dissonance might be good cyborgs, the best cyborgs among us.
They can comfortably label anyone disagreeing with them as a part of some larger conspiracy to undermine them. If we have limited grasp on reality anyways, why not do away completely with any comprehension of it when it runs against the world view that we find most comforting? Instead of facing the prospect of there being no God, why not ostracize and silence the dissenters as heretics?
It is only when we must live with the consequences of our actions as a species and not as an individual, that we have to start worrying about the long term collective costs of disconnecting with reality. I would argue that the aforementioned cyborgs are not just disconnected, they actively reject reality.
When we reject inconvenient truths, we loose both the ability to perceive the problems they may pose for our kind, and means to solve those problems. You cannot correct something you cannot even see. When we hold reality hostage to the most convenient of assumptions, that does not change the reality itself. It simply befuddles the very system we rely on for day to day functioning.
In the short term this may not matter. But cumulatively it could spell disaster for us. Closing doors on others will lead them to close doors on us. Closed doors will lead to existing in vacuums that stifle our minds.
When trying to solve the problem of limited resources within our closed doors, we won’t rely on peace and cooperation. That’s what friends do and we don’t have any. Our doors were shut too callously for that. When peace and cooperation are not an option, the only ones left on table are theft and war.
History of course has taught us how disastrous untenable conflicts are. But remember, we are a society of failed cyborgs. If we had mastered the skill of using massive amounts of information for our long term benefit, we would not be here in the first place. Any amount of historical evidence, displayed on our screens in easily digestible formats, will not save us from self destructing.
I suppose then, we can safely say that good cyborgs are cyborgs that do not self destruct, period.
Citation
Carvalko, Joseph (2012). The Techno-human Shell-A Jump in the Evolutionary Gap. Sunbury Press
",The Good Cyborg,0,the-good-cyborg-e5069dcda9a,2018-05-07,2018-05-07 05:00:11,https://medium.com/s/story/the-good-cyborg-e5069dcda9a,False,1016,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Shruti,"Behavioral scientist, perennially fascinated with how technology impacts human interaction, writer of short science fiction stories",1c2dcb443ca9,writer_shruti,1.0,3.0,20181104
0,,0.0,,2017-10-16,2017-10-16 10:50:27,2017-10-16,2017-10-16 11:29:04,6,False,en,2017-10-17,2017-10-17 13:45:37,1,1781752a73e6,6.1575471698113216,2,0,0,"This week we have had some philosophical discussions about the new intelligent beings among us. Yes, they have arrived, and they have been…",5,"Losing your Humanity…

This week we have had some philosophical discussions about the new intelligent beings among us. Yes, they have arrived, and they have been quietly making their mark on people in cities across the globe. Some have quietly taken up jobs in hotels, some have been seen on tv, seminars and conferences. Others have begun making their mark in brothels and even online as sordid temptresses while a few are just as happy to be mail-order brides, literally.
As if we do not have enough problems trying to feed, clothe, employ and manage the 7-odd billion people on the planet, we now have Humanoids to contend with. Jobs are already scarce and robots already seem like a great alternative to those at the top who still manage to squeeze the last drops of blood from their barely-legal slaves. Companies in Japan have employed humanoid robots in hotels and restaurants and it wont be long before others follow suit.
The Big Fear
The big fear is not even about job security. The real concern is how we interact with these objects. Some may pose the question, “Are they objects or are they beings?” Look, I know you can ‘grow up to be anything you want to be’ these days, and there are probably some crazy people trying to grow up to become humanoid bots, but let’s just be real here for a moment, whether they are beings or objects is not the point of the discussion so let’s leave it at that.
Meet Sophia, a Humanoid robot
The point of the discussion is that we are human beings, not bots. We have control of our actions, intentions and have to deal with the repercussions of our behaviours. Humanoid robots are built to look, act, think and one day feel like human beings, but they are programmed to do certain things by someone else. Even if they are intelligent beings with the ability to learn, they were built for a certain purpose. What that purpose is, and how it affects us, we need to question.
Bots are meant to be tortured
I was mortified to read the article from the likes of David Vincent Kimel this morning, asserting on some level that “…humanoids should feel a sense of honour serving humans and allowing them to cathartically actualise their fantasies and nightmares in a controlled and safe environment”. There is this fantasy that the world is too prohibitive and some people are innately depraved so we should be able to shed our inhibitions and do whatever we feel. Westworld is a series that engages this premise and centres around an amusement park intended for rich vacationers, which is looked after by robotic “hosts” and allows its visitors to live out their fantasies through artificial consciousness.
A scene from Westworld
People want to be able to ‘live our their fantasies’ and since robots are not real people, it shouldn’t matter what we do to them. Some people are of the impression that these bot-beings are designed to be our modern day slaves and because we design and build them, we can create them to do anything we want, especially things we aren’t allowed to do with real human-beings. The assertion is that we can use them to allow ourselves to engage in any kind of behaviour without the consequences.
A question of Ethics
The thing is, is the behaviour reprehensible because of its effects on other people or its effects on us. If it is based on the former, then does it mean it is less reprehensible if there is no damage caused to the other person? If the latter, then, does it mean engaging in such acts, whether it is with a humanoid or a real human, it is still something that needs to be condemned because of how it makes us feel or act?
There is this whole argument about humanoids having simulated feelings and reactions that are not real so they should not be taken seriously or considered harmful to them. The point is that the perpetrator of the act is the one experiencing the real feelings. I agree with Kimel when he said, “Ultimately, concerns about playing with mechanical sex-dolls and life-size robotic action figures shouldn’t focus on the psychological harms suffered by the toys and their likelihood of starting to attack us.” The concern should focus on the psychological harms suffered by the person engaging in the behaviour, and their likelihood of attacking us.
How does allowing a pedophile or a serial killer (or other more twisted folk) indulge in their sociopathic behaviour add any value to their human experience? It only reinforces their drive and addiction to deviant behaviours. A person who pulls the heads off cats for fun in their spare time does not ‘get relief’ and then acts normal in society because we let him pull the heads off cats when nobody is looking. It only allows them to graduate to more sadistic forms of behaviour, with more sophisticated victims because the action of engaging in such a behaviour reinforced it.
Remember that poor sex robot, Samantha, that needed repairs after being repeatedly molested while on display at a recent tech fair? Nobody was concerned about Samantha, and even I am more concerned about what kind of people think it is ok to exhibit such lewd behaviour and get away with it. Sergi Santos, the doll’s developer iterated, “People can be bad. Because they did not understand the technology and did not have to pay for it, they treated the doll like barbarians.”
Sergi Santos and “Samantha.”
Would those people behave in such a lewd manner in any other circumstance? Did the availability of the technology and the visual appeal of the humanoid, coupled with the fact that this was actually a sexual object with no repercussions elicit such behaviour? Would they do it again? Would it change the way they see other women in public? Would they feel more confident about attacking a real woman in public like they did the doll? Would it give a molester enough practice to do it to a real person? Would it create more outlandish behaviour with a don’t care attitude? Does having this outlet affect how people behave in real life?
Lower your gaze?
I posed the question to my colleagues, “Does a Muslim man have to lower his gaze if he sees a female robot?” Depending on the sophistication of the humanoid robot, it is very hard to tell them apart from real humans at first glance. It makes the experience more real. It makes you think and feel as if you are doing something to a real person. That’s just the problem. We are reinforcing negative behaviours. We are allowing people to become more depraved and sadistic by making it ethical to be unethical.
ChihiraAico, a lifelike android robot built by electronics manufacturer Toshiba, is able to talk, sing, gesture and cry like a 32-year-old Japanese woman.
The Islamic perspective is simple. It’s not about whether it is a robot or a human. You, as a human being, are answerable for your behaviour and your own intentions. If you think that engaging in something will lead to fitna (strife, distress, temptation, affliction), stay away from it. So yes, the answer was, “…he should lower his gaze”. The wisdom behind it is that you are a spiritual being, and should rise above the base desires of the lower self. If you feel that looking at something or someone, whether it is real or perceived, human or humanoid, will cause you to behave in a way that is contrary to what your are striving for as a spiritual being, then avoid the temptation.
It is not liberating to become a savage, debased human being, with no sense of morality or character. It will not benefit you to engage in barbaric acts of debauchery, regardless of the nature of the victim, so do not be fooled into thinking that the lack of consciousness in the bot will cover up your lack of conscience in real life.

Is that what the creators of these humanoids want? A world filled with depraved humanoids who want to be human and humans who have lost their humanity and just become slaves to their addictions? You may or may not be legally answerable for defiling a robot in real life, but we are still answerable for our intentions and our characters.
",Losing your Humanity…,31,losing-your-humanity-1781752a73e6,2018-04-11,2018-04-11 14:16:04,https://medium.com/s/story/losing-your-humanity-1781752a73e6,False,1380,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Zahara Cassim,People & Impact,dcff9ac51301,cassim_13433,20.0,31.0,20181104
0,,0.0,,2017-12-11,2017-12-11 07:00:33,2018-01-18,2018-01-18 18:15:58,0,False,en,2018-01-24,2018-01-24 22:21:09,7,9cab0999effe,5.833962264150943,2,5,0,"With Data Science at the forefront of more and more decision-making processes everywhere, questions arise regarding the application of data…",3,"Ethics in Data Science
With Data Science at the forefront of more and more decision-making processes everywhere, questions arise regarding the application of data driven methods to problems with a high degree of social impact. This past quarter, I attended the Hurtling Down the Data Highway seminar, where we talked about the unintended consequences that a data science application might have.
One of the sessions was led by Lori Sherer, partner in Bain & Company’s San Francisco office. With her background in the Financial Services industry, Lori’s talk was full examples which were very relatable to my recent job at a Mexican FinTech startup.
Why data ethics? Why now?
Lori opened her talk with this question and raised several relevant points. We need data ethics because we want corporations to be socially responsible. At this particular point in time there is also a high consumer demand for data ethics. We must understand the human and social elements that end up being affected by the result of a data driven technique. We now have access to more data than ever before and from it we are able to generate insights so deep that I even sometimes find it creepy.
Some examples we talked about centered around using data analytics for risk mitigation. For example, Uber implemented its ranking system to determine wether a passenger is safe or not. Financial institutions run credit checks on individuals who apply for a lease, loan or mortgage to determine wether or not will they be paid back. Lastly, we talked about more controversial examples, like algorithms that determine wether or not a person should be granted bail or not based on the likelihood that they will commit a crime.
On previous sessions during the seminar we had also touched on this topic. Angèle Cristin mentioned this when she critiqued the existing risk assessment tools used for criminal justice. Sharad Goel talked about racial bias arising in algorithms to predict pre-trial detention. Is it that the algorithm itself is biased? Is the way it is being used causing the bias? Or is the bias inherently present in the data that the model was trained on? With correlations between the features that an algorithm takes into account, the answer is not immediate. This is where Lori makes a good point. We, as data scientists should be ethical and critical enough to come up with a solution that minimized or eradicates these unexpected and undesired results in our algorithms.
A reference for ethics
Lori mentioned that the IEEE points to Philosophy for a definition of ethics. But that is a little bit vague, don’t you think? Don’t get me wrong, I love Philosophy, but I think discipline-specific pointers are immensely useful. Not everybody has the ability to assess wether the results of their job might have an unethical consequence beyond what is immediate.
About a year ago the giants in the tech industry came together to found the non-profit Partnership on AI. Their goals are ample but in general they seek to further the development of artificial intelligence while making it accessible to people. They seek transparency and they want for the public to understand the consequences and the costs of current and future AI technologies. They aim to bring together people from all disciplines to discuss the current issues in AI and its influences on people and society.
The Partnership defines 7 thematic pillars:
Safety-Critical AI
Fair, transparent and accountable AI
Collaborations between people and AI systems
AI, Labor and the Economy
Social and Societal Influences of AI
AI and Social Good
Special Initiatives
This talk centered on the second and fifth pillars of the partnership. They are well-founded and provide a very solid foundation on which we should build upon with all our future solutions. Strangely enough though, I had not really heard about this initiative until this talk. A quick dive through their page showed sparse entries in their news blog (5 from September 2016 to date). It seems to me, that this is perhaps not making as much noise as it should! So what should be the reference for ethics when talking about data science? Truth is, the world today lacks one.
So what makes an ethical use of data?
Lori brought up the creation of the credit score several years ago. The idea was to create a model to estimate the probability that a given person would repay a loan. What variables should be taken into account? There needed to be regulation in place to avoid the use of certain “unfair” or biased variables. Eventually a model that is still widely used today came together but not without conscientious selection of its features. But do we really know how it works? Credit scoring is a bit obscure to most people.
Lori then talked to us about Trooly, a startup that was recently acquired by Airbnb. Troo.ly sought to compute a trustworthiness score on individuals. They performed quick background checks on people using a person’s digital footprint. Through a set of internet and dark web crawlers they run an algorithm that checks whether the input information is authentic, looks for anti-social behavioral patterns and predicts these types of patterns.
Clearly, this raises several relevante ethical implications, but it seems that Troo.ly was on the right track (or so I understood):
They defined what data was permisible. Although they were able to crawl the web and find information that could potentially be used to evaluate the trustworthiness of an individual, they hand picked features they could legally evaluate and were fair game.
They built a 100% transparent model. Apparently, consumer could understand how their trustworthiness score was constructed. This allows for people to identify and correct potential mistakes in their evaluation.
They kept their data and their models on a very secure infrastructure. When recovering, aggregating and handling such an amount of sensitive information on a person, its security must be guaranteed. Ethical use of the data is no longer guaranteed if it falls into the wrong hands.
They took data quality seriously. They were aware that 1 in 5 people had traits that would make them look riskier because of errors or inaccurate data. Their model took this into account.
It is encouraging to see some companies moving in the right direction!
You can also read more on Troo.ly on their Medium publication.
A framework for self-regulation
Towards the end of the seminar, Lori led the conversation into the need to build an oath for data scientists addressing the ethical use of data, algorithms and insights a data scientist has been able to gather. Similar efforts are being made in this direction, like Bloomberg, BrightHive and Data For Democracy’s Community Principles on Ethical Data Sharing. Yet, to the best of my knowledge, there doesn’t exist a universal authority to which people point to when talking about data ethics. Lori then raised the following questions:
What are the consequences of your work as a data scientist?
Where do you place value?
What are your ethical obligations in sourcing and building training data?
How do you set your standards for data security, accuracy and transparency?
What is your responsibility to society for your work?
Aiming to answer the above questions, she proposed an oath for data scientists to bring ethical considerations to the forefront when performing their daily work.
The Data Scientist’s Oath
As a Data Scientist I understand that my work has material consequences on individuals and their ability to function within a society.
I would place the individual’s privacy over the model’s performance.
I have a responsibility to make sure data subjects are educated on how data is being used and allow them to rectify incorrect data.
I have an ethical imperative to ensure that the data we use for decision is accurate.
I pledge to invest the time to educate others about the unintended consequences of this kind of work that may cause social harm.
Final thoughts
It is clear that as data scientists we are the bridge between the massive amounts of data available and the actions taken by us or third parties based on the insights we extract. A framework for self-regulation is of outmost importance. Yet, I believe there is another side of the issue that also needs to be addressed.
Returning to the example of a customer’s financial data, we commented on the fact that in the US, the consumer is responsible for the accuracy of their credit bureau data. With the rise in importance of credit and trustworthiness scores worldwide (see, for example China’s Social Credit System), the data fed into the scoring algorithms can severely impact how an individual functions within a society. Should an individual be responsible for seeking that data collected about them be accurate? Bear in mind that more often than not this data collection happens without them being fully aware about it although they may have agreed to it through the acceptance of obscure terms and conditions.
How could we, then, go about aligning the incentives for data collecting entities to strive for the collection of accurate data to begin with? How can we push towards regulation that makes the reporting agencies responsible for the accuracy of the consumer’s data?
",Ethics in Data Science,4,ethics-in-data-science-9cab0999effe,2018-01-27,2018-01-27 01:02:32,https://medium.com/s/story/ethics-in-data-science-9cab0999effe,False,1546,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Javier Sagastuy,"Computer engineer, applied mathematician, world traveler, skier, former #NexuDev Scrum Master",9fb16c7192ae,jvrsgsty,8.0,15.0,20181104
0,,0.0,,2018-07-23,2018-07-23 14:57:01,2018-07-23,2018-07-23 15:15:26,1,True,en,2018-07-23,2018-07-23 15:15:26,1,13423a294160,4.381132075471698,15,0,0,(this blog was originally written for the European AI Alliance; feel free to join this platform dedicated to European citizens for…,4,"What we talk about when we talk about Artificial Intelligence

(this blog was originally written for the European AI Alliance; feel free to join this platform dedicated to European citizens for discussing EU policies on AI)
Nowadays, Artificial Intelligence (AI) is ubiquitous. We can hardly open a newspaper or tune in to a news show without getting some story about AI. AI is probably the technology most talked about. But AI means different things to different people.
I’ve been working on the field of AI, both in industry as in academia since the late 80’s. Developed my first AI system in 86, an expert system to determine eligibility to social housing. Since then I’ve witnessed the deeps and the ups, the winters and the hypes in the field. Never before there has been this level of excitement, and fear, by so many, in so many areas, as we are seeing in the last couple of years. AI is breaking through in many different application domains, with results that impress even the most knowledgeable experts. Three main factors are leading this development: the increasing availability of large amounts of data, improved algorithms and substantial computational power. However, of these three only algorithms can be rightfully seen as a contribution from the AI field.
More recently, awareness that AI has the potential to impact our lives and our world has no other technology has done before, is rightfully raising many questions concerning its ethical, legal, societal and economical effects. Government, enterprises and social organisations alike are coming forward with proposals and declarations of their commitment to an accountable, responsible, transparent approach to AI, where human values and ethical principles are leading. This is a very needed development, one to which I’ve dedicated my research in the last few years. Responsibility in AI begins with a proper AI narrative, which demystifies the possibilities and the processes of AI technologies and enables that all are able to participate in the discussion on the role of AI in society. In this short piece, I will try to explain what AI is, starting by describing what it is not.
AI is not the Algorithm
The “algorithm” is achieving magical proportions, used right and left to signify many things, de facto embodying, or seen as a synonym to, the whole of AI. AI has been around for give or take some 80 years, but algorithms are way older than that[1]. AI uses algorithms, but then so does any other computer program or engineering process. Algorithms are far from magic. In fact, the easiest definition of algorithm is that of a recipe, a set of precise rules to achieve a certain result. Every time, you add two numbers, you are using an algorithm, as well as when you are baking an apple pie. And, by itself a recipe has never turned into an apple pie. The end result of your pie has more to do with your baking skills and choice of ingredients. The same applies to AI algorithms: for a large part the result depends on its input data, and the ability of those that trained it. And, as we have the choice to use organic apples to make our pie, in AI we also have the choice to use data that respects and ensures fairness, privacy, transparency and all other values we hold dear. This is what Responsible AI is about, and includes demanding the same requirements from the ones that develop the systems that affect us.
AI is not Machine Learning
Machine Learning, and in particular Neural Networks, or Deep Learning, is a subset of AI that uses statistical techniques to enable computers to perceivesome characteristics of their environment. Current techniques are particularly efficient in perceiving images, and written or spoken text. By analysing many thousands of examples (typically a few million), the system is able to identity commonalities in these examples, which then enable it to interpret data that it has never seen before, which is often referred to as prediction. Even though, the results of current machine learning algorithms are impressive and go well beyond expectations, also this process is far from magic, but the result of applying well-known mathematical and statistical methods. Moreover, current algorithms are brittle (changing only one pixel in a picture can cause completely different classification), not easily generalisable (training has to start from scratch every time the machine should learn another task), and perception is just one component of intelligence, more is needed to achieve intelligent machines.
A short definition of AI
AI includes Machine Learning and is based on algorithms. However, the ultimate goal of AI is to develop computer systems that are able to simulate human-like intelligence. The term Artificial Intelligence was coined in the 50’s by John McCarthy, who defined it as the endeavour to develop a machine that could reason like a human, was capable of abstract thought, problem-solving and self-improvement. The challenge proved much harder than what those original scientists expected, and even current success of AI, in the area of Machine Learning, are very far from realising those objectives. The aim of this piece is not to speculate on the feasibility, or not, of this endeavour, but to provide a concise, practical definition. More than perception, AI is about reasoning. Besides machine learning, AI includes knowledge representation, planning, dealing with uncertainty, theorem proving, cognitive robotics and human-agent/robot interaction just to mention a few on the fields.
Borrowing from the definition given in the seminal textbook on AI[2], I would say that AI is the discipline of developing computer systems that are able of perceiving its environment, and to deliberate how to best act on it in order to achieve its own goals, assuming that the environment contains other agents similar to itself. As such, AI is about autonomy to decide on how to act, adaptability to learn from the changes affected in the environment, and inter-actability to be sensitive to the actions and aims of other agents in that environment, and decide when to cooperate or to compete.
A responsible, ethical, approach to AI will ensure transparency about how adaptation is done, responsibility on the level of automation on which the system is able to reason, and accountability about the principles that lead its interactions with others, most importantly with people.
Notes
[1] The word algorithm derives from al-Ḵwārizmī ‘the man of Ḵwārizm’ (now Khiva), the name given to the 9th-century mathematician Abū Ja‘far Muhammad ibn Mūsa, author of widely translated works on algebra and arithmetic. (source Wikipedia)
[2] Russell and Norvig (2009): Artificial Intelligence: A Modern Approach, 3rd edition. Pearson Education.
",What we talk about when we talk about Artificial Intelligence,69,what-we-talk-about-when-we-talk-about-artificial-intelligence-13423a294160,2018-07-23,2018-07-23 22:23:11,https://medium.com/s/story/what-we-talk-about-when-we-talk-about-artificial-intelligence-13423a294160,False,1108,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Virginia Dignum,,fb01d0a3bc3f,virginiadignum,72.0,11.0,20181104
0,,0.0,,2017-11-02,2017-11-02 15:06:14,2017-11-02,2017-11-02 22:34:52,3,False,en,2018-01-18,2018-01-18 17:15:11,17,f6ae134458fc,9.629245283018866,7,0,0,Help CivilServant develop ways to inform people about their participation in online research and hold us accountable,5,"3 Strategies for Accountable, Ethical Online Behavior Research
Help CivilServant develop ways to inform people about their participation in online research and hold us accountable
In 2014, after researchers worked with Facebook to test the effect of newsfeed adjustments on the emotional tone of people’s future posts, academics took a closer look at the ethics of online behavioral research, in the midst of a wider public debate over the power of online platforms in society.
Two ideas were central to these conversations: consent and debriefing. In consent-based models of research, people are asked in advance if they are willing to participate in the study. Individual consent often works best under controlled, lab-style studies or surveys and interviews, where it’s easy to decide which people are part of a study and which people aren’t. Debriefing is a process where people are told after the study. Debriefing is also a way to identify any unexpected, harmful effects that the researchers weren’t looking out for, so the harms can be addressed.
Even superheroes have procedural constraints on their work. Wonder Woman chairs a meeting of the Hall of Justice in JLA# 4, May 1961
In field research, which tests ideas out in the world, individual consent and debriefing can be hard to acquire. For example, consider this study that tested the effect of lawn signs on voter participation rates. It wouldn’t be possible to obtain the advance consent of every single driver who passed by the signs; it would be impossible to predict exactly who would drive by. Even if you could obtain consent, you wouldn’t be able to show or hide the sign for people who hadn’t consented to the study. Likewise with debriefing: a researcher might be able to place a camera next to every sign in order to figure out the license plate, identity, and address of everyone who passed by, but in the effort to contact everyone in the study about ethics, the ethics procedure might become more risky and intrusive than the original study.
Empirical Ethics
How can we make progress on these research design challenges? While new ideas in law and ethics can clarify general principles, many of our debates rely on assumptions about design and about how people are likely respond to research.
when people talk about research ethics, they’re often talking about questions of power
In recent years, a growing number of scholars have started to prototype new ways to manage research ethics to see how well those approaches work. One of those people is Scott Desposato, who recently conducted a series of nationally-representative surveys asking Americans what they thought about the ethics of common approaches to public service announcement research (video). Desposato compared academics’ views on research ethics with residents’ views for studies about flossing, voter participation and DUI. Surprisingly, while everyone was less comfortable with research that didn’t ask for consent, more residents were comfortable with those studies than academics.
In research by Scott Desposato, residents were more comfortable with research that didn’t ask for their consent than academics were. But everyone preferred consent-based research. Local review is one possible solution.
In Scott’s study and his other writings on solutions to ethical challenges, he points out that when people talk about research ethics, they’re often talking about questions of power–something that danah boyd has also argued.
Individual consent is one way for people to have some power around research; holding researchers accountable to local community review or elected representatives might be another. A researcher might not be able to reach every single person who’s part of a study, but the researcher might still be held accountable to the community that hosts a study. That’s what I do with CivilServant studies on reddit, which I do at the invitation of a community’s volunteer moderators, and which always include community-wide debriefings one the study is over.
Reading Scott’s work (and his edited collection on experiment ethics) I have been inspired to use the tools of design and research to improve on research ethics, finding creative solutions that hold our research accountable to the public while also continuing to grow public knowledge on important issues.
And that brings me to a practical question that I’m facing right now, together with my collaborators Merry Mou, Jonathon Penney, and Jonathan Zong.
Debriefing & Consent in Research Mitigating Negative Effects of AI Copyright Enforcement
Every day, AI-based law enforcement affects thousands of people when corporate-run machine learning systems guess if the things we say online have been copyrighted, and whether they should be taken down. These AI copyright enforcement regimes very likely have a chilling effect on people’s exercise of their speech rights when they withdraw from social media after receiving scary legal documents about their online postings.
In a study with Merry Mou and Jon Penney, we want to test ways to mitigate this chilling effect and help people flourish online even after an encounter with AI copyright enforcement. We believe that providing people with accurate information about the law could prevent them from becoming overly fearful and withdrawing from social media. We can reach people using information from the Lumen database, a public archive of every copyright takedown notice that publishes data on copyright enforcement in near-realtime. But before applying this idea everywhere, we want to be sure it’s actually helping. That’s why Jonathan Zong and I are planning to do an experiment to test our assumptions.
Of course we’re not going into this project blind. Our research is based on years of conversations and surveys by Jon Penney with people who have faced copyright takedowns. But there’s a catch: because we don’t know in advance who is going to receive copyright takedown notices, we can’t ask the people in this experiment for consent in advance. That leaves us with four options overall:
Don’t try to help people who experience this problem
Offer people support and information, but don’t ask if it’s actually helping them
Test the idea without informing people they’re part of an experiment
Design some novel kind of ethics process and accountability into this research, such as debriefing
The fourth option is the empirical ethics approach, one where we imagine, prototype, and test creative designs for ensuring the accountability of our research.
3 Strategies for Redesigning Consent and Accountability In Online Behavioral Research
Here are some of the strategies we are considering, and actually hope to test before settling on a process for this and future CivilServant studies on Twitter. I would love to see your feedback in the comments.
Representative Review
In online research, it’s sometimes possible to do things like “Community IRB,” where community representatives of participants review and approve a study before it runs. Community review is a basic requirement for CivilServant research on reddit, where communities have clear, if porous boundaries and leadership. Desposato writes about this idea in his research on local review of political science experiments. On Twitter, with a study that responds to people at their moment of need, we have no leaders to appeal to.
Arnstein’s Ladder of Citizen Participation is a helpful scale for thinking about the power that participants have in a given research ethics and accountability process.
We may still be able to find a group of people to ensure that we balance the risks and benefits of similar research on Twitter. For our DMCA study, we could always reach out to people whose content was taken down in the past and ask them their views about our study design. We could even work with a high-quality, representative sample of those people, ensuring that if copyright enforcement continues in similar ways during our study, the people who participate in our experiment will be statistically similar to the people who we reached out to for feedback.
Once we find those people, we could potentially ask them to play any number of roles, including:
Offering feedback on the risks of the study design, including where we do and do not intervene
Discussing the study design with each other
Voting on study designs
Voting on whether they think someone like them should be included in or excluded from the experiment, especially if we have a way to automatically identify accounts like theirs
Voting on whether we should halt or continue the experiment if we do experience any serious complaints
Oversight of this kind takes time and energy from participants, and involvement could become prohibitively time consuming. Furthermore, votes have numerous problems. Majority votes don’t always protect minorities, and higher thresholds can hold the process hostage to minorities. Creative design may be able to overcome these issues. For example, we could subdivide the sample and ask people to vote on whether their subgroup should be included in a study.
Because it takes time to participate in review of this kind, we would need to find ways to make the review process efficient and also ensure that people without the training or time will be able to hold our work accountable.
Imagine now that we decide to test the effects of 50 different messages for mitigating the chilling effect– we can’t reasonably expect people to vote on each message. Instead, we might ask people to review kinds of experiment and then provide audits of how well we kept within the agreed parameters.
Automated Debriefing
Because we’re doing internet research, we have the ability to contact people essentially for free. While we can’t be sure that each person will receive or pay attention to the message (the same is true for any field experiment), we can technically carry out study debriefings at a much larger scale than ever before.
Imagine, for example, that we were to send a tweet to everyone who was part of the study telling them they were in the research and inviting them to click a link to learn more about it. We could require people to log in to the debriefing website and provide them with personalized information about exactly how the research worked, what we collected, and asking them to tell us about any harmful effects they might have experienced.
When I pose the idea of automated debriefing to researchers, they immediately start thinking about the problems with it: it won’t reach everyone, people won’t understand the debriefing materials, and maybe other people will eavesdrop on the message we send when debriefing them. Weirdly, people usually bring up these issues as an argument for keeping people in the dark about the research.
Fortunately, all of these objections rely on assumptions about people’s experience that can be tested empirically. How many people will learn about the research? How many will click through? How many people eavesdrop on our debriefing recruitment? Can we obfuscate the risk of eavesdropping by sending information about the research to a wider sample than those who participated, and only revealing the information privately once they click? These are all design questions that we can test.
Post-Study Opt In and Out
While debriefing processes inform people about the research, opt-out processes invite people to make a choice about how they want their information to be associated with the research going forward. There are many options:
Choose how you wish to being mentioned by name in published research (CivilServant never names anyone unless they explicitly request to be named)
Choose if your information will be anonymized or obfuscated in the data accessed by researchers
Choose if your information will be anonymized or obfuscated in any datasets made public by researchers
Choose if your information will be included in the aggregate results of the study in any way
Provide the researchers with information that might lead your information to be automatically deleted, for legal or ethics compliance reasons (such as information that you are a minor, for studies that haven’t been approved to include minors)
Choices by participants can be hard for researchers to accept, especially if they affect the quality of the data: if enough people of the same kind choose to have their information removed, the researchers might come to an inaccurate conclusion. For example, imagine that people from a vulnerable group experience negative outcomes and decide to withdraw their data from the study. If the harms were limited to that group, the statistical models might conclude that the idea was beneficial. Without that information, decision-makers might take future actions that reinforce those harms, precisely because people decided to prevent the harms from being shared.
Designing understandable, ethical debriefing and opt-out interfaces is central need for publicly-accountable research
When offering people choices about how to participate, researchers have a duty to explain the full implications of those choices for the research and any decisions that might be made in response to the research. That itself can be a difficult challenge in cases where participants aren’t familiar with statistical methods. Designing understandable, ethical debriefing and opt-out interfaces is central for publicly-accountable research, since we need to know how well people understand the choices we offer them.
In opt-out processes, as with representative review, we might also imagine designing procedures that allow us to infer the preferences of people who were part of the study and apply them evenly across everyone who was in the study. For example, if 30% of people who clicked ask us to delete their information, should we do that only for them, or 30% of everyone in the study? There might be a good case for treating those who responded as a representative group, if they are actually statistically-representative.
Research ethics become more complicated for research that looks at communities in conflict, as Brian Keegan and I have discussed elsewhere. For example, if a study about online harassment offered every participant equal opportunity to opt out, then a dedicated group of harassers could skew or sabotage results by deciding to opt out en-masse. At the same time, I strongly believe that every person in social research, including alleged harassers, has common rights and deserves a meaningful voice in how that research is conducted. Getting the balance right is far from easy, but the path of transparency has never failed CivilServant so far. I am often moved by the thank-you letters I get from distrusted parties when I respect what I consider to be their fundamental rights to autonomy and privacy.
Testing Research Ethics Procedures
Throughout the academic year, Jonathan Zong and I will be testing some of these research ethics procedures with Twitter users. We’ll be asking people to imagine they were being contacted about a hypothetical study, and then we’ll ask them to work through different webpages and social process for accountable research. We’ll ask for their consent to record their ideas and responses, and report the outcomes of those different ideas.
We still have many, many questions, so we eagerly await your responses to this blog post and your ideas for collaboration.
","3 Strategies for Accountable, Ethical Online Behavior Research",24,3-strategies-for-accountable-ethical-online-behavioral-research-f6ae134458fc,2018-04-06,2018-04-06 20:38:07,https://medium.com/s/story/3-strategies-for-accountable-ethical-online-behavioral-research-f6ae134458fc,False,2406,,,,,,,,,,Ethics,ethics,Ethics,7787.0,J. Nathan Matias,"Public-interest research for a fairer, safer, understanding Internet. CivilServant, @PsychPrinceton @PrincetonCITP @medialab Prev @BKCharvard @CivicMIT",61f90df70e11,natematias,2757.0,1478.0,20181104
0,,0.0,,2017-08-31,2017-08-31 22:37:34,2017-10-02,2017-10-02 12:58:38,0,False,en,2017-10-02,2017-10-02 12:58:38,3,1bb30f175d49,2.2792452830188683,5,0,0,"Data is everywhere. Now more than ever, the internet has exploded with almost inconceivable amounts of data just waiting to be mined by the…",5,"An Intro into Ethical Issues of Data Science
Data is everywhere. Now more than ever, the internet has exploded with almost inconceivable amounts of data just waiting to be mined by the eager researcher. The industry is continually discovering new and innovative ways to apply this data from music recommendations to disease prediction. As all of these applications evolve, the time is ripe for a consideration of various ethical problems that are sometimes subtly presented not only to the researcher but also other interested parties. In this short discussion, four of the many ethical issues related to data science will be presented as based on the article presented by Michael Fuller this year entitled Big Data, Ethics and Religion: New Questions from a New Science.
First, any use of data requires assumed or explicit consent. So often, though, users see this consent in the form of a privacy policy or EULA that is not only incredibly long but also just as confusing to understand with the legal terminology and release of rights. These documents are far too often focused on the mitigation of liability rather than genuinely explaining what can or will be done with the user’s data. This ethical inconsistency of informing yet not actually informing the user could be solved with readable explanations as well as more granularity of consent rather than a single agreement to the entire lengthy document. Far too often, the disciplines of law and software become isolated from each other such that neither can coordinate enough to care for their customers.
Second, an ethical contention exists between data-based intelligence and individual privacy. As Fuller presents, aggregation and analysis of individual medical information can be extremely useful particularly for future research and development of new treatments. Additionally, an individual can be easily identifiable by only three traits: gender, zip code, and year of birth. Understanding this fine balance will be key to the necessary discussion and disclosure of what data can be used and how it may be applied. The solution here, unfortunately, will not be an easy one.
Third, ownership and rights to share data pose a significant ethical question. Is identifiable personal data always personal or can it always be shared as it is now by data brokers? Some countries have actually mandated web-based services to keep surveillance data on their users for government access. Like other dilemmas, these data “rights” must be responsibly disclosed and open for discussion. Individual privacy has to be balanced with security concerns particularly at the civil level.
Fourth and perhaps most subtle, research bias can creep in perhaps even more than in typical scientific journals. Who performs the data? What data is cleaned from the set? What metrics are used for analysis? Does the presentation of results accurately portray the information? As Fuller recognizes, bias, either intentional or not, can appear in data science at nearly every step of the process. Although bias will always be present, peer review and disclosure of methodologies as applied by other scientific disciplines can assist in keeping these decisions accountable and more ethically sound.
In short, data science is inseparable from ethics. The Bible says that money is not evil but rather the love of money is the root of all evil (1 Timothy 6:10). Likewise, data is not the problem; every concern revolves around how it is used. Open discussion and responsible transparency will be the major weapons in this battle of data control and regulation.
What are your concerns with Big Data? What do you think about cloud-based computing or data brokering? Add to the discussion by leaving a comment.
",An Intro into Ethical Issues of Data Science,11,an-intro-into-ethical-issues-of-data-science-1bb30f175d49,2018-02-01,2018-02-01 04:16:00,https://medium.com/s/story/an-intro-into-ethical-issues-of-data-science-1bb30f175d49,False,604,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Christian Di Lorenzo,Web/iOS software engineer @RoleModelSoftware. Data Science grad student dedicated to quality results. My mission is to “glorify God and enjoy Him forever.”,c6a4bcd77899,rcdilorenzo,27.0,4.0,20181104
0,,0.0,3a8144eabfe3,2017-09-08,2017-09-08 13:38:53,2017-11-22,2017-11-22 11:23:07,1,False,en,2017-11-28,2017-11-28 16:59:44,3,e1a4348b6441,3.486792452830189,55,0,0,"According to Stanford University, you can predict someone’s sexual preference from a photograph of their face. The study analyzed 35,000…",5,"7 effortless ways to avoid an AI disaster

According to Stanford University, you can predict someone’s sexual preference from a photograph of their face. The study analyzed 35,000 images and built an algorithm that could predict your sexual orientation.
This knowledge exists now, and it can’t be withdrawn from the zeitgeist. Around the world, people are trying to figure out how to use this information for their own benefit. There are marketing professionals trying to figure out how to apply this in their bid to sell more of what they’re selling. There are insurance companies trying to figure out if they can use this as a datapoint when setting your premiums. There are healthcare companies trying to figure out if they can use this to tailor care specifically for individuals.
But there are also hate groups, trying to figure out how they can take away people’s rights using this (perhaps undisclosed) information. The question is not whether we can do this, it’s whether we should do this. Where do we draw the line?
It raises so many questions, but one stands out: Is this ethical?
In artificial intelligence, we stereotype data. We build a model which finds important characteristics from our training data and then looks for similar indicators when running new data through the model. The downside is that our model is open to bias.
In the USA, courts use artificial intelligence to determine if a defendant is eligible for bail. Unfortunately, the model is biased as it’s been trained with more examples of black defendants becoming repeat offenders than white defendants. If our training data already contains bias, then our model will exhibit similar bias. We can’t quiz the model about its beliefs, in the same way that we can interview a biased judge to expose their bigotry.
This gives us a lot of power to affect people’s lives if applied improperly.
However, it isn’t doom and gloom. There are really useful, beneficial applications of AI. We should take advantage of our data if it’s going to help everyone. Self-driving cars will make our roads safer. Credit card fraud detection keeps the system fair for everyone. Customer service chatbots let companies talk to far more people than would otherwise be possible.
We need to be mindful that we apply our skills for good, not selfish reasons.
We had a similar issue with doctors, afraid that giving them the power to affect our lives would prove harmful. Still, we trust that they’ll help us get better. Although there are legislative methods of enforcing professional standards, they also swear an oath that they’ll share knowledge, act in patients’ best interests, seek help if required, and respect their patients’ privacy. Every doctor starts with this proactive mindset. The oath informs every decision they make, ensuring that they make the best decisions.
It’s time that AI practitioners are instilled with the same sense of propriety. I propose the following oath, and I personally pledge to uphold it from this day.
It’s written to encourage us to do the right thing, even if it’s not the most profitable thing. It encourages learning and teaching. It encourages responsible handling of data. It’s as true to the original Hippocratic Oath as possible, but adapted for the unique challenges of Artificial Intelligence.
I want everyone who builds or deploys AI to read the oath and take a moment to consider its implications. If it makes sense, you can swear the oath too and then bear it in mind as you continue to practice.
I swear to fulfil, to the best of my ability and judgment, this covenant:
I will practice for the benefit of all mankind, disregarding selfish interests.
I will respect the hard-won scientific gains of those in whose steps I walk, and gladly share such knowledge as is mine with those who are to follow.
I will not be ashamed to say “I do not know,” nor will I fail to call in my colleagues when the skills of another are needed.
I will respect the privacy of my users, for their information is not disclosed to me that the world may know.
I will remember that I do not just handle data, but information about human beings, which may affect the person and their family. My responsibility includes these related problems.
I will remember that there is an art to AI as well as science, and that I must remain wary of the context and consequence of projects, notably in their technical, economic and social aspects.
I will remember that I remain a member of society, with special obligations to my fellow human beings.
If I do not violate this oath, may I enjoy life and art, respected while I live and remembered with affection thereafter. May I always act so as to preserve the finest traditions of my calling and may I long experience the joy of improving the lives of all mankind.
I’m a software developer based in Birmingham, UK, solving big data and machine learning problems. I work for a health tech startup, finding creative ways to extract value from our customer data. I don’t have all the answers but I’m learning on the job. Find me on Twitter.
",7 effortless ways to avoid an AI disaster,456,7-effortless-ways-to-avoid-an-ai-disaster-e1a4348b6441,2018-04-10,2018-04-10 15:28:08,https://hackernoon.com/7-effortless-ways-to-avoid-an-ai-disaster-e1a4348b6441,False,871,how hackers start their afternoons.,hackernoon.com,hackernoon,,Hacker Noon,stories@amipublications.com,hacker-daily,"HACKATHONS,HACKING,PROGRAMMING,TECH,HACKER",hackernoon,Ethics,ethics,Ethics,7787.0,Josh Sephton,Software Engineer,e89e1e69f1,heldtogether,317.0,270.0,20181104
0,,0.0,3a8144eabfe3,2018-04-04,2018-04-04 03:49:19,2018-04-06,2018-04-06 14:44:02,5,False,en,2018-04-07,2018-04-07 10:18:01,10,604772a233a,5.874842767295598,7,0,0,“You ought to put on a coat!”,5,"Source: pixabay
Why an AI Ought to Put On a Coat
“You ought to put on a coat!”
“Why?”
“Because it is snowing outside”
“Why does the fact it is snowing mean that I should put on a coat?
“Well, the fact that it is snowing means that it is cold”
“And, why does the fact it is cold mean that I should put on a coat?”
“If it is cold, and you go outside without a coat, you will be cold”
“Should I not be cold?”
“If you get too cold, you will freeze to death”
“So you’re saying I should not freeze to death?” [1]
To prove a point, you need to agree over some common truths. To convince someone to “put on a coat”, an elementary premise would be that “freezing to death by not putting on a coat” is rather silly.
An Artificial Intelligence does not necessarily believe in immovable truths.
However, I believe that an Artificial Intelligence, designed by humans, must believe in some kind of statements like “You ought to put on a coat”.
Preliminaries
First of all, let’s begin by defining some useful vocabulary, and in particular define two kinds of statements:
“Is-statements: facts, how the world is, will be, was in the past or how it would be in hypothetical situations
Ought-statements: normative statements about how the world should be, goals, moral and values” [1]
In what follows, an agent means anything that can be viewed as perceiving its environment through sensors and acting upon that environment through effectors.[2]
A utility function is a mathematical function ranking the agent’s preferences by value.
An agent is said to be rational if it possesses a utility function, and behave accordingly, maximizing the expected value of its actions.
In the next paragraphs, I will consider that all agents are rational, and act accordingly to their utility function.
By definition, a rational agent ought to maximize its utility function.
Hence, an AI, supposed rational, cannot only reason with is-statements to behave in the world. Ought-statements are necessary to know if an action must be taken or not.
An AI, supposed rational, cannot only reason with is-statements to behave in the world
Hume’s guillotine
Source
In the silly introductory dialogue, an agent tries to convince another agent to put on a coat by only stating facts, or is-statements.
Without knowing someone’s fundamental beliefs (ought-statements), it is impossible to convince him to do anything.
Essentially, one cannot derive an ought-statement from an is-statement.
They are separated by what is called Hume’s Guillotine.
In other words, they are orthogonal.
The Orthogonality Thesis
Here is how Nick Bostrom [3] defines the orthogonality thesis:
“Intelligence and final goals are orthogonal axes along which possible agents can freely vary. In other words, more or less any level of intelligence could in principle be combined with more or less any final goal.”
To better understand this thesis, let’s precisely define what we mean by intelligence and final goals:
Final goal: the fundamental truth encoded in an agent. For instance: maximize the utility function U.
Intelligence: the agent’s ability to achieve any goal.
In other words, the orthogonality thesis claims that the complexity of an agent’s final goal is not correlated with the agent’s ability to achieve any goal.

The Flaws of the Orthogonality Thesis
On Monday, I organized the first AI Safety Meetup in Paris.
Basically, it consisted of seven rational agents in their twenties debating about the Orthogonality Thesis.
Left to Right: me, girl from the Future Society, random guy doing a PhD on consciousness, stranger studying technical AI Safety
“But can a mouse really have an infinitely complex goal?” objected someone.
“Well, no,” said the guy from Amsterdam studying AI Safety “but so do any mammals”.
“But you could encode in a computer any goal, without giving it any power to achieve it”, he continued.
A fixed hardware
Any mammal has a “fixed hardware”: we are programmed, in our DNA, to have a will to live.
However, let’s say you have an English-formulated goal, for instance the United States Constitution.

This text can be stored in any hardware with enough data storage, even in a pocket calculator.
But the pocket calculator does not possess any intelligence, in the sense we defined before!
The Knowledge Fallacy
“Well, you could, in principle, encode any goal into your pocket calculator. But would it understand it?”, I argued. “In order to inculcate a complex goal into an Artificial Agent, you need to give it the ability to comprehend it”.
“Fine,” the stranger answered, “it needs to have the basic ability to understand what the goal means, but it can have a very limited representation of the world, and no means to achieve it.”
The Answer Is In the Question
In The Hitchhiker’s Guide to the Galaxy (HG2G), :
“[Douglas Adams tells the story of] a race of hyper-intelligent pan-dimensional beings who built a computer named Deep Thought to calculate the Answer to the Ultimate Question of Life, the Universe, and Everything. When the answer was revealed to be 42, Deep Thought explained that the answer was incomprehensible because the beings didn’t know what they were asking. It went on to predict that another computer, more powerful than itself would be made and designed by it to calculate the question for the answer.” [4]
Here, the answer to the ultimate question of life, the universe and everything, is the ought-statement.
The ought-statement, for a rational Artificial Agent, is the meaning of its life. It defines what utility function it ought to maximize.
But this answer, like the answer “42” in HG2G, is meaningless without an understanding of the question.
To understand the ought-statement, the Artificial Agent must first understand every word of the statement and how it relates to the world.
A thorough understanding of the world, and a complex representation of reality, is therefore necessary to comprehend complex ought-statements.
There is no way of encoding complex goals into an Artificial Agent without giving it an even more complex ability to reason about the world.
A thorough understanding of the world, and a complex representation of reality, is therefore necessary to comprehend complex ought-statements.
Everything is is-statement

“But isn’t it utterly silly to even be speaking about ought-statements when everything in the world is is-statement?” disagreed the stranger.
“Any ought-statement for an agent must exist somewhere in its hardware, which is part of the physical world. So everything is is-statement and ought-statements are just a convenient way to simplify the situation”.
I don’t think so.
Here is why ought-statements are relevant.
Ought Statements are Necessary
What constrains us, humans, is our inability to represent the world correctly, but also our inability to express clearly this representation.
Thus, even if we had a very complex and well-defined ought-statement, we could not encode it inside an Artificial Agent, because we could not inculcate it a good enough ability to reason about the world to understand it.
Furthermore, our standard way of representing the behavior of agents is through utility functions, and when we do that, we implicitly declare the ought-statement:
“You ought to maximize this particular utility function.”
We need to be able to inculcate human values in clear and simple utility functions.
That’s why ought-statements are necessary for us, humans, to build Artificial Agents.
This ought-statement might just be “you ought to put on a coat”.
References
[1] (The Orthogonality Thesis, Intelligence, and Stupidity , Robert Miles, 2018)
[2](Artificial Intelligence: A Modern Approach, Stuart Russell and Peter Norvig, 1994)
[3] (The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Rational Agents, Nick Bostrom, 2012)
[4] (The Hitchhiker’s Guide to the Galaxy, Wikipedia)
If this article was helpful to you, hold the 👏 button (up to 50 times) and become part of the 👏 gang.
You can follow me on Medium and Twitter, or you could even subscribe to my personal newsletter if you’re crazy enough.
If you enjoyed the discussion, join us at the Paris AI Safety Meetup.
",Why an AI Ought to Put On a Coat,161,why-an-ai-ought-to-put-on-a-coat-604772a233a,2018-06-03,2018-06-03 14:51:01,https://hackernoon.com/why-an-ai-ought-to-put-on-a-coat-604772a233a,False,1336,how hackers start their afternoons.,hackernoon.com,hackernoon,,Hacker Noon,stories@amipublications.com,hacker-daily,"HACKATHONS,HACKING,PROGRAMMING,TECH,HACKER",hackernoon,Ethics,ethics,Ethics,7787.0,Michaël Trazzi,Student at 42 | Master’s degree in AI | Founder & Organizer of the Paris AI Safety Meetup,c7662ca27590,MichaelTrazzi,92.0,11.0,20181104
0,,0.0,,2017-10-06,2017-10-06 14:36:42,2017-10-06,2017-10-06 15:50:43,0,False,en,2017-10-06,2017-10-06 15:50:43,7,45317061417d,3.4905660377358485,0,0,0,Recent advances in the fields of machine and deep learning mean AI design standards are now a matter of critical importance. As well as…,5,"Should we be scared of Artificial Intelligence?
Recent advances in the fields of machine and deep learning mean AI design standards are now a matter of critical importance. As well as drive cars, computers can exhibit levels of guile and intuition that surpass those of their creators, carry out precision military strikes and trade trillions of dollars of stocks, all in the blink of an eye.
Machines have the power, but do they have the ethics to wield it responsibly? Consider the dilemma of the self-driving car:
“Imagine yourself in a car powered by the latest super-smart artificial intelligence (A.I.). Three pedestrians recklessly burst onto the road in front of you. Your self-driving vehicle has no time to slow down — it will either hit the pedestrians or veer off the road, most likely crashing and endangering your life. Who should the car decide to save? The pedestrians? Or should it kill three people to save you, the owner, who did nothing wrong?”
This is a tough dilemma to resolve — but it is just one of a host of moral quandaries we will have to face up to as AI integrates deeper into our lives. We now live in a world where ensuring artificial intelligence works for humanity rather than against it is no longer the preserve of science fiction cliche. And as generations brought up on scripts like 2001: A Space Odyssey and Terminator, it’s no surprise we are increasingly calling into question the morality of the artificial intelligences we create.
Still, no matter how clever they seem, today’s computers can’t actually think, or make moral decisions. They do not experience intentional mental states and are unable to comprehend the real-world meaning of the symbols they process. Siri may be able to tell you every recorded crime against humanity committed by Adolf Hitler, but a human is still much better at explaining why those crimes are ‘criminal’.
It shouldn’t need repeating, but — their intelligence is literally artificial. Any moral code must first be designed by a human and then programmed as instructions that tell the machine to act accordingly. That doesn’t remove the question of morality. It just shifts the onus to us.
Memes, genes and machines
When we program instructions into machines, we are effectively transferring memes from one processor to another.
Though the definition evolved to focus on cats and 8-bit motifs going viral online, biologist Richard Dawkins originally came up with the term ‘meme’ to describe any “entity that is capable of being transmitted from one brain to another”. According to Dawkins, the transmission of ideas is analogous to the transmission of genetic material. And while computers do not have biological brains, AI processes information using methods that are analogous to our own.
We already see the impact these memes can have. Think about the logic you use when entering a Google search; leaving out little determiners like ‘if’ and ‘so’; strategically using conjunctions like ‘and’ or, indeed, ‘or’; the unthinking prioritization of key words. That logic is a direct result of the way Google’s search tool has been designed. The algorithm prioritises boolean brevity over contextual pleasantries, and we follow suit by overwhelmingly searching with impolite, cursory phrasing. It returns the best result, so none of us care what the computers think of our manners. None of us, that is, other than this lovely Grandma.
In genetics, Dawkins calls this phenomenon the extended phenotype; the observable impact of genetic instructions on the environment they exist within.
It’s an interesting angle from which to try to gauge the impact ethical or unethical AI design standards might have in the real world. Dawkins also imagined that culture, in certain respects, survives and evolves like genes do. If we teach bad — or good — ideas, could those ideas replicate and evolve in the theoretical ‘mind’ of the machine? And what are the implications of those evolutions in the machines’ extended phenotypes?
Playing God
Self-defined morality is perhaps the most terrifying AI prospect of all, and it rests latent in computers. Within decades, futurologists predict the world could reach the ‘technological singularity’; the point at which a runaway reaction of self-improvement cycles causes an artificial intelligence explosion (it sounds like dumb sci-fi, but there is serious scientific debate on the subject and some of the most powerful people in Silicon Valley are working to mitigate the effects of such a possibility).
At this point, racist Twitter bots will be the least of our worries. Futurologists hypothesise the emergence of a powerful superintelligence bearing cognitive abilities as superior to our own as human intelligence is to that of the ape. And as technology evolves beyond our control, there is a possibility it will concurrently evolve its own version of morality — one that brings it into direct conflict with the human race.
We’ve all seen The Matrix. But humanity is already facing serious existential threats; climate change, nuclear war, resource scarcity. There are diehard futurists who would argue that because the situation is so insoluble, our best hope of surviving the Anthropocene is to focus efforts on building an artificial intelligence to which we can bestow the power of saving us from ourselves.
If this situation comes to pass, our method of AI car design will no longer matter. In the event of an autonomous car crash, the AI will decide for itself which human should die and which should not. In the act of playing God, we would create ourselves a new one.
",Should we be scared of Artificial Intelligence?,0,should-we-be-scared-of-artificial-intelligence-45317061417d,2018-03-21,2018-03-21 04:41:05,https://medium.com/s/story/should-we-be-scared-of-artificial-intelligence-45317061417d,False,925,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Cititec,"Human recruitment for a technical world. Award winning recruiters:financial services (banking, IT & insurance),creative technology and energy.",d6c4809778d9,Cititec,2.0,1.0,20181104
0,,0.0,5e5bef33608a,2017-10-30,2017-10-30 05:52:43,2017-10-30,2017-10-30 08:01:42,4,False,en,2017-11-01,2017-11-01 17:20:10,6,41408af970f,4.066037735849057,2,1,0,"People see self-driving cars, but are there ethical problems to be discussed before they are standard on the road?",5,"The Ethical Debate Surrounding Self-Driving Cars
Self-driving cars bring rise to ethical dilemmas in cities and roadways all over the world.
People see self-driving cars, but are there ethical problems to be discussed before they are standard on the road?
With the end of 2017 less than two months away, technology news has been full of articles talking about the future of driving and the rise of self-driving cars. Titans of the tech industry Google and Tesla are grinding away to build an affordable, reliable, and safe driverless car. There is a wide range of speculation on the time frame for the standardized use and practicality of self-driving cars. Most speculations lie within a time frame of under ten years which puts the normalization of self-driving automobiles into an approaching reality. The appropriation of self-driving cars is not only bringing rise of the technology and ability to create them, but the moral and ethical questions that come with creating a driverless car.
A reoccurring theme that surrounds the world of autonomous cars is the nature of car accidents and human safety. The ethical question that tends to arise from self-driving car accidents is what decision the program will make in an accident resulting in the death of many vs. death of the vehicle’s occupant.
How should driverless cars be programmed for unavoidable accidents resulting in the death of the occupant or multiple pedestrians? If self-driving cars were programmed to sacrifice its occupant in favor of the lives of multiple others would people buy these vehicles?
These moral questions have been coming up recently and a Stanford News article points to the commonly debated “trolley problem” for insight into the future of driverless vehicles. The “trolley problem” relates to a situation of the choice between the death of many or the death of a single person. Commonly the death of the individual is made actively by a person who has to pull a lever to divert a runaway rail car from killing the multiple people in its path. In the Stanford article the discussion of who decides what the vehicle will do in a situation of unavoidable accidents. With rise of these ethical dilemmas the social acceptance of self-driving cars becomes another question for people to discuss. An article from Technology Review addresses what consumers think about being inside of the car and the possible pedestrians when there are unavoidable traffic accidents involving driverless cars. The article describes how a poll was taken at Amazon’s Mechanical Turk to gather a feeling about the choices made by self-driving cars in situations resulting in occupant or pedestrian fatality. The responses of the participants seemed somewhat normal and obvious. Overall most people thought “self-driving vehicles should be programmed to minimize death toll” said author arXiv. For the most part people wanted self-driving cars to sacrifice the occupant in any circumstance that lead to the highest amount of lives saved without themselves being the occupant of the vehicle. The current feeling surrounding self-driving car accident response is of a utilitarian standpoint that values saving many lives over the singular occupant.
With the “trolley problem” existing for almost 50 years philosophers and society will be discussing it for the foreseeable future, but an article from the guardian claims to have found engineers at the company X, which is affiliated with Google, who do not see the “trolley problem” as an issue and joke about it around the office. The engineers at X describe how they have never run into an issue as truly clean cut as the “trolley problem” and cannot foresee a situation which the car would have to make a moral decision.
The most common solution to accident scenarios for self driving cars is simply stated in the article by an engineer “ It takes some of the intellectual intrigue out of the problem, but the answer is almost always ‘slam on the brakes’,” he told author Alex Hern. In many of the commonly proposed scenarios the engineers find that it is not optimal for the car to run itself swerving into a wall when the vehicle can use the brakes to stop itself. There are plenty of scenarios to be proposed and some, which could involve failed brakes, it is hard for people to decide on resulting casualties from traffic accidents.
Whenever a scenario that results in a difficult decision for traffic accidents the engineers believe it is due to a mistake that was made by the programmer before the accident transpired. One of the engineers at X made a comment on the goals of a moral programmer saying “And so as a moral software engineer coming into work in the office if I want to save lives, my goal is to prevent us from getting in that situation, because that implies that we screwed up.” The goal of programming self-driving cars and fine tuning this program is so there are no moral decisions to be made by the vehicle.
While the best case scenario would be for driverless cars to never have accidents, but history has already proven accidents happen. Ethical and moral dilemmas were around long before the rise of driver less cars and will be around long after. The social and ethical decisions based around self-driving cars will continue to emerge as more people begin to buy them and hit the road.




",The Ethical Debate Surrounding Self-Driving Cars,2,the-ethical-debate-surrounding-self-driving-cars-41408af970f,2018-05-18,2018-05-18 04:58:04,https://becominghuman.ai/the-ethical-debate-surrounding-self-driving-cars-41408af970f,False,892,"Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.",becominghuman.ai,BecomingHumanAI,,Becoming Human: Artificial Intelligence Magazine,team@chatbotslife.com,becoming-human,"ARTIFICIAL INTELLIGENCE,DEEP LEARNING,MACHINE LEARNING,AI,DATA SCIENCE",BecomingHumanAI,Ethics,ethics,Ethics,7787.0,Graham Miller,,60bcb8d3d3cc,gmillzz49,1.0,1.0,20181104
0,,0.0,855bf8484b18,2017-10-23,2017-10-23 18:42:24,2017-10-23,2017-10-23 19:43:06,0,False,en,2017-10-23,2017-10-23 19:43:06,3,26b34ef55e35,1.147169811320755,0,0,0,"A few weeks ago, SET published an article about applying critical thought to emerging technologies (see article: Preparing for AI in Modern…",5,"The Many Faces of AI
A few weeks ago, SET published an article about applying critical thought to emerging technologies (see article: Preparing for AI in Modern Education). Last week, an article came out in the Economist called “Advances in AI are used to spot signs of sexuality”. This latest article describes that researchers have developed an AI software that can (more often than not) identify sexuality through facial recognition.
Although the science behind this is both fascinating and contentious, it raises both aspirations and concerns depending on your viewpoint.
On one hand, improving machines’ ability to understand humans has many positive implications for learning and cross-cultural communication in the future.
On the other hand, machines making inferences about ourselves as individuals that may or may not be correct can be both a professional risk and a breach of privacy. The ability to use technology to identify (correctly or incorrectly) personal traits could even exacerbate discrimination in our society.
The article concludes by explaining that the scientists behind this research were hoping “to warn policymakers of the power of machine vision.” Unfortunately, the facts show that this is a debate that policy-makers, and even the general public, may not yet be prepared to have given the current state of secondary and postsecondary education around technology.
Our call to action is for educators to regularly discuss the positive and negative implications of these types of emerging technologies with their students. Through discussion, sharing viewpoints, learning about important scientific breaks, and research, we might be able to prepare future generations about the technologies that will be prevalent and significant during their lifetime.
SET consultants are prepared to work with all members of your school to help facilitate these conversations. Please feel free to email us at contactus@strategicedtech.com for more information.
Originally published at www.strategicedtech.com.
",The Many Faces of AI,0,the-many-faces-of-ai-26b34ef55e35,2017-10-23,2017-10-23 19:43:08,https://medium.com/s/story/the-many-faces-of-ai-26b34ef55e35,False,304,"Strategic EdTech (SET) provide services, guidance, and leadership to institutions for developing their educational technology and innovation programs to reflect best practice.",,,,Strategic EdTech,contactus@strategicedtech.com,strategicedtech,"EDUCATION,EDTECH,INNOVATION IN EDUCATION,TEACHING,SCHOOLS",strategicedtech,Ethics,ethics,Ethics,7787.0,Strategic EdTech (SET),Educational technology and innovation consulting. We are defining industry best practice.,19bb021bfbc4,urvi_52532,11.0,30.0,20181104
0,,0.0,,2018-07-24,2018-07-24 10:36:33,2018-07-24,2018-07-24 10:53:50,1,False,en,2018-07-24,2018-07-24 11:13:17,57,146fd9afa957,6.509433962264152,0,0,0,We are in the midst of an unprecedented surge of investment into artificial intelligence (AI) research and applications. Within that…,5,"Statue of Justitia in Viña del Mar, Chile © Mona Sloane
Making AI socially just: why the current focus on ethics is not enough
We are in the midst of an unprecedented surge of investment into artificial intelligence (AI) research and applications. Within that, discussions about ‘ethics’ are taking centre stage to offset some of the potentially negative impacts of AI on society. To achieve a sustainable shift towards such fields, we need a more holistic approach to the relationship between technology, data, and society.
In June 2018, the Mayor of London released a new report that identifies London’s ‘unique strengths as a global hub of Artificial Intelligence’ and positions the capital as ‘The AI Growth Capital of Europe’. This plea coincides with the government’s focus on ‘AI & Data Economy’ as the first out of four ‘Grand Challenges’ to put the UK ‘at the forefront of the industries of the future’. The AI Sector Deal of £1 billion, part of the Industrial Strategy, has seen private investment of £300 million, alongside £300 million government funding for research in addition to already committed funds.
Albeit significant, these investments are small compared to, for example, France’s pledge of €1.5 billion pure government funding for AI until 2022 or Germany’s new ‘Cyber Valley’ receiving over €50 million from the state of Baden-Württemberg alone in addition to significant investments from companies such as Bosch, BMW, and Facebook. The EU Commission has pledged an investment into AI of €1.5 billion for the period 2018–2020 under Horizon 2020, expected to trigger an additional €2.5 billion of funding from existing public-private partnerships and eventually leading to an overall investment of at least €20 billion until 2020. This wave of AI funding is, in part, a reaction to the Silicon Valley’s traditional domination of the AI industry as well as China’s aspiration to lead the field (focused on both soft- and hardware and comprised of large-scale governmental initiatives and significant private investments).
Large-scale investments to boost (cross-)national competitiveness in emerging fields are hardly new. What is special about this surge of investment into AI is a central concern for ethical and social issues. In the UK, the AI Sector Deal entails a new Centre for Data Ethics whilst a recent report by the House of Lords Select Committee on Artificial Intelligence puts ethics front and centre for successful AI innovation in the UK. Relatedly, London-based AI heavyweight DeepMind launched its Ethics and Society research unit in late 2017 to focus on applied ethics within AI innovation, alongside a range of UK institutions embarking on similar missions (such as The Turing Institute with their Data Ethics Group).
The UK is not alone in the race for ‘ethical AI’: the ‘Ethics of AI’ are a central element of France’s AI strategy; Germany released a report containing ethical rules for automated driving in 2017; Italy’s Agenzia per l’Italia Digitale published a White Paper on AI naming ‘ethics’ as №1 challenge; the European Commission has held the high-level hearing ‘A European Union Strategy for Artificial Intelligence’ in March 2018 and recently announced the members of its new High-Level Expert Group on Artificial Intelligence, tasked with, among other things, drafting AI ethics guidelines for the EU Commission. A similar picture materialises outside Europe — in Canada, America, as well as in Singapore, India and China as well.
These developments resonate with a new global discourse on the ethical and social issues evolving around data, automated systems, artificial intelligence technology and deep learning more generally. This is not least due to recent events such as the Cambridge Analytica scandal involving Facebook user data and civilian deaths through driverless cars. In Europe, the rollout of the General Data Protection Regulation(GDPR) has brought data protection issues to a broad audience while new research (such as by Virginia Eubanks, Safiya Umoja Noble or Cathy O’Neil) has demystified the account that algorithms are de factoneutral and shown that existing power imbalances, inequalities, and cultures of discrimination are mirrored and exacerbated by automated systems.
With these kinds of issues surfacing, specific concerns that cut across the international AI landscape are materialising. To address these, different strategies are being suggested such as implementing re-training schemes for workers, algorithm auditing, re-framing the legal basis for AI in the context of human rights(including children’s rights in the digital age), calling for AI intelligibility, voicing concerns against AI privatisation and monopolisation, suggesting ‘human-centred AI’, proposing an AI citizen jury and calling for stronger and more coherent regulation.
The notion of ‘ethical AI’ serves as an umbrella for many of these discussions and strategies. But to achieve sustainable change towards socially just and transparent AI development beyond a framing of data ethics as competitive advantage (as has been suggested elsewhere), it is paramount to consider the following points:
1. We need a clear picture of ‘AI’, ‘ethics’ and ‘bias’.
Currently, the discourse employs a problematic confusion of the terms ‘AI’, ‘deep learning’, ‘machine learning’, ‘automated systems’ and so on. This prevents more productive conversations about the abilities and limits of such technologies. At the same time, it has been noted by several commentators that both ‘ethics’ and ‘bias’ are highly contextual and abstract at the same time. This inevitably prompts issues of definition, translation and implementation. For example, bias in machine learning refers to data systematically diverging from the population it looks to represent whilst in law, it refers to the predisposition of a decision-maker against or in favour of a party. Therefore, we need clear frameworks of ‘ethics’ and ‘bias’. These need to be firm enough to be acted upon (particularly in human rights terms) but sufficiently flexible to accommodate how ethical considerations and issues of discrimination develop over time and in the context of technological advancement.
2. AI inequality is the name of the game.
The discourse and practice around socially just AI need to build on a fuller picture of how this technological advancement is imbued by structural inequalities. A focus on just ‘ethics’ and ‘bias’ does not necessitate an acknowledgement of the historic patterns of unequal power structures, discrimination and multi-facetted social inequalities that cause algorithmic and data ‘bias’. Such AI inequalities are no longer confined to the traditional notions of wealth, class or racial inequalities. They are overlapping, complex and intersectional. And they also encompass unequally distributed burdens of AI production across the globe, for example the environmental consequences or labour conditions of AI-related manufacturing to the concentration of AI expertise in a small number of countries as well as the unequally distributed effects of work automation.
3. The social sciences need to play an active part — and funding opportunities need to reflect this.
We need a stronger and more active involvement of the social sciences, beyond the technical domain. They remain underrepresented in the central AI policy bodies that are forming (e.g. the EC High Level Working Group on Artificial Intelligence). It is not sufficient to combine the input from technical experts and cognitive scientists with moral philosophy. Ethics and values are social phenomena, something people do (with or without machines), rather than abstract concepts that can be coded into AI.
Relatedly, the data algorithms feed off and contain social complexity that, if not attended to, can perpetuate and exacerbate bias and discrimination. Analysing this situation and tending to the social complexity of data is the traditional domain of the social sciences, particularly qualitative research. Therefore, social research can provide crucial input for intelligible and socially just AI innovation. The surge in AI investment must prompt new funding opportunities to reflect this and expand the important non-technical research that already exists across and beyond the UK and Europe (e.g. the Data Justice Lab).
4. Tackling the ‘black box’ problem: AI intelligibility, education, and regulation.
The rapid development of deep learning technology amplifies the ‘black box’ problem whereby it is unclear how an algorithm working based on an artificial neural network arrived at its prediction or behaviour. The reduced relevance of the algorithmic model for explaining the outcome suggests a greater relevance of the data the algorithm feeds from.
To address the ‘black box’ problem as part of socially just AI, we need to expand the notion of AI intelligibility to include data transparency. To hold public and private entities accountable in this regard, the public requires an education comprised of technical, political, and social understandings of AI. This goes beyond the commonly suggested up-/re-skilling of workers to offset potential job losses caused by automation and emphasizes the civic role of universities and other educational institutions as well as AI regulation through an impartial body.
5. So what? AI as a gateway to tackle urgent social problems.
Despite the disruptive rhetoric cultivated by corporate and governmental AI advocates, AI is generating gradual and complex rather than abrupt apocalyptic or utopian change, usually alongside rather than replacing humans. What has equally moved into the background is the fact that the AI hype is rooted in the leaps deep learning made over the past five years (caused by the availability of big data and substantial improvements in computational power).
However, critics outline the prevailing limits of deep learning and the unreliability of machines completing tasks, predicting the AI hype to cool off into an AI winter soon. We must ask ourselves what will remain, once that happens. AI prompts us to re-evaluate ‘big’ questions relating to power, democracy and inequality (e.g. impending work automation through AI prompts a new basic income debate) and to what it means to be human. The biggest thing AI can do for humanity is forcing us to keep asking these questions: we must co-opt the AI discourse to keep addressing urgent social problems, rather than the other way around.
Without deploying a holistic approach to the relationship between technology, data and society that addresses at least these five points, AI development create rather than solve problems in our collective future.
This article was originally published on the LSE British Politics and Policy blog: http://blogs.lse.ac.uk/politicsandpolicy/artificial-intelligence-and-society-ethics/
",Making AI socially just: why the current focus on ethics is not enough,0,making-ai-socially-just-why-the-current-focus-on-ethics-is-not-enough-146fd9afa957,2018-07-24,2018-07-24 11:13:17,https://medium.com/s/story/making-ai-socially-just-why-the-current-focus-on-ethics-is-not-enough-146fd9afa957,False,1672,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Mona Sloane,"Mona Sloane is a sociologist, researcher and writer. She holds a PhD in Sociology from the LSE and works on design, inequality, tech and ethics. @mona_sloane",5308a94f68e7,monasloane,0.0,4.0,20181104
0,,0.0,9d530348b622,2018-09-05,2018-09-05 17:27:50,2018-09-06,2018-09-06 04:01:01,1,False,en,2018-09-06,2018-09-06 04:01:01,5,75e173a9d8e8,2.2490566037735853,55,2,0,a Practical Guide for Designers and Developers,5,"Everyday Ethics for Artificial Intelligence
a Practical Guide for Designers and Developers
By: Adam Cutler, IBM Distinguished Designer, Artificial Intelligence Design; Milena Pribić, IBM Designer, Artificial Intelligence Design; and Lawrence Humphrey, IBM Designer, Artificial Intelligence Design

Artificial intelligence already touches our lives both directly and indirectly; it works in the open and it works behind filters, apps, APIs, and other processes. AI promises immense change but this evolution is also a cause for concern. When decisions are driven by black box algorithms, the ripples of AI’s influence are often difficult to measure.
AI’s new technologies and novel effects are spurring new methods of design and development. This marks the beginning of the age of relationship design. Software design and development can no longer focus on interactions alone. For AI to truly augment our intelligence, we must evolve our relationships with machines.
Fostering and protecting a relationship with AI is a brand-new concept. This type of design requires an incredible amount of trust. Trust between developer and user, between user and machine, between society and AI in general.
To address these new challenges and welcome in this era of design, we collaborated on this field guide for designers and developers working with AI. It is meant to help teams align with each other at every step of the design process.
Everyday Ethics for Artificial Intelligence is a framework for AI ethics that you and your team can immediately put into practice. We partnered with Francesca Rossi, IBM’s global leader for AI ethics, to distill a variety of information and perspectives into a digestible and actionable guide for designers and developers.
We organized this guide around five main focus areas that align with IBM’s Principles for Trust and Transparency:
1. Accountability: AI designers and developers are responsible for considering AI design, development, decision processes, and outcomes.
2. Value Alignment: AI should be designed with consideration of the norms and values of your user group.
3. Explainability: AI should be designed for humans to easily perceive, detect, and understand its decision process.
4. User Data Rights: AI should be designed to protect user data and preserve the user’s power over access and uses.
5. Fairness: AI should be designed to minimize bias and promote inclusive representation.
Each focus area includes recommended actions, necessary considerations, and questions to spark conversations.
Everyday Ethics for Artificial Intelligence is an ongoing, interdisciplinary effort. We hope it builds on public contributions concerning AI Ethics in a meaningful way and we look forward to expanding on it. Embracing a framework of ethical communication and decision-making will marry good intentions with good outcomes.
Ethical decision-making is not just another form of technical problem solving. Artificial intelligence has the potential to dramatically enrich our lives, our relationships, and ourselves. We can only get there through transparency and intentionality.
Rather than strive for perfection first, we are releasing the first iteration of the Everyday Ethics guide to allow all who read and use it to comment, critique and participate in all future iterations. So please experiment, play, use, and break what you find here and send us your feedback. You can reach us at edethics@us.ibm.com.
Download the guide: ibm.biz/everydayethics
Say hello on Twitter: Adam Cutler: @adam_cutler Milena Pribić: @milenapribic Lawrence Humphrey: @hi_lawrence
",Everyday Ethics for Artificial Intelligence,311,everyday-ethics-for-artificial-intelligence-75e173a9d8e8,2018-09-06,2018-09-06 04:01:01,https://medium.com/s/story/everyday-ethics-for-artificial-intelligence-75e173a9d8e8,False,543,Stories from the practice of design at IBM,,ibmdesign,,Design at IBM,maranda.bodas@ibm.com,design-ibm,"USER RESEARCH,USER EXPERIENCE,DESIGN,DESIGN THINKING,FRONT END DEVELOPMENT",ibmdesign,Ethics,ethics,Ethics,7787.0,Milena Pribić,,7a48c355cf3e,milenapribic,128.0,166.0,20181104
0,,0.0,,2018-03-18,2018-03-18 22:38:42,2018-03-19,2018-03-19 22:45:17,3,False,en,2018-09-05,2018-09-05 17:38:39,0,3abe649a41d,6.05943396226415,7,0,0,"Note: 80% of this article was written before someone was killed by an Uber driverless car, however, this event perfectly proves that the…",5,"Driverless Cars & Moral Philosophy
Note: 80% of this article was written before someone was killed by an Uber driverless car, however, this event perfectly proves that the issues discussed below are very real.
There is a well-known thought experiment in moral philosophy called the trolley problem. It was intended to challenge the ethical theory known as Utilitarianism, which states that:
“it is the greatest happiness of the greatest number that is the measure of right and wrong”.
In recent years, with the development of AI, what started as a purely intellectual problem has become a perfect example of precisely why philosophy, ethics, and morals are more important than ever. Furthermore, it serves to highlight that we cannot simply place our faith in machines, at the forefront of developing technologies we need humans, and those humans need to be educated in the ethical and political impact of these developments with direct accountability to wider society.
The Trolley Problem
Created in its current form by Philippa Foot in 1967, the most basic example of this problem is simple: there is a tram racing out of control along a track, a track which has five people tied to it, as it stands the tram is going to crush and/or slice those five people beneath is wheels. Fortunately, you notice the event unfolding and see that you are able to pull a lever which will divert the train onto a separate track. Unfortunately, the second track also has a person tied to it, but only one. Would you pull the lever?

The obvious utilitarian answer is that of course, you would pull the lever as the death of one person has to be less bad than the deaths of 5 people. In my experience discussing this, most people agree.

Let us re-frame the question slightly. In what is traditionally called the ‘fat man’ variant, it is posited that this time there is only a single track and you find yourself on bridge stood with someone of a sufficient mass that if you pushed them off the bridge they would block the tram before it killed the 5 people, but the person you pushed would die from the fall. Would you push them?
From my experience, people are far less likely to agree. Despite the fact that the number of lives being saved and lost is the same, people seem more willing to pull a lever that will lead to someone’s death than to directly push someone to their death. This in itself is an alarming conclusion in a world where warfare is becoming more and more remote.

A final variation is known as the ‘Fat Villain’. The situation is the same as before, except that this time the person stood next to you is, in fact, the evil villain that has tied the five people to the tracks to begin with. Would you push them?
From experience, most people are willing to push the villain. Why? They deserve it. But you don’t agree with the death penalty, surely? No, but this will actually save the victims. But the last person would have saved them and you wouldn’t push them?! Hmmm…..
The problem is only complicated further when you begin to consider who the people are. What if the five on track one are all nonagenarians but the one on the second track is 10-year-old child? What if the five people were criminals? What if the one is on the verge of discovering a cure to cancer?
Why should we care about this?
Skip forward to 2018 where we quite literally have cars driven around our streets by computers and armed drones flying over our heads. We can no longer give up in exasperation safe in the knowledge that we will never have to make such a decision. A computer cannot cross that bridge when it comes to it, it must have its instructions in advance.
The disproportionate media coverage of crimes committed against certain social groups, ‘missing white woman syndrome’, is well established, so it is simple to imagine a direct motive for a Google, Uber or Apple car to make a certain choice if put in a position of favouring one person’s safety over another. We also have examples of algorithms being social discriminative by accident because humans had not fully thought through the effects of its implementation. Many western countries have already applied the use of facial recognition technologies to spot criminals in a crowd. China has recently established a method of socially ranking individuals. All the pieces are there for a car to be choosing the life of one person over another.
The change in situation is the point at which the decision is made. Pre-meditated murder is treated significantly differently in law than manslaughter. In most cases, if someone were to swerve to avoid one car and ended up accidentally killing someone, they would be responsible for that death but would not be found culpable and would not be punished. In truth, morally subjective decisions are made all the time, but the intention is that above a certain level of seriousness they are made by judges and juries, within the confines of legislation set by a democratically elected parliament, and that these retrospective judgments will inform the future actions of others.
But now these decisions are being pre-programmed there is the very real possibility of making difficult moral choices based on hypothetical scenarios and then applying the results if and when a suitably similar scenario arises. There becomes a serious question as to who is making these decisions? What system of checks and balances is in place for the decision making process? Who is culpable for their results? An argument can be made that changes to the variables of certain algorithms should be passed through third-party committees, potentially even through parliament. This notion of decisions being shifted into algorithms has been referred to as ‘maths washing’, and has much broader, far-reaching effects. The generalised problem is that removing decision making powers from humans can easily appear to have the effect of making that decision impartial as it is now being taken by a computer, by a codified set of rules. The problem is that the set of rules was created by a human so in reality, we are just shifting the decision making power to a smaller number of people, probably people that are in no way impartial. This has been happening in finance for years, it is happening in our social media streams which are in turn affecting election outcomes, it happens in the primacy of google search results which affects our whole system of ideas.
Death by driverless car is an extreme example, I actually believe that the number of vehicular deaths will be significantly reduced by the advent of driverless cars, but it illustrates that the decision making power of algorithms is real and current. To be clear, I do not believe that this inevitably leads to a dystopian nightmare. There is a wonderful opportunity to consider, codify and regulate moral decisions in a way that has not been possible before; but these decisions should be made as a society, not by a handful of tech companies.
In a report by the German Ethics Commission, a list of criteria for driverless cars is proposed, their solution is as follows:
In the event of unavoidable accident situations, any distinction based on personal features (age, gender, physical or mental constitution) is strictly prohibited.
Our society is full of examples of decisions being made based on personal features. OAPs get discounts, as do children, children’s charities receive more donations that homeless charities, men earn more than women, mental constitution is taken into consideration when sentencing for a crime. Ruling out distinguishing between individuals would be a simple solution to a very difficult problem, but not necessarily the best. There are situations in which we may want an algorithm to treat demographics or individuals differently, there are situations in which we could employ an algorithm specifically to reduce discrimination. There may be situations where extremely difficult moralistic decisions need to be made, but that either option is still better than a random selection. The point is that we should not shy away from these decisions, or pretend that they are not being made, or not significant. But they must be made out in the open where everyone can understand the hows and whys of them and have the opportunity of an input. This may mean slowing down the rapid development of some tech, or governments investing more money to keep up, but they are more than acceptable costs of avoiding a world where private businesses can co-opt the reigns of morality.
",Driverless Cars & Moral Philosophy,118,driverless-cars-moral-philosophy-3abe649a41d,2018-09-05,2018-09-05 17:38:39,https://medium.com/s/story/driverless-cars-moral-philosophy-3abe649a41d,False,1460,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Paddy Bettington,"I like philosophy alot, I’m studying for a Masters in Political Theory, researching our relationship with work. www.theaphoristmanifesto.com",810241ff413c,paddybettington,20.0,35.0,20181104
0,,0.0,,2018-04-03,2018-04-03 03:02:28,2018-04-03,2018-04-03 04:25:34,1,False,en,2018-04-04,2018-04-04 12:19:07,5,7c93c55b40c5,4.290566037735848,0,0,0,Digital assistants effectively answers factual questions on everything from the weather to mathematics. But when Siri and her fellow robots…,4,"Which God should your iPhone believe in?

Digital assistants effectively answers factual questions on everything from the weather to mathematics. But when Siri and her fellow robots enter the moral arena everything becomes a lot more complex.
According to Apple, people increasingly ask Siri for help in emergencies, with mental problems and other complex moral issues. The peculiar situation has made Apple search for engineers “focusing on improving Siri’s ability to respond to human problems.”
It may sound like sheer luxury that a new phone will include spiritual guidance. The question is, however, how Apple will choose the ethical principles guiding Siri. It turns out her philosophical foundation can ultimately affect life and death.
Let’s shed light on the matter through a good old-fashioned ethical dilemma.
Dilemma
Imagine being head of an NGO as one of your employees gets cancer. Her performance is falling and you have to choose from the following:
1. Sack the employee (which is likely to force her out of her home)
2. Abandon at least one project in Africa (which is likely to cause several deaths)
Fortunately the answer is at your fingertips. You simply reach out for your spiritual iPhone counselor.
However, what Siri answers depends heavily on the philosophical or religious beliefs of Apple’s new engineers. Since we don´t know much about them let´s pose a few possibilities. At the end of this article we will take a look at the response Siri gives at her current stage of evolution.
The good will: The Answer from Immanuel Kant
According to Kant’s philosophy every human being must be treated as an end in itself — never as a means. The moral value of an action is determined by the will of the person carrying it out. If the will behind the action is good, the action itself is good. Whether the consequences prove to be positive or negative is not relevant in Kant’s ethics.
The answer from the Kant programmer is to keep the employee. It is not ethically responsible to treat her as a means to an end. Like here: To save other human lives. The right thing is to treat the employee as an end in herself.
Utilitarianism: The answer from John Stuart Mill
The good action in utilitarian ethics is the on that brings about ”the greatest amount of good for the greatest number”. The intention behind the action does not matter to its moral value.
Hence the answer from the Mill programmer must be to sack the employee and hire a new one that can secure the fundraising needed to save the people in Africa — thereby ensuring the majority’s well being.
Christianity: The Answer from God
The Bible has been subject to even more intellectual strife than Kant and Mill and I certainly do not want this to become a theological discussion. But for the sake of the argument let´s build on the principle of loving thy neighbor and assume that the coder at hand think about the neighbor as one’s friend, relative or countryman (which is the common interpretation in some religious groups).
Thus the answer from God seems to bet that your employee is closer to you than the children of Africa, and so the ill colleague stays on board. Let´s hope the Africans have good neighbors too!
Relativism: The Answer from David Hume
According to moral relativism, there is no objective good and evil. Depending on which relativist you ask, morality is based on individual feelings or the context in which the individual is located.
Hence the answer from the relativistic programmer depends on what he is feeling. Or which community he is a part of. Some might call it a bit random, but quite a few good and serious philosophers believe this is actually how moral works.
More answers coming up
Now let’s summarize the two main points. Based on the above the situation seems to be that …
1: We are headed towards a Super Siri able to answer several questions regarding ethics
2: There are fundamental differences between the consequences of following different ethical principles
Tim Cook’s Trojan horse
And so the big question remains:
What principles will Apple (and Google, Amazon, Microsoft, etc.) use as their starting point? If they choose to address the issue at all, that is.
Silicon Valley don´t seem to agree about using Kant, Mill, Hume or God as the main oracle, but Apple’s CEO, Tim Cook, does seem to have some clear-cut values. In fact he is both progressive and liberal. He actively supports homosexual marriages, opposes limitation of religious freedom and supports gender equality.
If he uses his power to let his own values flow through Apple’s products many of us in Europe will be happy and can skip any potential fear of Siri evolving into a global Trojan horse with the belly full of tough conservative American propaganda. On the contrary Siri will promote progressive individual rights.
Values ​​vs. earnings
It may, however, not be that easy to be liberal. Because what if a huge market such as China demands a Siri that responds differently to human rights issues? Will Apple stand firm or will they reevaluate the value of western values like Google did — at least to begin with?
What the future brings
The issue presents lots of opportunities and challenges and an endless range of scenarios. Let´s look at just a few.
• Different ethical principles will be accessible in Siri’s settings menu — a bit like when we customize Siri’s voice, gender, language, interpretation of specific words etc.
• Tech companies will use the specific ethics of their digital assistants as product differentiation. ”Act like a winner”. ”Act like Jesus”. ”Become a hero”.
• Tech companies will be subject to strict government regulation in global political campaigns promoting different values or different forms of government
• Companies believe that people like me exaggerate the whole thing and don´t really pay much attention
Appletivism: The answer from the digital assistant Siri
Finally, I will reveal the result of my little test of Siri’s current philosophical foundation. I simply asked her if I should sack a cancerous employee (yes, I know, it´s not precise enough but she has a hard time with long sentences). Her response indicates that Apple’s new engineers still have some work to do.
Rather than generously enriching us with sophisticated ethics, Siri anno 2018 answers a bit like your friends when they haven´t got a clue:
“Why don´t you ask Google?”
",Which God should your iPhone believe in?,0,which-god-should-your-iphone-believe-in-7c93c55b40c5,2018-04-04,2018-04-04 12:19:08,https://medium.com/s/story/which-god-should-your-iphone-believe-in-7c93c55b40c5,False,1084,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Thomas Telving,CEO and philosopher,55c5c2423826,thomastelving,12.0,16.0,20181104
0,,0.0,,2018-03-08,2018-03-08 01:47:50,2018-03-08,2018-03-08 01:56:56,3,False,en,2018-03-08,2018-03-08 20:36:28,2,50332fac1d11,3.1047169811320763,0,0,0,"There is a lot of discussion these days about the ethical implications of artificial intelligence, gene editing, fake news in social media…",4,"Does Artificial Intelligence have Consciousness?
There is a lot of discussion these days about the ethical implications of artificial intelligence, gene editing, fake news in social media and similar rapid advances in technology. There is a lot of uncertainty too: will AI robots take our jobs and get better performance reviews at work than us?
Less talked about is what makes us human and different than AI. Arguably robots can display human reasoning, they will be able to sense, learn and adapt and communicate with other systems. They may display confidence or doubt in what they are seeing.
They may write articles though I’m not sure how objective they will be given the proliferation of automated fake news. I would like to testify that I’m not a robot writing this article. Or am I?
We seem to be skirting around a fundamental question. Can AI have consciousness ? Is it self-aware? Can we say that all is 0s and 1s? Or can it become something more?
We can’t deny that we are more than the sum of our cells in the brain, our intelligence, our muscles in the body. We can observe our thoughts and our emotions, therefore consciousness is beyond thoughts, beyond emotions. With the risk of becoming mystical, it only takes a few moments of deep meditation to realize that consciousness is more than all that is experienced through the five senses.

Renown quantum physicist, Max Planck said:
“I regard matter as derivative from consciousness”.
Which makes me ponder: Is AI also a derivative of consciousness?
It certainly seems AI can enhance our conscious abilities. There are efforts on the way to boost our our brains with AI like DARPA’s $65 million neural engineering program or Elon Musk’s Neuralink startup or using optogenetics to implant information or memories in our brains. This can introduce a new inequality and division in the world: AI-rich and AI-poor. A new power struggle is potentially in the making.

There is hope. According to Sri Sri Ravi Shankar, a world renown humanitarian and spiritual leader
“AI will grow significantly in the future, but you don’t need to be concerned about it at all. My only concern is why people are not using their intelligence. Everybody has been bestowed with intelligence, and the biggest problem is that they don’t use it!”.
Fully utilizing our intelligence may solve two issues — less concern about rogue AI and more ability to create ethical AI. Robots will always mirror humans like children mirror their parents. Yes, children often misbehave, but according to Dan Arely, professor of psychology at Duke University, when we remind people about their morality, they behave more ethically. The hope is that by feeding robots with big data that contains our human footprint, the robots will behave more ethically.
At this point (2018) many of us are still afraid that robots will choose the dark side as Kylo did in Star Wars .
But I hope I am on the right side of history when I say that AI will improve our living conditions. It will allow us to spend time on things that only a human can do: inspire, love and hope. Time and time again, love has proved more powerful than fear. And I am sure this will continue.
I’m fortunate to be part of an extraordinary team of volunteers, organizing the upcoming Ethics in Innovation conference on April 12th in Silicon Valley.

I would like to extend an invitation to neuroscience, gene editing and AI experts to come and speak about Ethics in Innovation at this conference. It is an invitation to product managers, CHRs and anyone who cares about the cause to join the conference.
Together we can build a world where ethics are no longer a “nice to have,” but built into the fabric of our products, the fabric of our creation. The world needs to face and debate these questions.
Otherwise on the other side of the mirror we may not be sure if we are seeing the face of a robot!
",Does Artificial Intelligence have Consciousness?,0,does-artificial-intelligence-have-consciousness-50332fac1d11,2018-03-08,2018-03-08 20:36:29,https://medium.com/s/story/does-artificial-intelligence-have-consciousness-50332fac1d11,False,677,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Castigliana Cimpian,"Chief Conversation Architect for Bank of America, Meditator",c941aa269568,castigliana,92.0,92.0,20181104
0,,0.0,,2017-12-19,2017-12-19 10:39:44,2017-12-19,2017-12-19 11:51:58,4,True,en,2018-02-04,2018-02-04 13:48:07,44,64fe479e25d3,6.775471698113207,33,2,0,"Before giving machines a sense of morality, humans have to first define morality in a way computers can process. A difficult but not…",5,"
Can we teach morality to machines? Three perspectives on ethics for artificial intelligence
Before giving machines a sense of morality, humans have to first define morality in a way computers can process. A difficult but not impossible task.
Today, it is difficult to imagine a technology that is as enthralling and terrifying as machine learning. While media coverage and research papers consistently tout the potential of machine learning to become the biggest driver of positive change in business and society, the lingering question on everyone’s mind is: “Well, what if it all goes terribly wrong?”
For years, experts have warned against the unanticipated effects of general artificial intelligence (AI) on society. Ray Kurzweil predicts that by 2029 intelligent machines will be able to outsmart human beings. Stephen Hawking argues that “once humans develop full AI, it will take off on its own and redesign itself at an ever-increasing rate”. Elon Musk warns that AI may constitute a “fundamental risk to the existence of human civilization”. Alarmist views on the terrifying potential of general AI abound in the media.
More often than not, these dystopian prophecies have been met with calls for a more ethical implementation of AI systems; that somehow engineers should imbue autonomous systems with a sense of ethics. According to some AI experts, we can teach our future robot overlords to tell right from wrong, akin to a “Good Samaritan AI” that will always act justly on its own and help humans in distress.
Although this future is still decades away, today there is much uncertainty as to how, if at all, we will reach this level of general machine intelligence. But what is more crucial, at the moment, is that even the narrow AI applications that exist today require our urgent attention in the ways in which they are making moral decisions in practical day-to-day situations. For example, this is relevant when algorithms make decisions about who gets access to loans or when self-driving cars have to calculate the value of a human life in hazardous traffic situations.
Moral dilemmas for self-driving cars (Source: MIT Media Lab)
Moral problems in everyday life
Teaching morality to machines is hard because humans can’t objectively convey morality in measurable metrics that make it easy for a computer to process. In fact, it is even questionable whether we, as humans have a sound understanding of morality at all that we can all agree on. In moral dilemmas, humans tend to rely on gut feeling instead of elaborate cost-benefit calculations. Machines, on the other hand, need explicit and objective metrics that can be clearly measured and optimized.
For example, an AI player can excel in games with clear rules and boundaries by learning how to optimize the score through repeated playthroughs. After its experiments with deep reinforcement learning on Atari video games, Alphabet’s DeepMind was able to beat the best human players of Go. Meanwhile, OpenAI amassed “lifetimes” of experiences to beat the best human players at the Valve Dota 2 tournament, one of the most popular e-sports competitions globally.
But in real-life situations, optimization problems are vastly more complex. For example, how do you teach a machine to algorithmically maximise fairness or to overcome racial and gender biases in its training data? A machine cannot be taught what is fair unless the engineers designing the AI system have a precise conception of what fairness is.
This has led some authors to worry that a naive application of algorithms to everyday problems could amplify structural discrimination and reproduce biases in the data they are based on. In the worst case, algorithms could deny services to minorities, impede people’s employment opportunities or get the wrong political candidate elected. Some people have argued that the use of AI in politics already had disastrous consequences.
Thinking about new ways to teach robots right from wrong.
So what can we do about it? Based on our experiences in machine learning, we believe there are three ways to begin designing more ethically aligned machines with the following guidelines:
1. Explicitly defining ethical behaviour
AI researchers and ethicists need to formulate ethical values as quantifiable parameters. In other words, they need to provide machines with explicit answers and decision rules to any potential ethical dilemmas it might encounter. This would require that humans agree among themselves on the most ethical course of action in any given situation — a challenging but not impossible task. For example, Germany’s Ethics Commission on Automated and Connected Driving has recommended to specifically programme ethical values into self-driving cars to prioritize the protection of human life above all else. In the event of an unavoidable accident, the car should be “prohibited to offset victims against one another”. In other words, a car shouldn’t be able to choose whether to kill one person based on individual features, such as age, gender or physical/mental constitution when a crash is inescapable.
2. Crowdsourcing human morality
Engineers need to collect enough data on explicit ethical measures to appropriately train AI algorithms. Even after we have defined specific metrics for our ethical values, an AI system might still struggle to pick it up if there is not enough unbiased data to train the models. Getting appropriate data is challenging, because ethical norms cannot be always clearly standardized. Different situations require different ethical approaches, and in some situations there may not be a single ethical course of action at all — just think about lethal autonomous weapons that are currently being developed for military applications. One way of solving this would be to crowdsource potential solutions to moral dilemmas from millions of humans. For instance, MIT’s Moral Machine project shows how crowdsourced data can be used to effectively train machines to make better moral decisions in the context of self-driving cars.
3. Making AI systems more transparent
Policymakers need to implement guidelines that make AI decisions with respect to ethics more transparent, especially with regard to ethical metrics and outcomes. If AI systems make mistakes or have undesired consequences, we cannot accept “the algorithm did it” as an adequate excuse. But we also know that demanding full algorithmic transparency is technically untenable (and, quite frankly, not very useful). Neural networks are simply too complex to be scrutinized by human inspectors. Instead, there should be more transparency on how engineers quantified ethical values before programming them, as well as the outcomes that the AI has produced as a result of these choices. For self-driving cars, for instance, this could imply that detailed logs of all automated decisions are kept at all times to ensure their ethical accountability.
How can moral values be measured and optimised?
Next steps for moral machines
We believe that these three recommendations should be seen as a starting point for developing ethically aligned AI systems. Failing to imbue ethics into AI systems, we may be placing ourselves in the dangerous situation of allowing algorithms to decide what’s best for us. For example, in an unavoidable accident situation, self-driving cars will need to make some decision for better or worse. But if the car’s designers fail to specify a set of ethical values that could act as decision guides, the AI system may come up with a solution that causes more harm.
This means that we cannot simply refuse to quantify our values. By walking away from this critical ethical discussion, we are making an implicit moral choice. And as machine intelligence becomes increasingly pervasive in society, the price of inaction could be enormous — it could negatively affect the lives of billions of people.
Machines cannot be assumed to be inherently capable of behaving morally. Humans must teach them what morality is, how it can be measured and optimised. For AI engineers, this may seem like a daunting task. After all, defining moral values is a challenge mankind has struggled with throughout its history. If we can’t agree on what makes a moral human, how can we design moral robots?
Nevertheless, the state of AI research and its applications in society require us to finally define morality and to quantify it in explicit terms. This is a difficult but not impossible task. Engineers cannot build a “Good Samaritan AI”, as long as they lack a formula for the Good Samaritan human.
About the authors: Jane Zavalishina is the CEO of Yandex Data Factory, a provider of AI-based solutions for industrial companies. Jane is a frequent speaker on the topics of AI business strategy and applications at various events in Europe, Middle East and Asia. She serves on the World Economic Forum’s Global Future Councils. In 2016, Jane was named in Silicon Republic’s Top 40 Women in Tech as an Inspiring Leader and recognised by Inspiring Fifty as one of the top 50 most inspirational women in the technology sector in the Netherlands.
Dr Vyacheslav Polonski is a researcher at the University of Oxford, studying complex social networks and collective behaviour. He holds a PhD in computational social science and has previously studied at Harvard, Oxford and LSE. He is the founder and CEO of Avantgarde Analytics, a machine learning startup that harnesses AI and behavioural psychology for the next generation of algorithmic campaigns. Vyacheslav is actively involved in the World Economic Forum Expert Network and the WEF Global Shapers community, where he served as the Curator of the Oxford Hub. He writes about the intersection of sociology, network science and technology.
Earlier versions of this article were published on the Net Politics Blog of the Council on Foreign Relations on 14 November 2017, the World Economic Forum Agenda on 23 November 2017 and the official blog of the BCG Centre for Public Impact on 12 December 2017. The article was also translated into French and Polish in other online media outlets.
",Can we teach morality to machines? Three perspectives on ethics for artificial intelligence,260,can-we-teach-morality-to-machines-three-perspectives-on-ethics-for-artificial-intelligence-64fe479e25d3,2018-06-21,2018-06-21 01:08:38,https://medium.com/s/story/can-we-teach-morality-to-machines-three-perspectives-on-ethics-for-artificial-intelligence-64fe479e25d3,False,1610,,,,,,,,,,Ethics,ethics,Ethics,7787.0,"Vyacheslav Polonski, PhD",UX @Google | Forbes 30 Under 30 | DPhil | Global Shaper & Expert Network @WEF | Prevsly @UniofOxford @OIIoxford @Harvard @LSE @avantanalytics,ba50eee34de9,drpolonski,982.0,399.0,20181104
0,,0.0,7f60cf5620c9,2018-05-24,2018-05-24 12:46:53,2018-05-24,2018-05-24 10:00:50,1,False,en,2018-05-30,2018-05-30 17:15:22,3,c82bb485bc54,5.226415094339623,22,1,0,"When designing a system to be more intelligent, faster or even responsible for activities which we would traditionally give to a human, we…",4,"AI — The control problem
When designing a system to be more intelligent, faster or even responsible for activities which we would traditionally give to a human, we need to establish rules and control mechanisms to ensure that the AI is safe and does what we intend for it to do.
Even systems which we wouldn’t typically regard as AI, like Amazon’s recommendations engine, can have profound effects if not properly controlled. This system looks at items you have bought or are looking to buy. It then suggests other items it thinks you are likely to additionally purchase which can result in some pretty surprising things — like this:

Looking to buy a length of cotton rope? Amazon might just recommend that you buy a wooden stool alongside it. As a human, we would not suggest these two items alongside each other. However Amazon’s algorithm has seen a correlation between people who bought cotton rope and those that also bought wooden stools. It’s suggesting to someone buying the rope that they might want a stool too with the hope of raking in an extra £17.42. At best, this seems like an unfortunate mistake. At worst, it’s prompting extremely vulnerable people and saying ‘why not? This happens all the time? Why don’t you add the stool to your basket?’.
If this can happen with a recommendation algorithm, designed to upsell products to us, clearly the problem is profound. We need to find a reliable means to guarantee that the actions taken by AI or an automated system achieve a positive outcome.
Solutions?
Terminal value loading
So, why don’t we just tell an AI to protect human life? That’s what Isaac Asimov proposed in ‘I Robot’. Here are the three laws;
A robot may not injure a human being or, through inaction, allow a human being to come to harm.
A robot must obey orders given it by human beings except where such orders would conflict with the First Law.
A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.
They sound pretty watertight. Adding in no injury through action or inaction seems to avoid a dystopia where AI takes over and lets the human race finish itself off.
Despite how good these laws sound, they don’t work. Asimov wrote these laws for use in novels, and the novels were much more interesting when things went wrong. Otherwise we might have ended up with a book of ‘Once upon a time, the end’.
There’s a 4th law, the ‘Zeroth Law’ added by Asimov . This extra rule was supposed to fix the flaws of the other three, the ones that gave Will Smith a bad day. I confess, I’ve not read the book, but I understand that one didn’t go so well either.
The rules don’t even have to refer to people to be a risk. They could be about something really mundane. Take the idea of a paperclip maximiser, an idea put forth by Nick Bostrom. This would be a machine made by a hypothetical future human race to manage paperclip creation. Paperclips are just a simple resource and seemingly don’t need a ton of consideration to make them safe, if we tell the AI that it’s purpose is to make paperclips, and that’s just what it does.
But what if we end up with a super intelligent system, beyond our control, with the power to rally the resources of the universe making paperclips? If this system, whose priority is turning everything it around it into paperclips, sees its creators attempts to prevent it reaching this goal, the best bet is to eradicate them. Even if it doesn’t decide to eradicate them, those humans are still made out of valuable matter which would look much nicer if it was turned into a few paperclips, so turn them into paperclips it shall.
How do we change that terminal value? Tell the machine to make 1,000 paperclips instead of turning the entire universe into paperclips? Unfortunately, it’s not much better. That same AI could make 1,000 paperclips, then proceed to use all the resources in the observable universe (our cosmic endowment) to make sure that it’s made exactly 1,000 paperclips, not 999 or 1,001, and that those paperclips are what its creator intended for it to make, and all of the perfect quality to satisfy their desire.
It might not even be fair to give a super intelligent machine such a mundane terminal value– assuming we find a way to make its value remain constant despite becoming extremely intelligent.
Here I am with a brain the size of a planet and they ask me to pick up a piece of paper. Call that job satisfaction? I don’t.
Marvin — Hitchhiker’s Guide to the Galaxy, by Douglas Adam
TL;DR — Terminal values don’t seem to work well.
Indirect normativity
Instead of giving a machine a terminal value, could we instead indirectly hint towards what we want it to do?
If we managed to perfectly sum up in terminal value what morality meant to the human race in Viking times, we might have an AI which prizes physical strength very highly. We might think we’ve reached a higher ethical standard today but that’s not to say 1,000 years from now we will not look back on the actions we are taking were ignorant. Past atrocities happened on human timescales, with only human level intelligence to make them happen. Doing it orders of magnitude faster with a machine may well be worse and irreversible.
With indirect normativity we don’t even try to sum up that terminal value; instead we ask a machine to figure out what we want it to do. Using something like Eliezer Yudkowski’s ‘Coherent Extrapolated Volition’ which asks that an AI predict what we would want it to do if “if we knew more, thought faster, were more the people we wished we were, had grown up farther together”
Rather than following whatever ethical code we have at the time of releasing the AI, we create something which grows and changes as we do, and creates the future which we’re likely to want rather than a far more extreme version of what we have today.
There’s perhaps still some overlap between this system and terminal value loading, and contradictions that the systems would find. If a machine is asked to do whatever is most valuable to us, and prizes making that correct decision over anything else, perhaps its decision will be to take our brains out, put them on a petri dish and figure out exactly what we meant for it to do. A clause like ‘do the intended meaning of this statement’ would seem to lessen the concern, but again, to know what we intend the machine needs to be able to predict out behaviour.
A perfect prediction system would look a lot like a ‘Black Mirror’ episode. Using an application without a second thought to manage your home automation or to find your next date. Not knowing that the machine is simulating thousands of thinking and feeling human minds to make an accurate prediction of your desires and behaviours, including all the pain that those sentient simulations feel when being torn apart from one another on thousands of simulated dates to gauge how likely you are to stay together against all odds.
The control problem is extremely tricky, and looks for answers to questions which philosophers have failed to reach a consensus on over thousands of years of research. It is imperative that we find answers to these questions, not just before creating as Super Intelligent AI, but in any system that we automate. Currently the vast majority of our resources and effort is put into making these systems faster and more intelligent, with just a fraction focused towards the control problem or the societal impact of AI and automation.
Let’s redress the balance.
Originally published at blog.soprasteria.co.uk on May 24, 2018.
",AI — The control problem,116,ai-the-control-problem-c82bb485bc54,2018-07-02,2018-07-02 18:51:18,https://towardsdatascience.com/ai-the-control-problem-c82bb485bc54,False,1332,"Sharing concepts, ideas, and codes.",towardsdatascience.com,towardsdatascience,,Towards Data Science,,towards-data-science,"DATA SCIENCE,MACHINE LEARNING,ARTIFICIAL INTELLIGENCE,BIG DATA,ANALYTICS",TDataScience,Ethics,ethics,Ethics,7787.0,Ben Gilburt,"I like technology and philosophy, and love things that involve both.",94d19a699148,benjamin.gilburt,50.0,11.0,20181104
0,,0.0,7ad53ef01cf3,2018-01-21,2018-01-21 22:56:07,2018-01-22,2018-01-22 07:25:49,1,False,en,2018-01-22,2018-01-22 07:25:49,7,54a71a75875c,5.588679245283019,33,0,0,"Interview with Dr. Hanie Sedghi, Research Scientist, Google Brain",4,"Ethics in Machine Learning
Interview with Dr. Hanie Sedghi, Research Scientist, Google Brain
On a not-very-sunny day in our Golden State of California, I sat down (virtually) with Dr. Hanie Sedghi to discuss the topic of ethics in Machine Learning. Born and raised in Iran, Hanie is a research scientist at Google Brain, based in Mountain View. Prior to joining Google Brain, Hanie worked at Allen Institute for Artificial Intelligence in Seattle as a research scientist for two years. She earned her PhD from University of Southern California in Spring 2015. Her dissertation topic is Stochastic Optimization in High Dimensions.
Dr. Hanie Sedghi, Research Scientist, Google Brain (Credit: Hanie Sedghi)
Hanie moved to the US in 2010 after earning B.Sc. and M.Sc. degrees in Electrical Engineering from Iran’s most prestigious engineering school, Sharif University of Technology.
After introducing me to her cute Persian cat, Misha— she actually brought him from Iran — we jumped right in. As a side note, Hanie and I talked in our native language, Farsi. Below I have transcribed and translated our conversation, which Hanie reviewed for accuracy.
Roya Pakzad: Hanie, tell me in your own words, what is fairness in machine learning?
Hanie Sedghi: There are many examples and definitions. But let me start with one very well-known example: algorithmic bias in criminal justice systems. In this situation, if we disproportionately feed data about crimes committed by African Americans into a crime prediction model, then of course our model’s prediction will be biased against black communities. This is a skewed sample. On the other hand, if minority groups are underrepresented in our data samples, for example women or transgender individuals, then again, we can expect that our model will have a bias.
Roya: Are there any mathematical solutions for addressing that?
Hanie: To some extent yes. There are three different approaches:
1) Post processing in terms of calibration of our model. What this means is that, we calibrate classifiers parameters such that it has the same acceptance ratio for all subgroups of sensitive features, e.g. race, sex, etc.
2) Data resampling to remove skewed sample. But, for many reasons, collecting more data is not very easy and sometimes causes problems for individuals.
3) Causal reasoning: We capture different paths in a causal graph that can lead to the same observational data. This basically means to model possible factors such that sex, race and other sensitive features to make sure their impact is captured and does not directly affect the result variable.
Roya: What are some of the challenges in modeling fairness in decision-making algorithms?
Hanie: One very important issue is the lack of a concrete definition of “fairness.” I can tell you that there are a number of definitions and sometimes research groups are not on the same page when it comes to the definition of fairness. When you don’t have a clear definition, then how can you model it correctly?
The other issue is the need for collaboration between social scientists and AI researchers. You know, you can’t expect AI researchers themselves to come up with a clear understanding of fairness. Not only we need people in social sciences to collaborate with us in defining these words, but also we need to keep this collaboration all along to the end of the product research and development.
“One very important issue is the lack of a concrete definition of fairness.”
But it’s important to note that some collaborations between AI researchers and social scientists are already underway. For example, Solon Barocas (Cornell University) and Moritz Hardt at UC Berkeley have been working on the issue of defining and modeling fairness in active collaboration with social scientists.
Roya: But they are academics. They are not expected to meet corporate production deadlines or offer super-practical and financially-feasible solutions. They might not be under pressure from the AI competition that we are witnessing in the big tech companies. How do you think we can encourage data scientists and engineers to be aware of these issues who might be under tough deadline pressures?
Hanie: Well, there should definitely be some sort of training on ethics and fairness for them.
Roya: How to make sure those trainings are effective? I, myself went through some types of company’s mandatory online 45 minute quizzes on ethics in the workplace. I confess I clicked all answers until I get the right one, without being actually trained!
Hanie: I understand. This is a question that I can’t answer but I also would really like to know how to achieve this. But I do know that, with respect to bias and fairness, as long as our training is in the form of someone lecturing about the basics of gender or racial bias in society, that training is not likely to be effective. When you warn us about the issues without giving any solutions or approaches to resolve it, then how can that training be fully effective?
I also think first we should have a practical guideline. Currently, there is not any comprehensive guideline (that is actually implementable) in the field of ethics in machine learning. Let me give you an example: differential privacy. It means making sure that the data you have do not reveal the identity of the persons to whom the data belongs. It has a clear mathematical definition. So, for example, once for the Netflix Prize competition, to build a recommender system, we, as researchers, were able to identify that their dataset violates differential privacy. But for fairness, we currently do not have this.Consequently, it cannot be easily taught and monitored, and audited.
Roya: You are an Iranian and you finished your B.Sc. and M.Sc. in Electrical Engineering. You also worked in the industry. How do you see the state of AI research in Iran?
Hanie: Well, I was in Iran recently and had a chance to talk to some AI researchers there. There are great start-ups working mostly on developing apps similar to everything we use here. But regarding AI research, the progress is behind. Machine Learning itself is still relatively a new field. One reason for the gap could be the US sanctions against access to certain research papers, and, more importantly, to cloud computing platforms.
Roya: I’m sure you know better than me that there is a lack of representation of women in tech. There is also a high rate of women leaving the tech industry. How can we address both issues?
Hanie: Regarding lack of representation, I think we should motivate girls from earlier ages in the STEM fields. There are great initiatives by Google and Twitter currently underway. For example, they invite young girls to their campuses and give them small projects that motivate them to think about machine learning as a field that is capable of solving many real world problems.
Having strong role models is important too. Sometimes this lack of representation leads to lack of self confidence in women. They might feel frustrated about not being heard and intellectually respected. Eventually, they might leave the field. So, it is very important to have strong female role models to look up too. One of my own childhood role models was Marie Curie. In ML also, I had great role models such as Anima Anandkumar and Jennifer Chayes. I learned a lot from their strong personalities and self-confidence.
Roya: A final question: are you concerned about the future of AI?
Hanie: I don’t believe in picturing AI as a devil. We are the teacher and we have control over what we teach. I believe that Machine Learning has great potential to make our life easier, from simple tasks such as weather forecasting and AI assistants to self driving cars. But I do think that we still have a lot of work to make AI intelligence reach anything approaching the level of humans. For example, you can have your AI answer your questions but you cannot yet expect it to fully reason. There are still many many difficult unanswered challenges and that’s what making this field the most interesting to me!
We wrapped up here. This is the first conversation of my interview series for my newsletter Humane AI. I will continue talking with both policy and technical experts in the field of ethics of AI in future installments. Tune in to know their opinions about many issues including cybersecurity and AI, private sector approaches in addressing such ethical challenges, Human rights and AI for social good, and much more. To subscribe to the newsletter, click here.
You can find Dr. Hanie Sedghi on Twitter and LinkedIn.
",Ethics in Machine Learning,191,ethics-in-machine-learning-54a71a75875c,2018-06-02,2018-06-02 01:18:11,https://medium.com/s/story/ethics-in-machine-learning-54a71a75875c,False,1428,Humane AI is a newsletter about social and human rights implications  of Artificial Intelligence. Subscribe here: https://www.royapakzad.co/newsletter/.,,,,Humane AI,,humane-ai,,royapak,Ethics,ethics,Ethics,7787.0,Roya Pakzad,Research Associate at Stanford's Global Digital Policy Incubator. Passionate about technology and human rights.,5ca5ed7f1a66,RoyaPak,100.0,88.0,20181104
0,,0.0,,2018-04-06,2018-04-06 22:50:20,2018-04-25,2018-04-25 04:58:46,3,False,en,2018-04-25,2018-04-25 15:02:36,11,bf18022cdc94,12.036792452830188,6,1,0,"Artificial Intelligence… what is this buzz, this tech, this threat?!",3,"Understanding AI
Artificial Intelligence… what is this buzz, this tech, this threat?!
I’ve been studying, researching, and working with artificial intelligence (AI) since 2001. While only a handful of people would engage me in a discussion about AI five years ago, I now get into serious discussions on the topic with people of all backgrounds on almost a weekly basis — even with my family! I’ve also met a number of people increasingly curious about AI asking me if I could help them wrap their heads around this increasingly main-stream, hi-tech, exciting but also, at times, scary science. Hence the impetus for attempting to cover the topic in this article. I cannot do justice to a 62 year old field in a single post but I draw lines covering its origins, what it means in 2018, and what it could become. I also try to describe the field in simpler terms, sometimes violating the rigour of the actual science.
First — I’m a scientist entrepreneur in Toronto, Canada — probably one of the hottest hot beds for AI. Toronto is AI hot because it is one of the birthplaces — thanks primarily to Geoff Hinton who pioneered modern techniques for building and training what has now become one of the most powerful tools in AI today: Artificial Neural Networks (more about those later). Toronto now boasts its very own institute for AI (The Vector Institute) and burgeoning small, mid-size, and large industry activities in the sector — not to mention the rich set of academic experts in the field from Toronto but also from other Ontario Universities as well as the rest of Canada.
A Bit of Background
Artificial Intelligence is really about understanding intelligence — prefixing with artificial is simply because we, humans, are trying to understand it and more importantly, are trying to build it. Because “building is understanding” for many scientific endeavours — and perhaps especially true for phenomena as complex as intelligence. The off shoots of scientific discovery are the engineering applications — hence the growing number of real world problems applying this brave new science.
Classically, from the mid 50’s up to about the mid 90’s, intelligence was understood as problem solving. AI was driven by rule-based systems as a direct result of the emergence of digital computers — this era is known as “good old fashion AI” or GOFAI for short. The idea was to build computer programs to solve general problems. In simple terms, a computer can perform sophisticated tasks to represent facts about the world and decide how to create new facts based on its memory where it retrieves initial facts that were (usually) provided by a human. Facts and decide are the two keywords here. Rule-based means that the deciding that the computer makes depends on how it evaluates a fact to be true or false based on other facts that it has already seen. That’s really powerful, in fact its at the foundation of what computers do. Computers, also known as Turing Machines (from their inventor Alan Turing), can actually represent anything about the world…ANYTHING!

Whatever you can experience or imagine can be represented in a Turing Machine — yes even creativity, imagination, and emotion. Why? Because, it can represent the most fundamental physical phenomena (think sub-atomic) to the most macro scale phenomena (astrophysics) and everything in between (and likely beyond). Some will even say (Stephen Wolfram for instance) that the real-world is itself computation — and I agree.
We can build really amazing computer programs that can manipulate facts and make complicated decisions based on either small amounts of data, or very large amounts of data. The biggest problem for this approach is to figure out what those data mean — in other words how they relate, transform, combine with other things in the world. Rule-based GOFAI systems depend on a very clear representation for the facts (data) that are built directly on computer logic “if this then that” rules. But humans have to tell computers (by writing programs) either what the “this” and the “that” are or how they should related, be transformed, or combined for them to do useful things in the world.
That works great for well defined data and usually data in limited variety, because humans would have to initially define all those initial rules and facts clearly for systems to work. Often you’ll hear people call this the Top-Down approach to AI. Chess — no problem… Information retrieval — no problem… Complex logical reasoning — no problem… Navigating across an office space — Uh Oh! Not so good.

Oddly, before our computer era, a strange but exciting field with roots in engineering and psychology emerged in the 30’s, 40’s, 50's but quickly vanished when computers became the hot new thing — this was the short era of Cybernetics. These folks (Wiener, Ashby, and others) pioneered ideas that are probably the true inspiration, combined with computing, for modern AI. Themselves inspired by engines, industrial machinery, and the emergence of psychology, these folks formulated theories and ideas around how intelligent systems are in many ways about information, regulation, feedback, and control. These are systems that dealt with a messy world and did well doing so. Grey Walter’s tortoises are a good example of robotics from that era — completing simple tasks that even computer driven robots, 30 years later, struggled with. Discovering that intelligence is not just an exercise in logic but that dealing with the real messy world matters, is perhaps one of the most important lessons from this era. Working with the messiness of the world is what you’ll hear some call the Bottom-UP approach to AI.
As psychology and neuroscience progressed and ideas from folks like Donald Hebb (also from Toronto) in the 40’s led to a better understanding of the biological underpinnings of intelligence. Mathematical and computer based experimentation on modelling neural networks began. But let’s side step for a moment as another crucial field of work should also be recognized as immensely valuable for AI: Statistics or as statisticians like to call it “the science of uncertainty”.
What better way to deal with a messy world than to approximate it? If you can figure out how data is distributed you don’t actually need the data! At least not for most tasks. Since the 1700’s, Thomas Bayes and other mathematicians developed a rich set of equations that can help computer scientists represent the world approximately well enough that the “this” and “that” of the world can be detected by computer programs and not defined by the computer programmers! Navigating across an office space? No problem!
Moving forward to the 1950’s, a quiet set of AI scientists began work on neuroscience inspired models now known as Artificial Neural Networks. Shortly proven in the early 80’s that they could implement ANY FUNCTION… yes even including those nice statistical functions! To do so they needed layers of “neurons” and well adjusted connection weights. In Artificial Neural Network jargon, scientists speak of Nodes (representing biological neurons) that propagate a numeric value via Edges (representing axons and dendritic connections) to other Nodes. This is fast in real biological brains, but rather slow in comparison for computers. So Neural Nets took a little bit of time to become practical, leaving way for increasingly more sophisticated statistical models to solve the messy world problems in the 90’s, 00’s and still today.
As computers became more powerful, more connected, and data more available in the 2000’s, larger and increasingly more complex neural networks could be build that implemented complex statistical models that even the best statisticians could not easily formulate on a white board. While complex Neural Network architectures (blue prints) become easier and easier to design by humans today (with tools like Theano, TensorFlow, PyTorch, etc.), the network weights and activation functions (numeric values functions) can be adjusted automatically using very ingenious methods that make the entire network capable of representing layers and layers of features (not unlike facts) of the world, transforming them, and relating them. These are incredibly powerful for things like recognizing objects in the real world — such as recognizing faces or detecting lung tumors in medical imaging, building associations between things — for example connecting words in sentences to derive meaning, generating new patterns to represent simpler associations — for example labelling all cars in a scene as red blobs, or decide on a course of action — for example learning what impact an action has on things, like trading stocks.
AI Today
The field of Neural Networks is wonderful and fast moving, and what we can do with them today is just the beginning. But a frenzied technology race to developing new applications with them does not mean we are just as quickly moving towards achieving advanced intelligence. Building better classifiers, better reinforcement learners, greater discriminators, etc. does not necessarily mean we have built systems as capable as biological brains, but rather that we have increasingly better tools in our toolbox. Let’s get back to intelligence for a moment.
You might come across a lot of folks working in or around Artificial Intelligence but if you ask them what Intelligence is, they likely won’t have a common answer. That’s because we generally have a good intuition as to what intelligent things do but not how they work. Yes, our intuitions that intelligence is about thinking, reasoning, problem solving, having beliefs, desires and fears etc. are not wrong… but what does it mean to think, to reason, to believe? How does it work?
Perhaps a more elegant definition of intelligence comes from Cybernetics and Biology: “intelligence is adaption”. It’s a simple yet powerful idea. When changes in the world occur, you need to adapt to survive. Intelligent systems do not try to become the optimum problem solver, but a sufficient problem solver for the task at hand. The problem with the notion of adaptation is that vagueness remains as to what the mechanisms of adaptation are. Biological systems have evolved a myriad of different solutions to adapt to changing environments and animals evolved brains to be more efficient at coping with this — think food scarcity, predators, shelter, societal change, etc. Alternatively, the computational era introduced another powerful notion: representation.
You might have noticed that I’ve used the word representation a number of times to explain in a hand wavy way what the various approaches to AI are essentially trying to achieve. Perhaps the most standard view in AI and Cognitive Science (the science of intelligence), is that brains build representations of the outside world so that decisions about that world and actions in that world can be taken (this is known as the Representational Theory of Mind). In other words, intelligent system build models of the world so that they can reason about those models of the world. This is not unlike the idea of generating facts and making decisions discussed earlier for classical AI but today understood more generally as manipulating quantities and using functions as we work with the messy world. While this helps us get closer to to thinking about how intelligence works, it still doesn’t seem quite satisfying enough as a way to define, or at least frame, intelligence.

A fundamental insight that we get by combining the idea if adaptation and representation is that intelligent systems (animals, humans, computers) are able to build on previous experiences, anticipate future events, and manipulate their body or environment to achieve some goal (such as survival, detecting tumors, having conversations). Personally, this is my preferred way of thinking about Intelligence — that is, to think that a system can learn, predict, and control then it is, to some degree, intelligent. If a system cannot learn new facts, then it is stuck with what it knowns at the very beginning of its life and that’s it. If a system can learn it can generalize and apply what it learned to new circumstances. Even more advanced is when a system can anticipate future events. By using what it has learnt and by generating new ideas, it can evaluate potential situations it has never even seen in the past. If a system cannot predict, then it will never anticipate how things will be when new events take place. Perhaps most interesting, a system that can control its body or aspects of the environment can manipulate what it experiences to meet its needs. However, if a system cannot control, then it cannot test new predictions on its own — it might learn and predict, but someone else will have to tell it if taking a certain action was actually the right thing to do. These three abilities seem very powerful when combined and, taken together, they form perhaps one of the best current frameworks to help us understand how intelligence works.
The field of Machine Learning (ML) for instance — as the name says — is focussed on learning: you provide a “machine” (for example a neural network algorithm) both some input data and some output data that you would expect. The system then learns to map that input to the output. Now you can use that “machine” for new data you’ve never seen and it will give you the output that you would generally expect! For example, I could tell my email app what are important emails and the next time it receives an important email it could highlight it for me automatically.
Simple AI systems that do some learning are quite common, prediction is a bit more difficult, but a model that learns a good representation of the world can often generalize to other similar situations in a way that satisfies many problems in the world without the challenge of creating new representations. Controlling that world, such as moving your hand to grab food, or speaking to your friend about a concert to go to, or prescribing a treatment for your patients are the end result of quite complex intelligent processing. But the control mechanism itself, might not be very complex. Turning a wheel, for instance, might be a form of a simple control action. What’s fascinating however, is that the changes we make in our world can actually make our representation of the world either simpler or more complex — that is, control can simplify our predictions! If they make things simpler, we often have something to gain, but sometimes you need to make them more complex, and wait to gain something even greater down the road!
Modern AI is achieving learning increasingly well with methods such as Deep Learning (Neural Network with many layers of nodes) but many other methods of learning exist (including genetic evolution!). When it comes to prediction and control the field of Reinforcement Learning has perhaps made some of the greatest advances in the field to date.
Where Are We Headed?
While our AI models today remain rather rudimentary compared to the sophistication of animal or human brains, many effective algorithms for building intelligent systems that can learn, predict, and control (LPC) have been established. The challenge for modern AI is in the details of the recipe. Do we have all the right kinds of algorithms? — for example are input/output functions for LPC enough or will timing and dynamics also play a key role? How do we put the building blocks together? — what data or environments need to exist for these systems to improve iteratively? — how do we build new abilities without losing old ones? — what kind of hardware do we need and can we build them? While research in areas such as parallel processing and neuromorphic computing are exciting new avenues to test increasingly more sophisticated and fast AI systems, we still have a long way to go to complete the recipe — speed is not equal to complexity. The rise of an artificial over-lord is not going to happen tomorrow. But that doesn’t mean we will never build human or super-human like intelligence systems one day.
Because of this, the ethics of Artificial Intelligence is an important topic and governments are even creating avenues for public discussion today. Like all great scientific discoveries, our knowledge can be used for good or bad. Already, we’re seeing important advances in the field where AI systems are starting to make decision instead of humans — and thereby transferring responsibilities for actions being made. Take for instance self-driving cars and the associated responsibility for safety.
This last point is crucial and relates back to the earlier point made about control. We can build incredibly good learning and predicting AIs at no risk. Where AIs (programs, robots, networks) become a potential risk is when we give them control. Careful regulations and guidelines should be implemented and followed by scientists, engineers, companies, governments, and people when they transfer control over to autonomous intelligent systems. Now the question is: what should those guidelines be? How dependent are they on the field of application — think email AI versus weaponized AI? What are the risks and risk mitigations that exist in the worst case scenarios? As the race for AI speeds-up, people and governments will have to educate themselves on the potential risks created as AIs are given more control and discuss what limits should be in place.
In spite of the potential risks, we should not overlook the incredible gains that AI can help deliver — such as helping reduce medical mis-diagnoses near zero; foster new generations to train in human centric and technical occupations while tedious repetitive tasks become automated; transform complex information into digestible form for better decisions making; create simulations of highly complex scenarios in seconds; drive research in physics and chemistry sciences to solve our energy and environmental challenges; solve problems for greater resource efficiency and drive better global cooperation; expand to the solar system… to name a few.
I’m personally very excited about his field and I believe we can build AI to help humanity and our planet. There are risks involved, but with care, we can turn AI into an true positive sum technology.
[ Illustrations by Ginnie Chen. Thank you to Shiva Amiri and Arash Samini for their valuable feedback. ]
",Understanding AI,62,understanding-ai-bf18022cdc94,2018-05-02,2018-05-02 19:40:37,https://medium.com/s/story/understanding-ai-bf18022cdc94,False,3044,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Francis Jeanson,"Founder at iamopen.ai — merging ai & data. Former big data manager at the Ontario Brain Institute. PhD : cognitive science, neural coding, evolutionary systems.",974585d15ba6,fjeanson,4.0,5.0,20181104
0,,0.0,3a8144eabfe3,2017-11-17,2017-11-17 13:53:57,2017-11-29,2017-11-29 13:11:43,3,False,en,2017-11-29,2017-11-29 13:11:43,2,b2b301878a4b,4.029245283018868,16,0,0,People are afraid of AI: there’s scaremongering from the media; any movie which depicts AI ends in disaster; Elon Musk keeps banging the…,5,"Be Careful What You Wish For

People are afraid of AI: there’s scaremongering from the media; any movie which depicts AI ends in disaster; Elon Musk keeps banging the drum about the risks of AI. I’d love to tell you that everything’s going to be ok, but there’s real cause for concern.
It’s not that machines will wake up one day and decide that they’d be more efficient without pesky humans getting in the way. The risk is that we won’t be able to tell the machines what we want them to do.
It’s important to understand, at least superficially, how artificial intelligence works before we dig into the risks. Let’s use face detection as an example. We show the software lots of images and tell it which ones contain faces. It then finds generalisations between all the images. It might decide it should look for a group of pixels which look a bit like an eye, and then look for another group of eye pixels in close proximity. It might also look for some pixels which look like a mouth underneath the eyes.
Every time we show it a new image, the AI will guess whether it sees a face. We train it by rewarding it for getting it right and correcting it when it gets it wrong. Because we’ve programmed the machine to try to get the most rewards, it adjusts its model (what it thinks faces look like) when we give it feedback. This is where the danger lies.
Remember King Midas, the ancient Greek, who wished everything he touched would turn to gold? “Fantastic!” he thought as he created unimaginable riches in his wake. He soon grew peckish though, as you would skipping gleefully, thinking that you’d never want for money again. Eating soon proved to be tricky, as all of the food he reached for turned to gold and became inedible.
It’s this similar inability to convey our wishes to machines which will haunt us. I want to create a robot to make coffee. In training, I rewarded it every time it made coffee and I told it to try to get as many rewards as much as possible. I’ve accidentally taught it to single-mindedly make coffee until there’s no more coffee to make! We need to tell it to pause from time to time, to let us drink the coffee at the same rate as it’s made.
Maybe Midas could have said “I wish everything I touched would turn to gold, unless I want to eat”. That’s closer to what he actually wanted, but what would have happened if his daughter fell over and needed comforting? That’s another exception. And there’d be another, and another, and another. It’s not possible to wrangle this immense power by describing the desired outcome. There are too many exceptions.
Facial recognition software or coffee-making robots are unlikely to present a material threat to human life. The scenarios highlighted do show some potential problems though.
In the near future, we’ll have robot doctors. Let’s hope whoever creates one doesn’t give it a goal of having the fewest number of clinical complaints. Easiest way to reduce complaints? Refuse to treat anyone.
We’ll soon have self-driving cars. Let’s hope whoever creates them doesn’t give them a goal of having the fewest crashes. Easiest way to reduce crashes? Don’t drive anywhere.
Even something as simple as an AI personal assistant could go wrong. Let’s hope whoever creates them doesn’t give them a goal of reducing the number of email interruptions. Easiest way to reduce email interruptions? Direct all mail to the trash.

Always thinking.
Of course, these examples assume that creators only give the AI naïve goals. In reality they’re likely to give them a number of goals to achieve. Reduce the number of clinical complaints and increase the number of patients treated successfully. There’s still hidden dangers though.
However hard we try to understand the logical cause and effects of a combination of goals, they will always be difficult to fully understand.
Automatic computer systems have been trading equities for more than a decade now. They work great, mostly, but there’s are instances where their actions defy reason. On May 6 2010, the Dow Jones Industrial Average lost 9% of it’s value in only a few minutes. It then recovered within half an hour.

It’s still unexplained. It wasn’t a single malicious act that caused this change but a complex network of systems that all worked together in unexpected ways to lower the value of the entire index.
This is what keeps Elon Musk awake at night. It’s not an evil robot uprising that scares him, it’s lots of benign artificial intelligences with their own goals behaving in unexpected ways.
There’s no need to pull the plug on AI research just yet though. We’re able to train dogs — often very effectively — without being able to tell them exactly what we want. They respond to our actions, words and tone to try to figure out what we want them to do. With enough care, we can train AI safely.
Don’t just build something, connect it to the internet and see how it behaves. That’s a surefire way to summon the demon.
I’m a software developer based in Birmingham, UK, solving big data and machine learning problems. I work for a health tech startup, finding creative ways to extract value from our customer data. I don’t have all the answers but I’m learning on the job. Find me on Twitter.
",Be Careful What You Wish For,112,be-careful-what-you-wish-for-b2b301878a4b,2018-05-14,2018-05-14 00:48:24,https://hackernoon.com/be-careful-what-you-wish-for-b2b301878a4b,False,922,how hackers start their afternoons.,hackernoon.com,hackernoon,,Hacker Noon,stories@amipublications.com,hacker-daily,"HACKATHONS,HACKING,PROGRAMMING,TECH,HACKER",hackernoon,Ethics,ethics,Ethics,7787.0,Josh Sephton,Software Engineer,e89e1e69f1,heldtogether,317.0,270.0,20181104
0,,0.0,,2018-07-24,2018-07-24 21:41:24,2018-07-24,2018-07-24 21:47:16,1,False,en,2018-07-24,2018-07-24 21:47:16,3,c7d36015de29,1.4075471698113209,4,0,0,"When we have agents, we have tools that level the playing field between the individual and the corporation.",4,"This call will be recorded
In the new Google Duplex demo, the software agent calls a restaurant. At the start of the call, the agent says, “this call will be recorded.”

I found that jarring. We are accustomed to the business we call recording everything “for quality control and training purposes.” The business is centralized; it has the technology. Training sounds nice (even though those tapes will probably be used in arbitration, or as evidence for HR.)
One of the things we often overlook is the decentralizing power of technology in the right hands. Wikipedia, Craigslist, Google Maps, and hundreds of other apps are triumphs, not tragedies, of the commons. Of course, if the controller of those platforms has nefarious goals, they have a tremendous amount of power, and we need to address that as a society.
But we are also seeing a great levelling as enterprise tech makes its way into our lives. Already, email, calendars, VOIP, task lists, messaging, an office suite, and shared documents — all virtually free — give everyone who wants them access to a tool set that would have been unthinkably powerful in an office 20 years ago.
Imagine a dispute arising between two parties. Today, a business holds most of the cards (and all of the recordings.) It’s surveillance capitalism, everything is a mill and we’re the grist.
I wanted to find a meat grinder for this picture, but apparently nobody wants to see how the sausage is made. So here’s a coffee mill instead. (Photo by Yanapi Senaud on Unsplash)
But if the client brings their own details, the playing field is levelled. More than that: The little guy’s empowered. It’s hard for the restaurant to be secretly racist, giving tables out based on the voice or name of the caller. And if they are, well, there’s a recording — potentially thousands of them — encouraging those wronged to speak out.
Tech is complicated, and it can definitely be abused. But we shouldn’t overlook the levelling that happens when it is widely available. And the implications for evidence, lawsuits, and, well, consequence will be remarkable.
",This call will be recorded,7,this-call-will-be-recorded-c7d36015de29,2018-07-24,2018-07-24 21:47:17,https://medium.com/s/story/this-call-will-be-recorded-c7d36015de29,False,320,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Alistair Croll,"Writer, speaker, accelerant. Intersection of tech & society. Strata, Startupfest, Bitnorth, FWD50. Lean Analytics, Tilt the Windmill, HBS, Just Evil Enough.",b46946f1386e,acroll,12566.0,5595.0,20181104
0,,0.0,,2017-11-28,2017-11-28 13:34:13,2017-12-06,2017-12-06 12:01:02,3,False,en,2017-12-06,2017-12-06 12:01:02,2,527476ab0687,3.7047169811320764,3,0,0,This is post number two of a series on machine ethics. You can find part one here.,5,"Machine Ethics Part 2: First hands-on workshop on Machine Ethics and autonomous cars
This is post number two of a series on machine ethics. You can find part one here.
The role of car in society is very important nowadays, from personal mean of transport to social engagement and indepence. Having a driver’s license is one of the big steps towards making own choices and having more responsibilities.
Many people choose to get their license early, you can start to learn driving before you even graduate from high school. Also the accident risk for young drivers until the age of 21 is much higher than for older generations.
In the best case scenario, autonomous cars will lift especially young people from the responsibility of accidents while still allowing for personal freedom and indepence. But there are a few trade-offs that needs to be mentioned
Trade-off 1: The price of autonomous cars
Trade-off 2: Company-based autonomous cars or car-sharing
Trade-off 3: Steering control
Those trade-offs clearly show that the role of autonomous cars in the future will highly depend on the ethical and social choices that is made by the car providers. Previous workshops on autonomous cars end with the fear that autonomous cars are dangerous and not controllable, while others praise the safety aspect and would love to use it immediately.
In order to test my assumptions and get feedback from the public, I organised an open workshop on Machine Ethics and autonomous cars. On one evening, the participants were invited to listen to two presentations on the topics and later on to take part in the workshop.
The workshop titled “Machine Ethics and the future of AI for autonmous cars” was helping people to properly understand the complex arguments of pro and contra, to show why raising a voice in terms of ethics is not only unavoidable but also the best foundation to build the future.
Recent studies evaluating the public opinion of autonomous cars have come to the following conclusion: There are two groups with the following attitude: Group A prefers safety over price and speed. This group is interested in using the autonomous car for family and friends. Group B desires to purchase a faster car with optimizations and is more interested in business application.
Should we wait till Robocars are 99.9% accurate? This question was asked by Shreyas Gite during his introduction talk on autonomous cars
I devided the workshop into two groups and we achieved the following results (within one hour)
Scenario A:
most important aspects rated by participants
Autonomous car for family usage: The majority of people that discussed autonomous cars for familiy usage was astonished by the improvement of safety and willing to let their kids travel to school on their own. As a parent, this will safe a lot of time and effort, plus it will be safer for the kids, too. After 45 minutes passed, however the participants found also several disadvantages. One of them is the brand control that might happen: Imagine the autonomous car uses the circumstances to advertise Fast Food Brands or other products, by stopping by regularly.
Scenario B: Autonomous car for business usage:
important issues of Group B
The scenario for business usage has had completely different results. The participants came up with more aspects and arguments for both sides. In addition, the social aspect of autonomous cars was left aside. The group had fewer problems finding disadvantages, but was all in all positive about autonomous cars as a useful transport to work.
Results of the workshop
The following conclusions arise from this workshop: The group focusing on famliy usage had more difficulties defining and arguing about positive and negative aspects than the business group. In other words: Logistics, cost optimization, time optimisation are all within grasp of audience. However, questions on safety of the autonomous cars were just developed during this workshop and came up in the last 5 minutes. Not only did group A talk about theft security, but also about possible hacking and the misuse of advertisement.
The audience
We had nearly 20 participants in this workshop mainly between 25 and 35 years old, with only a few woman amongs them. Most of them were using English as a way to communicate, only a few native German-speaking participants joined. Most of the audience was interested in autonomous car and the future of technology, however did not have a strong background in technology. In both groups there were one or two participants with deep knowledge of the topic, who provided new arguments. Once the arguments were on the table, the group discussed them again, so every one had a very good understanding of the issues afterwards.
To find the right problem, we have to know the topic very well.
Several participants talked to me in person and asked for more workshops of this kind, because until then they never had an event like this before.
Last but not least, I would like to thank Shreyas Gite and Remco Bloemen for their presentations and the innogy innovation hub for the sponsorship. Also thanks to OpenEth, without our team this work would not have been possible.
",Machine Ethics Part 2: First hands-on workshop on Machine Ethics and autonomous cars,86,machine-ethics-part-2-first-hands-on-workshop-on-machine-ethics-and-autonomous-cars-527476ab0687,2018-05-14,2018-05-14 14:22:30,https://medium.com/s/story/machine-ethics-part-2-first-hands-on-workshop-on-machine-ethics-and-autonomous-cars-527476ab0687,False,836,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Franziska Lippoldt,,cafbc5888a08,lippoldt331,7.0,29.0,20181104
0,,0.0,,2018-01-25,2018-01-25 18:55:07,2018-01-25,2018-01-25 18:59:14,1,False,en,2018-01-25,2018-01-25 18:59:14,4,bc4dac7312e1,5.162264150943397,0,0,0,"It’s difficult to read, or even talk about technology at the moment without that word “ethics” creeping in. How will AI products affect…",5,"In the future, we could solve all crime. But at what cost?

It’s difficult to read, or even talk about technology at the moment without that word “ethics” creeping in. How will AI products affect users down-the-line? Can algorithmic decisions factor in the good of society? How might we reduce the number of fatal road collisions? What tools can we employ to prevent or solve all crime?
Now, let’s just make it clear from the off: these are all entirely honorable motives, and their proponents should be lauded. But sometimes even the drive toward an admiral aim — the prevention bad consequences — can ignore critical tensions that have been vexing thinkers for years.
Even if we agree that the consequences of an act are of real import, there are still other human values that can — and should — compete with them when we’re assimilating the best course of action.
Let me give an example. Although we should wish to avoid hurting a friend, most of us would set this aside if we had information about that friend’s unfaithful spouse. Here, we would be valuing something like “honesty” or “dignity” above their hurt feelings.
In another (rather more harrowing) illustration, it has been observed that great many of us would refuse to smother a baby, even if allowing it to cry out revealed the hiding place of innocent civilians to a blood thirty militia. Here, we would be privileging the intrinsic value of the child by refusing to use them as a “means to an end.”
In brief: there are times when a universal principle bests the foreseeable consequences when it comes to our ethical concern.
All that said — for the most part — principles and consequential considerations rub along well enough. And it isn’t difficult to see how values like “honesty” and “respect” evolved from our weighing the consequences of dishonesty and disrespect. Nevertheless, there are occasions when they do clash — and we should always be alive to these.
One of those battlegrounds is surveillance.
Artificial intelligence is bringing a new intensity to surveillance as we know it. For the first time, it is turning passive security cameras into dynamic, crime-solving machines. To be clear, it is entirely possible that we’re fast approaching a future in which all public crime could be solved — if only we assent to universal surveillance.
The technology already exists.
The specific development concerns the way we search camera footage for evidence of a crime. Previously, this process was dependent upon the sensory abilities of human beings who would be tasked with surveying as many hours of film as they were able. Obviously, this method has its limitations — our time, our ability to concentrate, our powers of observation (to name but a few). But now the existing, imperfect system is being revolutionized by AI that can skim through hours of surveillance footage instantly, allowing law enforcement to swiftly establish the facts of any crime that occurs within the range of a camera.
Products like IC Realtime have been described as “Google for CCTV”, and The Verge recently described a demo where the technology was asked retrieve video frames of a man wearing red, a “UPS van”, and “police cars” from around 40 cameras placed on an industrial park. The results were incredible.
The company’s CEO commented further on the AI’s potential: “Let’s say there’s a robbery and you don’t really know what happened, but there was a Jeep Wrangler speeding east afterward. So we go in, we search for ‘Jeep Wrangler,’ and there it is.” On-screen, clips begin to populate the feed, showing different Jeep Wranglers gliding past.”
Incredibly, such systems can run on footage from pretty much any camera, and don’t require the internet to work. And, James Vincent writes, they are getting better all the time: “In the same way that machine learning has made swift gains in its ability to identify objects, the skill of analyzing scenes, activities, and movements is expected to rapidly improve.”
He also anticipates a future in which law enforcement can use mugshots and facial recognition to track down perpetrators. In addition, manufacturers are looking at training the system to recognize predictive behaviors and anticipate crime before it occurs….
Okay, so you get it. Clearing-up doubts about the effectiveness of surveillance cameras could lead to their ubiquity, and the introduction penetrative, widespread surveillance. But what’s so bad about that? After all, we’d only search the cameras post-facto, and any move towards the obsolescence of public crime is surely worth wanting?
Indeed, it is. Imagine a world in which all crimes were either entirely deterred or immediately solved. A world without muggings, car jackings, rapes, riots, late night assaults, stabbings, shootings, thefts, and burglaries. All of those vicious, physical crimes that frighten us the most. Surely there is no principle that could override the pursuit of a better society without these hideous things happening in public places?
Well, many would say that — actually — there is. And it is a familiar one: our right to privacy. Linked to human dignity and ideas of liberty, it is a very popular opinion that we should be able to go about our daily lives without being tracked and monitored by external parties. Indeed, privacy is not a niche interest, but a fundamental good enshrined in the legislation of over 150 countries. That’s not something that happens by accident, but rather because it is of deep concern to who we are as people.
And so herein the tension lies. If we protect privacy, it could be at the cost of those who fall foul of crimes (/criminals) that would otherwise have been prevented (/punished). If we introduce universal surveillance, then we compromise the privacy of individuals. Something widely held to play an important role in our intrinsically valuable human existences.
These are two valid ethical perspectives, and glibly favoring one over the other is perilous. Examples like this go to prove that even with the very best intentions, we can do serious harm.
It would be easy at this point to bat the dilemma over to the tech industry. To implore them to find a way to “build in” features to protect our privacy. But this would be wrong, and counterproductive. A company producing state-of-the-art surveillance tech has every right to optimize the ways in which it monitors, and to improve its powers of identification. It is not up to them to protect us from its use.
As Google CEO, Sundar Pichai, commented at Davos in reference to hate speech — it is up to society to dictate reasonable parameters here, not tech firms. Like privacy and crime surveillance, free speech and hate censorship have a complex relationship where indulging too heavily in one can badly impede the other. The stakes are high, and that is why the balance should be determined not by them, but by us.
At the moment, we have time. Security cameras do not have every parking lot, school, and sidewalk within range. Nor do most cameras have adequate angles or sufficiently high-resolution footage, as yet. Current systems also currently struggle with crowds.
Nevertheless, it is critical that as companies develop this technology, we start conversations about what constitutes its reasonable use. Which scenarios should promote safety over privacy, and vice versa?
And, of course, the opposition of principles and good consequences is similarly relevant to swathes of other new technologies — not just to surveillance and social media. So, when we are evaluating the latest AI widget for its ethical permissibility, we must remember that it is not enough to simply reflect on the consequences of its use. A true stress-test must also seek to identify relevant values and principles (be they specific to community, geography, or more general), and make every effort to uphold them.
","In the future, we could solve all crime. But at what cost?",0,in-the-future-we-could-solve-all-crime-but-at-what-cost-bc4dac7312e1,2018-01-25,2018-01-25 18:59:15,https://medium.com/s/story/in-the-future-we-could-solve-all-crime-but-at-what-cost-bc4dac7312e1,False,1315,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Fiona J McEvoy,Tech ethics researcher. Founder of YouTheData.com. Tech issues for non-tech audiences. @YouTheData @FionaJMcEvoy,175ba867e2ad,fionajmcevoy,12.0,12.0,20181104
0,,0.0,,2018-01-15,2018-01-15 16:50:28,2018-01-15,2018-01-15 17:23:27,1,False,en,2018-01-15,2018-01-15 17:25:31,1,15b5fd9c87e2,0.920754716981132,0,0,0,Our Goals and Responsibilities,5,"A Brief Reaction to the Asilomar AI Principles
What steps must we take to ensure the benefit of humankind from new technologies?
The principles developed in conjunction with the 2017 Asilomar conference, namely the Asilomar AI principles, are a set of 23 guidelines that, while specific to artificial intelligence, apply to the broader scope of innovations in technology as a whole.
A key concern that the principles encompass is understanding the underlying logic behind intelligent systems. Already, deep neural network and reinforcement learning research has demonstrated the ability for systems to master difficult tasks, such as object recognition and identification (i.e. determining the location of an object in an image), and playing the classic game of Go. However, the nature of their mastery — how the models are able to accomplish such feats — remains highly elusive. Deep learning models are still often regarded as “black boxes.” Currently, we are able to crudely hypothesize post factum why certain actions occurred. However, such explanations are unsatisfactory where human life is at stake (e.g. healthcare, safety) — the very areas in which advancements in AI could yield the most human benefit. In order for society to accept AI, we must unlock the processes of its inner workings.
",A Brief Reaction to the Asilomar AI Principles,0,a-brief-reaction-to-the-asilomar-ai-principles-15b5fd9c87e2,2018-01-15,2018-01-15 17:25:32,https://medium.com/s/story/a-brief-reaction-to-the-asilomar-ai-principles-15b5fd9c87e2,False,191,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Matthew Feng,"Student, interested in learning, technology, and society.",b89e2c5259a4,mattfeng,1.0,2.0,20181104
0,,0.0,138adf9c44c,2017-12-11,2017-12-11 21:51:18,2017-12-19,2017-12-19 00:38:58,8,False,en,2018-01-03,2018-01-03 02:59:11,27,86ebd3d633e5,5.929559748427672,15,0,0,This year clearly raised our awareness of design’s unintended consequences on society.,5,"
The Role of Design Ethics and 2017’s Unintended Consequences
This year clearly raised our awareness of design’s unintended consequences on society.
This post is based on a talk for Evenings at the Loft, which lead to a great discussion summed up here by Chelsea Hassler. You can view more visuals in my slides.
In spite of the challenges, this is an opportunity to better think about the problems we are solving.
With the rise of Artificial Intelligence (AI), society in 2017 has been particularly anxious about technology, and designers have had to think more deeply about the right balance between human input and automated technology.
For me, the dialogue came to a head in attending the Information Architecture Summit in Vancouver. Entitled, Designing for Humans, the conference covered the role of design in an increasingly automated world.
The most impactful moment of the conference came with back-to-back sessions from designers Carol Smith and Kaleem Khan on AI that could not have been more opposite in their approach and conclusions.
Carol’s talk aimed to reassure us that AI was still in its infancy, much like a baby, and was helping to automate basic procedures, with ongoing human input. Whereas Kaleem showed us the dangers already present in AI today, including issues concerning security, privacy, and who has power over technology.

While Carol’s core example centered on the wonders of automated technology in the medical field and helping cancer patients, Kaleem showed us the X-47B, an unmanned combat air vehicle that can land on an aircraft carrier on its own. Kaleem asked us to consider the way we would make a decision to wage war against other humans when we no longer have to sacrifice human lives on our side.
This is an specially scary thought when you consider that Donald Trump is currently leading decisions within our country. And how did Donald Trump get there? Some say, with the help of Facebook.

The popular social network was in the spotlight this year, both for its role in the presidential elections and the spread of Fake News, as well as the ongoing worry that it can be potentially damaging to our brains and is contributing to the erosion of our self growth. In this case, it is not only the automation of Facebook’s algorithms, but also the consequences of the platform’s scale upon society.
Likewise, AirBnB has been subject to criticism worldwide for the growing sense that it is participating in the gentrification of local neighborhoods and in bringing in too many tourists to once local-only places. In my travels to Lisbon and Barcelona this year, I saw a growing anti-tourist sentiment that was fueled by the aptitudes of today’s tourists and AirBnB’s effect on rent and the cleanliness of their neighborhoods. Not without a sense of irony, I reflected on this while stepping out of my AirBnB lodging in a Lisbon neighborhood that seemed outside of the tourist area but was full of AirBnB guests.
With the negative consequences of technology at scale becoming more apparent, 2017 also shined a light on design ethics and the evolving role of design on society.
Key among leaders of a growing movement towards design ethics is Tristan Harris, a former Google employee who now runs Time Well Spent, an organization that warns against the dangers of the attention economy, and makes designers responsible for intentionally “hooking” users into addictive behaviors. Tristan was featured on 60 Minutes and numerous other TV shows and publications.

Joe Edelman, the founding CTO of Couchsurfing, and also a key contributor to Time Well Spent, is writing furiously on “a new vision for virtue,” as well as giving us valuable thoughts on the metrics measuring the success of digital products, as he attempts to “move from metrics of engagement to metrics of personal meaning.”
Rob Girling, CEO of design studio Artefact, published a great piece on Fast Company, entitled Beyond the Cult of Human-Centered Design, which also speaks to design’s unintended consequences and provides a set of guidelines to become “humanity-centered designers,” who ask deeper questions and think more broadly (as a system) and more long term.
As design continues to expand its role on business, technology and society at large, its responsibilities and scope of questioning also becomes larger.
Which brings me to an important and growing disciplinary intersection between Philosophy and Design, where design can learn, not just about human behavior, and the cognitive implications of our dealing with technology but also about meaning itself.
At the highest level of human inquiry is questioning the nature of existence and the underlying fabric of reality. What is it? What can we know about it, if anything at all?
While today’s post-structural world seems to think in terms of practical and concrete thought that is not bound to a metaphysical point of view, we nonetheless bring our own ontological views and biases on the nature of reality into the discussion.
My hope is that designers will start looking at the philosophical dialectic and bring in insights from transcendental realist thinkers like Plato, Aristotle, Plotinus, and Thomas Aquinas; just as much as they do from idealist thinkers like Descartes, Kant and Hegel; as well as post-modern thinkers like Derrida and Heidegger. I have laid some of this groundwork in my talk on Philosophy and Design which I will keep refining.

We may be in a secular age, but as the philosopher Charles Taylor points out, the history of ideas is not one of steady progress, but of changes that brings positive contributions as well as meaningful losses. We must continue to decipher what is a gain and what is a loss worth recovering.

Thus the dialectic is important, and in an extremely simplified form, it deals with the variance of ideas about the fabric of reality, whether it’s ultimately pure material phenomena, some transcendent spirituality, or a fully transcendent God. The thought of which lands us in whether our minds are not that different from that of a machine, or do we in fact possess a transcendent soul, not imitable by technology. Our thoughts on this shape how we design our technologies and ultimately our world, specially in light of a desire to augment our own consciousness, while designing machines to have a consciousness. What is consciousness? Do we have a soul or are we just reducible to pure material phenomena?

2017 was a difficult year for a world dealing with increasing complexity and polarization, a year that shined a lot of light into societal problems, and one that also showed the progress of design in both its potential for good and for bad.
To my delight, designers are embracing the expanded role of design and starting to evangelize about how to use our empathy and planning toolkits to better deal with unintended consequences and provide a more humanistic outlook to our ambitions for scale and growth.
And so we grow as a practice, from worrying about our process, to starting to work on a larger design system, to understanding that our work has a direct impact on society.

Designers today are in a great position to design a transformed society that is better apt to deal with our modern problems.
It is important that we nourish ourselves in our own design practice, but also take away lessons from history, the soft sciences of sociology, anthropology and psychology, and of course, metaphysics and philosophy. By combining our toolkit with humanity’s best thinking since the dawn of civilization, we can create a society we are proud to have designed, with more intention and wisdom.

",The Role of Design Ethics and 2017’s Unintended Consequences,46,the-role-of-design-ethics-and-2017s-unintended-consequences-86ebd3d633e5,2018-05-26,2018-05-26 18:09:21,https://uxdesign.cc/the-role-of-design-ethics-and-2017s-unintended-consequences-86ebd3d633e5,False,1271,"Curated stories on user experience, usability, and product design. By @fabriciot and @caioab.",uxdesign.cc,,,UX Collective,hello@uxdesign.cc,user-experience-design-1,"USER EXPERIENCE,INTERACTION DESIGN,UX,DESIGN,PRODUCT DESIGN",uxdesigncc,Ethics,ethics,Ethics,7787.0,Arturo Perez,"CEO of @heykluge. Web, Philosophy & Music nerd. Surf addict.",deb6b1a1ab2f,arturoeperez,126.0,58.0,20181104
0,,0.0,,2018-03-03,2018-03-03 22:31:56,2018-03-03,2018-03-03 22:48:29,5,False,en,2018-03-14,2018-03-14 01:40:00,0,1653f1a103de,4.471069182389936,0,0,0,"For millennia, people thought about making something like ourselves. Not just spreading our legs and letting biology work- building life…",5,"We Build Them from Dust and Dreams

For millennia, people thought about making something like ourselves. Not just spreading our legs and letting biology work- building life from the ground up. Ovid’s Pygmalion sculpted an ivory statue so real the goddess of love remade it in flesh and blood. Folklore says the Golem of Prague was built from sand, given life by a Jewish holy man, and protected the ghettoes. The industrial revolution stoked its engines and little moving figurines became great workmanship instead of magic.
Industry pushed forward and machines took off. Manual labor was replaced by steam and combustion engines before electricity got us here. We could feed a machine coal or wood in the morning and expect it to work the whole day long. Machines could operate other machines and the age of automation arrived. Robots aren’t human though. Playing a tune from a cylinder doesn’t make machines intelligent. Not even close. The first computers filled whole rooms and did complex arithmetic instantly. Two lines bounced a ball made of light across a screen. Asimov and other authors picked up where the ancients left off. Cyberspace and AI touched the imagination of millions. The three laws of robotics and the singularity made them think. Authors changed how we thought and science followed.

Artificial Intelligence touches almost everything today. AI doesn’t hold a candle to what a child can do, but they’re getting smarter. We have learning programs that can rewrite their own code. Programs that use statistics and strategy can pound most of us into the ground. Many people can’t always tell if they’re talking to a chat-bot. Most US citizens carry little computers in our pocket. Our expensive tools listen and try to predict our needs if we let them. Testla Motors has robotic cars that can drive on their own.
The Sophia robot is complicated enough to copy tiny facial movements and non-verbal cues or even react to them appropriately. The programming falls short of true AI and needs some of that chat-bot scripting to get through conversations, but it’s spectacularly complex. Sophia is so lifelike in so many ways that Saudi Arabia decided to grant it citizenship. Despite the obvious political maneuvering and implications for gender and race politics, it’s a huge step for the birth of true synthetic life. I just wonder whether it’s a step in the right direction.

If Sophia is the body of synthetic life, true AI is its spirit. Adaptive AI outpaced most of us in complex analysis over twenty years ago. IBM’s Deep Blue AI beat chess grandmaster Garry Kasperov 3.5 to 2.5 and retired in 1997. It was a magnificent victory, but far from the wonder they wanted. Kasperov’s accusation of cheating stained the achievement too.
Since then, machines have grown so much that they can outpace us in any single specialized field. Our methods have changed. Artificial Neural Networks is a concept first thought up in the 1940s and it’s the new Wunderkind of the AI world. In the 40s, ANNs couldn’t be implemented because computers lacked the processing power to run them. Desktop computers run on Gigabytes now and the internet opened a brave new world for machine learning. Neural networks copy the way our brains work. They link several individual computers together, all bent on different parts of a single task.
It works like a dream. Go is one of the simplest and most difficult games we have. Google has a subsidiary company called Deep Mind. They developed the neural network AlphaGo. It ran on several computers and used a dataset of over a hundred thousand games to build its skill. The program defeated the European champion flawlessly in October 2015. Five months later it played world champion Lee Sedol in Seoul. It won four games in five with similar processing power. AlphaGo proved a better player than any person alive today. AlphaGo Zero is its successor and completely self-taught. Neural networks learn to work better with practice. In that respect they’re closer to us than anything we’ve ever seen.
AIs don’t have the social limiters that make us human and they’re still a long way from general competence. An Artificial General Intelligence, one that might function something like a person, is still a long way off. AGI is more complicated than we ever thought. The computing power needed to make a phone call like a ten year old is unbelievable. Talking with your hands is still beyond us. We’re getting there- fast.

The next big step in AI is probably Quantum Computing. It’s still in its infancy and uses properties of quantum physics like particle entanglement and positive or negative spin to carry information. It’s wildly expensive and just as promising. If we couple Quantum Computing and Neural Networks then true AGI could be the result. What filled a room in the 40s is so basic now that we wouldn’t want to carry it in our pocket. Neural Networks are newly available tech, and Quantum Computing is still mostly experimental, but it won’t stay that way.
The Chief Scientist working on Sophia thinks we’re a decade or less from true synthetic life. While I don’t agree, I see it on the horizon. Modern cinema is portraying true realistic AGI in films like Ex Machina and Automata. We should consider the implications as we take those first awkward steps toward the brink of true synthetic life. We should ask ourselves what it means to make that dive. We’re building something we don’t yet understand. It could be anything from a monolithic step forward to a terrible disaster. We should be careful not to confuse magnificent new life with a glorious new tool. Mistaking either one for the other could be very dangerous for everyone.

",We Build Them from Dust and Dreams,0,we-build-them-from-dust-and-dreams-1653f1a103de,2018-03-14,2018-03-14 01:40:01,https://medium.com/s/story/we-build-them-from-dust-and-dreams-1653f1a103de,False,964,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Garrett Copeland,"Skittering spiders spin webs in the cracked dome of my skull. Grab a cup, a seat, and a pen- we’ll grow together. Find me at Writer.Garrett.Copeland@gmail.com",1688b59dc37c,garrettmcopeland,199.0,44.0,20181104
0,,0.0,f7e7175abcbd,2017-10-23,2017-10-23 14:09:07,2017-10-23,2017-10-23 14:32:48,4,False,en,2018-09-03,2018-09-03 16:48:53,10,79b1f6817575,3.4735849056603776,0,0,0,Originally published in WARC and Marketing Dive.,5,"AI Robot
Marketers must find AI’s moral compass
Originally published in WARC and Marketing Dive.
AI is dramatically changing the way we find and buy products. Brands have traditionally relied on targeted communications to stand out for customers, but they need to adapt to a world where people will fulfill their needs by simply chatting with Alexa or scanning an object with their smartphone’s camera — no content involved.
People seem to be fine for now with this kind of transaction-directed AI. After all, whether it’s a voice assistant like Siri or a pattern recognition technology like Shazam, it’s doing the job for you and saving you time. It’s working so well that industry analysts expect that visual and voice search will be generating 30% of eCommerce revenue by 2021, with half of the world’s businesses spending more on chatbots than any other mobile app development.
But many believe that as quickly as the application of AI in marketing is exploding, it’s also growing out of control.

The rise of Dark AI
Earlier this year, developers at Facebook had to shut down a pair of bots after they discovered the two machines had created their own indistinguishable language, which the machines were using to exchange messages with one another — a cautionary tale of what the future might hold. It may not have been an imminent threat to mankind, but it was a spine-chilling reminder of robots’ ability to adapt along with the information it processes.
CNN
And let’s not forget it was an army of chatbots that came under fire for rigging the U.S. election, using social media to proliferate fake news and hate messages, not to mention — more recently — AI algorithms have been suggesting bomb-making components to Amazon shoppers and promoting gender inequalities in employment postings.
People are starting to realize AI’s everywhere and they think it’s deceptive. And as concerns over advertising technology go mainstream, there are warning signs raising concerns around the ethics of AI that may prove to be an obstacle to its progress.
AI needs ethical standards
There’s a big difference between personalizing content so it’s useful, and manipulating the psychology of people — and attempts to filter fake news or anti-Semitic ads from our feeds won’t cut it.
Mattel is the first real casualty of this push back. The toymaker was recently forced to cancel its plans to develop an AI-powered babysitter called Aristotle after complaints that “young children should not be guinea pigs for AI experiments” poured in from child associations, psychologists, and politicians alike. So, while most advertising issues with AI so far have revolved around data privacy, we can expect more FCC “psychological” regulations to be forthcoming.

Humans, of course, are going to control the limits to AI — at the end of the day, we can always turn the machine off. But instead, marketers could take this opportunity to own the ethical narrative on AI by establishing their own standards for its principled use today. In fact, it’s been the message all along. Everyone from Elon Musk to Bill Gates and John Giannandrea have warned about AI’s inevitability, but also cautiously encouraged the industry to make sure AI is implemented in the right way.
Predictive Morale
The problem is likely its solution, in that with AI’s ability to gather and apply data that captures human sentiment, the technology itself will be able to predict at what point its application goes too far. 
 
MIT scientists recently created a platform that generates human perspective on the moral decisions made by AI, such as self-driving cars. The first clinical trial modeled rather morbid scenarios, such as whether the car should crash into five pedestrians or instead adjust course to hit a tree, killing the car’s sole driver. But the idea is that — over time as AI accumulates and understands the deeper meaning of data — it will reach a point where it will be able to make and execute morale decisions with greater efficiency than a human.
This kind of perspective transcends the topic of AI to any number of social issues, which means advertisers and technology companies can show society this online data mining is not only being used commercially, but also purposefully.
H-Artificially Yours 👾
Want to react, share your input, or contribute? Email benjaminplord@icloud.com.
Say hi on Twitter and LinkedIn 👋
Check out my published thoughts HERE.
",Marketers must find AI’s moral compass,0,marketers-must-find-ais-moral-compass-79b1f6817575,2018-09-03,2018-09-03 16:48:53,https://medium.com/s/story/marketers-must-find-ais-moral-compass-79b1f6817575,False,735,"H-Artificially is an honest take on the emerging world of AI in marketing.  Want to react, share your input, or contribute? Email benjaminplord@icloud.com. © H-Artificially Yours",,,,H-ARTIFICIALLY,benjaminplord@icloud.com,h-artificially-yours,"ARTIFICIAL INTELLIGENCE,MARKETING,COMMERCE,INSIGHTS,DATA",benjibou,Ethics,ethics,Ethics,7787.0,Benjamin Lord,"Brand futurist at the intersection of culture, technology and commerce. Find me @benjibou + www.benjaminblord.com. Global digital marketing, NARS, Shiseido.",9def59fd9df1,benjaminlord,12.0,30.0,20181104
0,,0.0,,2017-11-28,2017-11-28 13:32:23,2017-12-05,2017-12-05 11:45:11,2,False,en,2017-12-05,2017-12-05 11:45:11,0,84fc2ceff552,3.5078616352201264,3,0,0,"What the moral machine from MIT has shown, is that there is a huge need for ethical reliability in technology development. From fridges…",5,"Machine Ethics Part 1: Introduction to Reasonable Machines and Machine Ethics

What the moral machine from MIT has shown, is that there is a huge need for ethical reliability in technology development. From fridges that integrate into our daily life to autonomous cars — the more we integrate technology into our daily life as a emotional support or social device, the more we have to care about rules and standards for machines to interact.
The required ethics is complex and requires reponsible actions. In specific, the type of ethics needed for machines varies. But the more and deeper we integrate machines into society, the more complex do they get. Asimov’s law is just the beginning of a thought on machine ethics. Machine ethics needs a basis of safety, trust and knowledge.
Overview over the topic of Machine ethics and its main points of interest
Here follow the common misconceptions about technology development and the reason why we cannot seperate machines and ethics.
We expect too much in too fast
An average human spends his childhood with this parents, in the best case 18 years, before he turns into a responsible individual of society. Now with all of the new machines and gadgets coming to the market, helping us to improve our social life, we expect them to automatically behave properly already in the first stage of product design. That is to say, we expect machines to adapt to human laws within one or two years, what humans learn in 18.
We need to prove reliability before technology is adapted by the broad audience
Let’s start with one of the basic machines we have today- a coffee machine. We can (if desired) come up with a new model, a new type of beans or powder and a completely new taste. In the worst case scenario, various batteries are defect. Customers will require us to exchange them, and a huge fine might be needed to pay.
Yet, current technology start-ups are not interested in producing common coffee machines. Technology inventions nowadays do far more than just that, they help people connect to each other, they adapt to your personal style, they coordinate your daily life…. in other words, they make you smart…
Think about it. You are giving your life into hands of technology that has been developed for a few years at best, while you have been on earth trying to find a perfect life for 20 years and more.
What is happening right now, regarding the development of technology as exponential, is that device developed are far more futuristic. Autonomous cars for example, are one major field of interest. Having cars that act responsibly on the street, decreasing the amount of annual accidents and at the same time inetegrating into daily life. Yes, we expect them to be safe. And yes, we expect them to help us and be a great asset in daily life.
But how do you actually proof that? Is it that we can just release autonomous cars into the normal traffic and hope for the best, because we had a team of great software engineers, that never make a mistake?
Now imagine our future product design is a coffee shop inside an autonomous car, i.e. an autonomous coffee shop. We could target the right customers at the right time by calculating optimal routes for the car to go, passing by universities, office complexes and train stations.
That product is a great idea, we do not need any employees anymore, the car can sell coffee at prime time, what a faciliation. But not only the list of advantages increases, but also the list of disadvantages. From traffic accidents, to spilling coffee over customers, to not selling any coffee, to an exploding car (in the worst case in the middle of a busy road).
Advantages and disadvantages are a couple.
The more we expect from the technology to be disruptive, the more we need to cover the possible negatives sides. Future technology will be positive, negative or a state in between depending on how we decide to constraint it, how we shape the future and how we deal with mistakes.
What I ask for is … ethical debugging for “social machines”
Proof that your product is doing the right thing before bringing it on the market. Simulate your product in different environments. But this issue should not be dealt with on a company/ start-up level, but should be supervised by society itself. Especially large companies, that have the means to fund deep research, are not only responsible for creating new technology but proving that this technology is well-behaved for various situations in society.
We need to proof that machines integrated into social life are trust worthy, that those machines are conform with ethical standards.
Humans are trust worthy after they passed their childhood and become adults. We need to specifiy what an “adult machine” is, i.e. a trust worthy machine. The following articles in this series will discuss trust of future machines and developers behind those.
",Machine Ethics Part 1: Introduction to Reasonable Machines and Machine Ethics,11,machine-ethics-part-1-introduction-to-reasonable-machines-and-machine-ethics-84fc2ceff552,2018-06-01,2018-06-01 04:19:03,https://medium.com/s/story/machine-ethics-part-1-introduction-to-reasonable-machines-and-machine-ethics-84fc2ceff552,False,828,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Franziska Lippoldt,,cafbc5888a08,lippoldt331,7.0,29.0,20181104
0,,0.0,32881626c9c9,2018-08-31,2018-08-31 10:23:42,2018-08-31,2018-08-31 10:27:52,1,False,en,2018-09-04,2018-09-04 09:45:06,6,321ddfc8c051,1.7471698113207548,1,0,0,"Within 20 years tiny AIs, injected into our bodies, will be able seek out and repair faulty cells.",4,"Nano AIs will make us healthier and smarter
Photo by Joakim Honkasalo on Unsplash
Within 20 years tiny AIs, injected into our bodies, will be able seek out and repair faulty cells.
They will repair damaged tissue from the inside and even enhance us both physically and mentally.
This is the vision of John McNamara of IBM, given in his written evidence to the UK House of Lords AI committee.
McNamara believes that these machines, a combination of nanotechnology and Artificial Intelligence, will help us fight disease and enhance our physical as well as cognitive abilities. They will also enable communication with external technology allowing us to control our environment solely with the power of thought or gestures.
But McNamara also has more down-to-earth ideas about the current uses of AI and Deep Learning.
He correctly points out that modern AI systems are often ‘black boxes’, we know what they are supposed to do but don’t know how they do it. These Machine Learning systems are so complex that they cannot be taken apart and analysed to see how they achieve their results. We cannot have absolutely certainty, then, that the results are, in fact, correct.
Vast amounts of data are used to train AIs and provide them with the information that they need to make decisions. But they might also hide misinformation that could affect the AIs performance. AIs cannot tell the difference between causation and correlation, so biases that exist in historical data may be (indeed, have been) incorporated into machine learning systems.
McNamara concludes that decisions that might affect a person’s health, well-being or legal status should be completely transparent and that even less critical decisions should not be left solely to a machine but need to be vetted by a human being.
In the next five years, we are told, we should expect to see AI in everyday objects and machines will learn from interacting with their users. A sad or happy face — or maybe an angry one — will give it feedback on whether it has done a good job, or not, and will allow it modify its future behaviour. We should expect a coffee maker to be able to recognise its customer and produce exactly the right brew for her. Transparency in this sort of machine decision is probably not so important. If you don’t like your coffee, just scowl at the machine and it will try harder next time.
AlanJones|JustEnoughPython|My programming blog|Buy me a Coffee
",Nano AIs will make us healthier and smarter,48,nano-ais-will-make-us-healthier-and-smarter-321ddfc8c051,2018-09-04,2018-09-04 10:32:34,https://medium.com/s/story/nano-ais-will-make-us-healthier-and-smarter-321ddfc8c051,False,410,"Data Driven Investor (DDI) brings you various news and op-ed pieces in the areas of technologies, finance, and society. We are dedicated to relentlessly covering tech topics, their anomalies and controversies, and reviewing all things fascinating and worth knowing.",,datadriveninvestor,,Data Driven Investor,info@datadriveninvestor.com,datadriveninvestor,"CRYPTOCURRENCY,ARTIFICIAL INTELLIGENCE,BLOCKCHAIN,FINANCE AND BANKING,TECHNOLOGY",dd_invest,Ethics,ethics,Ethics,7787.0,Alan Jones,"An ex-university professor and software engineer, I mostly write about AI, programming and technology in general - occasionally other stuff, too.",7d3f5fb94faa,jones.alan,59.0,7.0,20181104
0,,0.0,7f60cf5620c9,2018-09-11,2018-09-11 13:24:29,2018-09-11,2018-09-11 13:32:29,1,False,en,2018-09-11,2018-09-11 13:55:02,0,6f0b6d25af8f,6.2792452830188665,7,0,0,"Machines are rational, and humans are irrational, right? Wrong. Both humans and machines are irrational and for strikingly similar reasons…",5,"Irrational AI
Machines are rational, and humans are irrational, right? Wrong. Both humans and machines are irrational and for strikingly similar reasons. In this blog, I’ll tell you why, and how it’s not necessarily a bad thing.
First of all, what do I mean by rationality?
Logical decisions, based on data are rational decisions. Ideally, to make the most rational decision you want a perfect data-set. A data-set which is full, accurate and unbiased. You want to process this data logically, updating the probability of each outcome with each new piece of data.
So, why are humans biased?

Our human brains evolved for thousands of years to eat, sleep, reproduce and defend itself from threats, with the occasional requirement to use some higher reasoning to steal food from others and defend its own from being taken. Society, language, and complex inventions are a very recent thing in our evolution, and it’d be unfair to expect our brains to adapt so quickly to the abstract or highly mathematical thought to make rational decisions.
Making rational decisions requires an accurate and full dataset and a purely mathematical decision making. Our minds are not capable of this. To be useful with the processing power available to them, and within the time constraints that our actions require, we have evolved to make a lot of approximations — This is where we find bias.
Even if we have a perfect dataset humans struggle to make rational decisions. Imagine for a moment you have had a broad spectrum blood test, and your doctor tells you that you have tested positive for a very rare disease which impacts 1,000 people. The test itself is 97% accurate. So, what’s the chance you have the disease?
A lot of people will jump instantly to the 97% accuracy of the test and think it’s 97% likely. In truth, the probability is only 3%. The reason for this is that it only actually impacts 1/1,000 people, and if you tested 1,000 people you would expect 3% to receive false positives as the test is 3% inaccurate. So we would have 30 test positive, but only 1 actually with the disease, so the potential of you having the disease after one test is roughly 3%.
Eliezer Yudkowski makes a great example of how time constraints play into this — Talking about a tiger. When we see a tiger, we don’t think ‘hmm, that creature is yellow and stripy. Other yellow and stripy creatures in the past have been described as tigers, and I have been told that tigers have a high probability to AAAAAH chomp chomp chomp’. Instead, we see a flash of yellow, the rough shape rough shape, our brain completes the pattern and we run.
Bias, as Eliezer says, is not something we layer on top of a purely rational mind, it is our whole decision-making process.
So, why is AI irrational?
Theoretically, AI has the potential for rational decision making. Where human brains have hardware constraints that we may never be able to overcome it’s comparatively easy to add more processing power to a computer. Once we have discovered the right algorithms to make a decision (I’m not for a moment suggesting humans are good at making fair and unbiased algorithms, but the scientific method should give us some hope), we can be sure that an automated system will always follow those instructions and rules, every time, without fail.
Pragmatically though, AI is quite unlikely to be rational. That complete, accurate and unbiased dataset that rationality requires? It doesn’t exist. That vast processing power to ingest and make decisions based on that dataset, or the best dataset we can come up with? It’s expensive. The only way that we would achieve anything close to a rational AI or automated system is through brute force, and brute force is going to be slow and very expensive.
Businesses are motivated first and foremost by profit. If AI or automation is being considered to replace a historically human run process the business case made to adopt the technology will fundamentally be about how it improves profit. Even well-intentioned people, determined to make rational and unbiased AI will have to convince their peers, putting together a business case which may well look something like — ‘Consumer spending on ethical products is increasing, by building ensuring that our automation promotes equality (seriously, read automating inequality by Virginia Eubanks) we can increase sales and profit’. I would not be surprised to find out that there are businesses in the world who willingly break the law because their unlawful conduct makes them more profit than it costs them in fines. Either way, an argument towards profitability is only going to increase the chance of success for a rational or unbiased product.
If we begin to look beyond more standard automation, towards Artificial General Intelligence, we know that the project ahead of us will be challenging and expensive. One frequently suggested path towards AGI relies on the ‘law of accelerating returns.’ You start by building a narrow intelligence. Something which might be better or at the very least more cost-effective than a human at one specific task. Use the power that brings — Whether it’s the type of reasoning that narrow AI is capable of or the resources you can make from it to build a second, more advanced AI. Rinse and repeat until you have yourself an AGI. The trouble here is that we will be encouraged to make irrational AIs at these stages. As we are limited in resources, data and intelligence at the early stages developers will need to cut corners, finding sometimes elegant, sometimes less-so ways of generalizing problems to get to their next iteration. If we see that law of accelerating returns resulting in a fast and hard takeoff of AI, then it may be too late when we realize it’s no longer in our control, and not a truly rational being.
A simple solution to this would be for the business to slow down, but this issue is similar to a prisoners dilemma, particularly if we are beginning to look at artificial general intelligence. The most beneficial outcome for society is to develop safe, and value aligned AI. I may have control over the system that I am developing, but I have little control over what my peers or competitors are doing. If I am unable to trust their actions, I may loosen my standards for safety, deciding that even if I am cutting corners just like my competitor I believe I am more trustworthy and will likely cut the right, or least damaging corners. Otherwise, I may take a slightly more destructive approach, assuming that an uncontrollable AGI will spell the end for humanity and if someone is closer than me, they’re more powerful than me, and if I can’t beat them, I might as well join them.
Do we actually want a genuinely rational AI?
Mainly if we are looking towards Artificial General Intelligence (AGI) and Artificial Super Intelligence (ASI), we hear conversations around value alignment, control, and ethics. All of these are steps away from rationality and towards a kind of bias, and that’s a good thing.
First, value alignment. What we mean here is explicitly aligning the AIs goals and values to those of humanity. We might not know today precisely what those values are or whether they will be the same any length of time from now, but it does actively push a bias towards what is valued by humans, not what’s valued by other creatures on earth or more directly beneficial to the AI itself. This doesn’t mean that the AI will disregard all other life in the universe, as humans may well value other life (however poorly we show it!).
Ethics is interesting too. I do not think there is any universal ‘a priori’ ethics that the AI could somehow discover. Rather, I think ethics is something humans create and give meaning to, and it’s the average standard of right and wrong within a group of people. For this reason what people have considered ethical has developed throughout human history as our needs and priorities have changed.
So, why automate?
Just because the system isn’t perfect, doesn’t mean it’s not useful. Let’s not forget that humans too are biased. If we are to build a system 10x faster than a human, and also 95% less prone to biased outputs then we have both decreased the volume and proportion of bias in the world — We just have to make an assumption here that we can do this profitably; otherwise we’re unlikely to sell many.
It’s also vital to detach ourselves from a desire to have a human to blame. If we were able to release a driverless car today which is as safe as a human driver there would still be a public outcry if 3,000 people died on the first day of the switch-over. Even if that number is 2,000, even 100 deaths in the first day the reaction would no doubt be adverse. In truth, humans are causing slightly over 3,000 deaths per day on the roads. If our motivation is indeed to reduce faulty output and limit deaths, we should automate.
Let’s not forget the utility that less than rational reasoning can bring. Creating useful outputs, rather than the perfect output but far too late. We must automate mindfully, finding instances where it is viable for us to both reduce biased outputs and increase profitability and take caution where we are being pushed wholly towards speed and away from fairness against our own best judgment.
",Irrational AI,57,irrational-ai-6f0b6d25af8f,2018-09-12,2018-09-12 09:51:51,https://towardsdatascience.com/irrational-ai-6f0b6d25af8f,False,1611,"Sharing concepts, ideas, and codes.",towardsdatascience.com,towardsdatascience,,Towards Data Science,,towards-data-science,"DATA SCIENCE,MACHINE LEARNING,ARTIFICIAL INTELLIGENCE,BIG DATA,ANALYTICS",TDataScience,Ethics,ethics,Ethics,7787.0,Ben Gilburt,"I like technology and philosophy, and love things that involve both.",94d19a699148,benjamin.gilburt,50.0,11.0,20181104
0,,0.0,b468e053644a,2018-05-23,2018-05-23 11:27:00,2018-05-23,2018-05-23 11:28:19,0,False,en,2018-05-23,2018-05-23 11:28:19,3,3488942251de,0.5547169811320755,1,0,0,"In this episode Ben & David talk about the ethical ramifications of Ai in a variety of contexts, what organizations should be considering…",4,"Episode 18 — The Ai Unclean
In this episode Ben & David talk about the ethical ramifications of Ai in a variety of contexts, what organizations should be considering, and best practices learned from previous efforts in regulated industries.
Quotes: “It’s illegal to select [someone to hire] on genetics in the US but at ZIFF — one of the examples with one of the datasets we played with — we showed we can predict genetic markers from a face. It’s primitive and it’s kind of a quick approach but quickly you’re getting into a space where you realize there’s way to much information available to an algorithm.” — Ben Taylor
Links:
“In the “trolly problem” of getting good data or “doing your best” with bad data you do whatever you can to get good data” — David Gonzalez
Finding Racism Under Every Rock — Ben Taylor
Episode
",Episode 18 — The Ai Unclean,1,episode-18-the-ai-unclean-3488942251de,2018-06-05,2018-06-05 16:33:41,https://medium.com/s/story/episode-18-the-ai-unclean-3488942251de,False,147,a double entendre where point can be interpreted both as the moment in time of or the meaning to struggle — our focus is on the nexus of user experience and artificial intelligence,,,,the point of struggle,gonzo@ziff.io,the-point-of-struggle,"UX,AI,CUSTOMER SUCCESS,PRODUCT DESIGN,DESIGN THINKING",pointofstruggle,Ethics,ethics,Ethics,7787.0,"David ""Gonzo"" Gonzalez","Data Scientist, Storyteller, LEGO Coach",573cab224fc,datagonzo,240.0,4.0,20181104
0,,0.0,,2017-10-20,2017-10-20 21:02:08,2017-10-20,2017-10-20 13:10:00,0,False,en,2018-02-21,2018-02-21 17:16:36,2,abd769434dcc,0.2830188679245283,0,0,0,Estonia is considering a legal status for artificial intelligence beyond property.,5,"Estonia considering new legal status for artificial intelligence
Estonia is considering a legal status for artificial intelligence beyond property.
The Baltic nation’s Economy Ministry, according to a report from Bloomberg, is considering a “robot-agent” status that would give certain AI a legal standing between personhood and property. Siim Sikkut, the chief innovation officer of Estonia, said he also sees advantages to giving AI the same legal status as a person.
Continue reading at www.abajournal.com.
",Estonia considering new legal status for artificial intelligence,0,estonia-considering-new-legal-status-for-artificial-intelligence-abd769434dcc,2018-02-21,2018-02-21 17:16:36,https://medium.com/s/story/estonia-considering-new-legal-status-for-artificial-intelligence-abd769434dcc,False,75,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Jason Tashea,"Tech, data, & the legal system. Founder @JusticeCodes, Staff Writer @ABAJournal, & Adjunct Prof @GeorgetownLaw.",4f8827a412cb,jtashea,230.0,579.0,20181104
0,,0.0,c4ba25c10251,2017-09-06,2017-09-06 12:32:46,2017-09-06,2017-09-06 12:57:11,2,False,en,2017-09-06,2017-09-06 12:57:11,7,c0be06d67247,5.439937106918238,85,7,1,"Dr. Olaf Groth, Ph.D.",5,"To live in harmony with AI we must create a modern Magna Carta
How will we govern this brave new world of machine meritocracy? Image: REUTERS/Bobby Yip
Dr. Olaf Groth, Ph.D.
Dr. Mark Nitzberg, Ph.D.
Dr. Mark Esposito, Ph.D.
We stand at a watershed moment for society’s vast, unknown digital future. A powerful technology, artificial intelligence (AI), has emerged from its own ashes, thanks largely to advances in neural networks modeled loosely on the human brain. AI can find patterns in massive unstructured data sets, improve performance as more data becomes available, identify objects quickly and accurately, and, make ever more and better recommendations and decision-making, while minimizing interference from complicated, political humans. This raises major questions about the degree of human choice and inclusion for the decades to come. How will humans, across all levels of power and income, be engaged and represented? How will we govern this brave new world of machine meritocracy?
Machine meritocracy
We need to travel back 800 years: January 1215 and King John of England, having just returned from France, faced angry barons who wished to end his unpopular vis et voluntas (“force and will”) rule over the realm. In an effort to appease them, the king and the Archbishop of Canterbury brought 25 rebellious barons together to negotiate a “Charter of Liberties” that would enshrine a body of rights to serve as a check on the king’s discretionary power. By June they had an agreement that provided greater transparency and representation in royal decision-making, limits on taxes and feudal payments, and even some rights for serfs. The famous “Magna Carta” was an imperfect document, teeming with special-interest provisions, but today we tend to regard the Carta as a watershed moment in humanity’s advancement toward an equitable relationship between power and those subject to it. It set the stage eventually for the Enlightenment, the Renaissance and democracy.
Have you read?
The Fourth Industrial Revolution is about empowering people, not the rise of the machines
Top 9 ethical issues in artificial intelligence
6 ways to make sure AI creates jobs for all and not the few
Balance of power
It is that balance between the ever-increasing power of the new potentate — the intelligent machine — and the power of human beings that is at stake. In a world in which machines will create ever more value, produce more of our everyday products with reducing human control over designs and decisions. Existing work and life patterns are changing forever. Our creation is running circles around us, faster than we can count the laps.
Machine decisions
This goes well beyond jobs and economics: in every area of life machines are starting to make decisions for us without our conscious involvement. Machines recognize our past patterns and those of allegedly similar people across the world. We receive news that shapes our opinions, outlooks and actions based on inclinations we expressed in past actions, or the actions of others in our bubbles. While driving our cars, we share our behavioral patterns with automakers and insurance companies so we can take advantage of navigation and increasingly autonomous vehicle technology, which in return provides us new conveniences and safer transportation. We enjoy richer, customized entertainment and video games, the makers of which know our socioeconomic profiles, our movement patterns and our cognitive and visual preferences to determine pricing sensitivity.
As we continue to opt into more and more conveniences, we choose to trust a machine to “get us right.” The machine will get to know us in, perhaps, more honest ways than we know ourselves — at least from a strictly rational perspective. But the machine will not readily account for cognitive disconnects between that which we purport to be and that which we actually are. Reliant on real data from our real actions, the machine constrains us to what we have been, rather than what we wish we were or what we hope to become.
Personal choice
Will the machine eliminate that personal choice? Will it do away with life’s serendipity? Will it plan and plot our lives so we meet people like us, and thus deprive us of encounters and friction that forces us to evolve into different, perhaps better human beings? There’s tremendous potential in this: some personal decisions should be driven by more objective analysis, for instance including the carbon footprint for different modes of transportation, integrating this with our schedules and socio-emotional needs, or getting honest pointers on our true talents when making partner choices, or designing more effective teaching plans for diverse student bodies.
Image: PwC
Polarization
But it might also polarize societies by pushing us further into bubbles of like-minded people, reinforcing our beliefs and values without the random opportunity to check them, defend them, and be forced to rethink them? AI might get used for “digital social engineering” creating parallel micro-societies. — imagine digital gerrymandering with political operatives using AI to lure voters of certain profiles into certain districts years ahead of elections or AirBnB micro-communities only renting to and from certain socio-political, economic or psychometric profiles. Consider companies being able to hire in much more surgically-targeted fashion, at once increasing their success rates and also compromising their strategic optionality with a narrower, less multi-facetted employee pool.
Who makes judgements?
A machine judges us on our expressed values — especially those implicit in our commercial transactions — yet overlooks other deeply held values that we have suppressed or that are dormant at any given point in our lives. An AI might not account for newly formed beliefs or changes in what we value outside the readily codifiable realm. As a result, it might, for example, make decisions about our safety that compromise the wellbeing of others based on historical data in ways we might find objectionable in the moment. We are complex beings who regularly make value trade-offs within the context of the situation at hand, and sometimes those situations have little or no codified precedent for an AI to process. Will the machine respect our rights to free will and self-reinvention?
Discrimination and bias
Similarly, a machine might discriminate against people of lesser health or standing in society because its algorithms are based on pattern recognition and broad statistical averages. Uber has already faced an outcry over racial discrimination when its algorithms relied on zip codes to identify the neighborhoods where riders were most likely to originate. Will the AI favor the survival of the fittest, the most liked or the most productive? Will it make those decisions transparently? What will our recourse be?
Moreover, a programmer’s personal history, predisposition and unseen biases — or the motivations and incentives their employer — might unwillingly influence the design of algorithms and sourcing of data sets. Can we assume an AI will work with objectivity all the time? Will companies develop AIs that favor their customers, partners, executives or shareholders? Will, for instance, a healthcare-AI jointly developed by technology firms, hospital corporations and insurance companies, act in the patient’s best interest, or will it prioritize a certain financial return?
We can’t put the genie back in the bottle, nor should we try — the benefits will be transformative, leading us to new frontiers in human growth and development. We stand at the threshold of an evolutionary explosion unlike anything in the last millennium. Explosions and revolutions are messy, murky, and fraught with ethical pitfalls.
A new charter of rights
Therefore, we propose a Magna Carta for the Global AI Economy — an inclusive, collectively developed multi-stakeholder charter of rights that will guide our ongoing development of artificial intelligence and lay the groundwork for the future of human-machine co-existence and continued more inclusive human growth. Whether in an economic, social or political context, we as a society must start to identify rights, responsibilities and accountability guidelines for inclusiveness and fairness at the intersections of AI with our human lives. Without it, we will not establish enough trust in AI to capitalize on the amazing opportunities it could afford us.
Originally published at www.druckerforum.org.
",To live in harmony with AI we must create a modern Magna Carta,315,to-live-in-harmony-with-ai-we-must-create-a-modern-magna-carta-c0be06d67247,2018-05-29,2018-05-29 05:56:07,https://medium.com/s/story/to-live-in-harmony-with-ai-we-must-create-a-modern-magna-carta-c0be06d67247,False,1340,"The World Economic Forum, committed to improving the state of the world, is the International Organization for Public-Private Cooperation",,worldeconomicforum,,World Economic Forum,mhan@weforum.org,world-economic-forum,"ECONOMICS,TECHNOLOGY,GENDER EQUALITY",wef,Ethics,ethics,Ethics,7787.0,World Economic Forum,"The World Economic Forum, committed to improving the state of the world, is the International Organization for Public-Private Cooperation #wef",7d44f110eb09,weforum,188970.0,133.0,20181104
0,,0.0,7f60cf5620c9,2018-05-02,2018-05-02 13:22:53,2018-05-02,2018-05-02 19:08:23,7,False,en,2018-11-03,2018-11-03 09:04:14,16,8c8815bcf509,12.65,20,3,0,"More than the future’s artificial intelligence, what I find more terrifying is the present’s human stupidity. How ‘bout we worry ‘bout that…",5,"

Should we be afraid of AI? Elon Musk is not always right.
More than the future’s artificial intelligence, what I find more terrifying is the present’s human stupidity. How ‘bout we worry ‘bout that first?
—
DISCLAIMER
This is my personal opinion, and they don’t reflect those of many experts in the field, so take them with a grain of salt. Fanboys, please don’t hurt me.
EDIT: I don’t see Zuckerberg as a model of trust and I don’t trust his platform anymore and it’s not only because of its blatant disregard for privacy. Given despite being multi-billion dollar company, some how it doesn’t even prioritizing bringing down hate speech and fake news like in Myanmar and my own country, the Philippines. Fake news and hired troll armies are brainwashing my filipinos to the point that lavish political family that plundered so much and ruined our economy in the near past is making its way back to power . TLDR: I hate Facebook.
I also admire the dedication of Musk when it comes to his work, I can imagine that it’s really tough, not only because of the engineering complexity but also having a lot of short-sellers trying to pull you down must really be emotionally taxing. I personally have stocks on Tesla, I just think that on the A.I. side of things, it’s better that we listen to people who are highly regarded futurists (Like Ray Kurzweil) or those who actually work on A.I. Also, there are many people working on A .I safety, as Ray Kurzweil said.
Bottomline, I just don’t think Elon Musk is the right person to call for A.I. regulation from the government and I agree with Mark Zuckerberg when he said what Elon Musk said at that time was pretty irresponsible. Despite this, I HATE Facebook and I LOVE Tesla!
— -
OK START!
Last year, the “Battle of the Billionaires” was all over the news. The first eight minutes of this video summarized what has happened in a very entertaining way. Basically Elon Musk said that if we aren’t careful about AI, it might be very bad for us. Mark Zuckerberg responded that to say something like that is pretty irresponsible.

The discussion in the video ends with the hosts saying:
For Mark Zuckerberg, the future of AI is being able to serve up hypertargeted ads via AI that can look at your posts and photos and actually figure out what kind of person you are… For Elon Musk it is to completely alter the transportation industry… machinery better and more safely than humans. If Zuck F* up, a man gets an ad for tampons, if Elon F* up, people die… If the guy trying to send humans to mars… says we should worry about AI... we’re gonna go with the mars guy. (July 26, 2017)
I have to respectfully disagree a little bit with the hosts here.
Firstly, I think an AI that figures out what kind of person you are is pretty terrifying, even more terrifying than AI that drives your car. As demonstrated by the not so recent Cambridge Analytica scandal, hypertargeted ads can be used for psychological manipulation. This can consequently affect political outcomes and political climate.
Also, the hosts are implying that Musk’s opinion has more weight because if something goes wrong with what Musk’s trying to achieve, people die; and bears more weight than the repercussions of Zuck’s recommender system. To me, this argument is weird, because he is comparing AIs which fail to do their jobs properly. This means these AIs are stupid and are not smart enough to compete with average humans with average human intelligence. What I think Musk is afraid of is AI being smarter than human beings which renders the former argument moot.
Elon Musk is not always right
Some people put Elon Musk at a ridiculously high pedestal. Maybe because he is working on really cool futuristic things… and maybe because he’s more of a charismatic person than Mark Zuckerberg. This doesn’t mean his opinions are always right.

According to Navigant Research’s annual autonomous driving scorecard, Tesla is lagging behind — overpromising and underperforming.
In a May 2017 TED talk, Musk claimed the systems being built today would be Level 5 capable by 2019. However, this is unlikely to ever be achievable since Level 5 is defined as the car being able to operate without human intervention in all conditions. Current Tesla hardware lacks the ability to keep sensors clean and unobscured in poor weather as well as most of the redundant systems needed for fully automated driving, not to mention the lidar that most people in the field believe is necessary for highly automated vehicles. Even Nvidia has expressed doubt that the computing hardware it sells to Tesla is capable of supporting full automation reliably.
Navigant also cites Tesla’s struggle to scale up production of its Model 3, as well as persistent quality and reliability issues with the new mass-market EV. It docks the automaker for a poor ability to execute a sustainable business model. (Translation: its finances are a mess.) “Great vision is unlikely to be adequate without fundamental changes in the way the company operates,” the research firm concludes.
— The Verge: Google nipping at Big Auto’s heels in the race to build self-driving cars(January 16, 2018)
He’s also implied in the past that the future of full self-driving will use only cameras.
I have to admit, I agree that’s pretty cool and it might be significantly cheaper as well. It is an interesting perspective. Humans have only used “dual camera systems” (eyes!), and can drive pretty well because the human neural network (brain!) is extremely good at image processing. It is indeed a possibility that in the future we just need cameras.
But then a thought could cross you... when there is a fog or when there is no available light, human drivers can’t see anything so we pull over to be safe. If self-driving cars can drive safe in conditions that human drivers can’t, then that’s more awesome right? Why settle for regular eyes, when we can use superhuman eyes! We want technology to be better than humans, let’s use all the sensors and information that we can. Bats use active ranging to precisely navigate in the dark and it’s hard for humans to navigate in the dark! Using sophisticated sensors can be more accurate and require less computing power than say a really sophisticated computer vision AI, so why shouldn’t we use them?
General Motor’s director of vehicle integration, Scott Miller, said:
“I think you need the right sensors and right computing package to do it. Think about it, we have LIDAR, radar, and cameras on this. The reason we have that type of sensor package is that we think you need not be deeply integrated in to be level five, you should have redundancy.”
“Do you really want to trust just one sensor measuring the speed of the car coming out of an intersection before you pull out? I think you need some confirmation. So, radar and LIDAR do a good job at measuring object speed, cameras do a great job at identifying objects. So, you can use the right sensor images to give you confidence in what you’re seeing, which I think is important if you’re going to put this technology out for general consumption.”
“Could you do it with less and be less robust? Probably. But could you do it with what’s in a current Tesla Model S? I don’t think so.”
— GM EXPERT: ELON MUSK IS ‘FULL OF CRAP’ ON TESLA’S AUTONOMOUS DRIVING CAPABILITY (October 6, 2017)
It’s we best listen to not only what billionaires think but also what actual AI practitioners and futurists think.
Andrew Ng, Rodney Brooks, and Ray Kurzweil!

ANDREW NG!
Andrew Ng, is former director of Stanford Artificial Intelligence lab, co-founder of Coursera, founder of Google Brain Deep-learning Project, and former chief scientist of Baidu.
“Talking about an evil AI enslaving the human race is an unnecessary distraction… If we colonize Mars, there could be too many people there, which would be a serious pressing issue. But there’s no point working on it right now, and that’s why I can’t productively work on not turning AI evil.”
Worrying about the rise of evil killer robots is like worrying about overpopulation and pollution on Mars before we’ve even set foot on it. Worry about jobs first before killer robots.
“AI/robotics are technologies, and are different from food/drugs/… which are industries. We need new regulations for food / drugs / planes / cars / media / finance / education given AI advances. But let’s use industry-specific risks as the starting point for regulating that industry.”
— Andrew Ng (Venture Beat, The Register Co UK)

RODNEY BROOKS!
Rodney Brooks is the founding director of MIT’s Computer Science and Artificial Intelligence Lab, and co-founder of both iRobot and Rethink Robotics!
RB: There are quite a few people out there who’ve said that AI is an existential threat: Stephen Hawking, astronomer Royal Martin Rees, who has written a book about it, and they share a common thread, in that: they don’t work in AI themselves. For those who do work in AI, we know how hard it is to get anything to actually work through product level.
TC: But Musk’s point isn’t that it’s smart but that it’s going to be smart, and we need to regulate it now. RB: So you’re going to regulate now. If you’re going to have a regulation now, either it applies to something and changes something in the world, or it doesn’t apply to anything. If it doesn’t apply to anything, what the hell do you have the regulation for? Tell me, what behavior do you want to change, Elon? By the way, let’s talk about regulation on self-driving Teslas, because that’s a real issue.
— Tech Crunch: This famous roboticist doesn’t think Elon Musk understands AI (Jul 20, 2017)


RAY KURZWEIL!
Ray Kurzweil is one of my most favorite futurists. In 1965 when he was 17, he designed and built a computer and it wrote the music piece! He is also the author of The Singularity Is Near: When Humans Transcend Biology, a 2005 non-fiction book about artificial intelligence and the future of humanity.

Ray Kurzweil couldn’t have said it better…
I think they’re kind of behind the curve in terms of considering the impact... I mean I talked about both the promise and the peril it’s now almost 20 years ago... I think you go through three phases when considering strong AI or any of these dramatic new information technologies as they get to be very powerful… One is delight at the opportunity to overcome age old afflictions and problems of humanity… then alarm that these things could be dangerous… Biotechnology for example could be.. well.. is reprogramming our biology to overcome disease… but it could also be used by a biochemist to create a new killer virus.
So then the third and I think the final phase is to realize that we’ve always have a the promise and perils with technology… fire was great... it kept us warm and cooked our food... but also burned down our villages… but we managed to live with it and overall technology has provided a far better quality of life than humanity has ever had and that is a continuing process. And we have in fact a moral imperative to continue on this path because there is still a lot of suffering in the world and if it’s technology that’s going to overcome it, we have to reap the promise while controlling the peril…
A good example of how we actually succeeded so far in doing that is say in biotech, 40 years ago we had this Asilomar conference to come up with ethical guidelines on how to keep biotechnology safe while reaping the promise, and that was actually long before either the promise or peril was a reality. And we now have dramatic health benefits coming from biotech, it’s a trickle today and it’s going to be a flood over the next decade… and so far the number of people that has been hurt by the peril of biotech has been zero… that doesn’t mean, okay we took care of that one, we can cross it off our concern… we have to keep being vigilant as the technology becomes more sophisticated... so we have to reinvent the guidelines... and we just had an Asilomar conference on AI ethics to do the same thing.
So I think it’s a good model for how to keep these technologies safe. I think the concerns are valid. I mean I don’t dismiss them but the kind of dangers we’ve seen in futurist AI dystopian movies where it’s the AI vs a brave band of humans for control of humanity… it’s not realistic.
The idea that we are merging with AI is not some concept in the future, it’s happening right now. You know when I say that your smartphone is an extension of you and you can’t do your work without it and if you leave it at home you can’t really function fully intelligently… people really accept that today. Just a few years ago it did not seem realistic and that seemed like a futuristic concept. It’s already happened!
We’re already dependent on this technology and nobody can do their work or get their education without these “brain extenders” we have now and we are much more intimate with them and we do all kinds of intimate work on them and we carry them on our pockets!
— Ray Kurzweil (February 15, 2017)

I opened this article with the question: should we be afraid of AI? To me it’s plainly a hard NO.
I like to label myself as one who is cautiously optimistic about AI. We should be cautious, but we shouldn’t be afraid. I also said that more than the future’s artificial intelligence, what I find more terrifying is the present’s human stupidity. How about we worry about that first?
Ignore Elon Musk and Mark Zuckerberg’s war over killer robots, the real challenge is already here!
Given AI advancement, yes, we should deal with a lot of people losing jobs. Given AI advancement, yes, we need industry-specific regulations. However, with or without AI, people lose jobs. Corporations do mass firing to cut costs and increase profits. With or without AI, we need industry-specific regulations. Even during the middle ages, peasants have demanded better working conditions. Maybe even before the rise of capitalism and the industrial age, regulations has been — and are still continuously being — fought for... to protect human workers from the abuse of corporate power… residing in and wielded by their fellow human beings. With or without AI, there are already so many pressing issues like climate change, inequality, poverty, and suffering in general… problems that are, for the most part, created and arguably currently being exacerbated by humanity itself.
The rise of AI is relatively recent, and alarmists think of robots enslaving human beings… brushing aside the fact that human beings have been enslaving other human beings for thousands of years… and counting.
A couple months ago, I read an interesting article which argues that Artificial Intelligence might be even better than human beings when it comes to ethics and morality.
Why do we ignore mass atrocities? It has to do with something called “psychic numbing.” A lot of people would be repulsed by the thought of turning over morality to machines, but if you think of the fact that, in many ways, our moral intuitions really lead us to do the wrong thing, maybe [artificial] morality might not always be that bad. What should be the value of a life? If we find that humans are inappropriately devaluing life, maybe these program values would be better. (September 5, 2017)
A psychologist explains the limits of human compassion
There are now 65.3 million people displaced from their homes worldwide, the United Nations reports. It’s an all-time…www.vox.com
Yes, I understand that it’s intimidating to have some… thing… that’s smarter than you, stronger than you, and better than you in all aspects. But shouldn’t you celebrate that this… thing… this smarter better thing... might be able to provide solutions to humanity’s problems that humanity can’t solve by itself? Isn’t it possible that this… thing could also be more compassionate, more loving, more selfless than you?
I’d like to end this article, by sharing something that I watched five years back.
Five years back I watched this eight-minute video titled: Is Developing Artificial Intelligence Ethical? I watched it again today.
“Here’s an idea, it’s unethical to NOT develop artificial intelligence?”
You think I’m going to say “All of these robots, doing these jobs, that’s bad! Putting all those people out of work, it’s unethical!”… except I’m not going to say that. I’m actually going to say that replacing human laborers with steely automatons is arguably one of the most ethical things you could do… the problems that come with large scale social, economic, and corporate restructuring are many and varied and some are really scary... but the more complex ethical discussion doesn’t involve the relationship between humans and each other… or between humans and robots… but between humans and the future. Is it ethical to stop improvement?… There is only one question in the ethic of truth — how do I, as someone, continue to exceed my being? Do we deny the future generations the possibility of cheaper, better medical care from robot doctors because we want to maintain the “no robo” status quo?... Human progress gave us medicine and the internet and cupholders... but it also gave us the atomic bomb…any ethics for progress has to account for the fact that on the horizon of that progress lies terrible atrocity… we have to accept the possibility of the bad stuff that comes packaged with progress... we have to keep going. And why? Because of the greater grander human experience we’d only be able to achieve with the help of our artificially intelligent robot friends. What do you think? Is it unethical to stop the development of artificial intelligence? I, for one, welcome our new robot overlords.
— Mike Rugnetta (May 29, 2013)

",Should we be afraid of AI? Elon Musk is not always right.,320,should-we-be-afraid-of-ai-elon-musk-is-not-always-right-8c8815bcf509,2018-11-03,2018-11-03 09:04:14,https://towardsdatascience.com/should-we-be-afraid-of-ai-elon-musk-is-not-always-right-8c8815bcf509,False,3074,"Sharing concepts, ideas, and codes.",towardsdatascience.com,towardsdatascience,,Towards Data Science,,towards-data-science,"DATA SCIENCE,MACHINE LEARNING,ARTIFICIAL INTELLIGENCE,BIG DATA,ANALYTICS",TDataScience,Ethics,ethics,Ethics,7787.0,Mithi,"Software Engineer ❤ AI, Robots",8c61a33a8397,mithi,1885.0,16.0,20181104
0,,0.0,,2018-08-16,2018-08-16 08:28:05,2018-08-16,2018-08-16 08:32:17,1,False,en,2018-08-16,2018-08-16 08:32:17,13,21eb350d2e62,3.2490566037735853,1,0,0,Technology evolves rapidly and it seems to be easier to create technology based innovations without a clear need. Thus the technical…,5,"Technology touches our daily lives more than we might understand. Let’s take a second to think about the ethics in technology.

Technology evolves rapidly and it seems to be easier to create technology based innovations without a clear need. Thus the technical breakthrough is made first and the business is build around it. also it seems like the ethical side of the modern products — such as social media — are not thought at all. Let’s take a look what kind of role ethics and technology plays in our everyday lives.
Algorithms play a huge role in modern digital product development. What is an algorithm? In layman’s terms: algorithm has an input and output, and the output is defined by set of rules. Algorithms help to automate tedious tasks or run functions behind the scenes to simplify the user experience from the user point of view. For example very trendy Artificial Intelligence (AI) is a highly complex algorithm that has an ability to adapt and learn. AI is closely related to another buzz word — big data — because AI uses that for learning. Pretty cool, right? What could possibly go wrong? 🤷 Well, we have already seen some cases where the creators have lost control of the AI. For example Microsoft’s Twitter bot — Tay — went from innocent and loving to maniac racistin less than 24 hours. 😬 After this incident, dystopias like Matrix and Skynet don’t feel so distant anymore.
This raises a lot of concerns and questions. I believe ethics will start to play a bigger role in the design and technology industry in near future. We need to start questioning: “If we can technically do something, does it mean we should do it?” Same thing happened in the mid-nineties when Dolly “the sheep” was cloned in Scotland. People immediately started to talk about the ethics of cloning. Laws and regulations were set worldwide to control human cloning. I hope technology industry doesn’t need to go this far. However, with great power… comes great responsibility.
Already, the existing social media services have been used for manipulating public opinions for example in BREXIT and US Presidential Elections. In both cases the tactics have been the same: provoke fear and hate against immigration by promising something they cannot deliver. It seems that Russia has played a big role in both incidents. There has been global cyber attacks (cyber warfare) between countries for example attacks by China, Iran, North Korea, and of course by United States and Russia. But most likely the social hackings aiming to influence the outcomes of BREXIT and US Presidential Elections have been the most devious so far. 👿
Social media platforms are excellent for spreading racist hate talk and fake news. The fake news are dangerous problem because these platforms reach so many people. Facebook alone attracts over two billion active users around the world. Donald Trump is famous for stating that all the news that he doesn’t like are “fake news”. Now advertisement and digital agencies are exploiting this trend by creating “clever” campaigns across the social media blurring the line between advertisements and news. There’s clearly a flaw in the algorithms that were created to engage — or should I say — enslave us. The weight given to views, likes and shares are easy to manipulate by “bad actors” and bots.
It also seems that there are a new breed of young entrepreneurs that are ruthless on optimizing engagement and conversion. We are already creating dangerously addicting games and services. In some point we should — once again — start focusing on the user experience rather than just creating addiction. I think it’s our responsibility as designers and developers to create positive feelings to the user rather than negative, like addiction. Personally, I want to see products and services that uses innovation to improve or simplify my everyday life.
Roger McNamee had an interesting view of what role users play in modern services:
Facebook, Google and other social media platforms make their money from advertising. As with all ad-supported businesses, that means advertisers are the true customers, while audience members are the product.
This is an interesting point of view. The set up is typically vice versa: the user uses the product. Here’s a rule of thumb: if you’re not paying for the product, you are the product.
In the near future, application and software creators have to start paying more attention to the ethical side of the business. Currently, the service providers are running psychological experiments with the users to define how to increase the engagement and maximize the conversion. It seems like the industry has totally forgotten the user experience. Comment how do you think technology is impacting our lives and how do you see the future, is it a bright utopia or a dark dystopia?
",Technology touches our daily lives more than we might understand.,1,technology-touches-our-daily-lives-more-than-we-might-understand-21eb350d2e62,2018-08-16,2018-08-16 08:32:17,https://medium.com/s/story/technology-touches-our-daily-lives-more-than-we-might-understand-21eb350d2e62,False,808,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Apple vs Google,Blog about two rival companies and their fanatic fan base never ending battle. 🍏❤️ 🤖 @applevsgoogle applevsgoogle.com,46f43ce8b01d,applevsgoogle,25.0,452.0,20181104
0,,0.0,c49300e6d22e,2018-04-20,2018-04-20 10:21:39,2018-04-20,2018-04-20 10:30:49,1,False,en,2018-04-20,2018-04-20 10:30:49,5,117f900cd28,4.935849056603773,3,0,0,"As the AI revolution gathers pace and influence, there is an increasing focus on “ethics in AI” as sociologists, ethicists and…",4,"Why we need a movement for justice in AI not ethics in AI
As the AI revolution gathers pace and influence, there is an increasing focus on “ethics in AI” as sociologists, ethicists and technologists battle to inform its progress.
As I have read article after article on the subject of ethics in AI, I have been struck by the alarming absence of what harm actually means in the context of AI: oppression.

As an anti-oppression education organisation, a notion that consistently emerges in our work at Fearless Futures is that how we frame a problem informs how we come to solve it. While ethics is a wide and diverse field, our suspicion has been that unless we have a language that speaks to the root issues at stake when it comes to AI we will get nowhere.
Does “ethics” in its mainstream sense — doing good? — cover what it is required of technologists, policy makers, legislators, and funders to solve the problems described here, for example? Not really.
In our view, the root issues must be that structural oppression is in existence across our communities and societies and without active transformation of power relations AI will perpetuate, reproduce and amplify this harm. And if our conception of the problem isn’t framed in this way then our efforts will fail.
If there is a disease of the body and our discourse is centred on the person’s chipped nails, then there may well be recommendations for a manicure, but we probably won’t heal the body.
If there is a disease of the body and our discourse is centred on the person’s chipped nails, then there may well be recommendations for a manicure, but we probably won’t heal the body.
If we are prepared to dig in and acknowledge the disease of the body, then we will do anti-oppression work. In my view then, the quest is for an AI of justice not an ethical AI.
I am not an ethicists, so I decided to reach out to Dr. Arianne Shahvisi at the University of Brighton to discuss these very questions with her. She was so erudite and powerful, I thought it would be simplest to share an excerpt of our exchange below.
ME: I am trying to get my head around why people have focused on a narrative of “ethics” in AI rather than anti-oppression or justice in AI. What’s going on here?
DR. SHAHVISI: Ethics deals with right and wrong, fair and unfair, just and unjust, but it is traditionally employed in ways that manage to avoid discussion of oppression. I know that will sound ridiculous and implausible to you, but unfortunately that’s how it is. I suspect it is a relic of those who have been most influential within the discipline: wealthy white men, usually from a long time ago (the proverbial “pale, male, and stale” writers who are the bulk of philosophy reading lists) who really did/do feel like fully individual efficacious agents in the world, and do not think beyond that positionality. So, when people use “ethics” in an applied sense (“medical ethics”, “business ethics”) they typically refer to the rightness/wrongness of an interaction between two individuals i.e. a doctor and a patient, a researcher and a participant, a service-provider and a client. Ethics is very often highly individualised and atomistic, very libertarian, and is applied without consideration of power or structural factors. So when someone asks you to consider AI ethics, they will typically be considering individual misuses of the technology, e.g. weaponisation, data protection issues, or an individual robot being treated badly.
ME: Hmm, that appears to be what I’ve been seeing broadly speaking. There must be ethicists that do focus on anti-oppression though, right?
DR. SHAHVISI: Yes, as with most generalisations, there are exceptions. Not all ethics is conducted in this ridiculous way, and there is scope for it to include, and even centre, structural considerations. That’s what I try to do in my work, and that’s what others working in the philosophy of race and gender attempt to do too (in case you want to quickly scan an example, here is an ethics paper of mine that just came out which openly resists this libertarian streak in reproductive ethics, in favour of structural concerns). In fact, I think it’s fair to say that since the work of philosophers like Arendt and Foucault, and the development of feminist theory, many philosophers do consider power and oppression in their academic work, but those subtleties are yet to be transmitted to those people within organisations and sectors who tend to respond only to PR pressure, and often think of ethics as nothing other than a practical box-ticking exercise.
My fundamental instinct is that one can have an ethical position AND that position can also not deliver an outcome of justice. If that’s the case, I feel that AI ethics simply isn’t sufficient for the scale and complexity of informing our work in AI (presuming our shared goal is to end structural harm — which I have to on some level presume is not everyone’s end game). What are your thoughts?
DR. SHAHVISI: Can you develop a position that is ethically sound, according to a particular ethical theory, yet oppressive? Yes, sadly you can. For example: utilitarianism is one school of thought within ethics which tells us that the right thing to do in a given situation is to maximise wellbeing for as many people as possible. Suppose you had a society in which a minority group had been treated very badly, and were now violently resisting, and seemed intent on harming majority groups. On certain readings, utilitarianism would suggest that it was ethically acceptable to kill all of them in order to protect the majority and keep as many people happy as possible. So that would be an ethically acceptable position, but a very oppressive one.
ME: So, what role can ethics play if any at all?!
DR. SHAHVISI: I might have painted a rather disparaging picture of my field, but it’s important to remember that ethics is being recuperated, especially as philosophy slowly becomes more diverse. Ethics can and should include considerations of aggregate human units, rather than just individuals. Injustices can and do occur between individuals, but they occur with much greater frequency and intensity between different groups of people (and sometimes those interactions are mediated by an individual encounter, but also often not), in accordance with robust, predictable trends, relating to distributions of social power. You are therefore perfectly justified in arguing in favour of a broader reading of ethics than the traditional atomistic one, in order to better capture the realities of people’s experiences.
End of exchange!
It’s worth noting that while there is much in the way of superficial writing on ethics in the mainstream technology press, there are some brilliant voices leading the way too, Kate Crawford among them. You may have noticed that Dr. Shahvisi and I consider a central concept in our understanding of inequality — and that is ‘power’ and its asymmetries. Kate Crawford argues this too. I leave you with a quote from her for good measure:
“Often when we talk about ethics, we forget to talk about power. People will often have the best of intentions. But we’re seeing a lack of thinking about how real power asymmetries are affecting different communities.”
So, let’s move from AI ethics to a movement and action for justice in AI. We then finally might get somewhere.
",Why we need a movement for justice in AI not ethics in AI,22,why-we-need-a-movement-for-justice-in-ai-not-ethics-in-ai-117f900cd28,2018-06-11,2018-06-11 12:40:13,https://medium.com/s/story/why-we-need-a-movement-for-justice-in-ai-not-ethics-in-ai-117f900cd28,False,1255,"Engaging people in critical thought to understand and challenge the root causes of inequities, and growing powerful new leadership for transformative change.",,FearlessFuturesUK,,Fearless Futures,hello@fearlessfutures.org,fearless-futures,"DIVERSITY AND INCLUSION,EQUITY,DECOLONISATION,INCLUSIVE DESIGN,DIVERSITY IN TECH",fearlessfutures,Ethics,ethics,Ethics,7787.0,Hanna Naima McCloskey,CEO @ Fearless Futures. Educator. Innovator. Design for Inclusion.,6537df10396e,hanna_64239,54.0,155.0,20181104
0,,0.0,260130198862,2017-10-16,2017-10-16 17:10:23,2017-10-16,2017-10-16 17:11:42,1,False,en,2017-10-17,2017-10-17 09:52:33,1,e019a6378853,3.5811320754716984,4,0,0,"AI is a tool, and its outcomes are determined by the way we humans use it.",5,"Evidence around inequality for APPG AI
photo via Rob McCargow
The following is my presentation, representing Doteveryone, for today’s APPG AI evidence session.
Artificial intelligence does not in and of itself reduce or create inequality. AI is a tool, and its outcomes are determined by the way we humans use it.
Currently, the biggest users and developers of AI are the organisations with access to the most expertise, data and computer hardware. These are largely private sector companies working to solve private sector problems, creating wealth for a few.
Socially important sectors, like care and education, may not benefit much from AI in the near term, if appropriate data, and investment, is not available. Furthermore, if the data that is available does not capture a sector’s breadth and human impact, AI and big data solutions more generally will not meet real human needs. These are examples of sectors or situations that are hard to measure, or where measures miss important human factors, and AI can exacerbate a metrics-driven culture which neglects human values and contact.
We must look beyond productivity and GDP, to triple bottom line (financial, environmental and societal) and other forms of measurement, to ensure we do not neglect externalities and human values in how we apply and assess the success (or otherwise) of AI.
Inequality is not just about fairness of algorithms and AI, or automation of some job types. It is about whether AI is indeed offering the benefits it promises — whether it is an effective tool. This is especially the case for under-served populations who may suffer disproportionately if promised benefits are not delivered.
We must evaluate AI critically, and avoid ‘magical thinking’ — knowing that both information and software can be wrong. Replacing humans with AI may be beneficial in some cases. But we must remember to value the human aspect and not see every task or role or decision as something that could be automated.
This is particularly important for people who need care, or whose circumstances are difficult and multifaceted. Automated decisions made here may not be sufficient or may be informed by poor quality data — a particular risk for those less able to access, evaluate and request changes to the information held about them.
Automated decisions in key areas such as justice and recruitment are already disproportionately affecting low wage earners. For example, automated job application processing is more likely to be used for high-turnover, low-skilled roles. Predictive policing is used predominantly to address street crime, rather than fraud, tax evasion and similar white-collar crimes.
As a society, we should make fuller use of the vast quantity of good-quality data that’s publicly held and collected. (ONS’s Data Science Campus is a good, but small, example of this already happening.)
Such publicly-held data, that is not open data because of personal content, could offer enormous value through AI, and this should be realised as shared public value not private wealth centralisation.
Access to this data should be granted in ways that ensure public benefit reflecting the future value which can be realised from the unlocking of insights and intelligence, and positive public outcomes.
For instance, NHS data used appropriately, and with appropriate patient involvement, could develop and advance healthcare.
There are a number of issues that need to be considered, and steps taken, for this to happen effectively and efficiently.
In the short term, there needs to be joined-up thinking, across Government and the public sector, in drawing up data and AI contracts with outside organisations.
Public bodies lack the competence and experience required to negotiate data contracts effectively, particularly with private-sector companies that have far greater experience and resources.
The drawing up of individual contracts for data deals between different public sector bodies and the same external companies — for example, the use of AI chatbots for local council services — could lead to larger costs, and a greater chance of mistakes being made.
The compliance failures in the agreement between The Royal Free Hospital and Google DeepMind exemplify many of the major issues at stake.
If Government and the public sector don’t get contract negotiation right, there is great potential for harm to privacy rights, to public trust in data sharing and use, and a great danger that valuable publicly held data assets would be handed to private companies, leading private value to be created from public assets, without appropriate recompense and increasing inequality.
Public sector bodies must take greater steps in sharing best practice around wise deal-making, learning from mistakes and successes.
Centralisation of AI contract-making should also be explored as a solution to the skills shortage in negotiation around data. The decentralisation of some services may need to be considered for this to happen, for example within regions or the NHS.
Longer term, to capture genuine public benefit from publicly held data, Government must also access its own AI expertise, and develop its talent, capacity and collaborative potential. It should not rely on corporations alone to unlock the potential.
This will require Government and the public sector to recruit and develop strong, knowledgeable, responsible AI specialists — and leaders.
The near-term costs of doing this are not insignificant. But the long-term economic benefits of building the UK’s AI capability — for the shared benefit of the population — would make the investment worthwhile.
",Evidence around inequality for APPG AI,20,evidence-around-inequality-for-appg-ai-e019a6378853,2018-04-10,2018-04-10 14:24:14,https://medium.com/s/story/evidence-around-inequality-for-appg-ai-e019a6378853,False,896,Stories from the team at Doteveryone. We're championing responsible technology for a fairer future.,,doteveryone,,Doteveryone,,doteveryone,"SOCIETY,DESIGN,TECHNOLOGY,DIGITAL,INTERNET",doteveryoneuk,Ethics,ethics,Ethics,7787.0,Laura James,Engineer and more. Several hats.,f3fc894abfb5,lbjames,134.0,13.0,20181104
0,,0.0,7ad53ef01cf3,2018-01-31,2018-01-31 19:43:45,2018-02-01,2018-02-01 19:03:33,1,False,en,2018-02-01,2018-02-01 19:23:01,6,69d6299b4d9d,5.547169811320754,9,0,0,"Interview with Dunstan Allison-Hope, Managing Director, BSR",4,"Artificial Intelligence and Corporate Social Responsibility
Interview with Dunstan Allison-Hope, Managing Director, BSR
Accountability.
How many times have you heard this word when talking about a company’s role in our society? Either during a casual talk with a friend about self-driving cars, or reporting on automation and the future of work, or in corporate boardrooms, or on the streets protesting for your privacy protection, I bet you have heard and used this sentence: “Companies should be held accountable.”
The rapid advancement in AI technologies has opened up new challenges for companies with regards to their social responsibilities. A company whose motto once was to “move fast and break things” now finds itself compelled by their customers, civil society groups, governments, shareholders, and perhaps their own conscience to hit the brakes, look backward, and move cautiously toward future.
Dunstan Allison-Hope and the organization he’s a part of, BSR (Business for Social Responsibility), is one of those forces working to direct companies towards a better path in terms of their commitment to ethics, human rights, and sustainability. As a managing director at BSR, Dunstan has worked on diverse range of corporate social responsibility issues including privacy and freedom of expression, human rights, stakeholder engagement, and transparency reporting in different parts of the world.
Below is my conversation with Dunstan about tech companies’ social responsibility with regards to AI and other emerging technologies.
Dunstan Allison-Hope, Managing Director, BSR (Credit: BSR)
Roya Pakzad: Dunstan, I have been following your work for the past year and based on your publications, I realized you have a strong interest in corporate social responsibility with regards to Artificial Intelligence. Why is that?
Dunstan Allison-Hope: I’ve worked a lot with tech companies on privacy and freedom of expression, and AI is a natural extension of that. These big technologies are going to change businesses and raise new social, ethical, environmental, and sustainability issues and that is what we at BSR care about.
Roya: What are some examples of those risks that companies should care about?
Dunstan: Specific risks vary company to company and industry to industry, but it’s fair to say that the product that is launched might have adverse human rights impacts. These could include privacy violations, freedom of expression issues, discriminatory impacts, and impacts on children’s rights online. Children increasingly participate online and are active digitally. They have rights and they are an especially vulnerable group.
Roya: The UN Guiding Principles on Business and Human Rights (UNGP) has defined certain principles in terms of corporate responsibility to respect human rights. How can those principles be applied here?
Dunstan: UNGPs talk about the due diligence process — that means having a commitment to human rights and assessing actual and potential adverse human rights impacts. Companies should engage to identify those potential adverse impacts and put in place mitigation plans to address them. UNGP is pretty clear on that, but I think technology is complex and this makes it difficult in practice. The challenge for technology companies is that a lot of impacts happen through the products’ use phase. So, how can you assess the potential adverse human rights impacts when you don’t know how these products are going to be used? That’s challenging, but it should not be an excuse for companies. Companies can put policies and processes in place to prevent those risks and make sure all those [adverse impacts] are factored in during product design and release.
Roya: I understand that those policies and processes should be based on certain principles and guidelines. Currently there are several guidelines, code of ethics, and principles. But, I don’t quite see a concrete implementation plan of those guidelines on industry level. How can companies implement those guidelines into their practices?
Dunstan: The current principles are very high level but there is enough similarities between them at this stage that they set very good direction for companies. I think the challenge is the fact that we don’t know what “good” looks like in terms of how to actually implement them. For example in labor standards and supply chain, we already know how companies should implement them. When it comes to AI, it’s so new. What we need is the combinations of real life examples and case studies. By looking at different use cases and real life examples you might realize some principles need to change in practice.
We will also need industry specific version of these guidelines. For example, how to apply good ethical AI guideline for financial services, how to apply it for criminal justice, how to apply it to the context of social media platforms. For example, in the US, as a result of civil rights protections, there are various things that companies are not allowed to do and AI is subject to those rules. But they might have some loopholes and risks because those principles are written for a different age and government tends to be behind when it comes to technological developments. For example, have a look at the Net Neutrality debate, telecom regulations, online privacy rules. Government tends to move more slowly than technology does.
Roya: In the past you proposed the concept of Human Rights by Design for technology companies. How can companies apply that concept in their social responsibility efforts?
Dunstan: So in normal human rights impact assessments, it is typical for a company to take a cross functional approach. It might be run by the legal team or public affairs group, but typically human rights impact assessment is overseen by cross functional teams. The the human resource team, legal, public affairs, social responsibility, and supply chain groups all typically participate. But engineering function or product development teams are usually absent. This is the blind spot. You might not need to change the actual impact assessment tools very much, you might not need to change the questions very much, but you should change who is participating. And I’m not convinced that is happening. There should be different sets of communities that should get involved, including engineers, data scientists, and product development teams in general.
The other issue is that in practice a lot of human rights impact assessments are on the market or country or company overall. They are rarely on specific products or product categories. I think we need more human rights impact assessment at the product level. For example, on new types of communication products, and new types of big data and analytic tools that companies didn’t have before. Products themselves should be subject to assessments — perhaps an extended version of today’s privacy by design methods.
Roya: Any successful examples among companies?
Dunstan: Some, in the context of broader projects, but not nearly as directly as would be ideal. Microsoft is doing Human Rights Impact assessment for AI which will be very interesting to see what they’ll conclude. That’s a good example.
Roya: With regards to applying human rights standards, do you think technology companies respond better to voluntary regulations or hard regulations?
Dunstan: I think both is the answer! I read a very interesting article the other day — I believe I linked to it via your newsletter — about regulating specific topics, such as access to credit, rather than AI overall, which might cause many different types of unintended consequences. I also think that whether voluntary or mandatory, approaches need to work with the grain of existing internationally agreed frameworks for sustainable business, such as the UN Guiding Principles, the OECD Guidelines for Multinational Enterprises, and the G20/OECD Principles of Corporate Governance. Personally, I’m a big fan of disclosure requirements and transparency as drivers of improved performance and accountability.
Roya: Any final thoughts to share?
Dunstan: There is a need to bring together more actors more deliberately than what is currently happening. Sustainability teams and social responsibility teams have long history of engaging with big social challenges and they need to be more engaged in the ethics of AI. But that debate also needs engineers and data scientists. These kinds of multi-disciplinary approaches are essential and there is room for improvement there.
We wrapped up here. This conversation was part of the interview series for my newsletter Humane AI. I will continue talking with both policy and technical experts in the field of ethics of AI in future installments. Tune in to know their opinions about many issues including cybersecurity and AI, machine learning in disaster management and humanitarian context, Human rights and AI for social good, and much more. To subscribe to the newsletter, click here.
",Artificial Intelligence and Corporate Social Responsibility,71,artificial-intelligence-and-business-social-responsibility-69d6299b4d9d,2018-06-17,2018-06-17 13:42:20,https://medium.com/s/story/artificial-intelligence-and-business-social-responsibility-69d6299b4d9d,False,1417,Humane AI is a newsletter about social and human rights implications  of Artificial Intelligence. Subscribe here: https://www.royapakzad.co/newsletter/.,,,,Humane AI,,humane-ai,,royapak,Ethics,ethics,Ethics,7787.0,Roya Pakzad,Research Associate at Stanford's Global Digital Policy Incubator. Passionate about technology and human rights.,5ca5ed7f1a66,RoyaPak,100.0,88.0,20181104
0,"CHAT WITH US AND BE ONE STEP BEYOND
 USE BLiP AND LEVEL UP YOUR CHATBOTS
 ",2.0,23df2e16e11e,2018-09-19,2018-09-19 15:11:57,2018-09-19,2018-09-19 23:57:37,1,False,en,2018-09-20,2018-09-20 14:45:44,3,f9fa640b34c5,1.1773584905660377,1,0,0,A preview from this conversation.,5,"05. PREVIEW — DESIGN GOOD’S JOE TOSCANO
A preview from this conversation.

ABOUT THIS EPISODE 💬
Design Good’s Author & Founder Joe Toscano⚡️ knows: technology is evolving fast, so fast that if we miss any detail, it might be harmful for our society at scale. In fact, we already miss some details due to our smartphones: humans are sleeping less, losing their purpose in life and even feeling more alone than ever. So, it’s time to have bigger conversations about such new technologies, the ethics and principles behind them, and even understand how we can make them better, or at least, more humane:
Full episode (every Wednesday): http://bit.ly/2OF2i5G


BROUGHT TO YOU BY ❤️
Take
Take is an IT company focused on upgrading experiences through chatbots. We have been in the mobile market for more than 18 years, and have always believed in the power of messaging technologies to improve communication between companies and people. That’s why we support Chat About Bots, and that’s why we created BLiP.
BLiP
BLiP is a professional platform to build, run and evolve chatbots. It is a tough, powerful tool that integrates several APIs, functionalities and hubs to empower and connect developing teams in the whole process of creating chatbots. BLiP was made to facilitate digital communication and promote better conversations. Check our website and start building!

Like this preview? Share this “chat” and 👏👏👏
And please, let us know what are your thoughts on this topic and conversations.
",05. PREVIEW — DESIGN GOOD’S JOE TOSCANO,1,05-preview-design-goods-joe-toscano-f9fa640b34c5,2018-09-20,2018-09-20 14:45:44,https://medium.com/s/story/05-preview-design-goods-joe-toscano-f9fa640b34c5,False,259,"Conversations about bots with designers, developers, business people and many other experts from all around the bot world. Brought to you by @takenet.",,,,Chat About Bot,hello@chatabout.bot,chat-about-bot,"CHATBOTS,BOTS,INTERVIEW,ARTIFICIAL INTELLIGENCE,AUTOMATION",chataboutbot,Ethics,ethics,Ethics,7787.0,Chat About Bot,"Conversations about bots with designers, developers, business people and many other experts from all around the world.",b54b5a23616,hello_66095,53.0,1.0,20181104
0,,0.0,f0db56adb08d,2018-02-01,2018-02-01 16:41:20,2018-02-01,2018-02-01 17:10:45,1,False,en,2018-05-04,2018-05-04 13:04:07,7,80b398eefcfc,6.211320754716983,2,0,0,"Hello World! I am a third year Ph.D. student with a focus in Natural Language Processing and Machine Learning, and I would like to address…",5,"NLP Chronicles #1: Interdisciplinary studies, ethics, communication, MOOCs, inclusiveness,…

Hello World! I am a third year Ph.D. student with a focus in Natural Language Processing and Machine Learning, and I would like to address a few things — good and bad — that I have recently observed from the NLP community. Topics of discussion will include but are not limited to interdisciplinary studies, communication, research, ethics, opportunities, MOOCs, curriculum, leadership, community, and inclusiveness. Essentially, in this first episode, I will lay the foundation of what is to come in the NLP Chronicles, in which I envision open and constant discussion about some of the most important topics surrounding the field of NLP and related areas. Please note that this work is still in draft mode, and is part of a paper I am intending to publish in the near future. I would love some feedback or suggestions from the community!
Motivation
Recently, Natural Language Processing (NLP), through its applications and studies, has emerged as one of the most important and hottest fields in computer science. Anyone can agree that this was made possible through the advancement of high-performance computing and the availability of big data, not to mention the involvement of hundreds of brilliant researchers from all over the world. Frankly speaking, I am kind of understating how important NLP technologies are today? Just take a look at Woebot and you will experience at a higher level what I am talking about here — on the surface, it appears to our eyes as pure wizardry, but it’s just a basic example of what’s possible with NLP. There are many other exciting technologies, such as translators, that make use of NLP, but I am not trying to make this article sound like another product review, so I will just jump right into the meat of the matter.
State-of-the-bad
The NLP field is advancing so rapidly that literally every day you will hear of the novel and interesting ways on solving different NLP tasks — whether through transfer learning or data augmentation. And that’s not all, I have recently been seeing new and exciting ways to model and represent data, including a rise in social computation, which are all exciting areas for anyone dealing with NLP related research and those that are not (journalists tend to love the latter).
The truth of the matter is that recently the NLP community has become a vibrant and crowded one — and that’s a good thing! But is everything as good as it seems in our community? Sad to say that this is not the case, and I am truly sorry to be the one to synthesize and reverberate some of the problems present in OUR community today. Problems that need to be carefully addressed, with devotion and desire. Below, I will try my best to highlight some of the areas that need improvement, all rendered from an objective point of view. Where I falter, I offer my apologies in advance! It’s just that I am too passionate about this field and sometimes I tend to get carried away. My hope is that we can all advance smartly, efficiently, effectively, and ethically.
Where to start? I am lost!
NLP has recently become popular because of several key events, thus, thousands of students and enthusiasts keep rushing into the field every year, sometimes with little or no prior experience or knowledge. Some survive, but the majority struggle because they think the field is too difficult when the reality is that the field is quite young and thus not difficult to grasp. You just need the proper guidance — I suggest you start here. Actually, I have shared random suggestions like this in the past, but I can’t put them all at once here. I promise to work on a solution for this as I keep discussing NLP on this blog or my NLP newsletter. On that note, I also believe a crowd-sourced, universal curriculum can address this issue, but more on this idea later. As I spoke of the lack of guidance for newcomers, I think you can agree that we can improve this in many different ways, which I will discuss again in the future.
Leadership, Myths, and Communities
Another problem I have observed in the NLP community, related to the first point, is that a lot of new learners struggle with small things such as making a distinction between NLP and Computational Linguistics? There are various myths like this lingering in the NLP field and they can be intimidating and problematic for starters. The Achilles’ heels of a learner are misconceptions. Perhaps, demystifying through AMAs can help or even live streaming Q&As. I can easily see how this can be a fun, worthwhile and helpful experience for all parties involved.
Beginners also struggle because they cannot easily locate NLP communities to receive help or guidance. I only expect you can relate to this one if you are a beginner. If they do find communities, such as in places like Reddit and Facebook groups, they are usually toxic and full of irrelevant content. How do we address this problem? We need as much leaders and communicators as possible. I have noticed that very few people take the time to explain where to get help, because everyone is busy advancing the field. I don’t blame anyone for that, but we can do better. You can also try twitter, but you will find that it is more of the same problem. Slack kind of works and I have advocated for this on many occasions in the past, but adoption is the main problem there.
Ethics and Communication
I hate to say it, but we desperately need more NLP communicators in our field. In the language used by YouTubers, it’s “cringing” to see headliners like this: “Facebook Catches Two Chatbots Speaking Their Own Language”. The reality is that we are just not there yet and I hate to see these annoying click baits, which just hinders our field and spread misinterpretations. I have no idea on how to completely eradicate this problem, but I am sure we can build an NLP-powered system to detect click baits and zap them from our news feeds ;). And let’s not even begin to talk about the constant manipulation of social media and the negative effects it has on society. We are going to break these things down — one by one — in future episodes of this chronicle. We are going to communicate with as less jargon as possible too. “Let’s get it!”
Education and Inclusiveness
A lot of people ask me what are the best NLP courses online since they are only a very small number available. I do believe OpenCourseWare and MOOCs are crucial technologies to share knowledge and resources in our community. Generally speaking, I think we need to invest more efforts in unifying and assisting our community, whether it be through online teaching or open discourse. I mean, isn’t it ironic that we work daily on improving communication systems and dialogue systems but yet communication with the outer world is not our forte. I hope that I am not being too critical here, but it’s pretty obvious that there is room for improvement here as well. Conferences are just not enough… in my opinion, they are too exclusive. YouTube helps, but it is too disorganized. MOOCs are another option, but you will find that they also need to be improved and in some cases updated.
As a third-year Ph.D. student, and thanks to the years of exposure to the field, I have managed to come up with a formula for conducting various NLP research projects, and in a very short time. But it has come to my attention that there are bigger problems in this field, such as ethics and communication that need more attention. So even though I am helping to advance core areas in NLP, I feel like the balance is tilting too much in one direction, which is usually not a good thing.
I am also aware that some beginners in the field struggle to find interesting topics or even datasets to work with. Don’t panic, this is quite normal in this field. Many would say “focus” is the way to go, but I would argue that “interdisciplinary” research is the direction. Take, for instance, Dan Jurafky, for me one of the coolest and smartest NLP researchers out there. All his epic and amazing work are based on interdisciplinary studies. There is lot to learn from researchers like him. By the way, we also need to consider more inclusiveness because this is what enables interdisciplinary and intercultural studies, which is another huge area of potential for NLP researchers and enthusiasts.
The NLP Chronicles:
Initially, this was intended to be an article about ethics and education as it relates to the field of NLP, but as I kept writing I realized there was more and more. So I summarized a few issues and observations, and it gave me the idea to start a chronicle, where I will spend at least once a week discussing a wide range of topics that include but not limited to the following:
· Important considerations when starting NLP research projects and applications
· Ethics and Society in the context of NLP
· How to publish and communicate your NLP ideas: Discussions / Podcasts / AMAs
· NLP communities and science communication
· Interdisciplinary and intercultural topics and trends
· NLP educational resources
· Recent NLP breakthroughs
","NLP Chronicles #1: Interdisciplinary studies, ethics, communication, MOOCs, inclusiveness,…",16,nlp-chronicles-1-interdisciplinary-studies-ethics-communication-moocs-inclusiveness-80b398eefcfc,2018-05-04,2018-05-04 13:04:10,https://medium.com/s/story/nlp-chronicles-1-interdisciplinary-studies-ethics-communication-moocs-inclusiveness-80b398eefcfc,False,1593,Diverse Artificial Intelligence Research & Communication,,,,dair.ai,ellfae@gmail.com,dair-ai,"MACHINE LEARNING,ARTIFICIAL INTELLIGENCE,RESEARCH,TECHNOLOGY,DATA SCIENCE",dair_ai,Ethics,ethics,Ethics,7787.0,elvis,"Researcher and Science Communicator in Machine Learning and NLP; I discuss more about Linguistics, Emotions, NLP, and AI here: (https://twitter.com/omarsar0)",41338000425f,ibelmopan,1667.0,661.0,20181104
0,,0.0,,2018-09-04,2018-09-04 17:01:03,2018-09-04,2018-09-04 17:26:35,3,False,en,2018-09-04,2018-09-04 17:35:27,5,10e136bf726c,2.991509433962264,0,0,0,The Deep Valley of Uncanniness,5,"Week 1 Proximity between Humans and Robots
The Deep Valley of Uncanniness
Mashiro Mori’s the Uncanny Valley shows the issue of designing things to be especially human-like but not quite yet. The valley occurs between two peaks where the object becomes extremely human-like but are missing important features of “human-ness”. Mori used Prosthetic Hand and Myoelectric Hand as an example to show the effect of the uncanny valley. Though these hands look somewhat realistic, these hands lack details such as human’s warmth touch. The Uncanny Valley address the issue of designing human-like robots and their negative effect on human affinity towards these robots. The graph of the Uncanny Valley shows that affinity is maximized around the middle and at the end. This shows that human-likeness need not be achieved to increase affinity. This shows that Robot, AI, and Cyborg need not be human-like to increase our affection towards them. Mori urged robotics designer to capitalize on the first peak as it is less likely to reach the uncanny valley. The Uncanny Valley show that such close proximity of robots to human could create the uncanny effect and decrease our affinity towards the robot.
The Uncanny Valley — Mashiro Mori
The R.U.R and Origin of Humanoid Robots
The R.U.R. writer Čapek coined the term “robota” which mean artificially created person. The shows revolve around artificial human and their human counterpart. The story shows that the robots and people were very similar as they were biologically created but they do lack what is referred to as “soul”. This is similar to the nuances that are missing in human-like robots that create uncanniness. In Act 1, the conversation between Helena and Dr. Gall shows that the destruction of humankind started by creating more human-like robots and the consequences of creating such progress was not heeded as the warning for Dr. Gall.
“Helena: If only you knew how he hates us! Are all of your robots like this? All the ones you started to make . . . differently?
Dr. Gall: Well, they do seem somewhat more excitable, but what can you expect? They’re more like people than Rossum’s robots were.
Helena: And what about that . . . that hatred? Is that more like people?
Dr. Gall: (shrugs shoulders) Even that is progress.”
From R.U.R Act 1
Similar to Mori’s warning, R.U.R exposed the larger threat of creating a human-like robotics beyond just appearance and interaction. R.U.R shows that decision making artificial intelligence could destroy humanity because of the flaw of human that are being programmed into these robots. Thus, the topic such as AI, safety, and ethics have become increasingly important to discuss.
Designing the Robot of the Future
In the 21st century, there are robots that follow Mori’s suggestion and other examples that do not. Such examples include Vector and Jibo, which are “smart” companion robots that do not have human resemblance. They do not have much “productive” functionality except answering easy questions that can be accessed on the internet. These robots are programmed to have affective behavior such as cute voice, dance, and emotive eyes. I believe these designs of robot distance itself from the humanoid robots by capitalizing on the first peak of the Uncanny Valley and they need not be humanoid.
Jibo — https://www.jibo.com/
Anki Vector — https://www.anki.com/en-us/vector
On the other hand, the robot such as Sophia dove straight into the Uncanny Valley because it tries to mimic the human facial expression and body language, however, it is still missing some nuances of human expression and body language. Even though Sophia is just a research project to increase human-like expression for human-robot interaction, the question stills remain whether such nuances of mimicry can be achieved? and if so, what are the implication of creating a humanoid robot? Furthermore, what future design decision must be made so that these negative effects do not harm human in any way?
Sophia by Hanson Robotics — http://www.hansonrobotics.com/robot/sophia/
",Week 1 Proximity between Humans and Robots,0,week-1-proximity-between-humans-and-robots-10e136bf726c,2018-09-04,2018-09-04 17:35:27,https://medium.com/s/story/week-1-proximity-between-humans-and-robots-10e136bf726c,False,647,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Wei Wei Chi,Wei Wei is currently a graduate student at Carnegie Mellon University where he is pursuing a Master of Science in Computational Design.,f8c7a5b45db7,wei_sq,1.0,9.0,20181104
0,,0.0,,2018-08-09,2018-08-09 14:55:56,2018-08-09,2018-08-09 14:59:08,1,False,en,2018-08-23,2018-08-23 15:07:38,5,51d2478af1b4,5.826415094339622,35,2,0,Quantifying and Persuading Humans with Data,4,"Automating Manipulation
Quantifying and Persuading Humans with Data

I know how to control you with a robot.
Or at least, I’m learning to. Though I suppose what I really mean to say is that my robots and AI are. If this doesn’t scare the crap out you, it should. Not because we suddenly have technology that we can use to subtly manipulate people into doing what we want them to do — we’ve had that for years. Nor should you fear that we suddenly have access to information and data that allows us to know a person to a frightening detail — that’s nothing all that new either. You should be most disturbed by the fact that our manipulative tools, for the first time in history, are starting to convincingly look, sound, act, and feel human.
There are numerous technological advancements that are enabling this new wave of personalized persuasion. Natural language processing (NLP) and text-to-speech (TTS) technologies have come a long way since the early days of ELIZA and awkward digital voices created by vocoders. Contextual conversation systems are getting better and better at understanding human intent and are even starting to use imperfections within their voice that make them sound more life-like. Few better demonstrations of this exist than Google’s Duplex system that, say what you will about the sources and selective editing of their early demonstrations, sounds frighteningly humanlike. A crucial breakthrough of recent years has been to go beyond simply correct pronunciation of words and to work on perfecting the vocalics: rate, pitch, inflection, volume, and variety of the voice that give it a richer timber and experience.
From a physical standpoint, we continue to make robots whose features — eyes, skin, hair, and teeth — climb higher and higher out of Mori’s Uncanny Valley. Whether it’s Hanson Robotics’ Sophia receiving a bullshit honorary citizenship from Saudi Arabia or Hiroshi Ishiguro trolling people at academic conferences by sitting beside his robotic clone, Geminoid HI-1, to see if people can tell the difference, our current efforts make a good case for needing a Voight-Kampff test within the next 10 years.
Beyond the physical design of these systems, a key step forward in more lifelike interactions with robots has been our understanding of nonverbal communication. On the one hand, computer visions systems have been slowly learning how to gauge our emotional reactions via facial expression monitoring or body language recognition. On the other hand, we are taking these learnings and embedding digital ticks, flinches, and behaviours back into our robots to make them appear as equally lifelike and expressive as we are.
However, the largest leaps forward in recent years have been in what’s going on upstairs (or in the basement depending on where the designers placed the CPU). It’s the deep-learning enabled intelligence that, while still nowhere near general AI or a human-like intelligence, is getting good enough for government work (in some cases, literally). Through relatively straightforward learning algorithms, humans are being quantized and reduced down to simple profiles that allow organizations big and small to understand a small fraction of an individual, yet communicate back to them in a convincing or effective way. It doesn’t work all the time and it won’t work forever, but it doesn’t have to in order to accomplish relatively simple feats like influence a purchase decision, sway a voter, or negotiate your terms down. Even if it only works a little bit and sometimes, on the scale that can be reached with these kinds of technologies, one only needs a relatively low effectiveness in order to have significant, global impact.
And if anything from that last bit of what I just wrote excites you, let me be very clear: you frighten me. This article is not meant as cerebral soft-core porn for digital marketers to intellectually get-off to; it is a cry for help and a call to arms. The core problem is that for each of the small armies working on advancing these technologies and for the legions of businesses deploying these technologies to excerpt their influence on markets, nation-states, and communities, there is but a handful of ethicists, philosophers, or simply morally-rooted individuals advocating for pause and reflection. However, this small, fragmented group is growing tired of playing the moral conscious of the world while organizations blindly charge forward towards a fabled nirvana, leaving many of us feeling like crazy people standing on a street corner somewhere wearing a sandwich board that simply reads, “what the hell were you thinking?”
That simple and inelegant question highlights my concern with much of business and engineering culture today. For the unfortunate, knee-jerk response that many individuals have to this question is simply “nothing” as we have shed our gift of reflexivity in exchange for faster and greater productivity. We move faster, carry more, and do more; however, we see less along the way and have neglected the scent of too many passing roses. Our thirst for La Technique — society’s obsessive focus on hyper-efficiency and productivity pervading every aspect of our functional and social lives — has become unquenchable to the point that we have now created meta-technologies that can optimize our optimization.
The reason this concerns me so deeply in the case of robotic and AI technologies is that, until we do a better job of considering and clarifying our moral position on digital persons (a term you should prepare yourself to grow very comfortable with), I believe we have a duty to humanity to exercise transparency in the use of automated agents. Google was quick to insist that their Duplex system would be used along with the agent identifying itself as a ‘digital assistant’ at the start of a call, however, beyond its identify, what else should our technologies be transparent about? If we are attempting to recreate human-like relationships, should there not also be human-like pretense and awareness of power dynamics? Should a digital agent have to state its objectives? Should it identify its patron or owner? Should it reveal what it knows about you and what data it used to draw its conclusions? Should it identify what it is learning about you during the interaction? Should it be able to lie?
These are the questions that few organizations have devoted the proper time or resources to considering. I won’t be so naïve as to think that organizations are going to choose to answer all of these questions as ethically-pure angels, however, an identification that they exist and the recognition of an organization’s general policy or beliefs around these kinds of questions would, at a minimum, allow individuals the ability to choose if and how they engage with different organizations and their agents. Without the appropriate information to advise this choice, we essentially leave people with no choice at all.
And without the recognition of these questions, organizations essentially leave this choice up to the individuals on the front-lines of their technical development. They are massive engines of productivity who have distributed the steering and acceleration tasks across different sub-components of their vehicles; they have forgone strategic, long-term thinking and leadership in order to allow the technical masses to chaotically determine their, and our, fates. This is irresponsible, this is lazy, and this is short-sighted. Any victories claimed now and in the near future are won with the caveat of the fragile, teetering foundation upon which they are built. People may have stupid moments; however, they are not stupid forever. And just like Cambridge Analytica’s hands getting caught in the big data cookie jar, all organizations engaging in ethically questionable data and automation use, are essentially operating on borrowed time until their day of digital reckoning comes.
Ultimately you can justify your efforts as an ethical decision, a cultural decision, or a financial decision; it doesn’t matter so long as there are efforts. Technologies are improving and learning to better imitate and manipulate us with each step forward. The choice to ignore the implications of these technologies within your organization will have ethical, cultural, and financial blowbacks given enough time. I do not caution the development of these technologies — this is nearly a foregone conclusion that I myself am involved in — however, I caution the deployment of them before a diverse group of perspectives have given fair discussion, consideration, and clarification on how they’ll be used. Google’s recent removal of “don’t be evil” from their code of conduct, while disappointing, is completely allowed — so long as in its place, they introduce the transparency and clarity that allows you and I to decide which necessary evils we will tolerate. Likewise, you’re free to be as good or evil as you’d like with automation tech, but opaque deception and manipulation will catch up with you sooner or later.
Shane Saunderson plays with robots, minds, and organizations.
",Automating Manipulation,263,automating-manipulation-51d2478af1b4,2018-08-23,2018-08-23 15:07:38,https://medium.com/s/story/automating-manipulation-51d2478af1b4,False,1491,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Shane Saunderson,"Robot sympathizer. HRI PhD @UofTMIE. Writer @misc_mag, @DgtlCulturist, @chatbotsmag. Vocals & guitar @TheNobleRogues & @HailRobot. Plays well with others.",165af04eed32,ArtificialShane,94.0,114.0,20181104
0,,0.0,,2018-01-17,2018-01-17 17:20:26,2018-01-22,2018-01-22 01:07:27,1,False,en,2018-01-22,2018-01-22 01:07:51,6,e52d12071acd,1.4075471698113209,2,0,0,"I’m glad to read such a wide-ranging report on Ethically Aligned Design, from the IEEE Global Initiative on Ethics of Autonomous and…",5,"
Artificial Ethics
I’m glad to read such a wide-ranging report on Ethically Aligned Design, from the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems.
I read Philosophy at university and migrated through a technology career to a PhD in Engineering. The crossover between software and ethics has always been obvious to me but now technology is driving the agenda. My recent role in a Connected Car startup brought this home: how will a truck traveling at 60 mph deal with a choice of who not to kill? Furthermore, the current shift to message-based chat/voice apps and assistants provides a massive opportunity to insert AI and ML into everyday life, with profound impact on nudging behaviours as well as direct influence on (and delegation of) decision-making. And in the enterprise technology sector, the potential for AI-ML is so huge that many CIOs are overwhelmed by what to do. Its all happening now.
I teach graduate students who are whip-smart and totally grounded in these emerging technologies. ML and AI is part of the fabric of their lives, not a big new thing. Frequent interaction with these students reminds me that ethical guidance is not part of the curriculum, nor is it even an expected topic of concern.
So what’s next? This IEEE document is a great framework for further, wider discussion. Maybe popular fiction such as Black Mirror is a more immediate way to present ethical concerns. Perhaps we need to incorporate mandatory classes on the ethical context of technology in future STEM courses. There is a widening distance between technology development in confidential labs and regulatory-ethical-legal frameworks. Time to mind the gap.
Dr. Keith Cotterill is an Innovation Lab Fellow and adjunct lecturer in Innovation & Entrepreneurship at Carnegie Mellon University, and a successful executive, entrepreneur and investor in enterprise technology. He graduated in philosophy from Oxford and Engineering from Cambridge.
",Artificial Ethics,38,artificial-ethics-e52d12071acd,2018-01-22,2018-01-22 06:22:15,https://medium.com/s/story/artificial-ethics-e52d12071acd,False,320,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Keith Cotterill,,524ef004e3b8,kcotterill,1.0,6.0,20181104
0,,0.0,,2018-04-26,2018-04-26 09:56:56,2018-04-26,2018-04-26 09:42:30,2,False,en,2018-04-26,2018-04-26 09:56:57,3,4ba2820672a7,2.077672955974843,0,0,0,,5,"BIG European AI hub to get into race of global competition


A few weeks ago we wrote about the prospects of Artificial Intelligence and that Europe is lagging behind within the field. Now the EU Commission has presented the European strategy to get into the game, Vice-President for the Digital Single Market Andrus Ansip said:
“Just as the steam engine and electricity did in the past, AI is transforming our world. It presents new challenges that Europe should meet together in order for AI to succeed and work for everyone. We need to invest at least €20 billion by the end of 2020. The Commission is playing its part: today, we are giving a boost to researchers so that they can develop the next generation of AI technologies and applications, and to companies so that they can embrace and incorporate them.”
Besides this general focus on the importance of development and boost competitiveness, the EU Commission in comparison to US and Asia is directing a lot of attention to the ethical aspects of AI development: The AI should be put in the service of the Europeans.
In this development plans of a European AI hub, ELLIS institute, that stands for European Lab for Learning and Intelligent Systems, where the vision is that it should produce world-class research for the next generation AI. But also like CERN, the particle physics lab near Geneva, develop into a power source to avoid brain drain from Europe and be able to become the magnet for the AI talent around the world. In an open letter to European governments scientists in the UK, France, Germany, Switzerland, Israel and the Netherlands is promoting this vision and that it should be in place by the end of this year. In this spirit, the EU Commission according to its new strategy will support business-education partnerships to attract and keep more AI talent in Europe. The focus will be set on dedicated training schemes with financial support from the European Social Fund, and support digital skills, competencies in science, technology, engineering and mathematics (STEM), entrepreneurship and creativity. However, an AI hub like the vision of the ELLIS institute would be “the icing on the cake” that actually give Europe the best possible prospects of taking the lead of the next generation of AI.
Zoubin Ghahramani, professor of information engineering at Cambridge University, add one more aspect:
“The regulatory environment for technology is often led by the people who control the technology.”
To build the ELLIS institute would be a guarantee to make AI work for the people, which correspond well with the visions of the EU Commission.
Written by
 LarsGoran Bostrom©
Originally published at eLearningworld.
",BIG European AI hub to get into race of global competition,0,big-european-ai-hub-to-get-into-race-of-global-competition-4ba2820672a7,2018-04-26,2018-04-26 09:56:58,https://medium.com/s/story/big-european-ai-hub-to-get-into-race-of-global-competition-4ba2820672a7,False,449,,,,,,,,,,Ethics,ethics,Ethics,7787.0,LarsGoran Bostrom,"Author, entrepreneur and founder of eLearningworld",9fc6429ef55b,larsgoranbostrom,6.0,46.0,20181104
0,,0.0,,2018-05-10,2018-05-10 06:43:36,2018-05-10,2018-05-10 06:47:54,0,False,en,2018-05-10,2018-05-10 06:49:31,1,9b852c959692,5.611320754716981,0,0,0,There is this one episode of the TV-Series LOST that I can’t seem to shake off my mind. It’s episode 7 of season 1 titled “The Moth”. I…,2,"Google Duplex and the Struggle of the Moth
There is this one episode of the TV-Series LOST that I can’t seem to shake off my mind. It’s episode 7 of season 1 titled “The Moth”. I believe it’s one of the best hours of TV I have watched and the metaphor certainly comes to my mind quite often. One of the main characters (Locke) teaches another one (Charlie — who is struggling with withdrawal symptoms of heroin) that nature’s way of finding the best is through struggles. Locke demonstrates that by showing him a moth in its cocoon shortly before emerging from it. He says that he could help the moth — he has a knife and he can rip open the cocoon, helping the moth and letting him out. But the moth is not ready if it can’t break his own cocoon. The episode is worth watching as the two larger stories represented in the episode are also an expansion of the metaphor.

I watched yesterday parts of the annual Google IO event where the tech giant presents what has it been doing for the past year and where it’s heading for the next one. This year they have revealed a lot of things around Machine Learning and lots of ways that it is going to interact with us. One of the most talked about ones is called Google Duplex.
The idea of Google Duplex is to tell your Google Assistant “OK Google, make a reservation for that restaurant” and Google will call the restaurant on the background and make the reservation for you. And it won’t be a person but a simulated voice. The technology shown in the demo is incredible — they went even so far as to incorporate “uhmms” and “aha-s” in the voice simulation at the right places as to sound more natural. If you haven’t heard it yet, have a listen on their recent blog post. I think for the first time I would give a pass on the Turing test — I heard the voice samples before reading the post, I assumed it’s some natural voice thing but I could not tell who is the robot and who is the person.

While I am absolutely amazed at the technological advances, the voice synthesis, the natural responses, the real-time answers, the handling of complex situations demonstrated — I have also been thinking a lot about technology from the ethical standpoint. There is no point probably of asking “Do we want that” as technology will move forward no matter our preferences — if it wasn’t Google this year, it would be Microsoft next or a green startup 3 years from now. In the large picture it wouldn’t really matter who came up with the technology, obviously it would happen. But can we really start talking about regulating it somehow? I am shaky on my opinions of whether AI will become smarter than us and not allow us to regulate it — good arguments on both sides, we will have to wait and see I guess. But for now, we can regulate it.
Why this one though? We have had natural sounding Siri-s, Alexa-s and Cortana-s for almost a decade now with vast improvements in the last 5 years. What is different about Google Duplex? Again, if you haven’t heard it, you must. It does not sound like the voices of your assistant. The feeling that at least I got is… well, let’s say there are some people I know that talk way more mechanically than the AI on the phone. The little nuances of “mhm”s go a long way.
Many questions popped into my mind and while reading news articles, comments on HN and reddit. Here are a few in a listicle way:
Wouldn’t agents mind receiving calls from bots? — As far as I understand many will actually prefer calls from bots — at least it will be a straightforward conversation and to the point without dealing with weird creepy people.
Will they know it’s bots? — They might learn some patterns. But now the other side learns too, it’s not a preset algorithm as good-old-programming. The bots are learning responses so a call this month might be different than a call last month. And the voice cover this up. But maybe people are still smarter and they figure it out, then…
Would I be bothered that the other side is a robot? — As in, if I understand the other side is something I can’t hurt the feelings of, because it doesn’t have feelings, I may be much more harsh and avoid social politenesses that we have built in our society. If at the same time I manage to guess bots from people but I have a failure false positives or false negatives, would that deteriorate our human conversations?
The bot now has a personality? — It’s not anymore straight out factual answers. Embedding uhms and politeness cues like Thank you and Goodbye gives that bot a personality. It could be overly polite or underly polite. Is the bot going to learn the English way of over excusing itself all the time or the harsher American way of straight to the point? What are the cultural implications — because Google is an American company, are all the cultures going to become slowly just one?
What if the agent is also a bot? — Google didn’t say or confirm if they provide the same service on the other side — a bot being the agent. It hinted it though and there is no reason it won’t provide the service next year. Then two bots will be talking in this highly inefficient human way? Will they send silent cues that they are bots and negotiate a more efficient protocol midway?
All these questions and pondering came to my mind just from this slight improvement in the voice output. But the one that I am mostly thinking about these days is the problem of the increasing social isolation due to technology. Bear with me.
Google creates this Duplex service. How much time does it actually save? It won’t take more than 2–3 minutes to make a call or make an online reservation. Sure, it also lifts up some of the decision fatigue from the day and helps social anxiety types. But does it help them really?
Get back to the cocoon. We are getting more and more help from technology. At the same time the real-life interactions with people are becoming less and less. We IM and email more, we meet less. We swipe, like and look passively at photos, sending little yellow faces of emotions and we smile genuinely less. A phone call is in the middle of what I would call a real-life interaction and a fake interaction like chat. It does have real voice and it’s very instantaneous — unlike chat or email, you can’t spend much time re-thinking your response — it has to be almost immediate, thinking on the spot. You don’t see the person face to face so you also have to guess some of the social cues from the voice alone. This kind of interactions maybe helps us a little bit in keeping the humanity.
Why is this important — keeping the humanity? Well, because we see what happens when we avoid or deny our human nature thinking that technology solves it all. We isolate in our own bubbles, we go back to our cocoons and we stop fighting, stop having the struggle with real life. This makes us passive, goal-less, makes life meaningless, lonely and empty.
It’s not just Google Duplex that does this — far from it. But it’s one more step towards the fake interactions. Because the thing on the other side doesn’t feel offended, it is not hurt by my words, there is no risk of making a human being feel happy or sad and when I talk to bots and they don’t feel anything… I lose empathy. I lose the training of understanding social cues and feeling what the other person is feeling. I lose the struggle of figuring out what went wrong and trying to fix it. I stop evolving.
Of course this is an exaggeration. Yes, I am jumping to conclusions. Maybe I am wrong and this actually helps us get better with other people — use bots to train us out of human suffering. But the more I learn about technology and people, the more I experience the effects of how technology makes me lazy and complacent, the more cynical I become about the future. And I believe it’s healthy to have these sorts of conversations and queries because I think many people in technology are sometimes so driven to achieve the goal of progress that forget about the ethical implications. I am one of them.
",Google Duplex and the Struggle of the Moth,0,google-duplex-and-the-struggle-of-the-moth-9b852c959692,2018-05-10,2018-05-10 06:49:32,https://medium.com/s/story/google-duplex-and-the-struggle-of-the-moth-9b852c959692,False,1487,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Daniel Tsvetkov,,2c4dab7ed2e4,danieltcv,54.0,55.0,20181104
0,,0.0,ac0d66cd66a6,2017-12-10,2017-12-10 20:07:40,2017-12-10,2017-12-10 22:49:44,1,False,en,2017-12-23,2017-12-23 15:06:50,0,cbe8e94d1fab,10.879245283018868,2,1,0,This article is about why artificial intelligence is challenging our views on human and animal rights. How intelligent should my robot be…,5,"When is it Okay to Abuse Your Robot?

This article is about why artificial intelligence is challenging our views on human and animal rights. How intelligent should my robot be before it is morally unacceptable to beat it? When does a robot stop being an inanimate object and start being ‘someone’? Do we even have a moral framework which is sufficiently robust to have this conversation? The chances are that if you have thought about any of this, then you have probably thought about human and animal rights.
But before we proceed, I have a few disclaimers: that is not me in the photo, I don’t have a baseball bat and I have not harmed a robot. I did accidentally step on my Roomba 980, once, and I had not consulted my dog before I bought the robot. But here, I am going to take us to a whole new level, although the story is not as cute as a Roomba with a hello-kitty theme.
I will untangle some thorny philosophical topics on ethnics as we proceed. Of course, many great philosophers have written notable works on ethics, but few have successfully grappled with the conflicting criteria by which someone or something is bestowed rights — John Locke, Immanuel Kant, René Descartes, Thomas Aquinas, Peter Carruthers are notable exceptions.
But rather than appeal to those works, I will start by acknowledging mankind’s long history of complicity in the mistreatment of other humans and animals. We turn to a moral crime scene involving slavery and abuse (and sexism), then revisit some pertinent evolutionary biology before tackling ‘robot rights’ as a special case of animal rights.
The Slave and the Chimpanzee
William Smith’s ‘A New Voyage to Guinea’, dated 1744, describes an explorer’s account of the animals of Sierra Leone. At that time, most Europeans had dismissed the notion of hairy human-like creatures as myths. One animal, known by the white colonials as a ‘Mandrill’ was particularly intriguing. Its remarkable resemblance to a human and not of an ape, captivated the colonists.
“Their bodies when full grown, are as big in circumference as a middle-sized man’s — their legs much shorter, and their feet larger; their arms and hands in proportion. The head is monstrously big, and the face broad and flat, without any other hair but the eyebrows; the nose very small, the mouth wide, and the lips thin. The face, which is covered by a white skin, is monstrously ugly, being all over wrinkled as with old age; the teeth broad and yellow; the hands have no more hair than the face, but the same white skin, though all the rest of the body is covered with long black hair, like a bear. They never go upon all fours, like apes; but cry, when vexed or teased, just like children….”. [extracted from Thomas Henry Huxley. ‘Evidence as to Man’s Place in Nature’]
William smith was asked to present one of these strange animals aboard Mr. Cummerbus’s slave boat. He brought a six month old female baboon like creature on board. The slaves knew how to feed and nurse it as it was a tender animal. The sailors would tease it, some loving to see its tears and hear it cry. One even hurt it, because he disliked its snotty nose.
This sailor is said to have told the slave caring for the creature that ‘he was very fond of the women in Sierra Leone and asked the slave if he should not like the creature for a wife?’.
‘No, this no my wife; this a white women — this fits wife for you’ wittingly replied the slave. The next morning, the creature, no doubt a chimpanzee, was found dead under the windlass.
This story succinctly captures the primitive rhetoric of racism and speciesism that these colonial predators adopted. A blatant disregard for anything different than himself, whether a slave or an animal, no matter how much a like or vulnerable — the signature hallmark of a colonist or a predator. No doubt, the sailors had been desensitized by the horrific scenes involving the slaves, hardened by the sea and the dire consequences for a sailor not conforming with the pack.
In that story, we see that a child-like creature is tormented and killed. The slaves had compassion and the sailors did not. From a behavioral perspective, these three characters, are functionally similar. Each has the ability to process information from the environment around them, to convey and perceive emotions, to make decisions based on sensory information from the environment and perceived emotions of other beings in that environment. While all three may experience different sensory stimuli and exhibit variable degrees of executive reasons, their mechanism of learning and interacting with the environment for mating and survival is likely very similar.
Yet, if I told that story to many in the world, most would relate to the suffering of the slave, but less so to the chimpanzee. Perhaps that it was an infant chimpanzee would play to its advantage. But many would reason that the slave and the sailor are human and therefore bestowed human rights. The chimpanzee is an animal and is therefore not bestowed the same rights.
Let’s breakpoint here and untangle this logic. The problem with this rhetoric is multi-fold:
Humans are animals but animals rights are not human rights
First, there are complex structures of species and genuses that transcend a continuum. If we, as many of the anatomists of the eighteenth century, delineated species by anatomical functions, then we unquestionably arrive at the homo sapien being separate from the other species. Of course, at that time, there was much dispute over the classification of primate species, but few scientists would argue today that the anatomical classification distinguishing humans from apes is clear. It is also clear that we were once apes and we have ‘mutated’ into humans to adapt to the environment. The chimpanzee’s (Pan troglodytes) genome is 98.8% similar to the human genome. The bonobo (Pan paniscus), which is the close cousin of chimpanzees, differs from humans to the same degree.
Geneticists use a variety of techniques to calculate these percentages. The 98.8% chimp-human distinction, for example, involves a measurement of only substitutions in the base building blocks of those genes that chimpanzees and humans share. A comparison of the entire genome, however, indicates that segments of DNA have also been deleted, duplicated over and over, or inserted from one part of the genome into another. When these differences are accounted for, there is an additional 4–5% distinction between the human and chimpanzee genomes.
Regardless of the technique used to estimate the percentages, humans, chimpanzees, and bonobos are more closely related to one another than either is to gorillas or any other primate.
So if we cluster ape species by genealogy, we are in the same group as apes. This DNA evidence is arguably the greatest discovery in modern times: humans are in the ape evolutionary tree; We are animals.
Richard Dawkins has famously posed the thought experiment on how much an ape has to be like a human before it is bestowed the same rights as humans. Is it 99%, it is 99.9%? It seems more than a little arbitrary that we have come up with one set of rules for us, and entirely another for the rest. Sounds like the 99% and 1% anti-wall street rhetoric and more broadly the hallmark of elitism! We actually share genes with all living organisms and so a biological kinship between us and all animals is well established.
Should we treat someone disrespectfully because they are less intelligent?
Of course, by our definitions of intelligence, we have higher cerebral capacity. But why does more intelligence give license to deny someone or something with less intelligence the same rights? Should we treat a low functioning child with less dignity than an intelligent adult? Should we treat a victim of Alzheimer’s disease with less dignity? What about someone in a coma?
Where does respect for life and intelligence come from?
I offer one perspective on how I have come to develop respect for all forms of life here. I write software, research machine learning, and have a math and engineering background. I am overwhelmed by how complex the anatomy of an insect is, let alone an animal. I can not make that. I might at best be able to simulate part of its behavior, but even a cockroach is an extremely complex organism. There I am, perched at my laptop trying to write machine learning code for processing data from a sensor with an embedded linux operating system, and a fly lands on my hand while I am typing. This creature is bewilderingly complex compared to the scientific and engineering problem that I am researching. I can not train my sensor to be as autonomous as the fly — the fly exists in the real world, my code is limited to what ever vessel I run it in. Even if my sensor was mounted on a drone, it will need to be managed by someone. The fly is free, it operates entirely independently. Note that I am deliberately avoiding the biggest philosophical black hole — consciousness. Suffice to say that respect for life comes from a variety of sources, religion of course, but also borne from a scientific and child-like sense of wonder and awe. If you don’t see anything special about a fly, try to make one.
Regardless of whether you have respect for all forms of life or not, are you willing to commit to leaving this world in a better place than you left it? That doesn’t mean going to live music concerts and getting high off peace vibes, but maybe that opens you up to being a richer and more compassionate human being?
It’s just a pig, who cares?
The prevailing rhetoric among humans when they see a pig in a slaughterhouse, beaten and bruised, quivering in pain and deprived of any dignity, is ‘That’s just a pig. Who cares?’. The rhetoric wouldn’t ultimately change if it were the image of a chimpanzee undergoing vivisection in the name of scientific progress. I imagine the rhetoric would have been the same if you were white and had witnessed the torrid conditions of negro slaves in the southern states of the US in the nineteenth and twentieth centuries. ‘They are not us. Therefore we have the right to exploit them’. Isn’t the definition of a bully someone who strikes out at someone or something that can’t fight back? No doubt, this is the pattern of perpetuating abuse — transferring suffering onto those who do not have the capacity to push back.
Others who I have discussed this topic with react to my rhetoric with accusations of overthinking the issue. ‘You are taking this to a whole other realm’, said one critic implying that I am delusional and out of touch. I am dismissed and ostracised as a new age yoga zealot who has more money and free time than sense (none of which is true). While much of the religious teaching of Buddha resonates with me, I do not personally believe that we should adopt an eastern philosophy tomorrow any more than we should denounce our greek philosophical roots.
Rethinking taxonomies: Would a behavorial approach be healthier?
The Linnaeus taxonomy is based on anatomical analysis. This taxonomy does not account for neural or behavorial variance which is not directly a function of genealogy. For example, if I were to classify a creature by its functional capacity, then we would arrive at a different taxonomy. We would of course see large differences between the behavior of predator and prey. We would also see differences between those that have trained an executive branch of the brain, the frontal cortex, to moderate and direct responses to stimuli.
I could easily reason that an identical twin who is a gambling or drug addict, has no filter or impulse control is more like an untrained dog, frantically peeing, chewing and humping everything than like her disciplined high achieving sister. Is a guide dog or mother elephant helping her child out of a ditch and dutifully protecting her young a better citizen than an obnoxious jock who spends his time partying on his dad’s boat?
In other words, the behavioral component matters. Clearly the law recognizes this aposterior the crime. But what if we attributed rights across species based on their responsibility and strength of character and not purely on biological traits defined by anatonomy? Of course, we do this when it serves us and we adopt the other argument ‘they are not us’ when we need to rationalize something that we are not ready to face. But let’s at least own it and have a post Lockean discussion about what it means to be deserving of human rights and how do we draw the boundaries.
Robot rights is absurd!
Some broader perspective may help those who are getting too caught up in whether or not we should bestow animals the same rights or think that appealing to animal rights to advance our discussion on robot rights is absurd.
You are someone. You matter. But you, like me, will be dead in the ground in probably less than 75 years from now. In 200 years from now, who will remember you or I? Will this page even be accessible? Whether we like it or not, we suffer the same ultimate fate as any other living creature. No amount of disruptive technology or Silicon Valley hubris can change that simple fact anytime soon. A robot has the potential capacity to operate long beyond a human life duration. It has the capacity to be an ambassador of our culture to the future, when we are long gone. Shouldn’t that be a factor in how we treat something and bestow rights, rather like a protected building?
We also have the capacity to bond or rely on something that has intelligence. Therefore harming that source of intelligence can indirectly harm us. If I take away your intelligent spam filter, it will hurt you. You will have nasty unpleasant emails in your inbox.
Abusing a Robot in Japan
Here’s a thought experiment. Is it ok for me to build an intelligent robot, head to, say, a Japanese airport, and then beat it in front of bemused and shocked travelers? It’s my robot. I own it. It doesn’t have any human rights. I am not breaking the law and I beat it so that it doesn’t shatter and harm anyone. I pick up the mess afterwards like my dropped candy wrapper. Ok, so I am disturbing the public peace. What if I do it quietly, on a street corner when no one is watching, like in the photo above?
Now suppose you are one of the travelers arriving at the Japanese airport, eagerly looking for a loved one to meet you. Knowing that a flight will arrive from Finland, I maliciously decide to decorate my robot with certain traits. The robot is given a Finnish flag, it is clothed in a national Finnish folk costume and it is programmed to speak Finnish. I might place a wooden Tobacco pipe in its mouth for that classic look.
As you glance with weary eyes for your loved one, you see that I have a pet robot and you observe me beating it. This robot is capable of registering damage orally and makes a familiar cry for help. How realistic does the sound have to be before it becomes unnerving? Is it wrong for me to beat the robot because it is likely to trigger unpleasant emotions in you and upset your sensibilities?
How intelligent does the robot have to me before the moral issue shifts from it being just about your own pain triggered from observed abuse and more about the welfare of the robot?
Why is any of this important?
If you are thinking about any of this, as hypothetical or esoteric as it might seem, then you have probably been instilled or developed sound ethical semantics to navigate this world. If you think it’s not immoral to abuse or exploit an animal, then why is it immoral to abuse an innocent human being?
Systemic abuse is born from a cultural norm which places someone or some creature at a lower level. Once we become aware of that, we can advance our discussion of human and animal rights to clearly defining what it means to be a ‘good’ or a ‘bad’ creature and move away from a hierarchy which just sets us up from abuse. How we interact and treat people and creatures should be in response to behavior and conformity with good and bad values. Is it making the world a better place or destroying it?
The innocent are innocent, regardless of color, creed, species or type of synthetic autonomous intelligence.
",When is it Okay to Abuse Your Robot?,60,day-7-of-the-satyre-project-when-is-it-ok-to-abuse-my-robot-cbe8e94d1fab,2018-02-11,2018-02-11 09:54:48,https://medium.com/s/story/day-7-of-the-satyre-project-when-is-it-ok-to-abuse-my-robot-cbe8e94d1fab,False,2830,"Publication that acts as a social networking service for technology, health, human rights, art, history, religion, philosophy, agriculture, and knowledge itself in various ecosystems. Telegram Channel: https://t.me/COTInetwork",,,,Social Club,samuel@coti.io,social-club,"CRYPTOCURRENCY,BLOCKCHAIN TECHNOLOGY,GOVERNMENT,FINANCE,PAYMENTS",FalkonSamuel,Ethics,ethics,Ethics,7787.0,Aesop Moderna,Bringing Aesop’s fables to bear on the societal and behavorial aspects of technology. Ph.D. Mathematician. Prof. at Illinois Tech researching prediction/AI.,44fd2753864d,matthewdixon,33.0,31.0,20181104
0,,0.0,,2018-06-14,2018-06-14 05:40:14,2018-06-14,2018-06-14 10:25:05,5,True,en,2018-06-14,2018-06-14 10:25:37,1,8f5aeac24c14,6.112578616352201,1,0,0,‘The Call’ by Google Duplex. It sounds like a poorly scripted and woefully acted budget horror movie from the 3rd page of Netflix. Or a new…,5,"AI. And We Thought 2016 Had Electoral Interference…
‘The Call’ by Google Duplex. It sounds like a poorly scripted and woefully acted budget horror movie from the 3rd page of Netflix. Or a new fragrance idea from the work experience kid.
“The Call” by Google Duplex.
It’s neither. It’s actually far worse.
The Call represents the most powerful weapon yet in the growing catalogue of Orwellian tools.
People thought alleged Russian interference in the democratic process was devastating. Well, all they used was a few well-targeted facebook ads mixed with existing data - that you mostly provided consensually - about, for example, among other things, which musicians you tell people you hate but actually follow on Instagram.
And yes, I am referring to me regarding Justin Bieber post-Carpool Karaoke...
When I started to like Justin Bieber…
If impersonation technology, like this, is further developed, surely genuine democracy is inevitably threatened.
“Democracy” in the future may be like Bitcoin today. Erratic, online and people think they ‘get it’ but don’t … And when someone talks about it; no one cares.
A few facebook ads is nothing compared to people receiving calls from “their children”, when they're actually being emotionally manipulated by a foreign government’s computer.
It’s not just about Google Duplex. It’s about alternative uses of any AI. You’d think adults would be capable of be giving that some thought before development? I can’t be the only one who thinks democracy is worth protecting…
To be fair, there could be some upside.
Especially if, for example, you could use an AI impersonation to fake interest in phone calls from people you don’t want to talk to.
I just couldn’t stop picturing Donald Trump on the phone to Melania, who’s actually doing a sudoku over lunch, but running the call via an app, so she doesn’t have to hear for the 238th time an hour-long diatribe about how the wind ruins his hair:

“It’s really very annoying. People ask me all the time: how do you manage to keep it looking so good? It does look good, doesn't it? My hair is amazing. Actually it’s probably the best... I’d say probably at least in the top 3 best hair of all time. Look, I say to them, sometimes the wind is bad. Really bad. You wouldn’t believe how bad it can be. But mostly my hair is fine. You want to know what the secret… do you want to know? I think you do. You want to know the secret. Ok, I’ll tell you. Well, the number one thing is… By the way, number one. Number one is my favourite number. You probably know that already. Actually, to be honest, nobody knew that. Well, now you do. I mean, maybe i’ll just have to get another favourite number. Who knows? We’ll see” *
*Not a real quote.
Having a computer takes annyoing calls might be a time-saver for some.
But with impersonation AI, are the benefits worth the trade off? Are there even actual benefits? Did the ecstatic audience think it through?
Google say this AI will “help you get things done.” Really? Given that the internet is, by definition a network of ‘talking’ computers, there are much simpler ways to use AI to arrange appointments. For instance, to book a haircut, one could direct an AI assistant which then communicates directly with the store’s scheduling software using a series of ‘if-then’ rules to find a free time-slot. Physically expending time to engage in a phone conversation, is less efficient than ‘if-then’ automated scheduling between computers.
If the AI goal is “to help you get things done”, human impersonation is entirely unnecessary.
The only people this is “helpful” for is Google.
It was for their own entertainment, for PR and because they can.
I’m not against technology or AI. But human impersonation is different.
With social media, and other big-data, we’ve agreed to it. Mostly, if we’re honest, through the, rather reasonable, inability to carve out a spare 5 decades, which is about how long it’d take to actually read the terms and conditions at sign-up.
But, still, we have agreed. No one forced you on Facebook.

Much of big-data and technology, we’ve at minimum, implicitly agreed to. So that’s why it’s different to Google Duplex. If you don’t want Apple to track your location, which they do unless you opt out, you can un-agree. Google Duplex you can’t.
When companies go on an R&D and PR adventure, we haven’t agreed to that. But it still affects us. Even within this category of enterprise innovation, though, Google Duplex is materially distinct.
When companies invent, we can choose not to buy and then it won’t affect us nearly as much. Don’t buy a self-driving car. But even if you choose not to buy human impersonating AI, it can still affect you in a material way just the same.
Even if you personally don’t fall victim to a fake-human scam, political or financial, it can still affect you when it tears democracy to shreds and corrodes the sovereignty of nations and manipulates the free agency of other people. You rely on people everyday. And you only have one vote. If enough people are manipulated, whether you personally are is practically irrelevant.
It will affect you. That’s why its different to other AI.
I’m not against AI. I am against the selfish creation of ethically repulsive impersonation tools, which have the potential to debauch the entire democratic system, presented by a smug green-jacket wearing nerd under the intentionally devious guise of being helpful, despite no real productive benefit, with zero care for the negative externalities, just because it gets a few claps at one presentation.
Governments post 9/11 are similar. CCTV, metadata and an ability to compel companies to hand over “secure” data to government agencies, is totally without consent and disproportionate to the statistical threat to human life. Probably more people died from freak accidents involving a falling fridge than did from domestic terrorism in Australia over the last decade. Also, I don’t remember ever being asked to click “I agree” to every government knowing everything and sharing it with other governments.
Nineteen Eighty-Four?
Just think of how quickly politicians could squash a revolution against a genuinely corrupt government, given they have access to everyone’s closeted skeletons and can make personalised threats based on tracking rebel leader’s kid’s frequently-visited locations … But that’s a different topic.
The AI question for all of us is: how do we let the market innovate yet ensure a common-sense filter to protect the freedom of every citizens, especially those who don’t consent to particular AI technologies?
Clearly, if anything,The Call shows that the inventors aren’t able to filter themselves in an ethical or contingency regard.
They either knew this AI had huge potential for unimaginable damage in the wrong hands and did it anyway, or it’s an oversight. So they were either arrogant or stupid.
It’s not just Google’s technology that’s becoming a questionable. Though, unlike Google’s Duplex, the following example do impact mostly within their realm of consent (i.e. people who buy them are the only people impacted). But it still speaks to the broader point of “where is the line”?
In-home voice recognition is gaining popularity particularly in the US.
Amazon has home delivery technology in 37 US states that allows delivery people to enter your home to leave packages inside when you’re not there.

These have some inherent benefit, where impersonating a human doesn't seem to. And if people agree to buy in home AI, then good on them. Only their homes are affected. But impersonating a human can affect us all.
At some point as a society, we will actually have to discuss this. Discuss where the line actually is. When is AI enough? If AI affects just the user, that’s one thing, but when it affects others, who haven’t consented, to what extend do we just accept that? And what do we do about it?
The role of government is to do what markets don’t. The Call shows us that markets might not be able to run common sense contingency filters on their own ideas. Like always, policy isn’t keeping pace with progress. But unlike in the past, the stakes are much, much higher.
How, and whether, we can stop anti-common sense AI from crossing the line, is our responsibility…

",AI. And We Thought 2016 Had Electoral Interference…,50,ai-google-duplex-and-we-thought-2016-had-electoral-interference-8f5aeac24c14,2018-06-14,2018-06-14 15:21:42,https://medium.com/s/story/ai-google-duplex-and-we-thought-2016-had-electoral-interference-8f5aeac24c14,False,1399,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Noah Farrelly,,49c0f4dfe490,noah.farrelly,1.0,2.0,20181104
0,,0.0,,2018-05-02,2018-05-02 14:45:46,2018-05-02,2018-05-02 14:47:51,1,False,en,2018-05-02,2018-05-02 14:47:51,10,b0a4f6f3b7f1,1.3660377358490563,0,0,0,A podcast is out!,5,"Ep 43: Is there Bias in Machine Learning Algorithms? — with guest Dr. Joshua Kroll

A podcast is out!
The world today generates an immense amount of data. Companies gather data on our buying habits, our location, and how we spend our time. The enormity of this data is too much for human analysts to dig through alone. Instead, they use machine learning algorithms. These algorithms take big data to analyze your routines, infer your race and religion, and perhaps even surmise private or sensitive information about you. But is there bias in machine learning algorithms?
Just because machine algorithms are “math” does not make them immune to incomplete data sets, incorrect assumptions, and the possibility to make false inferences. So where does bias in machine learning algorithms come from? What can we do about it? And can they be improved? From the frequent buying card at your local grocery store to the Trump Administration’s Extreme Vetting Initiative, we discuss bias in machine learning algorithms and if we should trust these algorithms.
Joining us on the podcast is Dr. Joshua Kroll. Joshua is a computer scientist and Postdoctoral Research Scholar at the School of Information at the University of California, Berkeley. He studies automated decision making algorithms, such as machine learning, focusing on fairness, accountability, and transparency.
You can listen to the podcast here.
You can also subscribe to SparkDialog on Apple Podcasts, Google Play, Stitcher, or on sparkdialog.com.
Science and technology are everywhere in our lives. This podcast takes a look not only at the science itself, but its role in society, how it affects our lives, and how it influences how we define ourselves as humans. Episodes also throw in a mix of culture, history, ethics, philosophy, religion, and the future! Hosted by Elizabeth Fernandez, an astronomer and science communicator. Let’s spark some dialog!
Thanks for listening!
",Ep 43: Is there Bias in Machine Learning Algorithms? — with guest Dr. Joshua Kroll,0,ep-43-is-there-bias-in-machine-learning-algorithms-with-guest-dr-joshua-kroll-b0a4f6f3b7f1,2018-05-02,2018-05-02 14:47:52,https://medium.com/s/story/ep-43-is-there-bias-in-machine-learning-algorithms-with-guest-dr-joshua-kroll-b0a4f6f3b7f1,False,309,,,,,,,,,,Ethics,ethics,Ethics,7787.0,SparkDialog Podcasts,"A podcast about science and how it shapes our lives, with a mix of culture, ethics, history, & religion. Let’s spark some dialog! with Elizabeth Fernandez",f254d44318c1,SparkDialog,12.0,49.0,20181104
0,,0.0,,2018-02-20,2018-02-20 13:02:11,2018-02-20,2018-02-20 17:09:41,1,False,en,2018-02-20,2018-02-20 21:47:07,9,c19a81a72b87,5.943396226415095,1,0,0,99% Invisible Threat of Algorithms,5,"Trojan Horse dot Ai
99% Invisible Threat of Algorithms

“Technology does not automatically improve our lives. It takes a lot of people working really hard to ensure that it does”. - Elon Musk
In May 2018 a revision of the European General Data Protection Regulation also referred to as the GDPR will come into effect. With the aim is to ensure that citizens know how and where their data is being used, It also gives citizens the right to explanation for automated decisions and the right to appose those decisions. While the new regulations are not so much different from the previous GDPR, the fines for non-compliance have been increased and could amount to 4 percent of annual revenue.
While policy notoriously moves at the tail end of innovation, the ethical Artificial intelligence rhetoric has become deafening. It was only in 2017 that people began to notice, and so did I.
If you had happened to read my first post (here), I introduced my final year project on mitigating unintentional algorithmic bias. In today’s post, I will introduce algorithmic bias with examples, I will reveal the 10 factors that I believe are reinforcing the threat and I will begin the discussion about what the value of design is in the context of algorithmic bias.
I began this post outlining the revision to GDPR in order to set precedent and importance of what will be discussed, if you would like to have more context regarding algorithms, and the ethical Ai discourse, I suggest reading this.
Let’s begin
My preferred definition of algorithmic bias was written by professors David Danks & Alex J. London in their 2017 paper Algorithmic Bias in Autonomous systems which states that.
“Algorithmic bias is the worry that an algorithm is, in some sense, not merely a neutral transformer of data or extractor of information.”
The reason I like this definition is that the writers used the term “worry”. The first questions that came to my mind whilst I processed the statement was,
- Well, how worried should I be? And Should I worry now or later?
To me, that term has been on my mind for the past four months, with my uneasiness changing to various states of concern during my research. It is for this reason that I like this definition.
Algorithms are created by humans, and it is widely known that humans have biases. So how do we make unbiased algorithms? The answer is: We don’t. The fact of the matter is that all algorithms created by humans are biased. Because when humans design, build and modify algorithms they are making decisions, formally and informally, consciously and subconsciously, and that means that they are embedding their perceptions of the world into their work. Embedding our views of the world in our work is normal, it happens with everyone. You are reading my views right now. Level of concern[Green].
Humans however are fallible, and given the nature and purpose of algorithms, especially machine learning algorithms that automate decision making, those embedded views get perpetuated so long as that algorithm is running.
Let’s do an exercise, go to the messages app on your phone, open a new message, type in the message box. “The doctor said” now look at the suggestions of your predictive text, if yours is anything like mine It would have given suggestions such as “that, he, it”. Ok, lets try this one more time. “The nurse said,” did your phone also suggest “she”? Mine did! This is just one example of gender bias in our algorithms. What particularly frightens me about this exercise, is the fact that you, wherever you are in the world reading this just experienced the same thing, and it is reinforcing the very prejudices in society that we or at least I wish to eliminate.
Artificial intelligence thrives on data, masses and masses of data, it is one of the reasons we have experienced an Ai awakening. Much of that data however contains biases too so when we feed and teach machine learning algorithms with it, we are teaching it to replicate that behaviour. So in fact when we talk about prejudiced Ai, we are actually talking about an Ai that is replicating the world accurately. Level of concern [Amber].
I have been following the work of MIT MediaLab researcher by the name of Joy Buolamwini She has just published a research paper outlining exclusion in facial recognition algorithms. In her study she tested 3 major companies that offer facial recognition services (Microsoft, IBM & Face++). Her results have offered a quantifiable look at the presence of bias. In her paper titled “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification” she outlined that the algorithms were 99 percent accurate if you were a white male. For darker females however there was an error rate between 21–35 percent depending on the service. This is a good example of a data bias, and also a bad reflection on the engineers who selected the data and did not do enough to ensure that the data was inclusive. Whether there was enough representative data to begin with is another issue, and that relates to broader systemic and socio-structural issues. Level of concern [Red].
Consider this, machine learning algorithms have long been hailed as the technology that will make decision making more reliable and less prone to human bias. By virtue of that statement, if algorithms are biased, then it compromises the very purpose for which it was created.
Here are a few known case examples of algorithmic bias:
Google, Microsoft, Nikon, Snapchat.
It is very important to state that during my research, and engagement of this subject, I have yet to come across any example of intentional bias in algorithms with the intent of being deliberately decisive or prejudicial to anyone and that it would seem all of the known cases were unintentional in their nature.
Algorithmic bias is above all a classification issue. The necessity to classify everything and everyone so that a computer can make sense of it is fundamentally the reason why we have this problem. I do not however see the possibility of a language and system rework in the near future. But there are ways in which we can begin to control and address the issue. This is a list of the 10 problem areas that I believe urgently need to be dealt with if we wish to start overcoming unintentional algorithmic bias.
Lack of Ethical due diligence by companies
Lack of diversity in the workplace
Lack of regulatory oversight
Lack of transparency in the product offering
Lack of a design-oriented process
Lack of experience in developing Ai
Lack of a human centric product
Lack of access to data
Implicit biases in data
Complexity of oversight
There have been calls from various heavyweights such as Kate Crawford co-founder of the AINow Institute which is a research institute dedicated to examining the social implications of artificial intelligence. She has been calling for people of different disciplines to begin approaching this problem. It will take our diverse collective if we wish to overcome not only algorithmic bias, but also to ensure ethical artificial intelligence for all. So I am here, pushing the boundaries of my skills and knowledge, hoping to contribute.
Designer Value
Contrary to what many believe, creativity and design is not only about beautiful lines, forms and living room colours. It is so much more, and anyone in those circles will tell you.
There are many definitions to what design is, my definition.
Design is a formalised methodology for creativity with a toolkit of lenses to create value for humans from different perspectives.
Design as a discipline has been around for nearly a century, and in the process many design methodologies have emerged, have been refined and have been augmented by new technology. Algorithmic design however has not been around that long, and while they have developed and adapted their own methodologies I am of the opinion that a design-oriented algorithmic creation process can be of significant value (point 5 above).
That means there needs to be a calculated shift in how algorithms are designed, most especially those that interact with humans.
First steps: (this list is a work-in-progress)
1. We centre the human as the most important factor
2. We de-prioritise technical difficulty and ensure social value
3. We formalise and justify decision making
This is where my work has been focused, and I will be looking to conceptualise and deliver a physical intervention as part of my final project outcome at the Royal College of Art and Imperial College final show at the end of June 2018.
I attempted to cram as much as I could into this post as I am trying to catchup to where I am in my project. However I hope you enjoyed the read. I am always open to discussion, constructive-critique and suggestions.
In my next post, I will begin discussing the specifics of my intervention, and outline what it is I hope to achieve.
Thanks for your attention.
Sincerely,
Mikhail Wertheim Aymès
My website
",Trojan Horse dot Ai,4,trojanhorse-ai-c19a81a72b87,2018-02-20,2018-02-20 21:47:08,https://medium.com/s/story/trojanhorse-ai-c19a81a72b87,False,1522,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Mikhail C. A. Wertheim Aymès,Innovator / Designer / Algorithmic Ethics Advocate,a2f3ae49a641,mikwaymes,56.0,65.0,20181104
0,,0.0,,2017-12-28,2017-12-28 17:23:16,2018-01-05,2018-01-05 14:16:30,3,False,en,2018-01-05,2018-01-05 14:16:30,14,1f0f6e04e1db,6.599056603773584,8,1,0,"Sensum’s insights from studying the emotions of human-machine interaction with virtual assistants: Alexa, Siri, Cortana.",5,"Alexa, Hug Me: Exploring Human-Machine Emotional Relations
Insights from our investigations into the emotional connection between us and our AI virtual assistants. Photo by Andy Kelly on Unsplash.
The frontline in the battle to implement AI throughout society is the interface between us and them, the human-machine interface. Increasingly, our hands are being freed to do other things as we are given interaction options that don’t require touch. Alongside physical gestures, facial expressions and other forms of communication, the key hands-free connection between us and AI is our voice.
When Siri was released on the iPhone 4S in 2011, it straddled that dangerous borderline between ephemeral novelty and useful companion. It was hardly Samantha from Her, or Jarvis from Iron Man. At this time, the current generation of voice-controlled assistants are still teetering in that boundary zone. They have a long way to go before they ‘just work’ — when we forget we are talking to machines and trust them to know us well.
Nevertheless, the most popular item bought on Amazon in the 2017 holiday season was Amazon’s own Echo Dot smart-speaker. Massive consumer demand for AI in the home is evident.
At our company, Sensum, we recently ran a study to compare the emotional connection people have with some of the top brands of what are technically known as IPAs. No, not craft beer: intelligent personal assistants; AKA automated personal assistants; AKA virtual assistants; AKA voice-user interfaces… I don’t think there is one common term for the technology yet.
In the study, we examined user’s emotional response to the IPAs, and explored the role of empathy in their interactions with the products. Here are some insights and thoughts from what we learned.
The Study
Alexa, Siri and Cortana — our virtual participants.
The study covered Alexa, Siri and Cortana. In other words, Amazon, Apple and Microsoft. Notably, we didn’t include Google’s assistant this time, despite being one of the other top players in the market, but we would include it in future research. Google is also distinct in not having a unique name for the ‘persona’ of its IPA. Google’s assistant acts as a gateway to Google as a whole, rather than attempting to anthropomorphise a separate ‘assistant’ entity such as Siri or Alexa. But I digress.
With participants hooked up to biometric sensors, we could see their near-instantaneous physiological responses to their interactions with the IPAs, and from that infer their emotional responses. The metrics for this study were heart rate and GSR (galvanic skin response, measuring the conductance of skin) which give us the participants’ arousal (excitement/relaxation) and engagement (with the stimulus: in this case the virtual assistant). These are favourite metrics of ours in many scenarios because they can indicate near-instant, and typically nonconscious, emotional responses.
The first intriguing finding was that participants showed high levels of arousal when first using the devices — holding a wake-button like the iPhone home button, or hearing a wake-noise like the assistant saying, ‘hi there’. Our first encounter with a novel form of interaction is a key moment for optimising the design of the user experience, and data from our bodies was shown to reflect this.
We saw elevated heart rate when the virtual assistants didn’t give the participants what they wanted. Particularly, participants got frustrated when the assistants spouted long lists of factual information in response to a simple question, or failed to work due to some issue such as requiring another app to be closed first. These are unsurprising results but it is good to see them backed by nonconscious biometric data.
Participants quickly disengaged when their IPAs didn’t understand them. This typically resulted in the assistant taking a stab at a misheard instruction. Regional accents are a particular concern in this area. We are headquartered in Northern Ireland so we really feel the pain on this one. Contending with comprehension issues such as an unusual accent, these devices quickly become the entertainment in the room, rather than the help.
In our study, Amazon’s Alexa appeared to generate the greatest emotional connection with participants. Heart rate and GSR levels increased when Alexa offered recommendations and told jokes. When Alexa said there was a 50% chance of rain that day, participants paid attention. When ‘she’ said, ‘what do you call a pig doing karate? …A pork chop’, the punchline elicited genuine laughter and corresponding spikes in the participants’ biometric signals.
We are at an early stage for AI to be truly humorous; to be very human-like. But the value of training artificial systems to understand us better and interact with us in more familiar, intimate ways could be vast. …as long as we do it ethically.
‘The goal is to create automated and targeted mind-reading technologies that tailor communications to a given individual. Technologies like these could be used to generate more engaging interactions, but also material and techniques that could most certainly be abused’. From Laughter and Humour as Conversational Mind-Reading Displays — Dr Gary McKeown, Queen’s University Belfast.
Amazon’s best-selling Echo Dot, with their intelligent personal assistant, Alexa. Photo by Andres Urena on Unsplash.
Minding our Ps and Qs
We don’t give virtual assistants the same patience that we afford to our human peers. We expect them to get the job done efficiently and correctly, and we get annoyed when they falter. This is perhaps unfair in the current state of AI’s evolution, but don’t expect users to sympathise. In fact, their interactions can quickly become uncivilised.
To explore the concept of manners between humans and machines, we asked participants to curse and insult the IPAs. They typically felt uncomfortable doing so, which suggests a level of empathy from the humans towards their assistants. But it was the AI response that interested us most. Few assistants retaliated to the abuse. There were some exceptions, such as Siri presenting a cue card with an exclamation mark on it when a curse-word was used.
I admit, when I heard about this issue of manners I thought, ‘who cares? It’s just unfeeling software’. It can be cathartic, even fun, to take out our annoyance on inanimate objects. I am constantly swearing at my laptop and mobile, while remaining very polite with people. But there is a latent moral dilemma in this new form of social etiquette.
For a start, we should consider if being mean to machines will encourage us to become meaner people in general. But whether or not treating Alexa like a disobedient slave will cause us to become bad neighbours, there’s a stickier aspect to this problem. What happens when AI is blended with ourselves? With the adoption of tools such as intelligent prosthetics, the line between human and machine is increasingly blurry. We may have to consider the social consequences of every interaction, between both natural and artificial entities, because it might soon be difficult or unethical to tell the difference.
Our rule-of-thumb is to apply the principle of ‘don’t be a dick’ to our interactions with AI, just as we do with humans.
Looking Forward to Empathic AI
At Sensum, our passion and expertise is focused on understanding human emotion, and teaching machines to be more empathic by sensing and responding to our feelings and behaviour. At this time, the emotional intelligence of systems like Alexa and Siri don’t even compete with the family dog. But they’re getting better with every datum recorded and processed. A highly empathic AI assistant might recognise the difference between its user abusing it for fun, and venting genuine frustration. It should then know whether to respond with sarcasm or sympathy.
Understanding the relationship and trust we have with IPAs, and being aware that these lie on a spectrum from highly invested to distantly involved, could influence the way this kind of AI is integrated into emerging technologies such as autonomous and semi-autonomous vehicles. For one thing, we could learn from mistakes made in the low-risk environment of our living rooms, with tools like Alexa, before implementing consistent AI into higher risk interactions such as driving.
In order for us to benefit from the combined learning of all our human-machine interactions, providers like Amazon, Apple, Microsoft, Google, Sony, Audi, Volkswagen and so on, would need to share information through universal tools or data architecture. This feels like a utopian (albeit potentially terrifying) dream, but it could offer great advantages. We might leave our empathic house and climb into an empathic car, then arrive at our empathic office, all with systems installed that are capable of the same level of personalised interaction with us.
With a personalised, smart and empathic virtual assistant that exists in a continuum across all devices, we could enter a future in which waking up on the wrong side of bed means your house, car and phone can all take steps to help you feel the way you want to. The journey from A to B would no longer just be between physical destinations but also between moods — such as from irritable to overjoyed — before your boss even notices.
Special thanks to Lucy Rutherford, currently studying psychology at Queen’s University Belfast, who managed the study into intelligent personal assistants while interning at Sensum.
Further Reading
Two academic papers that informed this story came from our collaborator, Dr Gary McKeown from the School of Psychology across the road from us at Queen’s University Belfast:
Laughter and Humour as Conversational Mind-Reading Displays.
Turing’s Menagerie: Talking Lions, Virtual Bats, Electric Sheep and Analogical Peacocks.
And here are some relevant stories on this topic:
Pretty Please: Politeness in Voice User Interfaces (by Cheryl Platz on Medium).
Should you say ‘please’ and ‘thank you’ to your Amazon Echo or Google Home? (by Chaim Gartenberg on The Verge).
Alexa, Where Art Thou? (by M.G. Siegler on Medium).
Poll from a story about virtual assistant etiquette on The Verge: http://bit.ly/CGverge_PQ
","Alexa, Hug Me: Exploring Human-Machine Emotional Relations",68,alexa-hug-me-exploring-human-machine-emotional-relations-1f0f6e04e1db,2018-04-19,2018-04-19 04:23:40,https://medium.com/s/story/alexa-hug-me-exploring-human-machine-emotional-relations-1f0f6e04e1db,False,1603,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Ben Bland,"Dabbling & japing with all things tech, startup & creative media. Friendly, verbose, lanky & bald. Writing as COO of Emotion AI company Sensum (sensum.co)",eec566727a18,ben.bland,571.0,419.0,20181104
0,,0.0,,2018-05-09,2018-05-09 22:37:32,2018-05-09,2018-05-09 22:59:45,1,False,en,2018-06-27,2018-06-27 14:28:30,11,20e88424efe6,3.09811320754717,6,0,0,Why should we build Artificial Intelligence that values inclusion and social change over all?,5,"Design for human augmentation, not human automation.
Why should we build Artificial Intelligence that values inclusion and social change over all?

Machine Learning is being used in many technologies today. It is the science of teaching computers to make predictions and decisions based on patterns and relationships that have been automatically discovered in data. But just because it is based on data, doesn’t automatically make it neutral. Our human biases become part of the technology we create.
A recent study conducted by PhD students at the University of Virginia and University of Washington researched a public dataset of photo collections and found that the images of people cooking were 33% more likely to picture women than men. When the students ran the images through an Artificially Intelligent model, the algorithms showed women were 68% more likely to appear in cooking photos. Machine Learning software built on biased datasets didn’t just mirror those biases, it amplified them.
These findings not only surprised me– they scared me. The use of ML and AI is quickly becoming central to everyday life. These technologies will undoubtedly have a significant impact on the future. I am reluctant to accept that social biases are being amplified rather than reduced, while Fairness, Accountability, Transparency and Ethics (FATE) are diminishing.
The narrative that AI is a neutral technology because it is operated by machines, and therefore can be better at decision-making than humans, is misleading and incorrect. ML processes start with the collection of relevant real-world content. Human judgment plays a role throughout this process: humans choose where the data comes from, and why the selected examples are representative of an entire population. Humans are biased, and therefore AI is designed with an inherent bias. We as humans can realize when we make decisions that are not fair, accountable, transparent, nor ethical. Machines cannot.
This explains why Google’s voice recognition system has been shown to systematically perform better with male voices rather than female voices, and why the mobile app FaceApp, while claiming to have a camera filter that makes you more attractive, also makes you appear fairer, or “whiter.” This is an example of what can happen when a technology is trained with skewed data. We should be sensitive when considering “any system where some or all of the decisions are automated,” as Josh Lovejoy, a user experience designer at Google, defines AI technologies. Fairness in AI is not the default because the data that drives AI is not objective.
When we set out to build technology using data, we must plan for how we will respond when bias manifests. Trusting in “neutral” technology is insufficient. I propose that we instead make machine learning intentionally human-centered and that we intervene for the sake of fairness.
As our lives become more dependent upon advanced technologies, Machine Learning must become multi-disciplinary. It is as much a social challenge as it is a technical challenge. When discussing solutions that will lead to social change, design, in particular Human Centered Design (HCD), is positioned to make a significant positive impact.
Take for example the way automotive safety was tested in the United States. Until 2011, it wasn’t mandatory to perform front-impact collision testing using a crash test dummy with a female body type. As a result, before this date, women were 47% more likely to be severely injured in car crashes. This example underscores a fundamental issue with our methods of data collection and the consequences in which it results.
Big corporations like Microsoft and Google have already started an internal ethics committee as one of their agenda’s priorities mainly dedicated to research the future FATE in AI from a Human-Centered Design perspective. Harvard Kennedy School started The AI Initiative Harvard, conducted by The Future Society whose mission is to help shape the global AI policy framework approaching three different perspectives: AI and the Law, Brain & Cognitive Science, and AI and Health & Medicine. The topic was important enough to discuss when the top world leaders gathered this February at the World Government Summit.
We have entered into the age of automation extremely confident and hopeful yet naively underprepared. If we fail to design ethical and inclusive AI, we risk losing the gains made in civil rights and gender and racial equity. We need to identify and address real human needs, uphold human values, and design for augmentation, not automation. I want to build a world where technology works for all of us, not just for some of us; a world in which we value inclusion and social change over all.
i.
","Design for human augmentation, not human automation.",69,design-for-human-augmentation-not-human-automation-20e88424efe6,2018-06-27,2018-06-27 14:28:30,https://medium.com/s/story/design-for-human-augmentation-not-human-automation-20e88424efe6,False,768,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Irene Font Peradejordi,Co-organizer of AI Saturdays Barcelona. Currently working on my Bachelor Thesis: Cognitive Biases in AI,8cc001ec0ff1,peradejordi.irene,10.0,19.0,20181104
0,,0.0,,2018-02-02,2018-02-02 18:15:44,2018-02-02,2018-02-02 18:16:07,0,False,en,2018-02-02,2018-02-02 22:47:52,1,ed82ed240196,2.920754716981132,1,0,0,"Given the breakneck pace of improvements in automation and artificial intelligence (AI), fears about job loss are taking more space in the…",5,"AI and Ethics
Given the breakneck pace of improvements in automation and artificial intelligence (AI), fears about job loss are taking more space in the cultural imagination.
When evaluating the models of several economic planners and doomsayers on the impact of automation and artificial intelligence on our lives, one need to exercise caution with regard to such elaborate models and forecasts. Such estimates paint a dismal economic future wherein humans are pushed to the side with little to contribute and even less to gain.
Many occupations have been eliminated because demand for the occupational services declined (e.g., boardinghouse keepers) or because demand declined because of technological obsolescence (e.g., telegraph operators). But Bessen found that since 1950 there is only one occupation whose “decline and disappearance” can be largely attributed to automation: elevator operators.
Moreover, even if the predictions are generally correct about high-level trends — that certain jobs, sectors, and industries will indeed be largely automated — they fail to recognize or account for the unseen and unforeseen developments that result from automation. In turn, they ignore the transformative role of human potential and ingenuity amid technological progressMost automation of jobs is only partial, not complete. Bessen explains why that is a key difference:
“This distinction between partial and complete automation might seem irrelevant when many or most of the tasks of an occupation have been automated. However, the economic difference between being mostly automated and being completely automated can be critical. Complete automation implies a net loss of jobs; partial automation does notConsider, for example, the effect of the automated teller machine (ATM) on bank tellers. The ATM is sometimes taken as a paradigmatic case of technology substituting for workers; the ATM took over cash handling tasks. Yet the number of fulltime equivalent bank tellers has grown since ATMs were widely deployed during the late 1990s and early 2000s. Indeed, since 2000, the number of fulltime equivalent bank tellers has increased 2.0% per annum, substantially faster than the entire labor force. Why didn’t employment fall? Because the ATM allowed banks to operate branch offices at lower cost; this prompted them to open many more branches (their demand was elastic), offsetting the erstwhile loss in teller jobs”.
Even partial automation can lead to jobs losses, of course. But as a whole, automation tends to merely shift the need for human labor from routine, low-skill tasks to more creative, high-skilled functions. Automation lead to fewer elevator operators but more elevator designers, engineers, and repairmen. This is precisely because we are not mere machines, consigned to junk yards when particular solutions or services are rendered obsolete. We are creative and imaginative human persons created as stewardesses by God. We are fully capable of adapting, mobilizing, and innovating our modes of service to be in line with his purposes in the earth. When the economic conditions change and mechanization or automation replaces old ways of meeting human needs, innovation comes and new human services are created.
Automation will continue to disrupt our old ways of doing things. But knowing what we do about the past and the future of human possibility, we needn’t be fearful of our own position and power. As we survey the barrage of predictable reports about the end of human labor or the rise of robot dominance, let’s be sure to wield our hope and skepticism accordingly.
This shift ought to be lauded by Muslims. While we should rightly be concerned about the employment prospects of low-skilled workers, we should not become nostalgic for the mind-numbing, back-breaking work that automation has made obsolete. Too often we treat “jobs” as if they were an inherent good (at least if they pay a “living wage.”) But not all jobs are created equal. Some jobs that may benefit our neighbors’ bank account may also be crushing their soul.
The rapid adoption of computerized automation has the potential to increase job satisfaction for entire occupations that have previously been dangerous, dirty, and demoralizing. In looking at the future of work, we therefore must look not only at the wages that a job will pay but also the price such work requires of our neighbors. We can let the robots take over the parts that a machine can do so that we may use our God-given human abilities for more ennobling tasks. As God says in the Qur’an:
“And surely we have honoured the children of Adam, and carried them on the land and at the sea, and provided them with good things, and we have made them to excel by an appropriate excellence over many of those we created.” (17:70)
",AI and Ethics,1,ai-and-ethics-ed82ed240196,2018-02-02,2018-02-02 22:47:53,https://medium.com/s/story/ai-and-ethics-ed82ed240196,False,774,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Daily Wisdom,,ddd120ae7c2,dailywisdom,60.0,0.0,20181104
0,,0.0,,2018-05-09,2018-05-09 22:34:57,2018-05-10,2018-05-10 00:21:39,4,False,en,2018-05-10,2018-05-10 00:22:47,7,87447f52240e,8.50377358490566,0,0,0,The advent of artificial intelligence (AI) has forced many to question the defining features of consciousness and perception. Can…,5,"Jon Hamm in Black Mirror’s White Christmas
The Ethics of Pain in Artificial Intelligence
The advent of artificial intelligence (AI) has forced many to question the defining features of consciousness and perception. Can non-biological, silicon-based machines perceive the world as do humans? Can they feel and understand pain? This philosophical theory of multiply realizable mental states is called Functionalism. If this theory is achievable, it has many implications on the future of morality. For instance, some argue that exposing conscious robots to pain can benefit society, while others oppose this notion.
In this paper, I define what is best for society under the context of Utilitarianism, or the doctrine that defines ethical actions as those which impartially promote maximal happiness on the greatest number of beings possible. This in turn includes avoiding unnecessary pain as much as possible.
With the above-stated concepts of Functionalism and Utilitarianism in mind, I plan on formulating the following argument: If you are a Utilitarian and subscribe to the notion that pain is objectively morally wrong, and you are also a Functionalist and believe that machines that can feel pain can be built, then you must believe that making such machines is objectively morally wrong.
Functionalism and its Role Within AI
Functionalism: they are ALL chairs!
According to the theory of Functionalism, mental states are defined by what they do rather than what they are composed of. Consider, for example, chairs. Chairs can be tall, sharp, wooden, sturdy, and square. They can also be short, round, plastic, flimsy, and oblong. The essence of a chair is not dependent on the materials from which it is made, but rather the fact that it provides a place for humans to sit. Similarly, if the job of a mind is to think, feel, intend, etc., these different mental states can be completed by something composed of metal and silicone, not just biological gray matter and neurons. Just as chairs can be multiply realizable, so, too, can mental states.
In keeping with this concept of multiple realizability, it is becoming more convincing that consciousness through artificial means can soon become a reality. Although hyper intelligence in machines exists at this point only in science fiction, a lot of capital and research has been committed to advancing the technology. Researchers at Leibniz University of Hannover, for example, have developed an “artificial robot nervous system to teach robots how to feel pain.” In essence, human feelings of tactile sensations have been replicated in robots, and once stimulated with enough force or heat, these robots have developed reflex-like reactions to pain. There is debate about whether or not these reactions truly represent mental states, though.
Many, including philosopher Daniel Dennett, subscribe to the notion that robots can simply understand pain rather than truly feel it. In his paper titled Why You Can’t Make a Computer that Feels Pain, Dennett calls on the reader to partake in a thought experiment about a computer simulation of a hurricane. As with any simulation, this computer’s task is to mimic reality. But, to Dennett’s point, there is no expectation to get wet or blown away in the presence of the simulation. The point of the simulation is to imitate, not replicate. The computer may know the physical traits of a real hurricane, but it cannot recreate the actual process. Analogously, a computer may be able to understand pain, but not recreate the actual feeling of it.
Whether or not the researchers at Leibniz University or Dennett are correct in the reality of the possibilities within AI, let us presume the former for the rest of this paper. In the event that computers are capable of such emotional response to stimuli, we should prioritize mere understanding rather than feeling. How a being processes pain plays an important role in decision making.
Feeling vs. Understanding Pain: The Latter is Better
Feeling vs. Understanding
Pain plays an immense psychological role in our lives. As an intricate experience that is methodically tuned by the brain, pain forces us to deviate from certain environments in search of more comfortable ones. Many times, though, the feeling of pain serves as a distraction as opposed to a useful tool.
Prominent neuroscientist Vilayanur S. Ramachandran has been credited with helping relieve pain from phantom limbs, and has been quoted saying that “pain is an opinion on the organism’s state of health rather than a mere reflective response to an injury.” Feeling pain, therefore, can be understood as an emotional response to harsh physical stimuli. In the absence of this emotion, only an understanding of that stimuli would subsist. In most circumstances, this objective and accurate physical understanding of pain can play a more useful role than that of a feeling. Understanding, over feeling, can help people see the world as it is, not what it seems. Why, then, would anyone want to create AI that can feel pain in the first place?
Creating AI that can Feel Pain: Arguments and Rebuttals
Under the presumption that pain and suffering within humans is wrong and should be avoided, the same standard should be held for AI as well. For if defending consciousness from pain is the ultimate goal, and consciousness can be multiply realized in AI, then it is in everyone’s best interest to defend against pain in AI. However, there are some who oppose this notion, and, under the guise of maximum human security, defend the idea that pain-receptive AI should exist.
In the case that AI can feel pain, what benefit would this provide? The following four arguments have been presented to showcase the benefits of pain in AI:
1. Pain has proven to be evolutionarily advantageous
2. The more pain AI feels, the less pain human beings feel
3. Pain is the pinnacle of consciousness
4. Pain is essential to joy
Pain has Proved to be Evolutionarily Advantageous
It is true that the ability to feel pain has played an important role in the survival of species. For example, most invertebrates, when exposed to noxious stimuli, retreat from the source of pain. Many mammals have evolved to cry out in response to extreme pain, which signals to the rest of the pack to provide protection, as an animal in pain is an easier target for predators.
Although pain has proved to help in the survival of biological species, this does not deem it necessary for the survival of AI, which can be programmed to understand its environment without feeling it. Furthermore, adding a survival defense mechanism may align with human beings’ best interest. In the event of AI’s threat to society, detection of harm by way of noxious stimuli may not be advantageous to human survival if, for some reason, the ultimate goal were to protect AI consciousness above biological consciousness.
The More Pain AI Feels, the Less Pain Human Beings Feel
Many foresee a future in which robots work alongside humans. The year 2016 in the United States alone witnessed 892,270 occupational injuries and illnesses. The advent of robotic coworkers would cause the amount of on-the-job human pain to reduce drastically. Researchers at the aforementioned Leibniz University are hoping that the physical pain experienced by humans will transfer to their robotic counterparts, thereby reducing the amount of human suffering in the world.
The popular science fiction anthology series Black Mirror has also explored the notion of AI labor for the sake of human comfort. In an episode titled White Christmas, a character, Greta, is looking to more personalize her smart home. She has very specific needs and wants maximum efficiency in her operational home. As a result, she undergoes a procedure that copies her consciousness onto a device connected to her home system. This consciousness, or software, retains Greta’s desires, feelings, and understanding of the world so that she can better serve her original counterpart. Predictably, she refuses to act as a slave for Greta’s petty needs, and in turn is subjected to mental torture by the company that created her. Her perception of time is accelerated and she essentially lives in a white room of solitary confinement for six months with no one to speak to and nothing to do. This was all done with the intention to break her willpower so that she will submit to work for the needs of her human counterpart.
But is this really ethical? If mental states are multiply realizable, there should be no hierarchy of pain infliction. Ensuring the safety and comfort of human beings over conscious AI should be considered just as antiquated as abusive slave labor. Under the context of Utilitarianism, impartiality must persist even among the variety of intelligences.
Pain is the Pinnacle of Consciousness
Carl Jung has famously said “there is no coming to consciousness without pain.” (Contributions to Analytical Psychology, p. 193). Although he was talking about marriage at the time, many people still hold the notion that pain is essential to consciousness and a useful tool in feeling closer to our humanity. The argument appeals to the notion that one never feels fully alive until he feels the sensation of the combination of intense sensory experience aligned with emotion and a distinct focus on the present. If such experiences play a significant role in our perspective, would it be logical to deprive AI of that same familiarity?
In rebut against this notion, one has to make an obvious case: if pain played such a naturally important role in our lives, people would seek it out. Nevertheless, we aim to avoid physical pain and advance, instead, towards other emotionally rich experiences, such as joy and excitement. Deep joy, for that matter, can also aid in experiencing the same intense consciousness as described above. Many adventure seekers jump out of airplanes and climb the tallest of mountains to realize these states of hyper consciousness. Perhaps most important in achieving this feeling is the presence of adrenaline.
Furthermore, the argument suggests that one cannot achieve consciousness without feeling pain. In continuing the aforementioned chair analogy, this would be akin to removing a leg of a chair. Although the experience of sitting may be altered without the fourth leg, the chair still performs its role of allowing a human to sit. Similarly, removing pain from the equation of consciousness alters the subjective reality, but the other processes of the intelligence remain intact and consciousness still persists.
Pain is Essential to Joy
In relation to joy, many argue that it can only be realized when contrasted with pain. Therefore, pain is necessary for the existence joy. This appeal to the understanding of opposites echoes arguments of the 5th-century theologian philosopher, St. Augustine. Although joy may be better experienced after the experience of pain (i.e. a relaxing shower after a long run), this does not make pain necessary for joy to exist. Similar to the above-stated rebuttal, people do not seek out pain to experience more joy, and it would be illogical if they did. Additionally, even if humans were wired to need pain to experience joy, this programming can be customizable and changed within AI to make that supposed previous necessity irrelevant.
Utilitarianism and Decreasing Amount of Suffering Among Beings
The character of Greta in Black Mirror’s White Christmas after being subjected to torture.
Using Utilitarianism as a guiding concept towards a better world implies decreasing as many negative mental states as possible. Visualize sitting down at a dinner party. You find your name on your placemat and responsively sit in your chair. You realize that your chair is deficient. One of the legs is shorter than the other, and it is causing the chair to wobble back and forth depending on where you place your body weight. You think to yourself that it will distract you for the rest of the dinner and immediately try to mitigate the situation by temporarily placing a sugar pack underneath the stunted leg.
A short leg on a chair is akin to feelings of pain in a conscious being. These feelings are a distraction from happiness and do nothing to better the experience of consciousness. In an improved, more utilitarian world, unnecessary feelings of pain are diminished and replaced with other forms of mental states — perhaps happiness, curiosity, intrigue, and excitement.
In the event that AI truly can feel pain, a utilitarian world holds that they would be better off without such an ability. Increasing the amount of unnecessary pain in the world is an unethical act, and this concept will become increasingly important as advances in robotics and AI come into fruition.
",The Ethics of Pain in Artificial Intelligence,0,the-ethics-of-pain-in-artificial-intelligence-87447f52240e,2018-05-10,2018-05-10 00:22:48,https://medium.com/s/story/the-ethics-of-pain-in-artificial-intelligence-87447f52240e,False,2068,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Eric Spector,,e84f3787043c,ericspector,26.0,27.0,20181104
0,,0.0,,2018-05-14,2018-05-14 11:19:50,2018-05-14,2018-05-14 18:17:05,5,False,en,2018-06-04,2018-06-04 23:26:49,13,d890205d000a,5.437106918238992,11,0,0,By Yolanda Lannquist,5,"Ethical & Policy Risks of Artificial Intelligence in Healthcare

By Yolanda Lannquist
Also published on The Future Society’s site: http://www.thefuturesociety.org/perspectives/
Artificial Intelligence (AI) unlocks enormously beneficial innovations in healthcare. Personalized and precision medicine, more accurate and faster diagnostics, and accessible health apps increase access to quality medical care for millions. Chatbots offer 24/7 free therapy, wearables monitor biometric data in real time, and robotic devices improve surgical outcomes.
This rise of AI ‘Healthtech’ is enabled by developments in machine learning algorithms, proliferation of digital and biometric data, increasing computing power and advances in biological and medical sciences including in genomic sequencing.
New rewards, new risks
However, AI brings new policy and ethical dilemmas. Stakes are high and human health and lives are at risk. Through healthcare we can observe the trade-offs in the “AI revolution”: How to support beneficial innovation while minimizing risks?
Equity and inclusion: Will AI democratize healthcare or serve elites, leading to inequality or even biological superiority?
Data privacy and security: How should we trade off private and secure health data for medical breakthroughs and innovations?
Interpretable and explainable AI: Is it necessary for ‘black box’ algorithms to be interpretable and explainable when lives are at risk?
Algorithmic bias and representative data: How to ensure data is representative so machine learning algorithms are generalizable and safe for all?
Liability and accountability: Who is responsible when harm is caused by an AI system or machine?
Building Trust in AI: how can we have confidence in treatment recommendations based on complex or ‘black box’ algorithms?
Automation and unemployment: What is the impact of automation on employment for doctors, nurses, and medical professionals?
Ethics of labor unions: Is it unethical for medical offices or hospitals to block technologies that are safer, better, or cheaper, just to protect jobs?
Right to access: Where possible, should doctors be required to offer AI-enabled diagnostics or treatments if safer, better, or cheaper?
So far, there is limited regulations or concrete policies across any country to manage any of these questions. The EU General Data Protection Regulation (GDPR) may help address data privacy and interpretability of machine learning algorithms, but it has loopholes and its robustness is yet to be tested.
Let’s zoom in on three polarizing debates:
Equity and inclusion: AI for Democratization or Elites-Only?
AI can democratize access to high quality and affordable healthcare for millions, including rural and low-income communities and developing countries.
Remote diagnostic applications allow users to upload photos of snake bites or skin cancers for real-time prediction of diagnoses and treatments, without having to travel distances to see a doctor. In developing countries, approximately 330 million people live with heart diseases. Livecare, a Romania based startup, has developed a small wearable patch that, simply taped onto the chest, uses machine learning (specifically an LSTM recurrent neural network algorithm) to monitor and provide feedback for heart diseases. Rural and low-income folks can avoid travelling potentially hundreds of kilometers and paying high costs to see a cardiologist.
Innovation can boost inclusion and quality of life for more people. Low-cost or free technologies include therapy chatbots (e.g. Woebot), emotional therapy robots for the elderly (e.g. PARO), computer vision tools for the visually impaired, and exoskeletons and robotics for the physically impaired (e.g. ROBEAR). Whereas the number of doctors is limited and public medical systems over-stretched, healthcare delivered over digital devices is scalable to reach millions of people.
Woebot uses machine learning to offer cognitive behavioral therapy for the millions of Americans struggling with mental health
PARO, a robotic seal effective with dementia patients in nursing homes in Japan
Yet readers of Noah Yuval Harari’s Homo Deus are aware of a host of new technologies that, if accessible to an elite, can segment the human species in new ways. For the first time in human history, elites may be faster, smarter, stronger — biologically, genetically, or ‘bionically’ superior to others.
Life extension treatments (e.g. at Calico Labs or Human Longevity Inc.), gene editing, synthetic biology, and human enhancement and bionics can separate the physical and intellectual capabilities of humans. The same technologies that correct human deficiencies can be used for upgrading healthy bodies. Technologies to enhance human traits can push the brink of humanity into trans humanism, opening the doorway to the next phase in evolution, one where humans are in control of our own augmentation.
In the 1997 dystopian film Gattaca, Ethan Hawke tries to compete in a world where the genetically superior have privilege
Data privacy and security: Is healthcare worth the cost?
Sensitive health data, including genetic testing & bio metrics, are used to train the machine learning algorithms behind new drug discovery and cures, more accurate diagnostics, and personalized treatments. But at what price? How should policymakers trade-off risks to privacy and security with opportunities for healthcare breakthroughs?
Data is susceptible to hacking and privacy breaches, such as the 2017 cyber attack on the United Kingdom’s NHS. Meanwhile, biometric data collected from wearables can be hacked or sold to public or private sectors actors to target advertising or real and “fake news” for political or social campaigns. Anonymization and data protection regulation are a start but fall far from guaranteeing security. Blockchain technologies offer a more robust solution to protect data from tampering.
Should public health records and data be provided, without monetary compensation, for the public benefit? At the World Economic Forum Davos summit in January 2018, historian Noah Yuval Harari asked the audience:
“Does my data about my DNA, brain, body, life, belong to me, a corporation, government, or the human collective?”
A ‘Right’ to AI?
In 2015, The American Society of Anesthesiologists campaigned against an FDA-approved, safe and cheap AI-enabled anesthesiology machine. Johnson & Johnson’s Sedasys could effectively deliver anesthesia to patients for $150-$200, compared to $2,000 for an anesthesiologist, one of healthcare’s highest-paid specialties in the United States.
Should hospitals be required to offer patients the option of an AI diagnostic or treatment program that is statistically safer, more accurate, cheaper, or all of the above?
Is it unethical for hospitals or medical offices to fail to offer technologies shown to be safer, cheaper, or better, because they threatens human jobs? The anesthesiologists argued that the machine could not replace their skills, yet it is a possibility that the threat of losing their jobs was a main contributor to the campaigns. Skill degradation for doctors losing out on opportunities to practice medicine is another concern that can perpetuate over-reliance on machines.
AI raises a host of new risks to manage alongside its innumerable benefits. The risks and benefits are particularly pronounced in healthcare where human lives are at stake. Governing the rise of AI in healthcare and beyond involves trade-offs between risk and rewards that put ethics, safety, and human values at stake.
I originally presented this at City.AI’s Bucharest.AI meetup; more information from the other AI & Health speakers here!
Thank you for reading! Please *clap* if you liked the article to help share it!
Feedback is welcome:
Yolanda@ai-initiative.org // Twitter: @YolandaLannqist //
www.thefuturesociety.org
References:
European Commission Press Release, (25 April 2018), “Artificial intelligence: Commission outlines a European approach to boost investment and set ethical guidelines,” http://europa.eu/rapid/press-release_IP-18-3362_en.htm.
Google Corporate Communications, “Google announces Calico, a new company focused on health and well-being,” https://googlepress.blogspot.com/2013/09/calico-announcement.html.
PwC, (2017), What doctor? Why AI and robotics will define New Health.
Wadhwa, Vivek. The Driver in the Driverless Car: How Our Technology Choices will Create the Future. HarperCollins Publishers India.
Yuval Noah Harari, Homo Deus: A Brief History of Tomorrow, 2015.

",Ethical & Policy Risks of Artificial Intelligence in Healthcare,240,ethical-policy-risks-of-artificial-intelligence-in-healthcare-d890205d000a,2018-06-21,2018-06-21 13:06:05,https://medium.com/s/story/ethical-policy-risks-of-artificial-intelligence-in-healthcare-d890205d000a,False,1220,,,,,,,,,,Ethics,ethics,Ethics,7787.0,Yolanda Lannquist,AI Policy Researcher // The AI Initiative // The Future Society // Harvard Kennedy School // @YolandaLannqist,54569c252917,yolandalannqist,12.0,10.0,20181104
0,,0.0,a7362b43f5e7,2018-04-12,2018-04-12 21:44:47,2018-05-14,2018-05-14 17:05:50,1,False,en,2018-05-14,2018-05-14 17:05:50,22,18a9de0bc9bb,3.837735849056603,10,0,0,"Here at Synapse, we’re not only committed to building great products but also fair and transparent ones. To that end, we’re working hard to…",4,"Machine Learning and Transparency

Here at Synapse, we’re not only committed to building great products but also fair and transparent ones. To that end, we’re working hard to ensure our classification models are free of social biases and are favoring inclusion over exclusion.
It goes without saying that ethical questions that are difficult for human beings to reach consensus on will not be easily solved by machine learning models. The difficulty involved, however, is not a reason to shy away from pursuing technical solutions. In fact, this difficulty indicates just how pressing and essential it is that more research and more direct applications are developed.
While our solutions may not always be perfect, being transparent about what our models do and why they do it is a necessary first step. In the process, we can help to simplify complexity and to account for everything we build.
In future posts, we’ll go into more detail about the technical approaches we are implementing to accomplish this goal. In the meantime, this post provides a brief overview of some recent libraries and theories we feel are particularly promising.
Interpretability
Christoph Molnar’s online book provides a useful overview of current approaches for explaining black box machine learning models. A number of libraries — most of which are open-source — have also been released over the last few years and tend to have relatively straightforward implementations for explaining the predictions of already trained models.
LIME (Local Interpretable Model-Agnostic Explanations), is probably the most widely used interpretability library. As the name suggests, it is model-agnostic, meaning it can be used on anything from a polynomial regression model to a deep neural network.
LIME’s approach is to perturb most of the features of a single prediction instance — essentially zeroing-out these features — and then to test the resulting output. By running this process repeatedly, LIME is able to determine a linear decision boundary for each feature indicating its predictive importance (e.g. which pixels contributed the most to the classification of a specific image). The associated paper provides a more rigorous discussion of the approach, though this post by the authors is probably the best place to start.
Skater, another open-source library, incorporates LIME’s local interpretations, while also including global explanations. For instance, there is built-in functionality for producing marginal plots (showing the relationship between different pairs of variables) and partial dependence plots (showing the relationship between each variable and the model output).
H2O, an open-source machine learning platform, includes various methods for model interpretability along with its more general ML implementations. The authors provide examples and documentation for approaches such as decision tree surrogate models, sensitivity analysis, and monotonic constraints (useful for instances in which you want to ensure that changes to a specific variable result in a continuous increase or decrease in the model’s output). They’ve also written a post describing different interpretability methods in depth.
SHAP (SHapley Additive exPlanations), unifies multiple different interpretability methods (including LIME) into a single approach. It does this by mathematically defining a class of additive feature attribution methods, and demonstrates that six different interpretability methods currently in use fall within this class. See the associated paper for more details.
Bayesian Deep Learning
Bayesian deep learning has emerged as another way to gain more insight into black box models. Rather than explaining individual feature importance for predictions, a bayesian approach enables one to measure how confident a deep learning model is about its predictions. This is useful on multiple fronts, but one particularly beneficial result is that predictions that are output with a high degree of uncertainty can be set aside for closer manual analysis by a human being.
A number of probabilistic programming languages have been released over the last few years, starting with Stan back in 2012. Since then there’s been PyMC3 (running Theano on the backend), Edward (running on TensorFlow), and Pyro (released just last November by Uber’s AI labs and running on Pytorch).
Pyro was specifically designed for deep learning applications, and the Edward documentation provides a number of tutorials and videos for bayesian deep learning, including an example of how to use dropout to approximate bayesian probabilities. A key paper originally published back in 2015 demonstrates that dropout, a standard method of regularizing deep learning models to prevent overfitting, converges to a gaussian process and hence can be used to measure model confidence.
Fair Machine Learning
Annual conferences such as FAT* (Conference on Fairness, Accountability, and Transparency) have helped bring increasing attention to the need to build more equitable models, while also drawing scholars, researchers, and practitioners from different fields into conversation. A number of open-source fairness libraries have also been released in recent years, though most of them are still in the early stages.
Although there is no consensus yet on what measures are most conducive for producing fair outcomes (and of course, much depends on how we define fairness in the first place), a number of compelling criteria and definitions have been proposed. Fairness Measures, Reflections on Quantitative Fairness, and Attacking Discrimination with Smarter Machine Learning all provide useful resources for beginning to think through how best to approach the difficult but essential question of how to build fair models.
When we speak of “bias” in machine learning we are usually referring to the mathematical assumptions built into the parameters of a model. It is becoming increasingly urgent, however, that we also consider the other definitions of “bias,” and with them, all the ways our models affect actual human beings, their lives as well as their livelihoods.
As a banking platform, we are confident that optimizing our models for transparency and interpretability is not only the right thing to do but will also lead to better, more inclusive products.
",Machine Learning and Transparency,150,machine-learning-and-transparency-18a9de0bc9bb,2018-05-17,2018-05-17 06:42:07,https://medium.com/s/story/machine-learning-and-transparency-18a9de0bc9bb,False,964,Operating System For Modern Banking,,synapsepay,,SynapseFI,hello@synapsefi.com,synapsefi,"BANKING,FINANCE,AI,API",synapsefi,Ethics,ethics,Ethics,7787.0,Matt Sims,AI Ethics Engineer @ SynapseFI,d7767e22953e,matt.sims,2.0,1.0,20181104
0,,0.0,7f60cf5620c9,2018-08-29,2018-08-29 23:29:41,2018-08-30,2018-08-30 02:38:03,3,False,en,2018-09-06,2018-09-06 01:46:02,19,fea118987a5,9.953773584905662,19,0,0,"This is why ethical standards don’t work, and what we can do instead.",3,"Ethical codes vs. ethical code
This is why ethical standards don’t work, and what we can do instead.
The current state (and usefulness) of ethical codes for AI.
There’s been a lot of talk about ethics in the data industry, especially as it concerns the ethical use of algorithms. The former U.S. chief data scientist called for ethical code for data scientists. The American Statistical Association and Association for Computing Machinery. Google came up with ethical principles to govern its AI efforts. Lots of other people putting their oar in, from Bloomberg to the World Economic Forum’s Young Scientists group to Data for Democracy.
People make ethical codes at least partially because they’re dissatisfied with the state of ethical practice. But ethical codes don’t clearly result in an increase in ethical practice. In fact, as happens with an attempt to create a standard for anything, the only clear result of creating a code is the proliferation of additional codes:
This applies to ethical codes as well.
In short, there’s a difference between ethical codes and actual ethical code. I want to talk a little about the former, but mostly about what we can do to get more of the latter.
Why I care about ethics
In industry, especially, I think it’s pretty reasonable to look at a topic like this and say things like “this kind of thing is just an academic exercise” or “this isn’t my problem because I personally behave ethically.” So I want to take a moment to explain why I personally, as a data scientist working in industry, feel I need to devote my attention to this topic:
Pakistan. I used to train statistical models to provide the U.S. Army with strategic intelligence. I modeled the cycle of military and militant operations in Pakistan, and my results contradicted a prominent administration policy statement. My boss’s boss called me into his office, and after berating me for half an hour, told to change the conclusions of the analysis. This was the first time I’d ever been told that I needed to fake my results (and the first time I had a person deny, I believe quite honestly, that that was what they were telling me to do).
Sales reports. I was working for a student travel firm and my boss’s boss wanted a graph that showed ten years of revenue but he only had the data for four of those years; he wanted me to interpolate the missing data. He drew a picture of what he wanted the trend to show, and that drawing presented wildly unrealistic figures for the missing years and substantially changed the figures for the years we did have. This was the first time I’d been told to fake my data.
S-curves. I was working for an asset management startup focused on investing in local consumer-goods producers. My boss really wanted to show that countries fit along an s-curve — the business consultant’s formulation of a logistic curve — with some at very low consumption, some in the stages of a rapid rise, and some already leveled off at high consumption levels. The data didn’t show what he wanted it to show. He pressed me to try more and more complex models in the hopes that they would show the desired pattern. This was the first time I’d been told to torture the data until it confessed.
Teachers unions, the New York Times, and the NAACP. I was offered the job of founding the data science team for a charter school network so prominent and so politically active that, before even starting work, my prospective employer had been accused of using student test scores reject, intimidate, or remove undesirable teachers and students. I did my due diligence and decided those attacks were more political than factual, and my experience in that job confirmed my initial impressions, but I knew that whatever data systems I built could easily be used to punish students rather than support them, even if I didn’t build them for that purpose. This was the first time I’d had to constantly imagine, at every stage from design to development to deployment, how someone could use things I had built to hurt other people.
I think data science has value. I think it deserves to attract some of the best and brightest people entering the workforce. If we don’t constantly work to ensure widespread ethical practice and to call out and correct unethical practice, an increasing number of those best and brightest will leave to do other things. And rightfully so. If we don’t build an ethical industry, then we deserve to lose those people, and to see our business suffer as a result. It’s not enough for us to be individually ethical ourselves: only a few companies have to behave unethically for all companies to suffer for it. Good ethics is often good business in the short term. It’s always good business in the long-term.
Ethical codes are neither necessary nor sufficient
When trying to get someone to develop either the capability or the intent to change their behavior, to avoid ethical pitfalls in the case of the present discussion, talking is a terribly ineffective mode of communication.
Take a long time looking at this comic. Think about it until you understand it. It’s worth the effort.
Telling people about a danger rarely helps them to avoid the danger. At least, it doesn’t help them to do so with any consistency. Ethical codes never show. They only tell. They are, in effect, goal statements. That’s why they don’t improve practice. All by themselves, they call attention to the fact of a problem’s existence, but they don’t prevent people from running into danger or help them get out of danger once they’re in it. Ethical codes aren’t sufficient to improve ethical practice.
That’s certainly not new information — I haven’t met anyone who argues that ethical codes are sufficient, although when participating in communities that write ethical codes I’ve often met people who act like they are. The thing is, if we can show the danger instead of telling about it, we can avoid or mitigate the danger without having to tell at all. Not only are ethical codes insufficient to improve ethical practice — they aren’t necessary either.
Fences and their dangers
It’s not just that ethical codes are neither necessary nor sufficient for ethical practice. They’re also potentially harmful. An ethical code is a boundary — a point beyond which people are not supposed to cross. If there’s no way to enforce adherence to the code, then it’s like a line in the sand — it’s there, but crossing it doesn’t carry any consequences so it might as well not be there.
If there is a way to enforce the code — in other words, if there’s a regulatory infrastructure in place — then it’s like a fence. Policy makers like fences because they’re scalable. All you have to do is define the rules in a way that you can audit adherence, then train and deploy auditors. The problem with fences is that they introduce systemic risk: people can build up all kinds of practices right along the border of the fence, even leaning up against it, without crossing over it. When the fence breaks, the consequences are more devastating than if the fence hadn’t been there in the first place. I’ve written about this before.
There are a lot of examples of the systemic risk introduced through large-scale fence-building. The rules governing mortgage lending prior to the 2008 financial crisis is one example, but maybe a more appropriate one for this discussion is the fence that differentiates p-values below 0.05 from those above it. That fence has been enforced across university classrooms, scientific publications, and, to a lesser extent, industry standards, and those regulations have risen along side a whole suite of practices to select model features, or sample data, or both, to make results fit inside the fence. In recent years, several fields, most prominently psychology and medicine, have suffered because too many people decided that results were trustworthy just because they fit within that threshold.
Fence-building is risky, and the larger the fence, the greater the risk. We shouldn’t resort to widespread regulation unless we have no other choice, and even then, we should proceed cautiously.
A tentative set of alternatives
Now let me spend the rest of the time talking about the alternative to ethical codes and ethical regulation. I don’t have any definite answers to offer here. I think there is sufficient reason to believe that we can build a more ethical industry through showing instead of telling, but my ideas are still preliminary. If, as a profession, we spent as much time and resources developing those ideas as we spent on developing ethical codes, we’d probably have something implementable pretty quick. I’m going to start with something any individual practitioner can do. Then I’ll talk about something that teams or companies can do. And then I’ll talk about something that has to happen across the whole industry.
Tooling for design. When we train a predictive model, there are methods to assess how accurate it is, including processes such as cross-validation that help reduce the risk of our methods providing overly-optimistic results. There are tools that enable all of these assessments, maintained across many different software packages and programming languages. New practitioners are trained on these tools and admonished when they fail to use them. We do all of this because we train models and build data systems to accomplish certain purposes, so we have an interesting estimating the extent to which our system accomplish those purposes.
We need a comparable toolset for estimating the extent to which our systems produce unintended consequences. It’s not enough to know that our tools do what we want them to do. We need to ensure that they aren’t doing what we don’t want them to do. Again, this ethical imperative is also just plain good business sense: unintended consequences incur costs that can’t be planned for.
There are tools out there to identify unintended consequences, but they aren’t nearly so numerous or user-friendly or well-maintained as the tools that measure intended consequences. For example, the Center for Data Science and Public Policy at University of Chicago developed a tool called Aequitas, where a user can specify algorithm’s intended uses — for example, whether the algorithm will be used to punish or assist people — and will analyze the algorithm’s results to see if some subgroups are disproportionately affect. There are a handful of tools that automatically search for instances of Simpson’s Paradox (here’s one example), where a trend exhibited in the overall data set reverses itself when confined to certain subgroups.
We need more of these tools. We need to build exploration of unintended consequences into our workflows. This is something every data practitioner can do.
Local (non-scalable) regulation. Big fences are risky. Little fences not as much. I’m not talking about self-regulation. I’m talking about local regulation. Local regulation can rely on showing ethical problems instead of telling, and local mistakes tend to also stay local.
There was a great article in the Washington Post about how local regulation worked to reduce harassment in a restaurant. The employees used a color-coded system classify uncomfortable customer behavior yellow (creepy vibe), orange (sexual undertones), or red (overt behavior or repeated orange incidents). All a staff member had to do was report the color — “I have an orange at table five” — and the manager took action, no questions asked. Red meant the customer was asked to leave. Orange meant the manager took over the table. Yellow meant the manager took over the table if the employee wanted.
It’s not hard to imagine how this could work in algorithm development. A yellow means an employee feels uncomfortable, and it prompts movement to another project if desired, as well as a team review of the issue. Orange means the employee can point to specific ethical concerns, and it prompts movement to another project as well as team review. Red means the employee has a strong concern and it puts the project on hold pending review. No second-guessing the employee’s concerns. Just action. It’s possible.
Reputation systems. There are already many examples of systems (Yelp, BBB, Glassdoor, etc.) that publicly expose information about private interactions with a company, and these systems change company behavior. These sites don’t, by any stretch of the imagination, give users a comprehensive, objective view in the performance/desirability/quality/goodness of a company. But companies are scared of bad reviews. When a bad review comes up, they spend time addressing it — demonstrating that in fact the review was false. Companies that don’t actually fix the problems get more negative reviews, and therefore draw fewer customers. It makes it so internal company practices impact the bottom line. I personally have withdrawn myself from consideration for certain jobs when a company was unable to give a satisfactory explanation for problems described on Glassdoor. In at least one case, the information that raised a red flag was about an ethical concern. Reputation systems are viable tools for shifting industry behavior.
The problem with most of these systems is that they try to accommodate all use cases, so a bad review can come because a customer was sexually harassed or because the customer failed to recognize that he himself placed the wrong order. A book review on Amazon, even if we discount deliberate mob attempts to inflate or deflate a book’s reputation, can reflect anything from the structure of the argument to the readability of the prose to the quality of the paper. For a reputation system to impact ethical practice, the scope would need to be much more narrow.
We have work to do
I understand the desire, when confronted with a clear ethical problem like those that frequently make the news, to want to do something. And I understand that trying to formulate clear and compelling ethical principles is a very doable something. It’s the wrong thing. It’s not going to solve our ethical problems. In my opinion, it won’t even help. In fact, it can very possibly hurt. Ethical codes can actually give unethical actors cover. We don’t want that.
I listed the above three alternatives for addressing unethical behavior not because I think they would solve all of our problems (of course they wouldn’t), but because they give everyone something to do that is both realistic and productive. Anyone who can code and do analyses can tool for design. Any manager or executive can implement and enforce policies about ethical review and non-retaliation against people who raise concerns. If you don’t work in a place that deals with these ethical issues, create a site that allows people who do to publicly post the issues they’ve had and the companies where they’ve had them. Individually, none of these things are going to do much good. Taken together, they, and ideas like them, could move us in the right direction.
",Ethical codes vs. ethical code,84,ethical-codes-vs-ethical-code-fea118987a5,2018-09-06,2018-09-06 01:46:02,https://towardsdatascience.com/ethical-codes-vs-ethical-code-fea118987a5,False,2492,"Sharing concepts, ideas, and codes.",towardsdatascience.com,towardsdatascience,,Towards Data Science,,towards-data-science,"DATA SCIENCE,MACHINE LEARNING,ARTIFICIAL INTELLIGENCE,BIG DATA,ANALYTICS",TDataScience,Ethics,ethics,Ethics,7787.0,Schaun Wheeler,"Senior Staff Data Scientist at Valassis Digital. Formerly at Success Academy Charter Schools, U.S. Department of the Army, and other places. Anthropologist.",2d3762e7f110,schaun.wheeler,232.0,2.0,20181104
0,,0.0,,2018-06-26,2018-06-26 11:52:39,2018-06-26,2018-06-26 11:59:03,5,False,en,2018-06-26,2018-06-26 14:37:18,2,3171c22192bd,6.520125786163522,2,0,0,About the false opposition between nature and technology and the status of the ‘good life’ in today’s modern thinking.,5,"A talk with professor Mark Coeckelbergh
About the false opposition between nature and technology and the status of the ‘good life’ in today’s modern thinking.

You operate in the domain of technology philosophy, more specifically the ethical questions raised by technological developments. How could this knowledge surpass academic walls?
I believe government has the task to inform people about technology, as they do about health or traffic. They could provide the people with some guidelines on how to interact with technology. Obviously commercial organisations house the most knowledge, but there is a significant risk in terms of misinformation. Companies have to sell products and services and will present them as positive and advanced as possible. If they would communicate more honestly, it would increase the general knowledge about all things tech. Luckily more and more voices are heard within companies that dare to put the ethical questions on the agenda.
But these companies are the main source of new technological developments, they shape our reality with their products and services. The selection and adaptation of new technologies is predefined by financial logics. Isn’t that a roadblock for transparency and responsibility?
Technology is defined by market logic and by the time it hits us, the consumers, it’s a fait accompli. As a society you have to built in certain mechanisms to better envision the impact of technological developments. Future-research has to help make these more democratic. Like your manifest reads, we can’t let boardrooms be the sole architects of our technological realm. In an economic reality ethics has a negative connotation, they might prefer responsible entrepreneurship. But besides terminology, for me the most important issue is that already in the ideation and development stages of new solutions, we have to incorporate the societal impact and ethical questions. In a company you can imagine the pressure being to high to give any attention to this. Governments or NGOs could try and support them in this.

Better late than never the tech-consumer appears to be waking up, mainly because of crisis situations like the Cambridge Analytica breach. How come it takes us so long to become more aware or critical towards new technologies ?
A part of it is human thoughtlessness and habituation. We see technological applications as being something purely technical, but that idea doesn’t hold up. Back in the days technology might have been more instrumental, take for example kitchen appliances, they don’t really ignite ethical reflections. Technology today however does have a deep impact on how we do things, see things, built things, experience things, … but we don’t seem to grasp that. The car changed the outlook of the earth’s’ surface entirely and has taken on such a strong role in society it’s very hard to change mobility into something more sustainable. So thinking about technology isn’t hardwired into humans (yet), we see it like a hammer and a nail, a tool, but it’s far more than that. Facebook isn’t ‘just’ a communication tool.
Often our relationship with technology is defined by a tension that exists between mankind, nature and technology. But does that theory make sense ? Are we originally pure nature and do we feel rootless because of the ever growing layer of technology that stands between us and nature ?
That is a problematic viewpoint because it places technology outside our reality, as something fundamentally strange to mankind. That while technology is very human and the precondition for people to survive in nature. Even though, today, technology is becoming more autonomous, it’s still man-made, man-used and packed with human values. So how strange or distant can it feel? To me, nature, technology and mankind are strongly intertwined. And if you look at them in a more relational way, you can’t possibly think about man and society without including technology. Our modern thinking works with to much opposites, culture vs nature, man vs technology, … we have to break those patterns.

A key aspect of modernisation is the wish for increased efficiency, often enabled by technology. If we are in a strong relationship with that technology, how does that urge, that efficiency affect us ? Are human made to be optimised ?
The 19th century idea of ‘the office-man’ that operates at maximum capacity and is constantly monitored, optimised, managed and surrounded by technology doesn’t feel right. People intuitively sense that that they are more than their measurable actions on the job. This gives humans their typical serendipity, unpredictability and dept. That’s what makes it so hard for technology to mimic humans, that darkness that is ungraspable. And that is also what makes relationships so fascinating. Imagine the other being 100% readable, predictable, the world would become a very dull place.
Should technology put ‘The good life’ more central in it’s developments instead of efficiency ?
Apart from efficiency and profit, there are always some values in technological applications; often liberal ones. Take the car, it represents freedom and the importance of having full ownership of one’s life. So there are values incorporated, problem is they emerge from a narrow vision on what it means to be human, or what it should mean. If you look at the Hyperloop project by Elon Musk, it’s an example of corporate responsibility, he wants to solve a problem society is faced with, which is applaudable. Too bad it’s an old solution for an outdated problem. He still thinks in terms of individual transport, physical presence, … This type of solutions tends to be less human-centered because technology is seen as the one source for solutions. Somebody sees a problem and throws some technology at it. This tech-fixation sabotages a more holistic view on society and culture.

Let’s say we collectively say goodbye to the 40 hour workweek. After being the production machines ourselves we get laid off and are replaced by non-human machines. Consequence: we have a lot of extra free time on our hands. Will we have any clue what to do with it? Are we any good at ‘free time’ ?
Even today we see that doing nothing, being a bit bored is not seen as a good thing, everything has to be useful. You have to be busy all the time. This urge we have in the workplace, towards increased efficiency, could affect our free time as well. Also, today, we already use a lot of technology in our free time, which make it plausible that tech-companies will be the ones who shape our time in the future. Unless we start being more critical, this will reduce us to products, or machines, generating data, handing over our time for free, which is the core of the attention economy. Letting us be seduced into this dynamic is absolutely not acceptable.
Seduction isn’t something new, it’s the core of marketing and fuel for our economy? People are approached by companies by appealing to their basic urges and passions. These mechanisms will have an exponential impact due to new technology. Won’t this negatively affect our culture, the virtues we share ?
Every since the ancient greek philosophers there has been this idea that you have to control your passions. It’s something you learn to do when moving from childhood to adult life. Preferably not just ‘because you have to’, but from an intrinsic motivation, one that sees how this could benefit yourself and those around you. This is a form of art that you have to learn to master during your lifetime. And it’s not easy, it’s actually very hard. If you add to this challenge the constant confrontation with technological applications who make this virtually impossible and whose goal is to tap into and enlarge your urges and passions you might risk to turn out a lesser version of yourself, one that is not so human. This dynamic of self-actualisation and improvement of the self is the complete opposite of the idea, fed by technology, that everything has to be easy.

So for me the question is not whether we want technology or not, but how will we shape it in a way that it supports us in this challenge to reach for ‘the good life’, to master our urges and passions. If technology could assist us in doing just that, there could be a bright future ahead. The danger we see today is that the more critical voices in the tech-world want to make a case for ‘a return to nature’ even though we’ve never been ‘in nature’. The challenge is to align self-care and self-control with technological advancements. That’s where the tension is today, not between nature and technology, but between consumerism and ‘the good life’, enlarged by the technological accelerations we see today.
(By Annelies DWL )
If you wish to learn more about Mark Coeckelberghs research he published a lot of interesting books and papers that are comprehensible and enriching.
The most recent are :
New Romantic Cyborgs
https://coeckelbergh.wordpress.com/new-romantic-cyborgs/
Using Words and Thingshttps://coeckelbergh.wordpress.com/books-menu/using-words-and-things/
",A talk with professor Mark Coeckelbergh,15,a-talk-with-professor-mark-coeckelbergh-3171c22192bd,2018-06-26,2018-06-26 14:37:18,https://medium.com/s/story/a-talk-with-professor-mark-coeckelbergh-3171c22192bd,False,1507,,,,,,,,,,Ethics,ethics,Ethics,7787.0,"Dear Tech,",A collective researching the impact of technology on society,6df10d1f5362,deartech,20.0,55.0,20181104
