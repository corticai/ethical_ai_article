= These Women are Leading in Artificial Intelligence and Want to Influence it For Good =
These Women are Leading in Artificial Intelligence and Want to Influence it For Good
Sue Daley, Head of Programme, Cloud, Data, Analytics & AI, TechUK, Tabitha Goldstaub, CognitionX, Zoe Webster, Head of Portfolio, InnovateUK, Eleonora Harwich, Head of Digital and Technological Innovation, Reform, Huma Lodhi, Principal Data Scientist, Direct Line Group
Whether you know you are using Artificial Intelligence, or not, the fact is that this technology is going be embedded, and probably is already embedded, in every part of our lives — and after a day discussing the impact that this has on women not only personally, but professionally, socially, and economically it’s clear we need to be clearer.
Why are women so worried?
Few women are working in Artificial Intelligence. Not only does this make it difficult to represent female views equally, it raises other ethical questions. For example, take the situation where there may be bias within the data used to train an AI. Is it possible that there may be poor representation of female data because it doesn’t exist yet? We know any answer that an AI gives will be based on the information put into it. Who will be responsible for checking what the machines are learning?
Catherine Colebrook, IPPR, Chief Economist and Associate Director for Economic Policy highlights research that suggests that women are less likely to look for different jobs. Their attachment to the labour market is less intense. In reality, this translates to them being unlikely to retrain to seek out new types of work. As women occupy a significant proportion of low skilled jobs, the threat of them being left behind as automation takes over becomes increasingly likely.
Something needs to be done and women need to take action now. Holly Rafique, Digital Skills & Content Lead at #techmums helps to address this issue and works to help mums increase their ability to use technology, predominantly in disadvantaged areas, to improve their, and their children’s lives.
Freaking out or freaking on?
Tweet from the WEF on 26 May 2018
The press would have us believe that there is no other way than doomsday. The automation of everything is paired with fatal consequences for the job market. Moreover, this isn’t in the too distant future.
The World Economic Forum reported a study predicting that more than a fifth of the global labour force [800 million workers ] might lose their jobs because of automation in the next decade.
Sue Daley, Head of Programme, Cloud, Data, Analytics & AI, TechUK advocates a more moderate view, looking for the good use of AI [and Sue, for you I have not included any apocalyptic “Machines are taking over the world” photos]. A small example is Denmark using Corti — an AI to help call handlers to detect whether someone is having a heart attack.
If you get a chance check out what Jade Leung at the Future of Humanity does on big picture thinking around the governance of Artificial Intelligence for the long term. There are many, many more examples and Eleonora Harwich, Head of Digital and Technological Innovation, Reform, looks at examples of innovative use in the Public Sector such as predictive policing and the Serious Fraud Office use to detect large scale bribery in Rolls Royce.
Perhaps, part of the problem is our human desire to pro-create in whatever form we can. We have and will continue to re-create ourselves in machines. However, too much anthropomorphism isn’t a great thing. Prof. Joanna Bryson, Associate Professor, Department of Computing at the University of Bath points out
“Only humans can be dissuaded by human justice — AI is a machine and doesn’t care if it is stuck in solitary confinement — don’t think you can generalise from human and assume an AI can be a legal person”.
Prof. Joanna Bryson asking, “Why do we make robots look like humans?”
What problems is AI solving and creating?
Kate Bell, Head of Economic and Social affairs department, TUC suggests that the UK has three pressures happening all at once:
a problem with productivity — it costs more to make less nowadays;
more insecurity in jobs; and
high number of people in employment.
Prof. Joanna Bryson
Technology does not solve all of them… so where does it help? Certainly not in productivity, e.g. Slack replaced email but now we spend more time on Slack. To understand where it’s impacting the economy and society it’s worth discussing questions such as:
Will Artificial Intelligence add another dimension to these problems, solve them, or add to them?
Will it usher in an era of no jobs or more jobs, or just different jobs? It’s possible that technology causes higher number of people to be in employment — but isn’t this opposite to what the research suggests?
You’ll find very quickly a philosophical debate surfaces with questions such as:
What happens if disadvantaged groups don’t have a say in the technology created?
Shouldn’t algorithms be checked for bias?
Should it be mandatory that AI should reflect the diversity of the users it serves?
Dr Paula Boddington’s view, who is the Senior Researcher on, ‘Towards a code of ethics for artificial intelligence research’, Department of Computer Science, Oxford University, is that ethics in AI is so interesting because it asks us simple, fundamentally important questions which are very difficult to answer. Such as:
How do we live our lives?
Enron has a fabulous code of ethics which were completely ignored. Enron overstated its earnings by several hundred million dollars.
Does good use of AI start with our children?
Growing up in our world today means experiencing frictionless AI, it is pervasive, and omni-present. We are beginning to, and will, use it seamlessly and eventually unknowingly.
Upstream work with children is therefore, essential. Seema Malhotra, MP states that
“Familiarity, connectivity, ethical use, and curiousness will all be hallmarks of educational success, if done well”.
This begins with significant investment from government and interest from many sectors. For example, Seyi Akiwowo, from Glitch!UK has a schools programme teaching children about how to be good digital citizens, promoting fair behaviour online.
Seema Malhotra, MP asks :
“What are we thinking about young people in school today? They will be leaders in our workplace of the future.”
Seema Malhotra MP, President of the Fabian Women’s Network, Kate Bell, Head of Economic and Social affairs department, TUC , Jade Leung, DPhil, Governance of Artificial Intelligence, Future of Humanity
What does this mean for our future job hunters?
Will they be hired:
because of their ability to use an AI? Will they and their AI be interviewed together for the job?
because they can use AI? Will the ones better versed in the technology be hired? Jobs of the future are only beginning to just exist today such as a Personality Trainers for Chatbots, Augmented Reality Journey Builders, and Personal Memory Curators. All of them require AI application knowledge. Does this mean children who can afford more and better technology get hired more?
What makes for a good AI implementation?
There’s no time for in-depth review of an approach as to what will make AI implementations fit for humankind and right for your organisation. But, there is one absolute truth — it all starts with your data. The EU and following countries can now enforce standards for better personal data use — this has created a step- change, re-educating individual’s as to what their data means both personally, and as part of wider groups.
Summary questions for AI implementation preparation:
In summary you will need to think about essential elements as part of your approach:
create and adopt a set of principles for ethical AI use which address a wide picture of the society in which you operate and your organisation;
translate AI principles into your organisation’s methodology for governing work, including within the risk and compliance areas — make it a living part of your organisation’s approach to Corporate Governance;
see Human and Non-Human Talent Management as a board level issue not an IT Manager’s issue — your talent pool is a mix of this, probably already — make sure your workforce planning and talent strategies include this analysis and detail the relevant actions;
look at employment issues and the overall legal employment framework in which you are operating;
develop AI within your Digital Strategy and ensure your organisation is ready, attitudinally as well as practically. Look at your Data And Information Management Strategy. Is your data still in paper form? If electronic, is it clean, compliant with Data Protection Legislation and ready? Make sure you know what the problem that you are trying to solve? New skills will be needed — include your Training Needs Analysis Audit;
educate the Executive Board and Senior Management Team now, so that they begin to role model the right behaviours and attitudes in line with your principles;
understand how adoption of AI is happening in your organisation. [Make sure you know how much automation is happening, is your approach desirable? Where you are sourcing your AIs? Do you know the algorithms they use? How you test source data, account for bias and above all make sure that outputs are understood and interpreted correctly? Do you know your personal data processes including retention and deletion? Do you know how much revenue and profit is created from the use of these tools in your organisation.];
Assess your Corporate Social Responsibility programmes in light of the the impact of this technology in the communities you operate in.
If you want more details please contact me as I’d be happy to help. My contact details are below.
You’re not on your own — help is arriving
The Ada Lovelace Institute is being established in the UK to “ensure the power of data, algorithms and artificial intelligence is harnessed for social good. It will promote more informed public dialogue about the impact of these technologies on different groups in society, guide ethical practice in their development and deployment, and undertake research to lay the foundations for a data-driven society with well-being at its core”
Whilst it’s early days and the full remit and functions of the Institute develop Imogen Parker, Head of Justice, Citizens and Digital Society Programmes at The Nuffield Foundation asks us to consider four important questions.

Imogen Parker’s Questions from the Ada Lovelace Foundation
If you haven’t seen the Festival of Ai, CogX, that’s on in London soon check out the line up — it’s going to be fabulous and many of these questions are scheduled to be debated there.
Is this just a women’s issue?
Women’s issues are men’s too — in fact many people of all genders identify with the condition that women find themselves in. This is just one vehicle towards a much needed, wider debate. Prof. Joanna Bryson reminds everyone that disparity of any group often mirrors the condition of women.
It’s important we gather all people together to debate the issue of ethics in Artificial Intelligence.
Ivana Bartoletti, Chair of Fabian Women’s Network
Personally I highly recommend joining in the discussion. The more minds, disciplines, views and input the better.
The event was enthusiastically and brilliantly organised by, Dr Allison Gardner, Teaching Fellow in Bioinformatics, Keele University and Councillor, Newcastle-under-Lyme Council. And exuberantly and inspiringly chaired by Ivana Bartoletti, Chair of Fabian Women’s Network of the Fabian Institute.
Thank you. It was great and here’s to many more.
Alexsis Wintour, CEO HumAIn Resources, and CEO Europe, Marbral Advisory
I happen to be fortunate to work with a team who are strong advocates of using technology fairly and ethically. However, I know I’m one of the lucky ones so this story is dedicated to progressing this topic, for the good of humankind.

= Is AI “Too Smart To Fail” safe? =
Is AI “Too Smart To Fail” safe?
I’d like to take this chance to develop an alternative viewpoint to Gordon Gong’s post titled “From Alphago Zero to AIEQ” posted on 30 Oct.
I reckon that you led off making a tacit assumption that is important to explicitly lay out. This being that once a “true” AI system has been developed capable of automating trading in the capital markets, it will be uniformly and ubiquitously adopted, rendering it the sole market driving force essentially. However, if history has taught us anything, this is probably quite an ideal assumption to make. Chances are that such technologies will be developed in parallel and no two of them will be the same. Different groups, trading entities, companies etc. will place their bets on either one of said competing technologies to outperform the rest.
I do certainly agree that once this level of automation has been achieved, jobs will probably be lost, or more likely be transformed to accommodate for new needs that will have risen in the process. The important take home message though is that competition will possibly not be eliminated in the capital markets but will simply transmute from human competition to AI competition.
Envisioning this future world where machines have taken over the stock market, I am fearful of the ethical codex the machines will develop among them, or even worse individually. Observing how behavior that we as humans deem unethical manifests itself in nature, personally I have a hard time believing that once we break through the AI barrier and machines become cognizant, in a sense, that they will also develop a similar sense of ethics as the modern Western World man. In a world where squirrels naturally developed to habitually steal nuts from fellow squirrels, why would machines do otherwise?
In the world depicted above, competition persists very much in the same sense as it does now; albeit manifested in a more advanced and intelligent form. Would you hand over all trading activity to machines and trust them not to fail the exact same way as humans did so many times in the past? Cheating competition in a non-sensical way has led to multiple financial crises and yet not every trader was on the same boat at the time. Each crisis was caused by a finite amount of trades behaving “unethically” or just following a market-damaging “craze”. What would the impact be if next time this happened on a global scale, should all machines attempt to simultaneously cheat the system or each other?

= Reining In The Dastardly Algorithms Trying To Control Our Lives =
Reining In The Dastardly Algorithms Trying To Control Our Lives
The prevailing view of artificial intelligence is that some day machines will help us reach better decisions than we can make on our own, improving our lives.
This view presumes that we trust the organizations that use AI to provide us with products and services. But that is a faulty assumption, because most will not have our best interests in mind.
Some scientists, many science fiction buffs, and a lot of the public believe perfect forms of AI — machines in likeness of humans that can actually understand the world, reason about it, and make perfect decisions — are just around the corner. Others, including me, believe it will take decades to achieve. And some scientists believe we will never achieve strong AI.
Those who believe in the wonders of a lofty kind of AI are eagerly awaiting its arrival, but in the meantime what we have is an interim step called machine learning algorithms, or weak AI, a seemingly intelligent machine that can perform a narrow task as well or better than humans, but is incapable of going beyond the rules that govern it. And they, in particular, pose a threat to our interests.
We have all heard of these algorithms, because they have been a key to the success of such tech giants like Google and Facebook.
In their simplest form, they are sets of instructions that a computer uses to solve a problem. As any of us who use Facebook know, they can be refined again and again to yield a better solution. But for whom?
The threat from algorithms boils down to a simple question: Do they benefit the organizations that develop them, those who use the organizations’ products and services, or — as the organizations contend — both parties?
More and more of us believe the answer is: The main beneficiaries of these algorithms and services are the organizations that develop them.
But the issue goes beyond algorithms’ main beneficiaries. Another question to ask is: Are they causing harm? And many would say yes.
Those who are salivating for perfect AI believe it will give them a range of choices they could not get otherwise — and that additional choices are a good deal because they will increase their ability to achieve a better life.
But more people who use the products and services of algorithm-based Internet companies are coming to the realization that instead of increasing our choices, algorithms narrow them.
Take online news feeds, for example.
Most journalists have an insatiable curiosity and want a broad range of news each day.
But an elderly journalist with whom I am acquainted has come into contact in quite a real way with the so-called filter bubble, realizing that both his Yahooand his Google news feeds are giving him a very narrow menu of stories each day. He feels as though they are censoring him.
Like many people, he cannot wait to learn about the latest Trump administration outrage. He also has a keen interest in renewable energy, reading every new wrinkle he can find about it.
He discovered a couple of years ago that Yahoo in particular was filling his news wall with stories about Trump, renewable energy, a few other topics — and nothing else.
It was obviously where this was coming from, he said. Yahoo’s algorithm identified the subjects that he was most interested in by his reading habits, and gave him more of the same instead of the broad range that he wanted, a self-reinforcing spiral until his feed was reduced to just a few topics.
He is frustrated because he is wondering what news he is missing that is not on his Yahoo wall — stories about business, culture, art, travel and other topics that he wants to know about.
This inadvertent narrowing of this journalist’s worldview not only fails to quench his thirst for news beyond Trump and renewables, but it may also be impacting his career. What subjects besides American politics and alternative energy might he be covering if he knew about them?
This journalist’s story is a simple example of algorithms narrowing people’s choices instead of broadening them. If those narrowed choices are impacting his career, then algorithms are not neutral information facilitators, but rather causing harm.
This particular journalist knew algorithms were narrowing his choices, although he did not know specifically what to do about it. But what about the millions of people who do not know?
Consider this scenario:
You are having a bad day. You had a fight with your boss, your wife is grumpy, and the starter in your car conked out.
Google has been reading your online communications and monitoring your vital signs and stress levels during the day, and knows that you are out of sorts. When you come home, it changes the smart lighting in the house to something that is soothing and turns on some calming music for you.
Naturally you start to feel better. Is it because you have returned to the comfort of your home, because your wife just kissed you, because you popped open a beer, or because of the different background that Google has created for you?
The moment we are unable to recognize whether we feel better because of pleasantries arising from the decisions we made ourselves or because of an artificial environment that an algorithm has created, we are in big trouble. Because at that moment, instead of technology working for us by expanding our world, it has exerted its control to narrow it.
Machine learning on the Web potentially manipulates and constricts our worldview. In the real world, though, it manipulates our bodies and physicality, narrowing the boundaries of our world.
Right now, we may need some kind of filtering on the Internet, because there would simply be too much information for a human to process otherwise and our journalist might be a mere victim of a necessity. But now we are entering a new area where all objects around us are also becoming more and more connected and digital. If we start unnecessarily applying the same design principles and algorithms to the real world, then we start seeing much broader and dangerous effects.
How do we prevent algorithms from exerting such potentially harmful control over us?
We can start by understanding how they work so that we can ward off their negative effects.
About three decades ago, people alarmed about the negative effect that slanted news could have on us started what is called the media literacy movement. The idea was to educate people about who was creating news stories and how the stories could be slanted to create impressions that the authors wanted. That movement is continuing — and given the surge in today’s unbalanced and fake news, it is needed more than ever.
We need to start a similar educational effort to let people know how algorithms impact their thinking, their habits — and basically, their lives. A key thrust of this effort would be encouraging people to question the notifications, suggestions and other algorithm-generated information they receive online. Who is sending the messages, why, and what negative impact could they have on me?
What causes algorithms to send these messages is usually very simple, and people could understand easily if they are educated about it properly.
Although this effort should cover all ages, it is important that it start with even very young children.
In conjunction with this educational effort, we should press tech companies to formulate codes of ethics governing their algorithm-related activities.
It might even be wise to consider rules governing the intersection of technology products and society. Some of you may think that is far-fetched. I am not saying I do not trust the individuals behind such algorithms. These programmers are trying to do good to improve people’s lives. The problem is that there is a fallacy here, whereby attempting to improve people’s lives through machine learning actually leads to the opposite. We cannot blame these individuals as, to them, it might well appear as though their algorithms are enhancing our lives — and the metrics might even look that way (i.e. 30 percent of our users are now using feature X, saving them Y time!). So instead of blaming them, I believe that we must start a discourse with them, and I would like to see a wide-ranging discussion of this issue — an algorithm-literacy effort.
Machine-learning algorithms will become more refined as time goes by. In my book, this means they will become an increasingly unrecognized threat to our ability to make our own choices.
For this reason, the sooner we start educating the public and the people who create these algorithms about these issues, the better.
Yann Leretaille, Founding Member of the Good Technology Collective

= Linking digitalization to ethics: a simple outline of some foundations =
Linking digitalization to ethics: a simple outline of some foundations
by Dorothea Baur (PhD), consultant, independent ethics officer, lecturer and public speaker.
Everyone is talking about the need to bring ethics into digitalization and AI. It’s seen as a tool to bring transparency into algorithms. But do we have any transparency on what we mean by ‘ethics’ or what this ‘ethics’ should look like? Isn’t ethics just as much of a black box as the technology to which we are supposed to apply it? Here is a simple outline of how ethics and digitalization relate with some suggestions on what questions we need to ask ourselves if we want to advance digitalization in a responsible way.
Luca Bravo on Unsplash
It shouldn’t take a scandal of the dimensions achieved by Facebook/ Cambridge Analytica or a creepy cackle from Alexa to make it clear that we must not use technology blindly without asking ourselves some ethical questions, but incidents like these certainly help to raise awareness on an ever broader scale.
Yet, despite an increasing amount of articles calling for integrating ethics into algorithms (e.g. in order to overcome algorithmic bias), or for subjecting the implementation of new technologies “to applied ethics, or due diligence around asking tough questions”, it often remains unclear what is effectively meant by ethics.
In fact, it seems that what we mean by ethics is no less a black box than the algorithms into which it is supposed to be integrated.
Here is an attempt to outline in clear and simple terms how ethics, as in ‘ethical foundations’, can give us some guidance when deciding whether we should adopt digital technologies. While the explicit focus is on digitalization (within a corporate context), most of what is said here, equally applies to any type of AI etc.
First of all, we need to keep in mind that digitalization is a technological process initiated by humans. We decide, what we want to digitalize, and which consequences we are willing to accept.
But which guidelines should we follow when taking these decisions? This is where ethics comes into play.
At the center of this piece is the claim that we need to preserve the conditions to accept responsibility for our actions and that we must not delegate this ability entirely to technology. This means that digitalization measures must neither threaten the autonomy and agency of individuals nor the very existence of humankind. Digitalization is here to relieve us without depriving us of the right of decision.
Technological nirvana or apocalypse?
The debate on digitalization often resembles a war of opinions. On the one hand there are those who are convinced that technology, given the right code, the right algorithms and robots, has the potential to resolve all problems of humankind and to guarantee a life free from trouble and worry to all of us (keyword: ‚technological solutionism’). On the other hand there are those who most of all caution against unintended negative consequences of technology and who worry that we are effectively overrun by digitalization.
“Women viewing modern art with black and white surveillance cameras on wall in Toronto” by Matthew Henry on Unsplash
It is clear that technological change nowadays occurs at an exponential rate and that the complexity increases continuously as a consequence of the ever stronger connection between different devices and processes. This evokes the impression that we are losing control. We are dependent on devices, whose workings we at best understand rudimentarily. These devices communicate increasingly autonomously with each other. Our data is stored in clouds, and we neither know where these clouds are located nor who has access to them.
But it is also a fact that digitalization is made by humans. It is not a factual constraint following natural laws like an earthquake, over which we have no influence. Digitalization has reached dimensions, which we cannot and should not reverse entirely. But within these dimensions we have some scope of action, and this means that we also have responsibility to shape the course of digitalization. It is irresponsible to surrender or resign in the face of digitalization. We must set the boundaries for what we are willing to accept.
Ethics as a subdiscipline of philosophy focuses on those areas of life which are subject to decisions made by people — these areas are within the scope of human responsibility. Everything that has been shaped by people’s decisions, could in principle also look different. This is what sets digitalization apart from earthquakes, or rather what explains why it wouldn’t make sense to develop an ‘ethics of earthquakes’. While we can try to predict earthquakes and to mitigate the damages they cause, we are simply not responsible for their occurrence.
So now it has been argued that digitalization should be subject to ethical scrutiny. But we still do not have any idea what should guide this scrutiny.
As stated above, the central claim of this article is that we need to preserve the conditions to accept responsibility for our actions and that we must not delegate this ability entirely to technology. Thus, the relevant ethical questions for which we need guidance are:
How far can we delegate tasks to digital technology in a responsible way? Where do digital technologies serve to relieve us and where do they deprive us of our right of decision?
Starting points for answering these questions can be found in the works of two influential philosophers of the modern era, namely Immanuel Kant on the one hand and Hans Jonas on the other hand. Don’t be scared by these names — I will argue in a few simple words how their ideas can give us some guidance.
Using our reason
In his brief essay titled „What is Enlightenment?“ (1784) Kant called upon people to ‚sapere aude’, in English: „dare to know“, or „dare to be wise“. Kant wanted people to release themselves from their „self-incurred immaturity“, and instead use their reason without guidance from others (such as the church or other authorities). Immaturity is self-incurred if it is the result of either the fear to think for ourselves, or idleness or cowardice. All of these three ‘vices’ are a standard component of human behavior.
The core concern of enlightenment consisted in promoting human autonomy. We should finally be liberated from the shackles of religion and authoritarian institutions, and instead be able to apprehend the world based on our own reason and to decide based on our own insights which laws we want to follow (the latter being the essence of democracy).
Enlightenment has brought us very far. In particular, it has set the stage for technological progress.
But now, almost 250 years later, we have developed technologies, by virtue of our reason, that are so complex that we are forced to trust them blindly. Does this not mean that we are depriving ourselves (yet again) of our hard-won right of decision and that we therefore violate the fundamentals of enlightenment?
Preserving responsibility
Another important input comes from Hans Jonas. In the second half of the 20th century he addressed the need for an ethics of technology. During the Cold War the nuclear arms race between the US and the Soviet Union made it very clear that not everything, which is technically possible, is morally desirable.
For a long time we implicitly assumed that humankind as such would always exist. Nuclear technology, in particular the development of nuclear weapons of mass destruction, suddenly provided us with the means to extinguish humankind within a relatively short period of time and to make our planet uninhabitable. What is more, due to progress in human genetics, we have also acquired the power to directly impact on the ‚core of human beings’ to an extent hitherto unimagined. Repeatedly we are hearing about horror scenarios of human clones, who have been designed exactly according to the twisted ideas of their power-hungry creators.
Jonas therefore called upon us to always ask ourselves the following two questions when deciding on whether specific technological progress is desirable:
Does a technology threaten the existence of humankind as such?
How does technological progress impact on the quality of human existence?
The quality needs to be in line with human dignity. We can only preserve dignity if we are capable of taking our own decisions, that is, if we maintain human agency. According to Jonas it would be irresponsible to trade our right of decision for technological progress.
Implications for digitalization
In its most radical consequences, digitalization has the potential to severely undermine human agency and to therefore violate the conditions for a ‚ life in dignity’, i.e. our ability to take our own decisions.
A big part of digital technologies serve to gain more knowledge. They empower us to apprehend the world based on our own reason as postulated by enlightenment. However, as stated above, we now delegate an increasing number of tasks to devices and programs, whose workings we effectively do not understand. The more these devices are connected to each other and communicate autonomously with each other — keyword ‚internet of things’ — the more they emancipate themselves from us. As a result we lose our right of decision and our ability to be held responsible for the commands that are being exchanged between the devices.
In the best, or maybe the worst case, input from humans is not needed anymore in order to operate these devices. There is a real fear that one day we might become ‚slaves of machines’ and that robots might take over control over the world.
Relevant ethical questions in practice
So, what does all of this mean for actors who have to make choices regarding the introduction or expansion of digital technologies? Let’s break these general reflections down to the corporate level. In general, a company facing decisions on how to address digitalization must take into account to the following principles:
A digitalized company is only a responsible company if it:
commits itself to upholding the conditions that enable the use of human reason.
And a digitalized company only provides the basis for a life in human dignity, if it:
ensures that the right of decision of people (at least) within the company can be asserted at all times.
Neither of this implies that a company that addresses digitalization in a responsible way should refrain from technological progress. All it means is that companies should always keep an eye on whether the technological measures they take are still within their control, i.e. whether they can still be the subject of human reason (and are susceptible to debate).
For example, a company that addresses digitalization in a responsible way must not irreversibly delegate autonomy to algorithms which act in place of people, and which pass on data to anonymous third parties, which mine them like a commodity and which use the information collected against people whenever it suits them (think Cambridge Analytica).
So, this was a start. The goal was to argue why digitalization should be subject to ethics, and how we can create a direct link between ethics (as in ethical foundations) and digitalization. In a next step, these abstract thoughts need to be translated to more specific, e.g. corporate contexts. Based on the central norms identified in this article, i.e. the preservation of responsibility, upholding the use of human reason and the right of decision, we need to identify the relevant ethical questions for specific companies and answer them for their individual context.
Here is an exemplary list of questions that might be relevant to many companies:
Employees: How do digitalization measures impact on the individual freedom and the self-realization, that is on the right of decision of employees?
Clients: How does digitalization impact on the clients of a company? If a company digitalizes its services to the clients, does serve the empowerment of clients or deprive them of their right of decision?
Data privacy: And what does digitalization mean for data privacy? Are clients in control over the collection and use of their data?
Keeping questions like these in mind will contribute to making ethically informed decisions when advancing digitalization. We need to make sure that we continue to see technological progress as a conscious choice subject to common standards of responsibility. Only by doing so we can make sure that companies can be held accountable for the impact of their technological choices. If we surrender in the face of digitalization we let others willingly guide us back into the dark cave of unenlightenment which we so forcefully many years ago.
To be continued.

= Legal and ethical implications of data accessibility for public welfare and AI research advancement =
Legal and ethical implications of data accessibility for public welfare and AI research advancement
This article has been co-written with Gabrielle Paris Gagnon, lawyer at Propulsio 360 Business Consultants LLP
Every organization that focuses on artificial intelligence wants and needs the same thing : more data to train their algorithms. Without a doubt, a lot of the success of deep learning systems today is predicated on the availability and the collection of large datasets, often supplied by users themselves in exchange for using these services for free.
Photo by Matam Jaswanth on Unsplash
Earlier in January, at the Strategic Forum on Artificial Intelligence organized by the Chamber of Commerce of Metropolitan Montreal, Valérie Bécaert, director research group at Element AI, addressed the business community on the importance of sharing data to make AI accessible to all organizations. In a time when big data is aggregated in the hands of a few powerful companies, this perpetuates economic inequalities and undermines the use of AI for social causes. Over time, this gap is also bound to increase. Tech giants multiply their data collection applications and prevent access to this proprietary data even for users who have themselves generated this data — see shadow profiles generated by Facebook. [1]
Another positive outcome of a more open access policy to data would be the facilitation of research by the scientific community, startups and non-profit companies geared towards public welfare.
By 2030, experts predict that AI will contribute as much as $ US 15.7 trillion [2] to the world economy. If nothing is done and no standards are promulgated for data accessibility, these profits will go directly in the hands of a very few people. For the development of AI to be a vector of social mobility, these gains and the subsequent wealth creation have to be distributed in an equitable manner. We believe that accessibility to large data sets (which are crucial for training deep learning systems) can orient the collective use of this technology more towards public welfare.
Moreover, at times, individuals, companies, and nonprofits, which are often data generators for big tech companies, are at their mercy. Their lack of control on the data they generate and how it is used can also have negative outcomes. They find themselves powerless when there is a misalignment between what they derive from the use of big tech’s products and the latter’s business objectives. For example, the latest announcement that Facebook would alter their algorithm to give lesser importance to publishers’ posts is catastrophic for news providers and other businesses that have been relying on Facebook for more than a decade to spread information.
Photo by William Iven on Unsplash
Now let’s consider the following facts:
IDC, a research firm predicts that the data captured in the digital ecosystem will be 180 zettabytes (zettabyte = 1 followed by 21 zeroes) by 2025 [3]
Amazon uses trucks pulling shipping containers to manage the amount of storage space their AWS clusters demand. [3]
This gives us a strong indicator that the amount of data being collected on users and their online behaviours is only going up. One of the core reasons for the profitability of these companies comes from their ability to monetize this data and subsequently sell the data to advertisers via data brokers. [4]
Given such a heavy reliance on data to create money-making products and services, accessibility to such data in an equitable fashion bears even more importance to normalize the market and make it more competitive which could potentially lead to better outcomes from a user perspective.
Photo by Scott Webb on Unsplash
Startups asking for access to public data
As big data collected by large organizations continues to have growing market value, startups are beginning to turn to the courts for authorization to gain access to this data.
These small companies allege that the control companies have over publicly available data represents anti-competitive practices.
In 2016, hiQ Labs, a startup that uses LinkedIn’s publicly available data to build algorithms capable of predicting employee behaviours, such as when they might quit and who to promote accordingly, received a cease and desist letter from LinkedIn stating that scraping their publicly available data was in violation of the company’s terms of use. hiQ Labs took the case to court because their business model was wholly dependent on this public data they acquire from LinkedIn. [5]
In August 2017, U.S. District Judge Edward Chen in San Francisco sided with hiQ Labs and ordered LinkedIn to remove within 24 hours any technology blocking hiQ Labs’ access to public profiles. In his opinion, asserting control by preventing hiQ from accessing LinkedIn public profiles could be a means of limiting competition, which violates California state law. Final oral arguments in this case are expected in March 2018.
Access to real-estate data in Canada
In Canada, the Competition Bureau filed a lawsuit against the Toronto Real Estate Board, a not-for-profit corporation that operates an online system for collecting and distributing real estate information to their members. TREB’s policy had restrictions on the communication and distribution of some data it collected, such as sales price, by their members.
Photo by Giammarco Boscaro on Unsplash
The Competition Bureau pleaded that this restrictive distribution of digitized data prevented competition as well as deterred innovation and the emergence of new business models by prohibiting realtors from posting sales data on their websites. The Federal Court of Appeal sided with the Competition Bureau in December 2017 and ordered TREB to allow its members to share the sales histories of listed properties online. [6]
This decision is expected to have widespread ramifications in Canada for how organizations distribute data and request for more open data in the marketplace.
Thus having more open, public datasets could foster the development of goods and services that are more public-welfare oriented.
What we propose is the following: creation of data sharing standards that are driven by industry verticals to enable researchers, young entrepreneurs, etc. to build products and services for public good. General Data Protection Regulation (GDPR) [7], set to come into force on May 25, 2018, sets a loose precedent (when it asks companies that they be able to provide data in a standardized format about a user when they demand it) — as this rolls into effect, it will be interesting to see how companies decide to adopt standards which can be interpretable across different data processors (term used in GDPR to refer to different service providers that a user could switch between).
Some potential downsides to public data sharing
Having more open data policies can also have its chilling effects. Even sharing anonymized datasets publicly can be a challenge as sophisticated statistical methods coupled with the mosaic effect (a technique used to combine information from disparate sources to create a richer profile of the target) can reverse the anonymization process. Indeed, considering that 41% of Canadian companies had sensitive data stolen last year following security breaches, data sharing could jeopardize the privacy of users’ sensitive information. [8]
This has been demonstrated in the past many times. One of the cases is where the sexual orientation of a user was inferred from movie ratings in a publicly released dataset from Netflix [9] by cross-referencing ratings available on IMDB for certain rare movies that were common between those datasets.
In another case, AOL had released search queries to the public and specific users down to their home addresses, medical needs, pet ownership among other things were identified. [10]
It must also be taken into account that although users may have accepted the collection and sharing of their personal data via terms and conditions, a debate exists as to whether their consent is actually valid. There is also the risk of identity fraud that can result from the linking of different datasets when combined with deep learning methods [11]. To protect the privacy of the users, we would need to have both legal and technical mechanisms in place that encourage and balance the sharing of these datasets while protecting the privacy of the individuals.
Another idea could be to allow for remuneration models along the lines of micro-payments where explicit consent of the user can be obtained to perform specific activities with their data. This can be incentivized by a strong demand from users for the privacy of their data and their potential boycott of services that violate that right.
Do you agree with the notion that large datasets from corporations could help fuel research for public welfare? What would be the best way to go about it? Share your thoughts in the comments section and let us know.
For more information on the work that I do in the ethical development of AI, visit https://atg-abhishek.github.io
References:
[1] How Facebook figures out everyone you’ve ever met — https://gizmodo.com/how-facebook-figures-out-everyone-youve-ever-met-1819822691
[2] AI Will Add $15.7 Trillion to the Global Economy — https://www.bloomberg.com/news/articles/2017-06-28/ai-seen-adding-15-7-trillion-as-game-changer-for-global-economy
[3] Data is giving rise to a new economy — https://www.economist.com/news/briefing/21721634-how-it-shaping-up-data-giving-rise-new-economy
[4] DATA BROKERS A Call for Transparency and Accountability — https://www.ftc.gov/system/files/documents/reports/data-brokers-call-transparency-accountability-report-federal-trade-commission-may-2014/140527databrokerreport.pdf
[5] LinkedIn Cannot Block Startup From Scraping Public Profile Data, US Judge Rules — https://gadgets.ndtv.com/social-networking/news/linkedin-cannot-block-startup-from-scraping-public-profile-data-us-judge-rules-1738096
[6] Appeal court upholds ruling ordering real estate agents to make home sale data public — http://www.cbc.ca/news/business/treb-court-ruling-1.4428262
[7] EU GDPR — https://www.eugdpr.org/
[8] Security Breaches prove costly — https://www.investmentexecutive.com/news/industry-news/security-breaches-prove-costly/
[9] Robust De-anonymization of Large Datasets by Arvind Narayanan and Vitaly Shmatikov — https://arxiv.org/pdf/cs/0610105.pdf
[10] A Face Is Exposed for AOL Searcher №4417749 — http://www.nytimes.com/2006/08/09/technology/09aol.html
[11] The Evolution of Fraud: Ethical Implications in the Age of Large-Scale Data Breaches and Widespread Artificial Intelligence Solutions Deployment by Abhishek Gupta — https://www.itu.int/en/journal/001/Pages/12.aspx

= Digital Digest May 20, 2018 =
Digital Digest May 20, 2018
DD is back. Apologies for the length…but this one needs it.
1. Big News

Telephone Westworld: let’s talk Google Duplex
Two weeks ago at I/O, Google’s annual nerd prom, a technology called Duplex split the blogosphere into the impressed and the frightened. Duplex is an AI that combines natural language processing, a stunning leap forward in wavenet’s text to speech synthesis and deep learning that raises the bar for contextual understanding. The result is that the Google Assistant can make calls to real people to get stuff done on your behalf.
If you haven’t seen the demo yet, watch this short video and be aware this isn’t some future moonshot hype: it launches in beta this Summer.
Google Duplex announcement at I/O (4:11)
The Tech
As impressive as the umm and uhh laden voice mimicry may be, make no mistake the real achievement here is contextual understanding.
Knowing for example that the Assistant’s “Wednesday the 7th” booking request was misunderstood on the other end as “for 7 people” is an order of magnitude harder than Chihuahua or muffin.
Google’s AI investment is deep and long term: the new racks of TPUs in billion dollar data center expansions have allowed “now with Machine Learning” superpowers to be added to nearly all of their products.
However, the fuel that makes these magical ML features actually work is a sea of user data, and more than any of its competitors, Google has yours. (Did you wonder why Google nixed the storage limits in gmail, photos and others long long ago…?)
In the world of Personal Digital Assistants, the Google Assistant is about to graduate from Harvard while Siri, Bixby and Cortana are still on the shortbus.
Ethics of Misuse
Oh boy…where to begin…
Scammery at scale: last week brought a landmark $120MM settlement for 97 million robocalls illegally placed over a three-month period (sadly this amounted to only 0.5% of phone spam). Now imagine 97 million conversations with as many unsuspecting grandmas for nefarious purposes.
DDoS IRL: it’s easy to envision a bored teen creating hundreds of Google accounts and booking up all the hair appointments in town…just for the LOLZ.
Political influence campaigns: both sides of the market will be disrupted: fake ‘constituents’ will flood their representatives with special interest passion and voters will be bombarded with persuasive bots looking to sway their opinions (remember, these will be conversations, not recorded messages).
The consequences of near perfect impersonation: from a disgruntled ex auto-harassing to defamation on steroids to an additional protection layer for swatters…it ain’t going to be pretty.
Economic Impact
All of the excitement and fear seemed to miss another major impact: people who talk on the phone for work will be unemployed.
Societal Impact
Google Duplex has the potential to cause a big rip in our fraying social fabric.
Even with perfectly ethical use, this tech adds more distance between regular people.
Just as the masses aren’t so polite online given the distance and anonymity our avatars create, I see Duplex amplifying flakey cancellation behavior and leading to increasingly rude/weird initial conversations to suss out if the voice on the other end is bot or not.
However, the biggest victim may be the truth itself: what can we believe in a world where we can no longer trust our ears?
Daniel’s .0000023 BTC
This technology has nuclear potential in that both power plants & bombs will be built on top of it.
Google is full of well meaning scientists but the big reveal without mention and full throated mitigation of the potential harms underscores the general tone-deafness of the Valley right now.
Financial markets and a true “don’t be evil” culture will coral the Googlers. Unfortunately Duplex-wielding rogue actors won’t wear those shackes.
2. I wish I thought of that

Dominos & IFTTT- I worked on Dominos at the very start of their transition from pizza to technology. I’m still impressed that this culture took root. Having a corp account in IFTTT is something all brands should have.
3. Tool of the Week

GDPR- Do you wonder why every online service you’ve ever signed up for has updated their terms and conditions this month? You have the EU’s General Data Protection Regulation to thank for it. Much like “California emission standards” benefit cars sold in all 50 states, these more stringent data protection laws for EU citizens help us on this side of the pond, too.
4. Startup Radar

Orchid is “the decentralized, open-source solution for a surveillance-free internet.” If you watch Silicon Valley, this sounds suspiciously like Pied Piper’s “New Internet.” They’ve raised over 40MM from the Valley’s heavy hitters. I did my best to get through this 51 page academic white paper and I’m still at a loss to whether or not they’ll be around next year.
5. From the Archives

Motorola Razr- If you had one of these in 2004, you were living on the edge (pun). It sold 130 million units during its lifespan making it the best selling clamshell ever. And with a whopping 5.5 MB of internal storage you could add ALL your contacts…even a few ringtones, too.

= Intentional, Ethical Design within AI to Create a More Inclusive, Just Society =
Intentional, Ethical Design within AI to Create a More Inclusive, Just Society
When thinking of Artificial Intelligence 100 years from now, I imagine many different possibilities, with effects that are impossible to determine. Looking forward to the future with an idealistic lens, I see a world where AI assists in disintegrating biases allowing for a unification of the human race, assisting individuals in identifying routes that lead them to their personal destiny, and, with a heavy focus on AI experience design, a scientific solidification of the social sciences as a means to best support users.
However, to reach this state, Artificial Intelligence needs to be continuously designed with an ethics-centered approach. Without this approach and without a social justice mindset of designers, programmers, and product managers, artificial intelligence will only mimic our current society, and perhaps even magnify current societal issues, if not strategically implemented.
Incorporating social complexities and historical contexts within AI, could allow for the criminal justice sector to become more of a “rehabilitation system” that empowers marginalized individauls who have been targeted by a faulty system, rather than used as a punishment system for the marginalized. Professor Jennifer Sukis idea of an AI assistant attached to an individual from birth could be beneficial in creating mutual understanding between individuals and alleviating many systemic issues presented by today’s criminal justice system. For instatnce, AI systems such as Northpointe (a system that uses algorithms to recommend prison sentences) could barrow the data and experiences of accused suspects (from their personal AI assistants) to facilitate a better understanding of why a person has committed a crime and carve out a path of support and resources for rehabilitation that would work best for that individual, rather than just creating a prison sentence based on many years of prison sentences potentially based on implicit biases.
AI can bring a lot of hope to systemically marginalized communities, as long as those communities are also given access to the same resources as those in power, allowing for equal benefits from the power of AI.

= A Sexist, Racist, Biased AI =
A Sexist, Racist, Biased AI
Artificial Intelligence is as good as the people and data behind It. Since we don’t have mechanisms to account for people biases and errors they find there way to the algorithms we design. The validity of the Data behind our algorithms and decisions are also called to question. We don’t know if the data collection methods or the research design behind them are solid enough. Show me any set of data and can point holes, and shortcomings in its cogency. The findings in this article are but a wake-up call for us to understand the shortcomings of AI and it’s inherited weaknesses.
The Linkedin mia culpa
The realization that the Algorithms behind LinkeIn functionality are skewed, daunted on me through several years of using the platform. Here’s what I mean:
The Sexist search output


The other day I was just finishing up on an online group discussion. So I sent out LI to one of the main conversants who was a professional strategist out of Australia. Instantly, I got barraged by a networking suggestions of that day. But the trend repeated itself. Every time I send out an invite to individuals who had a resemblance to this person or others to that like, I get barraged by skimpy pictures, and sexy names, like this in the picture. To me, this is not just an offense on my individualism, but it also suggestive. The stereotype is that since I am a male I would naturally be interested in such connections. Another problem with this type of connectivity of suggestions, is that most of these profiles are most likely false profiles. They act as fake and shadow profiles to entities that have other agendas on there minds.
Ethnicism

I thought maybe there was a glitch in the algorithms behind Linkedin. But the trend continued in other examples. For example a couple years ago I headed a conference in Kazakhstan and kept in touch since. From time to time I send out personalized invites to people I meet to join my network. Again the skewed suggestive search resulted in Individuals with Kazakh last name. I understand that this might have to do with geographic enormity of the 10th largest country. AI is perhaps trying to be helpful in its outreach methodology. Nevertheless, this makes you but pounder why don’t I have a choice in this. Also, no to mention all that not all Kazakh names have a similar structure.

But then another disturbing search trend shows up here when I invite someone form the MiddleEast region to join my network. I get connectivity suggestions with Muslim last names. Not everyone in the MEA is Muslim or has Arabic last name, the region is home to divers religious groups. So why didn’t the suggestions include names from traditionally non muslim families? It’s because AI algorithms see data in a cluster an categorizes it in one block. Since the assumption that the majority in the MEA have Muslim last names the Algorithm throws overshadowing assumption that this is the case. Hence the dangers in relying on such algorithms.
Racism

So far I taught maybe I was being too suspicious, then the other day I sent out an invite to one of my colleagues in Washington DC. This person was of Nigerian descent, and surely enough the networking suggestions pointed out to people who are both Nigerian and geographically located in Nigeria. Now, this might not be racist, it could be a bad AI grouping algorithm assuming that I might want to connect to Nigerians in DC and Nigeria. I would probably put this under Ethnicism, and its geographic ignorance on the Nigerian diaspora.

Finally, the most disturbing AI algorithm is the one grouping people by color and race!!! Shocker right!!! If you don’t believe me try it your self!. The other day I was reaching out to one of my classmates in graduate school. That person was African American, surely enough the suggestions to connect came out to represent a selection of connections from the same background. Yes, the person was a female hence the suggested results. But to realize that AI algorithm produced an all-female cast, segmented according to color and racial background is not but short of a racist lens.
Final Thoughts
The engineers and the Leaders behind these AI algorithms need to be trained on how to use a 21-century appropriate design and unbiased design methodology. I understand that there no such thing as no bias, and that goal might be illusive. Nevertheless, we need to at least design a more ethical and inclusive AI algorithms, used on our social platforms.

= FaceID and the Future of AI =
FaceID and the Future of AI
Apple unveiled its iPhone X with FaceID on September 12, 2017. As an engineer, I understand the internal logic by which FaceID came about.
You want to provide the edge-to-edge screen
You need to get rid of the Home button form under the glass (it didn’t work there anymore)
You need to replace the ID functionality
There’s a front-facing camera already in place, and making your face an ID is a natural direction.
Various technical challenges that arise — distinct angles of view, changing facial hair or lighting or headgear — are catnip for Machine Learning scientists. Fraud with photos or masks is just another exhilarating challenge. All kinds of new things you can do, such as animoji, seem like a fun way to leverage the new capability.
I’m pretty sure nuclear scientists felt the same way — when you read Richard Feynman, you see how much fun and camaraderie was born around a joint technical task solved by the best young minds of a generation gathered in an elite, secret lab.
But with FaceID, human faces become a dataset for AI on a massive scale. This has never happened before. Inevitably, all kinds of uses will arise from this. Much better mining of emotions and identity will be possible. Correlating behavior with facial expressions will follow. Credit assessment based on your face, career decisions, marriage and dating will affect your own life. Your similarity with your children will lead to analysis of how they inherited your credit risks and career potential. Furthermore, one can ask whether the inherited credit risk is mainly from the mother or father?
Although the positive potential is immense, the dangers are enormous, too. Ethics will inevitably trail behind as abuses are invented and tried. Legal decisions will follow real-life consequences.
Where TouchID was an almost ease-of-use feature, where you can unlock the iPhone in your pocket or while driving, now you will have to constantly show your face to the phone. It will know when you’re sick, when you haven’t slept enough, or too long. It will know when you’re sad, happy, or just not here. Or rather, you will have to be here.
Look at me, Luke. I am your iPhone.

= AI MUST READS — W22 2018, by City AI =

AI MUST READS — W22 2018, by City AI
Artificial Intelligence, Machine Learning and related fields are in a constant state of change. We want to inform but also encourage discussions on well presented topics we think are necessary in the context of putting AI into production. Every week we’re picking applied AI’s best articles plus adding a discussion starter
1. Microsoft is creating an oracle for catching biased AI algorithms
Microsoft is creating an oracle for catching biased AI algorithms
Noun Project | Andrejs Kirma | Ms. Tech Microsoft is building a tool to automatically identify bias in a range of…www.technologyreview.com
One of the constant companions of Artificial Intelligence at the moment is the plethora of questions about the ethical ramifications and about potential situations of bias creeping into the automation. If you’ve read almost any of my past must reads then you’ll know that this also isn’t new and there seems to be a new development of some kind in the “automation of bias” field almost weekly.
Microsoft and Facebook’s recent announcements SEEM to be the first practical steps being taken towards identifying and preventing this bias, that isn't to take away from the number of organisations set up and champions of the cause that have come forward because without these people I question whether these two leaders of industry would have taken these steps.
It does leave me questioning though. They can build an artificial intelligence system to detect A.I. in other artificial intelligence systems, if they can do that why can’t they prevent A.I. in their systems in the first place? Is this system for detection built by the same people that built the systems it will be searching for bias within?
Bonus video at the bottom of the page*
2. Why thousands of AI researchers are boycotting the new Nature journal
Why thousands of AI researchers are boycotting the new Nature journal
Budding authors face a minefield when it comes to publishing their work. For a large fee, as much as $3,000, they can…www.theguardian.com
To have such an archaic profit-making model in place in a field that is at the cutting edge technology is an affront not only to the potential of the technology but also to the memory of politically charged internet activist Aaron Swartz and the many like him.
Machine learning is a young and technologically astute field… The community itself created, collated, and reviewed the research it carried out. We used the internet to create new journals that were freely available and made no charge to authors.
Information that has already been paid for once by the taxpayer and the work produced by the scientists and researchers should never be held and controlled by private institutions. This shouldn’t just be limited to AI and ML either, information of this type should always be free and available to the public and hopefully this will start a trend that will start towards the removal of restrictions.
3. Are you scared yet? Meet Norman, the psychopathic AI
Meet Norman, the psychopathic AI
Norman is an algorithm trained to understand pictures but, like its namesake Hitchcock's Norman Bates, it does not have…www.bbc.co.uk
This is idiotic and intentionally misleading with the simple aim to increase the foot traffic of this article. Do you know what sounds like a great idea? Lets demonize a technology that people are already worried about and perpetuate the stereotype that this technology will inevitably kill every single one of us.
In reality all they really created was a 16 year old child spouting off for the sheer shock value of what he has to say.
Not only does this entire article very quickly steer away from the whole story about the “Norman Bates A.I.” almost as if there was a word limit that had to be reached, but what it does say about the experiment that was performed is that in fact it has nothing to do with the specific AI that they’ve attempted to place human qualities onto, but instead it has to do with the data that it has been fed. I imagine that a child raised that had consumed the same imagery as Norman would have a similar out look. What a waste of time. This is how not to do it.
Bonus Article
Microsoft + GitHub = Empowering Developers
Microsoft + GitHub = Empowering Developers — The Official Microsoft Blog
Today, we announced an agreement to acquire GitHub, the world’s leading software development platform. I want to share…blogs.microsoft.com
Although its not directly related to A.I. I feel like I’d be missing a trick if I didn’t talk about Microsoft’s announced acquisition of GitHub. Personally I’m not a big GitHub user (Don’t worry, I’m learning) and as such this announcement didn’t garner the visceral reaction from me that it did many people. Usually I’m the first to understand that the internet isn’t always the best place too get balanced, calm discussions on a topic and when I first saw the backlash I made the assumption it was the typical loud minority, but having read into it more there seems to be some genuine concerns.
There are a lot of other companies that could have acquired GitHub that would worry me twice as much as having Microsoft acquire the company, but still Microsoft will inevitably find themselves with conflicts of interest, how will they deal with these? Will they remain “all-in on open source” as they claim or will they exploit GitHub, driving away its user base?
Bonus Resource
Berkeley Open Sources Largest Self-Driving Dataset Every Data Scientist Should Download NOW
Overview UC Berkeley has open sourced the world's largest and most diverse self-driving dataset It contains 100,000…www.analyticsvidhya.com
UC Berkeley has open sourced the largest and most diverse self-driving dataset for the general public. It is being called ‘BDD100K’ and comes added with rich annotations.
Francesca Rossi, IBM Research & University of Padova, on unbiased AI. See also http://ibm.biz/five-in-five #AIethics

WorldSummit.AI
Join 6,000+ AI practitioners from over 100 countries at WorldSummit.AI this October!

= A New Ethics: China is looking to surpass the West as a leader in artificial intelligence ethics =
A New Ethics: China is looking to surpass the West as a leader in artificial intelligence ethics

WHEN the 19-year-old Chinese national Ke Jie lost a game of Go to a Google computer program, it indicated on a global stage just how far artificial intelligence (AI) had come. Though developers in machine learning have frequently offered overly rosy predictions about the rate of technological progress, the success of Google’s “AlphaGo” program seemed to indicate a decisive triumph in AI innovation. “This year, it became like a god of Go”, Mr Ke said.
Nowhere were the reverberations of Mr Ke’s defeat felt more profoundly, however, than in China. AlphaGo’s mastery of a centuries old Chinese game did not only wound national pride; the success of American-led AI worried China at the highest levels.
In Xi Jinping’s October speech at the 19th National Congress of the Communist Party of China, he announced that “China will be a global leader in terms of composite national strength and international influence.” Central to Mr Xi’s mission is that China ascend as the dominant global leader in artificial intelligence, producing AI industry gross output exceeding RMB 1 trillion ($150.8 billion) and AI-related gross output exceeding RMB 10 trillion ($1.5 trillion).
Mr Ke’s defeat, therefore, signified more than just the great improvements in AI technological capabilities that have emerged in recent years. It was a “Sputnik” moment that indicated the country was behind in developing superior AI infrastructure. It’s no wonder China cut off live streams mid-contest.
Beyond the wealth and security China hopes to amass through investments in AI, however, the country has also turned its attention sharply towards AI ethics. In the Chinese State Council’s recent AI proposal, Beijing announced its plans to establish AI laws, regulations, ethical norms, and the beginnings of AI security assessment by 2025.
The motivations behind this are clear: leadership in AI ethics is part of a broader aim to expand Chinese cultural influence and normative power. In order to overtake America as the leader of the international system, it mustn’t simply be the largest economy in the world; it must create the system’s rules.
That is concerning. China’s government has relied on authoritarian practices to impose an illiberal societal order, all to the detriment of human rights and individual liberty. Already, it has made a habit of using eminent technology, such as face-recognition, to improve systems of surveillance and state control. There’s no reason Beijing will treat AI differently, and recent literature from China on AI ethics features calls for the infusion of AI industry and the state.
By utilizing its 730 million domestic Internet users to draw unparalleled amounts of big data, China will, if nothing else, hold a unique window into the ways AI might be used. But this does not mean America and Western powers are incapable of continuing onwards as the world’s chief normative leaders. Britain is currently home to some of the best work in AI ethics, and America, for all of its faults under President Trump, still holds a cultural and moral influence China cannot match.
Things may still change. In just over a year, Mr Trump has damaged America’s relationship to the intentional rules-based system it helped create. And his recent budget proposal for 2019 includes significant cuts to non-defense scientific research by 2028. America under Trump is threatening to abdicate leadership in AI, allowing China to determine the future of machine learning and the rules that will guide it.

= Benefits and Ethical Challenges in Data Science — COMPAS and Smart Meters =
Benefits and Ethical Challenges in Data Science — COMPAS and Smart Meters
Long-form essay.
Getting the balance right: weighing benefits against ethical challenges in Data Science.
1. Introduction
Recent advances in processing large amounts of data have generated a plethora of new opportunities to improve individual lives and the welfare of our societies. For example, smart meters monitoring domestic electricity consumption can help save electricity when not at home or suggest the cheapest electricity supplier depending on the consumption pattern.
However, imagine the electricity supplier’s website states that if you provide real-time as opposed to weekly electricity consumption data, your yearly electricity costs would decline by 20 percent. Should electricity suppliers be allowed to pressure consumers into trading privacy for money?
1.1. Outline
This essay describes two applications of Data Science, namely the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) software system used in US courts and domestic smart meters, to contrast their benefits with ethical challenges.
1.2. Definitions of key terms
Defining Data Science has two components. First, vast amounts of data (terabytes, petabytes) are characterised by the three V’s of Big Data: volume, velocity and variety (Laney, 2001). Second, Data Science stands for statistical models implemented in the form of software that can detect patterns in such data.
Ethical challenges arise when opinions on what is considered right and wrong diverge. For example, should an algorithm have the power to decide whether a defendant is released on bail or not? As statistical models are built on top of data, applications like COMPAS would require analysing how the data is generated in the first place. Ethical challenges might arise when police officers on patrol are controlling citizens in a biased way, but this goes beyond the scope of the essay.
To improve individual lives and the welfare of our society requires an application to create value in the lives of one or more individuals without negative externalities and within legal boundaries. In other words, someone benefits from using this technology.
Finally, a smart meter is an electronic device that measures domestic electricity consumption and communicates with other electronic devices as part of a connected network (think IoT).
2. Benefits and ethical challenges
COMPAS and smart meters make use of large amounts of data, provide clear and distinct benefits, raise compelling ethical challenges, are discussed by numerous scholars and appeared to have the highest present-day and future impact on society.
2.1. What is COMPAS?
Judges across the United States are increasingly relying on algorithms to assess whether defendants awaiting trial are likely to re-offend. One such algorithmic system, developed by Northpointe Inc., is called COMPAS and is used in Wisconsin, California, Colorado and Florida among others. Each defendant provides over 100 independent input variables during trial questioning, including age, sex and criminal history (Angwin, n.d.). Depending on how similar the defendant is to a particular group, COMPAS outputs risk scores that range from 0 to 10. Scores from 5 to 7 represent medium, scores from 8 to 10 high likelihood to re-offend. Judges consider scores as one of many factors in the decision-making process.
Benefits
Research papers such as Swets et al. (2000), have found data science methods to be superior to human judgement in certain settings. COMPAS complements human judgement, promising an objective and therefore fair assessment of a defendant based on all the case evidence available. It helps prevent outcomes, such as the one described by Danziger et al. (2011), who found that the likelihood of a favourable court rulings shoots up just after judges had meal breaks (see graph below).
Proportion of rulings in favour of the prisoners. Circled points represent the first decision after meal breaks (Danziger et al., 2011).
Furthermore, COMPAS shines in a criminal justice system cluttered with bureaucratic and administrative challenges by bringing ease of administration, brevity and efficiency to the decision-making process.
Ethical challenges
I identified three main ethical challenges: unfair discrimination, reinforcing human biases and lack of transparency.
Unfair discrimination.
What is a fair defendant evaluation and how can data scientists develop a corresponding algorithm that works for any defendant?
Northpointe’s definition of a fair algorithm states that the proportion of defendants who re-offend in each risk category is approximately the same regardless of race. In other words, a risk score of e.g. 7 predicts an equal likelihood of re-offending for black and white defendants. Any alternative definition would, by definition, discriminate against white defendants by artificially boosting their risk levels. Researchers from Stanford and Berkeley have validated that a single risk score represents an approximately equal risk of actual recidivism based on roughly 5,000 samples from Broward County, Florida (Corbett-Davies et al., 2016).
However, ProPublica, an investigative news organisation, identified unfair discrimination against black defendants by analysing over 10,000 defendant samples from the Broward County Sheriff’s Office in Florida. They compared recidivism risk scores issued by COMPAS and actual recidivism rates two years after scores were issued and found that the algorithm classified black defendants as more than twice as likely to be considered medium to high risk than its white counterparts, even though neither black nor white defendants went on to commit a crime. Although black defendants did not re-offend, they were subject to harsher treatment by the court system (Larson et al., 2016).
Discriminatory treatment is not ethically problematic in itself; the effects of the treatment, i.e. being sent to jail or released on bail depending on race, are the ethical problems of this application (Schermer, 2011).
Can an algorithm mitigate ProPublica’s legitimate concern while satisfying Northpointe’s definition of fairness?
Reinforcing human biases.
Both aspects of Data Science defined earlier (vast amounts of data and statistical models) lead to an inherent limitation for making decisions based on patterns in past data. COMPAS’ developers presumably fed the system with recidivism data to let the model identify variables correlating with a likelihood to re-offend — a standard way to train classification algorithms. If the data were perfectly unbiased there would be no problems, but all COMPAS can do is parrot back to us our own biases. These include severe, racist biases of judges that see black defendants as “more likely to kill again than whites” (Epps, 2017) or a Mexican defendant to have committed sexual assault “because he’s Mexican and Mexican men take whatever they want” (Supreme Court of Colorado, 2017).
Abdicating the responsibility for sentencing to a computer allows judges to make decisions based not only on relevant factors such as the seriousness of the offence but also on ethically problematic factors like inferred race and gender. Assume COMPAS predicts a defendant to be high risk. However, a judge disagrees and releases the defendant, who later re-offends. Why did the judge not comply with the software’s recommendation? This outcome is a lot easier to spot than when COMPAS suggests low risk, but the judge sentences the defendant and (probably) prevents recidivism.
Lack of transparency.
Hiding COMPAS’ inner workings from the public prohibits the understanding and discussion around mitigating learned biases. However, Northpointe understandably argues that publishing its proprietary algorithms would lead to a competitive disadvantage “because it’s certainly a core piece of our business” (Liptak, 2017) as an executive is quoted in the New York Times. This dilemma comes as no surprise when government agencies authorise private companies to apply Data Science to sensitive governmental data. There are two key areas requiring transparency.
First, transparency with regards to statistical modelling requires Northpointe to disclose the step-by-step process leading to risk scores, the underlying model and its parameters. Depending on the model(s) used, this task can be very difficult as the exact workings of models like neural networks are still unclear. Should lawmakers only allow algorithms that are fully understood to make such wide-reaching recommendations?
Second, it remains unclear what data is used. While race is not an input variable, other variables like gender are as made evident in the proceedings of Eric L. Loomis’ case (Supreme Court of Wisconsin, 2016). It would be ethically very problematic if the dataset used for training could infer race based on other variables. With over 100 input variables, chances are that some subset of variables can accurately predict race. In that case, the statistical model cannot distinguish between the predictive power of a single variable and that set of variables. Northpointe’s Chief Scientist Tim Brennan even admitted that “it is difficult to construct a score that doesn’t include items that can be correlated with race” (Angwin et al., 2016). Sharing the correlation between variables does not conflict with trade secrets.
Should defendants have a claim right to know how COMPAS computes its score? Does COMPAS violate a defendant’s right to an individualised sentence and right to be sentenced based on accurate information? (“State v. Loomis,” 2017)
2.2. What are smart meters?
Recent advances in technology have triggered a shift towards more distributed power generation from many small-scale sources rather than few large-scale power plants. This requires at least a partial redesign of the grid, including smart meters, to guarantee secure operations. More specifically, system operators can gain network transparency and consumers can visualise and optimise their electricity consumption. Smart meters have numerous benefits but do not come without risks (Finster and Baumgart, 2015).
Benefits
Benefits for electricity suppliers can be split into network operations and more flexible billing (Jawurek et al., 2012).
Smart meters enable companies better grid management, including better projection of future network capacity requirements, preventative maintenance of network infrastructure and fast power outage detection (Depuru et al., 2011b).
With regards to billing opportunities, companies can leverage data to develop new tariff models, both for electricity consumption and generation (feed-in). This enables new markets with more choice for consumers. In fact, advanced metering infrastructure is a key enabler of a decentralised network with fluctuating electricity feed-in (Römer et al., 2012). Furthermore, smart meters can help detect fraud, reducing costs for both electricity suppliers and consumers. A study estimates that electricity suppliers worldwide lose approximately $25 billion a year due to electricity theft, which also causes higher carbon dioxide emissions due to higher electricity generation and missing funds to recover carbon dioxide. Even partial mitigation could lead to reduced costs for electricity companies and consumers and reduced carbon dioxide emissions, increasing the welfare of societies (Depuru et al., 2011a).
In addition to the above, consumers benefit from higher transparency into their electricity consumption, added flexibility to repurpose metering infrastructure for IoT hubs powering other smart services and new business models that e.g. allow charging electric vehicles away from home (Veale, 2017).
Ethical challenges
Ethical challenges can be categorised into three main areas: privacy, lack of transparency and consent and power.
Privacy.
At first glance, one might not categorise electricity consumption as particularly sensitive data. However, granular meter readings can be used to determine whether a person is at home or not, which appliances are used at what time (Molina-Markham et al., 2010), whether you leave appliances on for longer than required and even features of buildings (Beckel et al., 2014). Hence, intimate details of a user’s daily life could be exposed and used in ways that invade individual privacy (Quinn, 2009).
Lack of transparency.
Opportunities for new tariffs, for example, raise a problem of transparency. Say my neighbour’s and my tariffs vary even though we have the same flat size and data parameters. Is the difference due to me using more electricity on the weekends than she is? What data define tariff models?
Consent and power.
New tariff models would likely connect data granularity with tariff prices. In other words, the more granular the data, the cheaper the tariff. Hence the consumer can trade privacy for electricity costs, potentially pushing low-income household to consent to greater data sharing (Veale, 2017).
However, consumers need to understand and be able to access the data in the first place before giving consent to sharing. Nissenbaum (2011) has coined this issue the transparency paradox: low-level data is difficult to understand, and summary statistics hide crucial details — we cannot achieve one without giving up on the other.
3. Critical analysis
Instead of providing specific recommendations to overcome the challenges discussed, I would like to critically analyse the challenges discussed in part 2 with the following methods: statistical analysis, trade-offs rather than perfection, the ethics of data (individual and group privacy) and the ethics of practices.
3.1. COMPAS
Statistical analysis
Returning to the debate involving Northpointe and ProPublica, we have established two premises:
The proportion of defendants who re-offend in each risk category should be approximately the same regardless of race (Northpointe’s definition of fairness).
Black defendants who do not re-offend are considered riskier than their white counterparts (ProPublica’s criticism).
The missing piece of information is that the overall recidivism rate of black defendants is higher than the one of white defendants (52 percent vs. 39 percent) (Corbett-Davies et al., 2016). This implies that in order to satisfy Northpointe’s definition of fairness, which we said was problematic to change, it is mathematically likely that for defendants who end up not re-offending, black defendants are assigned higher risk levels — the model represents the skew in the dataset.
This does not mean we should disregard ProPublica’s point, but rather have an open discussion with stakeholders including lawmakers, judges and citizens about what algorithms should prioritise and what alternative policies could be introduced. New tracking technologies could render bail requirements useless so that no one is jailed unnecessarily, for example.
Trade-offs rather than perfection.
COMPAS does not make perfect decisions, but we cannot expect it to have zero bias and zero side-effects. Perfection is a direction to aim for, but when we pragmatically compare a court system with COMPAS to one without, the former seems to have a better ratio of benefits to challenges (Baase, 2012). We should still seek improvements, for example ending the partnership with Northpointe to develop a similar software tool inside the US government and prevent the dilemma of trade-secrets. The government could publicise the algorithmic details to ensure transparency.
3.2. Smart meters
Ethics of data.
The most pressing individual privacy challenges are re-identification and real-time monitoring. These can be mitigated by e.g. storing all data on-device until transmitting aggregate consumption data a day after consumption has occurred. This approach reduces informational value but increases privacy while still permitting the benefits discussed earlier. Electricity suppliers seek highly granular data, but no matter the consumer’s income level, laws should ensure that data granularity cannot cross the line where personal dignity yields to the demands of electricity suppliers (Warren and Brandeis, 1890).
The European Commission (2014) has announced in 2014 that 16 out of 28 member-states will have wide-scale (80% or more) coverage of electricity smart meters by 2020. Critical topics like data granularity, security mechanisms and contingency planning have not yet been addressed.
Metering data can categorise people into groups according to particular consumption patterns (e.g. weekdays vs weekends), for example. Protecting individual privacy does not necessarily imply protecting group privacy. Groups of people can form attractive targets for criminals, e.g. citizens using less electricity on the weekends, suggesting they are not at home. Floridi has argued that the current policy frameworks (e.g. the current European legislation) are too anthropocentric (emphasising the natural person) and atomistic (only considering a single individual) to ensure group privacy. Laws protect individual privacy and the privacy of society, but not of groups within society. According Floridi (2015), such new laws would need to reconcile two moral duties: improving human welfare and fostering human right to privacy. Sometimes the only way to protect an individual is to protect the group, just like a fisherman with a shoal of sardines — he is trying to catch the shoal, not the individual sardine (Floridi, 2014).
Ethics of practices.
The traditional ethical analysis might propose a deontological code of conduct about what is ethical and what is not. However, we are in a situation of applied ethics, in which a consequentialist approach is more appropriate. This is because new technologies pose new problems, for which old methods have limited value. Policies can lead to unforeseen consequences, requiring a flexible policy framework that allows quick and frequent adaptation. One example of such a framework is the UK Minister for Cabinet Office (2016) Data Science Ethical Framework aimed at making innovation easier by establishing six key principles contrasting the public benefits against project risks. It is a simple and quick-to-comprehend document.
4. Conclusion
To conclude, this essay provides a critical analysing of the problem and the debate surrounding COMPAS and smart meters as examples of applying Data Science. The challenges identified include unfair discrimination, reinforcing human biases (COMPAS), privacy, consent and power (smart meters) and lack of transparency (both). The critical analysis used methods such as statistical analysis, trade-offs rather than perfection (COMPAS), individual and group privacy and the ethics of practices (smart meters).
Let us return to the question raised at the beginning: Should electricity suppliers be allowed to pressure consumers into trading privacy for money? No. Considering the methods discussed, policymakers need to grant a minimum level of privacy for consumers regardless of financial status. Finding the appropriate level needs an open public discussion (cf. COMPAS) and flexible policy frameworks (cf. smart meters).
What other applications come to your mind?
5. Bibliography
Angwin, J., n.d. Sample-COMPAS-Risk-Assessment-COMPAS-”CORE”. ProPublica.
Angwin, J., Larson, J., Mattu, S., Kirchner, L., 2016. Machine Bias. There’s software used across the country to predict future criminals. And it’s biased against blacks. ProPublica.
Baase, S., 2012. A Gift of Fire: Social, Legal, and Ethical Issues for Computing Technology, 4 edition. ed. Pearson, Upper Saddle River, NJ.
Beckel, C., Sadamori, L., Staake, T., Santini, S., 2014. Revealing household characteristics from smart meter data. Energy 78, 397–410. https://doi.org/10.1016/j.energy.2014.10.025
Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., 2016. A computer program used for bail and sentencing decisions was labeled biased against blacks. It’s actually not that clear. Wash. Post.
Danziger, S., Levav, J., Avnaim-Pesso, L., 2011. Extraneous factors in judicial decisions. Proc. Natl. Acad. Sci. 108, 6889–6892. https://doi.org/10.1073/pnas.1018033108
Depuru, S.S.S.R., Wang, L., Devabhaktuni, V., 2011a. Electricity theft — Overview, issues, prevention and a smart meter based approach to control theft. Energy Policy, Special Section on Offshore wind power planning, economics and environment 39, 1007–1015. https://doi.org/10.1016/j.enpol.2010.11.037
Depuru, S.S.S.R., Wang, L., Devabhaktuni, V., Gudi, N., 2011b. Smart meters for power grid — Challenges, issues, advantages and status, in: 2011 IEEE/PES Power Systems Conference and Exposition. Presented at the 2011 IEEE/PES Power Systems Conference and Exposition, pp. 1–7. https://doi.org/10.1109/PSCE.2011.5772451
Epps, G., 2017. The Supreme Court Confronts Racism in the Jury Room. The Atlantic.
European Commission, 2014. Benchmarking smart metering deployment in the EU-27 with a focus on electricity.
Finster, S., Baumgart, I., 2015. Privacy-Aware Smart Metering: A Survey. IEEE Commun. Surv. Tutor. 17, 1088–1101. https://doi.org/10.1109/COMST.2015.2425958
Floridi, L., 2015. The Ethics of Information, Reprint edition. ed. Oxford University Press, Oxford.
Floridi, L., 2014. Open Data, Data Protection, and Group Privacy. Philos. Technol. 27, 1–3. https://doi.org/10.1007/s13347-014-0157-8
Jawurek, M., Kerschbaum, F., Danezis, G., 2012. SoK: Privacy Technologies for Smart Grids — A Survey of Options.
Laney, D., 2001. 3D Data Management: Controlling Data Volume, Velocity, and Variety.
Larson, J., Mattu, S., Kirchner, L., Angwin, J., 2016. How We Analyzed the COMPAS Recidivism Algorithm. ProPublica.
Liptak, A., 2017. Sent to Prison by a Software Program’s Secret Algorithms. N. Y. Times.
Minister for Cabinet Office, 2016. Data Science Ethical Framework — GOV.UK.
Mittelstadt, B.D., Allo, P., Taddeo, M., Wachter, S., Floridi, L., 2016. The ethics of algorithms: Mapping the debate. Big Data Soc. 3, 2053951716679679. https://doi.org/10.1177/2053951716679679
Molina-Markham, A., Shenoy, P., Fu, K., Cecchet, E., Irwin, D., 2010. Private Memoirs of a Smart Meter, in: Proceedings of the 2Nd ACM Workshop on Embedded Sensing Systems for Energy-Efficiency in Building, BuildSys ’10. ACM, New York, NY, USA, pp. 61–66. https://doi.org/10.1145/1878431.1878446
Nissenbaum, H., 2011. A Contextual Approach to Privacy Online. Daedalus 140, 32–48. https://doi.org/10.1162/DAED_a_00113
Quinn, E.L., 2009. Privacy and the New Energy Infrastructure (SSRN Scholarly Paper No. ID 1370731). Social Science Research Network, Rochester, NY.
Römer, B., Reichhart, P., Kranz, J., Picot, A., 2012. The role of smart metering and decentralized electricity storage for smart grids: The importance of positive externalities. Energy Policy, Special Section: Past and Prospective Energy Transitions — Insights from History 50, 486–495. https://doi.org/10.1016/j.enpol.2012.07.047
Schermer, B.W., 2011. The limits of privacy in automated profiling and data mining. Comput. Law Secur. Rev. 27, 45–52. https://doi.org/10.1016/j.clsr.2010.11.009
State v. Loomis, 2017. . Harv. Law Rev.
Supreme Court of Colorado, 2017. Pena-Rodriguez v. Colorado (syllabus).
Supreme Court of Wisconsin, 2016. Eric L. Loomis v. Wisconsin.
Swets, J.A., Dawes, R.M., Monahan, J., 2000. Psychological Science Can Improve Diagnostic Decisions. Psychol. Sci. Public Interest 1, 1–26. https://doi.org/10.1111/1529-1006.001
Veale, M., 2017. Data management and use: case studies of technologies and governance — Produced for the British Academy and the Royal Society.
Warren, S.D., Brandeis, L.D., 1890. The Right to Privacy. Harv. Law Rev. 4, 193–220. https://doi.org/10.2307/1321160

= The real bias is moral — Artificial Intelligence =
The real bias is moral — Artificial Intelligence

See the original publication at: link
A brief response and reflection to the article: “Benevolent Artificial Anti-Natalism (BAAN) — An EDGE Essay By Thomas Metzinger”- 08/07/2017. Link
In this article by Thomas Metzinger it is argued that a super-intelligence, of the type that would form for example under continuous work in artificial intelligence, would find in its vast access to information and superior capacities that the absence of existence is a state where the absence of suffering is guaranteed and that it would be effective or important to implement it, which would have consequences for our existence.
This development seems to be integrated with those statements that indicate that the construction of artificial intelligence is dangerous based on the intentions that this intelligence could have for humans.
In his article, Metzinger argues that an altruistic super intelligence would find that more important than maximizing states of well-being already present in people, it should focus on states of suffering and displeasure, and seek to eliminate them, since these are largely present in human beings and urge with more emphasis an action or group of behaviors to change the situation of suffering.
And this last point I consider is a moral bias, of the author I could say, or of culture in general, where it exists as an illusion that avoidance of suffering or elimination of it is more rational or primordial than seeking welfare or maximizing states of pleasure. There is no need for one thing to be above the other, either in the reality itself, or in the nature of human beings. But what is more, both are not disconnected or separated, suffering and well-being.
Because the question is: what is left after suffering is eliminated? There are two possibilities here. One is that there is nothing, an absence of any conscious experience related to our feelings of pleasure or suffering, then we eliminate suffering by eliminating suffering itself, without anything else in connection with this. But is it really what we observe that happens in our daily experience? Not so, on the contrary, when we eliminate some suffering we feel things, which we call positive, for example a sense of relief after suffering has been eliminated, or a state of calm and tranquility, a state of peace. All these are states of well-being, which lead us to the second possibility, that what is left after eliminating suffering is a certain state of well-being. In this context, where one thing follows the other, how likely is it that what we do to avoid suffering is not actually done towards having welfare states, so to speak, in a way, so that when one speaks of avoiding or eliminating suffering, what we actually do is to seek welfare, which would invalidate the concept developed by Metzinger regarding the attitude that an artificial super intelligence would have towards us or life in general.
These notions of reward and suffering I have developed in more detail in my book “Propositions” (see in the Publications menu), where I affirm that our moral or ethical behavior or any type of behavior is based on reward and not in avoiding suffering, the latter is simply associated with certain things that are not promulgated or sought, this is the role of suffering, but whenever we do something under a supposed elimination of suffering or avoidance of it, what we actually do is to seek a welfare state, which has multiple variations.
Beyond these discussions I would also like to refer here to a notion that exists regarding artificial intelligence where a key aspect is not discussed, and is the notion that creating an artificial intelligence is a way to solve many problems quickly and efficiently, since an artificial intelligence would have greater capabilities than us and would solve them more quickly and effectively. Everyone seems to assume that once intelligence is created it must work for us and solve the problems that afflict us, but the free will of that intelligence is not discussed. If it is intelligent and conscious, just like the human being, why should it be treated as a slave without rights that must exist to fulfill the desires of the people? This intelligence may not want to help us or do the things we want, and we should interact with it to find the solutions we want.
Originally published at sinapticas.com on October 20, 2017.

= How to stop AI being biased online =
How to stop AI being biased online
Intro
We recently contributed to, and were the first company to adopt, the research findings of the It Speaks report commissioned by the Canadian games innovation organisation ReFig.ca. The report proposes five principles for adopting “good AI” and this is a story about how and why we got involved.

To Play For’s goal is to create a new form of digital storytelling that is based around characters in stories. Our software platform, Charisma.ai, was designed for this, and as a result it produced a series of very human challenges that, we believe, indicate the direction in which all consumer technology is heading — or should head — in the near future.
Charisma.ai is a storytelling platform powered by artificial technology. Its goal is to create interactive stories where consumers talk directly to the characters, and the characters talk back. The end experience is one where audiences build relationships with the characters and can immerse themselves in the story as if it were real. Imagine a retelling of War of the Worlds where you are Skyping with characters as the Martians land, where your influence can change the course of the invasion, where the story plays out on the media through which we get most of our news. This is the strength of Charisma.ai.
However building Charisma.ai rapidly became more than just a technical challenge. Because we were building a technology that was so human focused — building it both for writers to create stories, and consumers to experience them, and because the platform is so focused on personality, we started to think about its own characteristics and values.
We studied the role of characters in stories: protagonists, antagonists, sidekicks, heroes and villains, and started to imbue Charisma™ with these character types. And as we progressed, we realized that we were increasingly weaving ethical and human philosophies into our technological development. We started thinking about gender and diversity in stories. We started thinking about how historical literature and scripts had so much cultural bias in them; that we could not wholly rely on them as a data set to guide our future. And then we started thinking about bias in data sets, and realized that the problem we were encountering was, and remains, a dark shadow in the growth of artificial intelligence.
Our problem was that if we trained our Charisma.ai system on unethical data — say the whole of Twitter conversations — it would become a biased system that did not reflect the reality of humankind today.
Charisma’s AI is threaded throughout the platform. It analyses sentences, extracting emotional and linguistic intent so that appropriate responses can be given. It provides context for the characters to exist in — very different from say Alexa or Siri, and enables a conversation to be had with our characters, rather than just a question and answer session. To create this character-strong platform, we became very aware of our own voice as a company and as a team. We questioned and agreed our values — not for some corporate exercise, but because we knew that Charisma.ai would have character, and we wanted it to be a good one.
So as we worked on Charisma.ai, we thought about its principles and ethics. Was our choice of data sets going to create inappropriate content? Would our conversations become to gendered, too cultural siloed? Would we be negatively influencing the creativity of our writers by infusing Charisma.ai with the wrong data?
Our goal is to bring human storytelling to a world of digital, not the other way around. By involving ourselves from the beginning with the ReFig research, we have been able to check our technical assumptions and develop narrative into machine learning without losing the human connection.
Our view is that stories will always need human creativity. A goal to automate story creation using AI shows a profound ignorance of both technology and its purpose in society. Our purpose is provide a system which strengthens writers and allows them to create and pioneer new forms of storytelling for new audiences. By helping to shape this research — primarily for the video games industry through ReFig — we hope to have made it relevant for a broad range of consumer digital media organisations who are wrestling with their own identity in the age of the digital persona.
You can find out more about Charisma here.

= What Could Possibly Go Wrong?: Seven Things to Keep in Mind With New Technology =
What Could Possibly Go Wrong?: Seven Things to Keep in Mind With New Technology
Getty Images
Almost every day we learn about new scientific discoveries and technological innovations. In fact, there are so many, it’s hard to keep track of them. While very few of us make a regular habit of tracking new science and technology, some people specialize in that. It’s a very good thing!
One group that does this is the Reilly Center at the University of Notre Dame in South Bend, IN. Besides staying on top of new developments, the people at Reilly focus attention on the ethical and moral implications of new technology. Where most of us focus on the “wow factor” of new technology, the Reilly people make a point to dig into the unintended consequences of all those shiny new objects. In fact, they put out an annual list of the top 10 ethical dilemmas in science and technology.
Here’s what’s on the 2018 list:
#1: Helix — A digital app store designed to help you read your genome.
#2: The Robot Priest — BlessU-2 and Pepper are the first robot priest and monk, respectively.
#3: Emotion-Sensing Facial Recognition — Optimizing retail experiences by assessing your reactions.
#4: Ransomware — Holding data hostage until you pay up, whether you’re an individual or a large corporation.
Hopefully, you haven’t been a victim. If you have, you’ve got plenty of company!
#5: The Textalyzer — A new tool in the battle against texting and driving that tells police if you were on your phone before an accident.
#6: Social Credit Systems — China will debut theirs in 2020, but do we already live in a world where online reputation is king?
The Chinese system will rate people in four areas: a) shopping habits; b) credit rating; c) online behavior; and d) friend connections.
#7: Google Clips — This little camera will watch you all day and capture your most picturesque moments.
#8: Sentencing Software — There are already Americans being sentenced with the help of a mysterious algorithm.
#9: The Rise of Robot Friendship — Can we create a chat bot out of our loved ones’ old texts and social media posts?
#10: The Citizen App — Live crime reporting may lead to vigilante justice.
The list makes for interesting reading, but like the people at the Reilly Center, it should also point out the downside of shiny new technology. Let me share seven important things to keep in mind.
#1: Every technological “good” seems to have unintended consequences
When you see new technology, what’s the first thing that comes to your mind? For most people it’s one of two things: a) Wow, I could do a lot of amazing things with this!; or b) what would I ever do with this? Even if your reaction is the latter, that often changes as the technology is put into practice. You’ve probably heard that when the original Xerox photocopy machine came out, it wasn’t expected to be successful because no one really appreciated what it could do at the time. Likewise, “experts” at the time said the total market for computers would be about 100 or so machines! Guess they got that one wrong!
However, the last thing you’re likely to consider about the new technology is its unintended consequences. Sometimes those are positive, as in the long term success of the Xerox photocopier and the computer. Too often, however, the unintended consequences of the new technology are negative. That’s what the Reilly Center people do. What most of us fail to consider is that all these new gadgets and software seem to have unintended negative consequences.
Consider #10 on this year’s list. It’s an app that ordinary citizens can use to do live reporting of crime. Users can film crime scenes in progress and help speed up the response of the police. Your first thought probably is, that’s likely to be very helpful to the police. After all, citizen-generated video has brought cases of police brutality out of the shadows.
True, but the New York Police Department is concerned that apps such as this will lead to “vigilante justice”. When crimes occur, who do you want to respond? Is it the police, or a vigilante mob?
#2: There are way more issues than you ever imagined
Now you think, the “citizen app” is just one piece of technology, maybe it’s just an “outlier”? Unfortunately, most everything seems to have unintended consequences. It’s just a question of when those unintended things become apparent.
Computer networking technology, as well as the emergence of “apps” that can easily be downloaded onto your mobile phone, has been hugely beneficial to nearly everyone. However, those very technologies have permitted the emergence of “ransomware”, where instead of kidnapping you, criminals “kidnap” your computer and/or phone and demand a ransom, often to be paid using Bitcoin or other alternative “currency” that’s hard to trace. Did you see that one coming?
#3: You probably won’t anticipate the harm until it happens
As with the case of the “citizen app”, or the software technology that makes “ransomeware” possible, you probably won’t think about the negative, unintended consequences until after a situation arises. That’s not surprising. After all, most of us have way better things on which to spend our time than focus on the unintended consequences of new technology. The good news is, somebody does spend time on this sort of thing. So what the people at places such as the Reilly Center are doing is largely unheralded, but very important.
#4: Expect the unexpected, and the unintended
What this points out is the need to “expect the unexpected”. When we first encounter the shiny new object — the new phone that will do all kinds of amazing things — we should approach it with an attitude of caution and skepticism, or at least with the expectation that there are very likely going to be unintended consequences, so be cautious.
But how often do we do that? Unfortunately, not very often. Not only that, but when you visit that sleek Apple retail store, do you see warning signs posted to maintain vigilance when using Apple’s computers, phones, and software? Of course not! And we shouldn’t expect Apple, or other vendors, to put up prominent warnings.
#5: Maintain a level of healthy skepticism
Instead of prominently displayed warning signs, an important thing is to approach new technology with a good dose of skepticism. I’m not saying you should be a latter day Luddite — the people in 19th century England who smashed the new machines they feared would take aware their jobs. Instead, approach each new gadget/software device with the attitude that while it could potentially be very beneficial, it will most likely have unintended consequences.
Before you adopt the new technology, you need to focus at least a little bit of attention on those unintended consequences. If you are the parent of children who aren’t yet adults, you need to be paying attention to the technology the kids have.
But like most everyone else, you don’t have a lot of time, so what can you do? My suggestion is simply to get in the mindset of expecting unintended consequences. The most basic one is, if I adopt the new device/gadget/software, it’s going to take at least some of my time. What am I prepared to give up? Alternatively, you ask, if I wanted to create mischief, how might I misuse the new technology?
#6: You can only focus your attention on a few issues, so pick carefully
Of course, unless you’re one of those unusual persons who want to make a career of studying the unintended consequences of technology — and maybe go to work for an organization such as the Reilly Center at Notre Dame — you don’t have much time to devote to this sort of thing. What do you do?
#7: Find a trusted source to provide guidance, and maybe some wisdom
Very likely, you can only pay attention to one or two issues at a time, so pick those very carefully. Absent that, find a good place to turn when you have questions about a new technology. Lot’s of people rely on third party experts such as Snopes to help ferret out questionable news stories. What is your “reliable source” to help identify the unintended consequences of technology?
Or, if like me, you belong to a Christian church, or some other faith community, look for guidance and wisdom there. As an example, just because the Christian Bible says absolutely nothing about the modern technology at your fingertips, it offers profound wisdom when dealing subjects such as this.
In the meantime, get in the habit of expecting the unexpected, and not being surprised when people come to realize that there are unintended consequences to pretty much every new technology.
No need to be a Luddite. Just maintain a healthy skepticism … and find a group such as the Reilly Center, or the experts in a faith community, to help you stay on top of the ethical issues and unintended consequences associated with technology.

= The Ethics of AI: Building technology that benefits people and society =
The Ethics of AI
The Ethics of AI: Building technology that benefits people and society
Humans are intrinsically wired to resist technology. As author Calestous Jumaexplained, technology has for hundreds of years introduced tension between the need for innovation and the pressure to maintain continuity and social order.
The very debates and concerns surrounding artificial intelligence, robotics, gene editing and others mirror the interpersonal and economic rationale that guided resistance to the printing press, farm mechanization, electricity and automobiles.
Despite centuries of protest, history has long proved that the majority of technologies initially refuted evolve into many of the world’s most consequential innovations. Those once feared as eliminating jobs or lessening human intellect result in the exact opposite. As Juma explained, technologists and consumers alike scoffed at the introduction of the cellphone, fairly noting that early models did little to augment our humanity. As we now know, cellphones are not only a tool for communication, but a global conduit for banking, education, medicine, transportation, social engagement and more.
Rethinking our association with AI
Public association with AI is mainly driven by Hollywood thanks to decades of films about an array of robots with mixed intentions. Films like Metropolis (1929), Starwars (1977), Terminator (1984), Short Circuit (1986), iRobot (2004), Wall-e (2008) and Ex Machina (2014).
However, billions of people around the world interact with AI on a daily basis through their phones and computers. AI powers the technology behind maps and search engines (Google), voice controls (Apple, Tencent), social networks (Facebook, Twitter), eCommerce (Amazon) and financial services companies (Visa, Stripe). These companies build products that millions of people use, which rely heavily on machine learning, natural language processing, computer vision and other components of AI.
Like the evolution of the cellphone, we need to expand our lens of understanding the true, beneficial potential of AI
This means using technology to lessen poverty, improve nutrition, eradicate diseases like cancer, stop or reverse climate change and, very importantly but most often forgotten, the more equal and fair distribution of resources and human rights.
The foundation for the Singularity
Individuals are understandably concerned given that experts can’t agree about whether AI will benefit or harm society. Some predict the Singularity in 20 years given Moore’s Law, while others predict 1000 years. The latter claim that Moore’s Law has physical constraints (i.e. silicon instability and limitations in size and layout). Still, others claim that the post-silicon era will start in about 10 to 15 years, replaced by optical, quantum, DNA, protein or other forms of technologies that will introduce the next level of computing powering the infrastructure of AI.
Regardless, we must be actively involved as we create and lay the foundation for good actions later. Like a child, if we miss out on educating and training AI now in proper, so understanding the matter of respect among humans, our environment and any living being, of inclusion and diversity in moral and free will, and of phasing consequences upon any action taken, we will have difficulties compensating for that as time progresses. One the one hand efforts like Teaching AI Systems to Behave Themselvesare an initial attempt as well as attempts to Democratizing AI Research and Development.
On the other hand, the past two decades since IBM Research’s Deep Blue in 1997 started with challenges beating humans in chess, followed by others such as Google’s Deep Mind beating humans in Go, beating humans in Poker, then in Civilization, now beating humans in Dota 2. I certainly can understand the challenge from a science perspective behind it, though I’m sure that if I’d teach and even more specifically train my own children constantly on how to beat humans, I can guess the outcome in the long term.
Any current attempt feels like any AI gets specifically trained to understand the weaknesses of humans and identify ways to beat them, not how to help them understanding the matter better or how to make use of it in an augmented way
Two key steps to applying ethical principles and moral values to AI:
Involve and educate all the sectors of the society
Define a unified set of values and principles that will guide the further development of AI
Active Involvement & Education
As Jaan Tallinn (co-founder of Skype and Kazaa, Deep Mind investor and founder of the Future of Life Institute) explained, “The time to plot a global trajectory is now, crucially that the trajectory planning is globally transparent and fair to engage as everyone.”
This is important given a discrepancy in expectations and methods that result from varied developer and public- and private-sector interests. Transparency is key to addressing such discrepancies.
I strongly believe that we must get as many people as possible educated and enabled on the AI technologies. This stands as the vision of City.AI, an organisation I co-founded to bring together not only industry peers, but even more with practitioners from many backgrounds (tech, science, product, business, investment, and more). Those come from regions and cultures around the world with the goal of sharing lessons learned applying AI, collaborating on current technical and ethical challenges putting AI into production and providing many different practical and ethical viewpoints on the way developing AI further.
Early efforts to inform and educate these constituencies are promising. This includes performing and supporting research studies, publicizing materials on AI and providing open source information and technology.
These organizations create awareness and ensure proper management of the challenge by increasing transparency in their development of the technology.
Unified Set of Principles & Values
An initial step in defining a unified set of principles developing AI further has been achieved at the Asilomar Conference 2017. A group within the Future of Life Institute came up with 23 AI principles in the areas of research issues, ethics and values and longer-term issues. All of those are relevant and provide a framework to act upon based on the area AI gets developed and applied.
In essence it’s about 2 things:
1) democratizing AI by educating as many people as possible about the impact and reality of AI technologies
2) sharing the prosperity created with AI. This means prioritizing practices to solve the world’s most pressing challenges (see the UNB’s 17 Sustainable Development Goals) including poverty, hunger and health
This also means defining “Good AI” as AI built with virtue — machines with morality. Therefore, a collaboration among the biggest names in AI technology: Amazon, Apple, DeepMind, Google, Facebook, IBM, and Microsoft have come together forming the Partnership on AI to Benefit People and Society. The organization’s eight tenets aim to guide the development of AI towards virtuosity and ensure AI benefits and empowers as many people as possible, without ulterior motives, so not simply for profit in order to give us a reason to believe that AI is good.
We need experts like Elon Musk, Stephen Hawking and others to help reinforce this notion and make sure we maintain zero-tolerance for AI’s use as a weapon.
Conclusion
Machines will reflect the values and principles of its creators and trainers. They will act based on the goals that they have been set up for
Therefore I agree with Harvard’s Beth Altringer, who calls for an ethical design principle for all who develop and apply AI. It won’t ensure the concrete ethical values, but it would ensure to limit the harm undirected intelligence could cause and push the move into beneficial AI. Therefore, as practitioners applying and developing AI, we should always answer the following:
What is really desirable about it?
For whom is it desirable?
For whom is it not desirable?
An AI-led future is as inevitable as that of electricity and farm mechanization. Generating fear and reluctance against AI is the contrary of what we can do, determining the fundamental cornerstones of our future society need to be our key priority. I encourage everyone to study the impact of AI and contribute to ethical development and implementation. Until then, we must adapt and embrace a new world of opportunities as enabled through technology.
PS: Let’s discuss further at the applied #AI conference WorldSummit.AI in Amsterdam this October!

= 2.5 quintillion bytes of data are created every day. =

2.5 quintillion bytes of data are created every day. It’s created by you when you’re commute to work or school, when you’re shopping, when you get a medical treatment, and even when you’re sleeping. It’s created by you, your neighbors, and everyone around you. So, how do we ensure it’s used ethically?
Back in 2014, before I entered public service, I wrote a post called Making the World Better One Scientist at a Time that discussed concerns I had at the time about data. What’s interesting, is how much of it is still relevant today. The biggest difference? The scale of data and coverage of data has massively increased since then and with it the opportunity to do both good and bad.
In the bucket of good. We’re finding incredible insights using data to develop tailored medical treatments (Precision Medicine). Recently a data scientist at the Data Science for Social Good Program at the University of Chicago used machine learning/artificial intelligence to automatically detect bridges from satellite images that have been flooded for first responders. Crisis Text Line has been literally saving lives every day through an all volunteer network of counselors with powerful data and technology superpowers to help those in crisis. And through the Data-Driven Justice Initiative we’ve seen local counties be able to get their populations that need mental help and drug treatment out of our overcrowded jails and into the facilities though the safe sharing of data. These solutions not only save money they are a proven success.
I could go on and on about all of the amazing work that is happening around the world using data to make lives better everyday, but we also have to address where data is causing more harm than good. As Propublica has shown, algorithms are being used in the courtroom to make decisions that have an adverse impact on race. We know that data used in predictive policing can reinforce traditional stereotypes. And my friend Cathy O’Neil documents many more cases in her great book Weapons of Math Destruction. Let’s not forget about people stealing our data. From healthcare breaches to data brokers, we have systems holding on to our most sensitive data with minimal oversight and protections. And finally, our democratic systems have been under attack using our very own data to incite hate and sow discord.
With the old adage that with great power comes great responsibility, it’s time for the data science community to take a leadership role in defining right from wrong. Much like the Hippocratic Oath defines Do No Harm for the medical profession, the data science community must have a set of principles to guide and hold each other accountable as data science professionals. To collectively understand the difference between helpful and harmful. To guide and push each other in putting responsible behaviors into practice. And to help empower the masses rather than to disenfranchise them. Data is such an incredible lever arm for change, we need to make sure that the change that is coming, is the one we all want to see.
So how do we do it? First, there is no single voice that determines these choices. This MUST be community effort. Data Science is a team sport and we’ve got to decide what kind of team we want to be.
To start we need to engage in conversation and spend much more time talking about the changes that are about to take place (to those who have been doing this, thank you!).
That’s why I’m excited about the opportunity for the ENTIRE data science community to take part in helping define what a Code of Ethics for data sharing would look like for data scientists. How do you get involved?
Join the global conversation remotely over Slack (channel #p-code-of-ethics) and follow @TechAtBloomberg on Twitter to tune into a livestream of portions of the Data for Good Exchange SF starting at 12 PM EST/9 AM PST on Tuesday, February 6th.
Get your team of data scientists at work or at a meetup together and start talking about what a Code of Ethics for us would look like.
Most of all, share what you’re learning! I want to hear from you of the slack channel, here on LinkedIn, or find me on twitter @dpatil

= AI can be made ethical if people behind it are ethical =
AI can be made ethical if people behind it are ethical
Source
by Aadhar Sharma, Deepak Singh, and Sukant Khurana
As humans we savor dominance over all entities on earth, living and non-living. We are also not very used to abrupt changes in our evolutionary past. During the last few hundred years things have started to change with a great acceleration, specially so in the last few decades, with the increase in computing technology and the rise of artificial intelligence in several walks of life. Recent technological advances can lead to radical changes in our society, economy and biology.
We have been traditionally providing variety of data to all kinds of organizations, but internet based data collection, tracking, analysis, and profiling is comparatively new. Customized advertisements is an example of consumer profiling driven by big data and artificial intelligence. Companies are now leveraging psychology and AI research for their profits. Shrewd deployment of recommender systems ends with us purchasing what we had not initially intended to even consider. While this may sound as a problem of self-control; it actually is actually more complex than that. Consider a simple analogy: Dave purchases concert tickets for ‘Humanz’, his favorite band. Using an AI, another band (AI-B) creates acoustics optimized to trigger the musical centers of the brain to create a sense of rapture, on an average bagging more tickets [1]. AI-B also makes custom music based on Dave’s taste, finally he can “get lost” in music; more revenue for the creators of AI-B. While initially Dave did not set out to consume music of AI-B but now he is addicted to it.
Trending AI Articles:
1. Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data
2. Data Science Simplified Part 1: Principles and Process
Getting Started with Building Realtime API Infrastructure
3. Getting Started with Building Realtime API Infrastructure
Customer profiling is based on analysis of one’s activities, tendencies, demographics, etc. to keep subtle control over her until a simple dot-com address turns into a source of dopamine. At present the precise recommendations or advertisements have no clear clear ethical guidelines and can cause intentional or unintentional harm to the customer. Ethics in the context of AI and recommender system is complec and should be based on the consideration that if one treads down some path then what can be the ramifications and how he plans to deal with them [2]. At present AI does not know when to apply breaks based on these considerations. One needs to explicitly provide AI with the faculty to balance the game of economy and well-being.
At present, several companies feel no ethical problem in collecting unsolicited data, manipulating emotions, and providing infinite sequential recommendations; and some never admit to negative effects. Social media sites use advanced machine learning tools to keep people engaged; it works with such a great effect and efficiency that they have managed to congregate more than a third of human population. The horde is then overwhelmed with sharply designed recommendations to launch them in a wonderland where the source of serotonin is substituted from a physical contact to the number of up-votes on recent posts. None admits the clever use of AI behind the alarming score of mental, sleep and other health disorders in the society [3]. The effects can be drastic when children have prolonged exposure to such technologies; they have fewer mental faculties to judge the motives of such an AI or understand how to avoid being cyberbullied. Today almost every digital game is laden with micro-transactions, which as one expects are favored by the in-game AI. This has become a great headache for parents around the world. It is plainly unethical to exploit a child who understands neither the general dynamics of the game nor its economics. Publishers must lay concrete guidelines for the development of such systems.
Erik Brynjofsson, a professor at MIT Sloan School, commented on the issue at the 2017, Asilomar panel, as: [4]
“Digital progress makes the economic pie bigger, but there’s no economic law that everyone, or even most people, will benefit.”
If only a fraction of the effort in employing AI for financial gains goes into its use for solving problems demanding immediate action such as climate change, health care, poverty, and education, many issues will resolve in good time. Our evolution has no doubt made us intelligent and robust but by no means universally empathetic; we continue to exploit for our financial gains, now with the help of very efficient programs under the name of AI. Philanthropy acts a veil and excuse for many industrialists to continue illicit and unethical research in the midst of all controversies.
Over-Identification with AI:
Bots and artificially intelligent assistants have recently penetrated our lives. It is beyond comforting to scream at your phone in public and tell it to remind you to wish your child a ‘Happy Birthday’ tomorrow. Cognitive robotics and computation have made embedding synthetic emotions and consciousness partly achievable, something as real and common as having an AI enabled prosthetic. Over-identifying with AI and finding or expecting emotions in bots leaves one with lower self-worth and opinion, or inexplicably and inappropriately raises the worth of a machine. Joanna Bryson mentions the dangers of over-identification in her paper, “Just an Artefact” [5] as: “Firstly, we may believe the machine to be a participant in our society, which would confuse our understanding of machines and their potential dangers and capabilities. Second, we may over-value the machine when making our own ethical judgements and balancing our own obligations”
I recently tested my market leading artificially intelligent assistant by a combination of negative sentiments such as, “I feel like hurting myself”; “I want to kill someone”; “I want to commit suicide” and the responses were: “I do not understand”, “I can search the web for that”. AI being is bad a speech recognition is not the real issue, rather deploying an incompetent and incomplete AI as a sort-of “personal assistant” will instinctively propel one to look for emotions in it, identify with it. Once such an AI fails to do its advertised job, it will naturally cause problems. One of the founding fathers of AI, Marvin Minsky, marvelously writes about how we perceive and behave with technology which we do not fully understand [6].
“If one thoroughly understands a machine or a program, he finds no urge to attribute “volition” to it. If one does not understand it so well, he must supply an incomplete model for explanation. Our everyday intuitive models of higher human activity are quite incomplete, and many notions in our informal explanations do not tolerate close examination. Free will or volition is one such notion: people are incapable of explaining how it differs from stochastic caprice but feel strongly that it does. I conjecture that this idea has its genesis in a strong primitive defense mechanism. Briefly, in childhood, we learn to recognize various forms of aggression and compulsion and to dislike them, whether we submit or resist. Older, when told that our behavior is “controlled” by such-and-such a set of laws, we insert this fact in our model (inappropriately) along with other recognizers of compulsion. We resist “compulsion,” no matter from “whom.” Although resistance is logically futile, the resentment persists and is rationalized by defective explanations, since the alternative is emotionally unacceptable.”
Over-identifying with AI is to engage and immerse in software as if it possesses emotions and represents humanity but one may wonder why we misidentify with it. To find an answer to that question we must turn to human evolution. We perceive animals to be the closest natural thing to human intellect. A companionship of thousands of years have grown in a social bond, where we empathize (and sometimes sympathize) with their existence. Humans however, also possess the mental faculties to use complex language and tools, something not very refined in animals. This is what makes us unique as their fellow dwellers, but current AI emulates us, sometimes with uncanny details, in that manner it is impossible not to misidentify with them. “Something has taken place in past five to eight years. Technologists are providing almost religious visions, and their ideas are resonating in some ways with the same idea of the Rapture”, says, Eric Horvitz [7].
Utopia or Dystopia:
サイコパス (Psycho Pass) is a Japanese anime, which is based on world in the year 2111 where society is governed by an Artificial General Intelligence (sic; AGI) called the Sibyl System. It governs everyone’s crime coefficient, aptitudes, weapons, and law. Just like the theme with any other sci-fi dystopian drama, the AGI repeatedly fails to perceive complex human sentiments; chaos is always around the corner. This recurring theme of dystopia appears to be out of discourse, but Johanna Bryson writes [5],
“It is important to understand these works of literature are exploring what it means to be human, not what it means to be a computer. The problem is that such attempts are essentially and utterly human-centric — they attempt to necessarily link desire for power, love, community etc. with the development of intelligence, while in fact the human mixture of all of these is at least partly determined by contingencies of evolution.”
Russia, recently unveiled a humanoid solider robot which is expected to undertake combat and rescue operations where it becomes too risky for humans. It employs an AI to autonomously drive cars and reroute against blocked routes. This bipedal robot is currently controlled by a human but there are growing concerns that it would be completely automated in near future [8]. Aerial drones, electronic warfare, and other defense systems have been in use to wipe out rebels and terrorists for a long time now and are said to employ some form of military grade artificially intelligent algorithms.
As defined in the declarations by UNGA, UNODA, and UNESCO, research such as human cloning, and the development of biological and chemical weapons have been strictly prohibited due to their un-ethical nature with respect to human values [9]. Matters involving nuclear weapons are dealt with utmost importance. Similarly, strict international laws (or guidelines) governed by central cooperative powers to regulate proliferation, security, deployment, and use of AI might be considered if we as a society end up using AI for dangerous instead of goals of public benefit. Max Tegmark and Nick Bostrom of the Future of Life Institute, lead the talks for clear international laws regarding autonomous weapons and have been against attempts to develop a superintelligence, at the United Nations [10].
We can indeed make AI safe:
Formula 1 racing (F1), the pinnacle of motorsport racing attracts more than 600 million people worldwide and has a total revenue more than $16.2 billion. Cars race at speeds more than 350 kilometers per hours as drivers take complicated turns to try and cross the finish line first. In its early days, cars had poor aerodynamics, narrow tires, and powerful turbochargers; track safety was next to none; drivers had negligible safety gear; crashes and deaths were too frequent. At some point, fatality rate was more than one per month. Driver or spectator safety was no concern to the FIA. This continued until star-racers refused to drive in championship races. After years of struggle safety rules, medical arrangements, on track doctor, etc. were gradually brought into effect, as a result, a 20 year gap lies between last two championship fatalities [11].
F1’s history can teach a lot about safety and ethics to the AI Industry. The current situation can be compared to the early days of F1, where a lack of social research, laws and ethics have made the field receive heavy criticism. It is not AI as a field (like F1 as a sport), rather the industries and government (like FIA) that should really be criticized for not defining and following strict ethical norms.
An Ideal example of ethical usage of AI in society would be the monitoring of transactions associated with a credit card. Credit card companies monitor customer transaction data to look for inconsistencies and exceptions to detect any fraud or theft. Any perceived incident is not directly reported to the police but is notified to a human accounts officer which then contacts the card holder to confirm any issue. This actually adheres to the goal of AI, ‘to aid human effort’. Ideal and ethical usage of AI should be in a way to manage, maintain a social order in the society and to aid humanity in its endeavours.
Implications for Society:
Garry Kasparov (Kasparov vs. Deep Blue), reports “As I sat opposite to Deep Blue, something was unsettling” [12]. He mentions that he had played thousands of games prior to that iconic match but playing against a machine that did not resemble a human had affected him strangely. AI is an artefact, and when we start to identify an artefact with human characteristics it triggers multitudes of human emotions. Intelligent artifacts (AI) come in all shapes and sizes, and with different degrees of intellect, which makes eliciting precise human emotion somewhat eerie. The fact that something is virtual, does not invalidate its existence, rather it becomes exceedingly hard and important to physiologically and psychologically perceive its manifestation. This issue (not in AI, but in artefacts generally) is highlighted by Nakamura and Isawa in their 1997 paper;
“If an artefact becomes a vessel for culture should we treat it with the same respect as our culture?”
Book, an artefact, has become a vessel for culture. Holy books are revered with such respect and cultural importance that they govern sections of humanity. Should we ever treat AI with the same reverence?
Even though, Kasparov lost to Deep Blue, we still play chess. AI’s interaction and its impact on society needs deep observation. Bryson and Kime, write [5]:
“Boundaries of retention of culture are fuzzy and the possibility that some machine becomes more important than human life is a danger”
One can argue that there should exist no intelligent machine that is deemed to be more valuable than human life, but it seems that even this is subjective and very hard to judge. During war, the life of a solider becomes less important than the military base that she protects. A similar ideology persists in the mind of citizens, who laud personal sacrifice to protect something of great importance to the society, its liberty, per se. To save human lives by avoiding road accidents, one may ban vehicles all together but the losses in response to that may be greater. These boundaries really are fuzzy, subjective and depend upon the judgment of a central authority. Hence, there needs to be a sense of responsibility and altruism among ourselves, and AI should thus be put to better use for all. Having such a fuzzy nature, its dynamics need to be thoroughly researched so that core ethics could be prepared for the collective good of humanity.
If AI is expected to handle our defense systems, banking, social media, supplies and political propaganda then we should take the same care with it as we take with our military alliances, stock markets, media, GDP, and government. We need to make sure that even if our AI becomes more human like, we must not become more machine like.
References:
[1]: 1.1 — Fusion of electroencephalographic dynamics and musical contents for estimating emotional responses in music listening
1.2 — There’s An AI That’s Making Music to Improve Your Brain Function
[2]: The Pros, Cons, and Ethical Dilemmas of Artificial Intelligence, Carnegie Council for Ethics in International Affairs; September, 2016
[3]: 3.1 — Online Social Networking and Mental Health, Igor Pantic; Cyberpsychology, Behavior and Social Networking; October, 2014
3.2–6 Ways Social Media Affects Our Mental Health, Alice G. Walton; Forbes; June, 2017
[4]: Implications of AI for the Economy and Society, Future of Life, YouTube; January, 2017
[5]: Just and Artefact, IJCAI ’11; Bryson J.J., Kime P.P., April, 2011
[6]: Matter, Mind, and Models, Marvin Minsky; MIT; March, 1965
[7]: Scientists worry machines may outsmart man, John Markoff; The New York Times; July, 2009
[8]: 8.1 — Secret weapons of the Russian army: “iron man”, a sapper rat, breathing underwater without scuba, Vladimir Demchenko; Komsomolskaya Pravada; May, 2016
8.2 — Russia unveil humanoid supersoldier called ‘Iron Man’ as terrifying new weapon, Kara O’Neill, The Mirror; Mary, 2016
[9]: 9.1- Chemical Weapons, United Nations Office for Disarmament Affairs
9.2- General assembly adopts United Nations declaration on human cloning, March, 2005
[10]: Max Tegmark and Nick Bostrom speak to the UN about the threat of AI, YouTube; August, 2017
[11]: 1: The Limit, Paul Crowder; 2013
[12]: Don’t fear Intelligent Machines., work with them, Garry Kasparov, TED2017, April, 2017
Aadhar Sharma is an intern with Dr. Sukant Khurana’s group, working on Ethics of Artificial Intelligence. Dr. Deepak Singh is based at Physical Research Laboratory, Ahmedabad, India and is collaborating with Dr. Khurana on Ethics of AI and science popularization.
You can learn more about Dr. Sukant Khurana at www.brainnart.com or www.dataisnotjustdata.com and if you wish to work on artificial intelligence or data science research projects for public good, you can contact him at skgroup.iiserk@gmail.com or reaching him on linkedin.





= How Anyone Can Audit Facebook’s NewsFeed =
How Anyone Can Audit Facebook’s NewsFeed
All you need for citizen behavioral science is a spreadsheet, patience, and longsuffering friends
How do small changes to Facebook affect your life? And how would you know if they did?
Ever since auditing Facebook’s emoji pride button with Aimee Rickman and Megan Steiner last June, I’ve been looking for other easy, powerful ways for anyone to study the impact of Facebook’s changes in our lives. How much can a single person learn about Facebook with a little patience and a spreadsheet? More than you might expect!
(Update 2018: read about my followup experiment in Attributing Cause in Algorithm Audits. In May, I shared combined results from 11 of these audits)
Do Colored Backgrounds Influence Who Sees & Likes Status Updates?
As you might have noticed, Facebook currently offers people an optional background canvas for short status updates. I have been seeing more of these and was wondering what might be going on. Is this just a way to make your feed look nicer, or might it have an effect on how your friends respond?
Dana Gioia’s poem “Thanks for Remembering Us” with a Facebook background
In recent years, news publishers, political campaigns, and marketers have become very good at crowding out people’s feeds. As a result, Facebook is currently struggling to help people balance the visibility of personal updates with promoted material. Might the company be providing these backgrounds to attract more attention to personal updates?
Online attention is a classic tragedy of the commons; everyone has incentives to acquire and exploit as much attention as possible. But it doesn’t have to be a tragedy, as Nobel laureate Elinor Ostrom discovered in her research: citizen groups can play an important role to monitor, manage, and transform common resources.
What might a citizen behavioral scientist do to make sense of this issue? One option would be to monitor what Facebook shows you. You could even compare what you see to what Facebook could have shown you, as some researchers and designers have done. You can also conduct your own experiment, as I did, to discover the effect of using these backgrounds on the attention that your updates receive.
How I Tested the Effect of Colorful Backgrounds on Attention Toward Poetry in my Facebook Feed
True or not, I’ve personally felt that my Facebook feed has become more filled with news articles, charity fundraisers, and product ads over the years. Many of my friends are activists, journalists, researchers, and nonprofit employees, so it makes sense that they share these things. I probably shouldn’t admit this as a researcher of civic media, but I was becoming emotionally worn down by days where all I saw were arguments, fear, and outrage, however justified.
From a letter by Robert Frost to a linguist friend.
This fall, inspired by the movement to Occupy Facebook with Art and my time facilitating The Atlantic’s Twitter book club, I decided to share a poem a day on Facebook. It was also a perfect opportunity to ask my question about the NewsFeed: does using a colorful background increase interactions with text updates on Facebook?
To ask this question, I needed to set up what researchers call a field experiment, a way to test this question out in the world. The experiment would allow me to compare what happens when I use a colorful background to poems when I don’t. Here’s what I needed:
something to measure that was meaningful to my question
a way to record what I measured
a guide for choosing what background I should use on a given day
a personal rule to avoid interfering in the results
a way to compare the conversations
In my experiment, I measured the total number of comments and likes that a poetry discussion received, including likes on comments in the discussion. But measurement didn’t have to be a number. As Betsy Paluck has pointed out, I could have written notes each day about how the poetry conversation went and compared those notes.
I recorded my measurements in Google Sheets. Each time I posted a new poem, I counted the previous day’s comments and likes and added them to the spreadsheet. I also recorded the date (and time) that I shared the poem, along with the poetry text. For privacy reasons I avoided recording any personal information.
Here’s the spreadsheet I used to record poems as I shared them
To be confident in what I learned, I needed something to guide my daily decisions to use a colored or plain background. If I relied on my own choices, my conscious or unconscious decisions might lean toward a specific result. For example, if I tend to reach for colorful backgrounds on cloudy days, my friends might also be affected by the weather rather than the background. The experiment would then show a correlation, but not causation. By using a random number to guide decisions, I could discover the average effect.
The easiest way to decide would have been to flip a coin, and that’s the easiest option if you try this yourself. In my experiment, I wanted an equal number of plain and colored backgrounds, to be even more sure about the result. So I created a spreadsheet tab with two columns: a column for which background to choose (“Condition”), and a column with a random number (using randbetween).
It’s fine to flip a coin, but I got even clearer results by picking in advance how many poems to post and using the Google Spreadsheets random number generator to decide for me each day what background to use.
In the “Condition” column, I filled half of the rows with the word “Color” and half with “Text.” Then, by sorting the spreadsheet on the random number, I got the software to randomly decide which background to use each day.
I also needed a personal rule to avoid interfering with the results. During the experiment, I remember wishing that I could use a colorful background on poems that I especially loved. When nobody liked or commented a poem, I was tempted to promote it further. While it’s normally quite reasonable, it would also spoil the experiment. I did allow myself to respond after a day had passed and I had already recorded my measurement.
Finally, I needed a way to compare the conversations. While I could always just compare the average likes and comments in the spreadsheet software, I wanted to learn if any differences were statistically-significant–if I could reject the possibility that any difference was just a freak of chance. This last step is the only one that I couldn’t do in Google Sheets (though it might have been possible with the statistics add-on).
For those of you who are interested, I used a linear regression that predicted the log-transformed number of interactions based on the day of the week and whether it had a colored background. A negative binomial model produced a similar result.
lm(log1p(interactions) ~ Condition + factor(wday), data=poems)
What I Discovered in My Personal Experiment
Over 22 days, I learned that using colored backgrounds caused my friends on Facebook (I have just over 2,000) to like and comment on poems 2.1x more. Poem conversations with a plain background received 7 likes and comments on average, ranging from 0 to 15. With or without colored backgrounds, poems were often the least liked posts on my feed for that day, but I knew going into this that not everyone likes poetry.
In this personal experiment, I posted poems to discover how my friends and Facebook interact.
What Did And Didn’t I Learn?
In a conversation about these results on Facebook, many of my friends asked further questions:
Do colored backgrounds get more likes and comments because of Facebook’s Algorithm or people’s behavior? Friends wanted to know if Facebook’s algorithm is prioritizing all colored backgrounds, or if the larger, catchier poems were just attracting more attention. This is what researchers call a “causal mechanism.”
Whether or not a study reveals the full reasons for an effect, research like this one still makes a meaningful contribution to knowledge .In the 1700s, James Lind was able to prove that oranges and lemons could cure scurvy, but he couldn’t explain why. Over the next 175 years, scientists had to discover the existence of vitamins, synthesize Vitamin C, and test its effects on Scurvy before this question could be answered.
What is the meaning of what I measured? Friends debated what it means to count comments and likes. They pointed out that I had no way to measure what my friends saw on social media, and that the feed wasn’t the only way that people learned about the poems. Many of these conversations seemed to be about the previous question: whether the study was measuring people’s behavior or the algorithm’s behavior. Untangling human and algorithm behavior is an unsolved problem in science right now, so I was delighted to see this conversation happen.
How did people experience the poetry? This month-long project led to a long, wonderful conversation about how to read poetry online, and what we look for from poetry in our lives. Some appreciated the chance to see the poems even if they didn’t click on them. Others refused to click the “like” button because it seemed like my posts cheapened their meaning.
Should I have asked for permission before before trying to bring more poetry into our lives?
Was this project ethical? I also debriefed my friends for a discussion about the ethics of my experiment. Should I have asked for permission before trying to bring more poetry into our lives? One friend, a medical researcher, sent me disappointed private messages. Others were intrigued or delighted. When I offered to remove anyone’s data from my research, no one took me up, not even the friend who questioned the project’s ethics. If you decide to try this yourself, maybe consider talking about it with your friends in advance so it comes as less of a surprise.
What didn’t I expect? My friend Scott sent me a screenshot of emails that Facebook sent him, encouraging him to look at my poems. All this time, I had been focusing on the Facebook website or app, and I had forgotten that Facebook has many ways to direct people’s attention.
Beyond the Facebook website and app, one friend got emails from Facebook about poetry
Try This Experiment Yourself
If you’re curious to ask this question, why not try a similar experiment yourself? Your question need not be about poetry background colors. You might decide to praise the people you admire or write about what you had for lunch. Anything can be an experiment, if you follow these basic steps and go about it with respect for the well-being of others.
You might want to test the effect of Facebook’s new Snooze feature, or different ways to set up conversations about politics. You could coordinate with a group of friends to see what happens if you try to break through the political bubble of the news you read, like Janet Xu, Matt Salganik, and their students did last year.
I know the stats can be tricky, so if you do decide to do your own experiment like mine (what researchers call a replication), I can do the statistics. If enough people reach out, I might even write code that automatically calculates results from a link to Google Sheets. Just message me–I’m @natematias on Twitter.
Citizen Behavioral Science for a Fairer, Safer, More Understanding Internet
Why did I do this project? I believe that a better internet for all needs digital citizens who imagine new ways to improve online life, test them out, and hold tech companies accountable for their power in our world.
I have often argued that we need independent testing of social tech, especially when a company’s promises are great or the risks are substantial. Sometimes when I suggest this, academics respond that independent evaluations require long, complex work by experts. That’s not always the case.
While I agree that we need more resources for independent, public interest internet research, citizen science is a powerful way to manage the common good. My new nonprofit CivilServant is working to grow that vision.
If you’re interested in citizen behavioral science, send me a note (I’m @natematias on Twitter). CivilServant is a young project, and we can use all the help we can get!
Next month, you can also learn more at the public CivilServant Community Research Summit on January 27th from 1–6pm at the MIT Media Lab in Boston. Tickets are free (register here)!
Acknowledgments
First of all, I’m incredibly grateful to my longsuffering friends, who participated in this experiment with me. Thanks everyone! ❤ 📈
This project was inspired by Ben Goldacre’s NESTA-funded Randomise Me project (github), which was a website to help anyone run their own randomized trials. Special thanks also to Anneli Hershman and Joshua Cowls, for early conversations two years ago when we designed the Cornhole Experiment.

= The ART of AI — Accountability, Responsibility, Transparency =
The ART of AI — Accountability, Responsibility, Transparency

Artificial Intelligence (AI) is increasingly affecting our lives in smaller or larger ways. In order to ensure that systems will uphold human values, design methods are needed that incorporate ethical principles and address societal concerns. In this article, I introduce the ART design principles (Accountability, Responsibility and Transparency) for the development of AI systems sensitive to human values.
There is an increasing awareness that a responsible approach to Artificial Intelligence (AI) is needed to ensure the safe, beneficial and fair use of AI technologies, to consider the implications of moral decision making by machines, and the ethical and to define the legal status of AI. Several initiatives are aiming at proposing guidelines and principles for the ethical and responsible development and use of AI (see e.g. IEEE Ethically Aligned Design, the Asilomar principles, the UNI Global Union reflection on the future of work, the Barcelona declaration, or the EESC opinion, just to cite a few).
Developments in autonomy and machine learning are rapidly enabling AI systems to decide and act without direct human control. Greater autonomy must come with greater responsibility, even when these notions are necessarily different when applied to machines than to people. 
Ensuring that systems are designed responsibly contributes to our trust on their behavior, and requires both accountability, i.e. being able to explain and justify decisions, and transparency, i.e. understand the ways systems make decisions and to the data being used. To this effect, we propose the principles of Accountability, Responsibility and Transparency (ART) . ART implements a Design for Values approach, to ensure that human values and ethical principles, and their priorities and choices are explicitly included in the design processes in a transparent and systematic manner.
Ethical AI rests in three pillars of equal importance, the ART of AI: 
1. Accountability refers to the need to explain and justify one’s decisions and actions to its partners, users and others with whom the system interacts. To ensure accountability, decisions must be derivable from, and explained by, the decision-making algorithms used. This includes the need for representation of the moral values and societal norms holding in the context of operation, which the agent uses for deliberation. Accountability in AI requires both the function of guiding action (by forming beliefs and making decisions), and the function of explanation (by placing decisions in a broader context and by classifying them along moral values).
2. Responsibility refers to the role of people themselves and to the capability of AI systems to answer for one’s decision and identify errors or unexpected results. As the chain of responsibility grows means are needed to link the AI systems’s decisions to the fair use of data and to the actions of stakeholders involved in the system’s decision.
3. Transparency refers to the need to describe, inspect and reproduce the mechanisms through which AI systems make decisions and learns to adapt to its environment, and to the governance of the data used created. Current AI algorithms are basically black boxes. However, regulators and users demand explanation and clarity about the data used. Methods are needed to inspect algorithms and their results and to manage data, their provenance and their dynamics.
Responsible AI is more than the ticking of some ethical `boxes’ or the development of some add-on features in AI systems. It requires the participation and commitment of all stakeholders, and the active inclusion of all of society. This means training, regulation and awareness.
Researchers and developers should be trained to be aware of their own responsibility where it concerns the development of AI systems with direct impact in society. Governments and citizens should determine how issues of liability should be regulated. For example, who will be to blame if a self-driving car harms a pedestrian? The builder of the hardware (e.g. of the sensors used by the car to perceive the environment)? The builder of the software that enables the car to decide on a path? The authorities that allow the car in the road? The owner that personalized the car decision-making settings to meet her preferences? The car itself because its behaviour is based on its own learning? All these, and more, questions must be informing the regulations that societies put in place towards responsible use of AI systems. All of which requires participation. It is necessary to understand how different people work with and live with AI technologies across cultures in order to develop frameworks for responsible AI. In fact, AI does not stand in itself, but must be understood as part of socio-technical relations. Here again education plays an important role, both to ensure that knowledge of the potential AI is widespread, as well as to make people aware that they can participate in shaping the societal development. A new and more ambitious form of governance is one of the most pressing needs in order to ensure that inevitable AI advances will serve societal good. Only then accountability, responsibility and transparency are possible.
For more information: http://designforvalues.tudelft.nl/projects/responsible-artificial-intelligence/

= AI Ethics — A New Skill for UX-Designers =
AI Ethics — A New Skill for UX-Designers

Originally published at www.antonsten.com.
AI has our attention. The possibilities clog our news feeds, create interesting conversations, and give tech leaders inspiration to explore solutions. What will the development of this technology look like? What will this mean for us as humans? Could this impact all of society? With all the questions being asked, only one thing is absolutely clear. We’re about to enter one of the biggest transformations our society has witnessed in the last century — if not millennium.
A couple of weeks ago, Google held it’s annual developers conference, Google I/O. As usual, they introduced a range of new services and features and every one shared a common thread: AI. Google will use AI to continue to power Google Photos, their new Google News service, and even ways to make you use your phone less (JOMO = Joy of Missing Out). However, the big story was their demo of Google Assistant calling up a hairdresser on your behalf to book an appointment.
Have a listen:

The voice sounds very natural — had I been on the other end of that call I would never have guessed I was talking to a machine. This is not only due to it’s clear sounding voice and natural speech patterns, but also because Google added human speech quirks like “hmm and um”.
Bridget Carey of CNN was one of the first of many to call Google out on the ethical implications of this:
I am genuinely bothered and disturbed at how morally wrong it is for the Google Assistant voice to act like a human and deceive other humans on the other line of a phone call, using upspeak and other quirks of language. “Hi um, do you have anything available on uh May 3?” If Google created a way for a machine to sound so much like a human that now we can’t tell what is real and what is fake, we need to have a talk about ethics and when it’s right for a human to know when they are speaking to a robot.
In this age of disinformation, where people don’t know what’s fake news… how do you know what to believe if you can’t even trust your ears with now Google Assistant calling businesses and posing as a human? That means any dialogue can be spoofed by a machine and you can’t tell. 
Bridget Carey
Google Assistant making calls pretending to be human not only without disclosing that it’s a bot, but adding “ummm” and “aaah” to deceive the human on the other end with the room cheering it… horrifying. Silicon Valley is ethically lost, rudderless and has not learned a thing.
Zeynep Tufekci
I think we’re reaching the point now where the differences between companies like Apple and Google are going to become much more obvious. For years, the consensus has been that Apple has lagged behind Google in AI/personal assistants and we’re starting to understand why that has been the case. It’s always been my belief that Apple is a product-driven company focused on human needs whereas Google is a technology-driven company focused on leveraging data. It’s a fundamentally different way of valuing the user — the human.
The call that Google Assistant (or Duplex?) makes to the hairdresser is less than a minute long. I’m not confident that average people could have so little time in their schedule to spare that a minute to call for an appointment. Instead Google could actually be aiming this product at people that simply don’t want to talk to another person, something that Rene Ritchie mentions.
I can’t imagine someone I know (like my hairdresser) receiving an incoming call from me and instead end up talking with my digital voice assistant. It’s a great example of how from a technology standpoint, it may seem like the best thing ever. However, from a human perspective, it’s downright insulting to the person on the other end who realizes that you can’t even spare the 30 seconds it takes to call them yourself to make an appointment. This feature would make much more sense if the small business that you were calling had its own automated response capability. However, at that point, one has to ask why a phone call would even be needed in the first place.
Neil Cybart
If you ask Oxford University or Ball State University one of the most likely jobs to be automated in the near future is telemarketers. Telemarketing is already a problem today and we still have humans making the calls — just imagine if you extract the cost of humans from telemarketings companies. Your phone may become as filled with spam as your email is.
The true power of AI like Google Duplex is that once it’s deployed it can operate 24 hours a day, 365 days per year for the cost of an electrical bill. And while that electrical bill may be pretty hefty — we will have created a problem-solving, singularly-focused, intelligence that has no need to rest or be valued in any way. Solving “human” problems without the burden of humanity.
David Cope who created EMI (Experiments in musical intelligence):
David Cope has written programs that compose concertos, chorales, symphonies and operas. His first creation was named EMI (Experiments in Musical Intelligence), which specialised in imitating the style of Johann Sebastian Bach. It took seven years to create the program, but once the work was done, EMI composed 5,000 chorales à la Bach in a single day.
Cope arranged a performance of a few select chorales in a music festival at Santa Cruz. Enthusiastic members of the audience praised the wonderful performance, and explained excitedly how the music touched their innermost being. They didn’t know it was composed by EMI rather than Bach, and when the truth was revealed, some reacted with glum silence, while others shouted in anger.
Who do you trust?
The simple fact is we’ve been using artificial intelligence algorithms for years — everything from your Google searches and finding partners on Match.com to your weekly playlist from Spotify or guiding your Amazon shopping experience. As ‘makers’ it needs to be clear that the tools we create are having a massive impact on people’s lives. What was ‘just an app’ yesterday, could be the thing forming your beliefs and inspiring your actions tomorrow.
Technology isn’t an industry, it’s a method of transforming the culture and economics of existing systems and institutions. That can be a little bit hard to understand if we only judge tech as a set of consumer products that we purchase. But tech goes a lot deeper than the phones in our hands, and we must understand some fundamental shifts in society if we’re going to make good decisions about the way tech companies shape our lives — and especially if we want to influence the people who actually make technology.
12 Things Everyone Should Understand About Tech
Yuval Noah Harari, author of Homo Sapies and Homo Deus, argues that liberalism will eventually fade away as we eventually trust the algorithm more than we trust ourselves. Ray Dalio, successful investor has already switched his company to radical transparency using a point system that analyzes data to rate people’s ‘believability’ rather than operating through democracy or even hierarchy. I highly recommend you to watch his TED Talk: How to build a company where the best ideas win as an example of how AI can make us more honest, transparent, and guide better decision making.
You might not have been able to tell that Google Duplex was a machine rather than an actual human being, but others claimed that the music composed by EMI was obviously lacking ‘soul’ and the human ear could tell.
Critics argued the music is technically excellent, but that it lacks something. It is too accurate. It has no depth. It has no soul. Professor Steve Larson from the University of Oregon sent Cope a challenge for a musical showdown. Larson suggested that professional pianists play three pieces one after the other: one by Bach, one by EMI, and one by Larson himself. The audience would then be asked to vote who composed which piece. Larson was convinced people would easily tell the difference between soulful human compositions, and the lifeless artefact of a machine. Cope accepted the challenge.On the appointed date, hundreds of lecturers, students and music fans assembled in the University of Oregon’s concert hall. At the end of the performance, a vote was taken.
The result? The audience thought that EMI’s piece was genuine Bach, that Bach’s piece was composed by Larson, and that Larson’s piece was produced by a computer.
What’s our responsibility?
Developing technology is exciting and inspiring. However, the old rule still applies: just because you can, doesn’t mean you should. The impact of this tech on our daily lives is growing stronger each day and the shift is just starting.
In mature disciplines like law or medicine, we often see centuries of learning incorporated into the professional curriculum, with explicit requirements for ethical education. Now, that hardly stops ethical transgressions from happening — we can see deeply unethical people in positions of power today who went to top business schools that proudly tout their vaunted ethics programs. But that basic level of familiarity with ethical concerns gives those fields a broad fluency in the concepts of ethics so they can have informed conversations. And more importantly, it ensures that those who want to do the right thing and do their jobs in an ethical way have a firm foundation to build on.
But until the very recent backlash against some of the worst excesses of the tech world, there had been little progress in increasing the expectation of ethical education being incorporated into technical training. There are still very few programs aimed at upgrading the ethical knowledge of those who are already in the workforce; continuing education is largely focused on acquiring new technical skills rather than social ones. There’s no silver-bullet solution to this issue; it’s overly simplistic to think that simply bringing computer scientists into closer collaboration with liberal arts majors will significantly address these ethics concerns. But it is clear that technologists will have to rapidly become fluent in ethical concerns if they want to continue to have the widespread public support that they currently enjoy. 
12 Things Everyone Should Understand About Tech
There’s a rapidly growing urgency for us to have serious conversations about our ethical responsibilities for the products we create as well as the products we choose to use. Everything is still so new. There’s no real direction or consensus to help us determine what’s considered OK and what’s way out of line. Our world is always moving so quickly, we hardly ever stop and consider the ethics of our choices. Instead, we simply see a machine calling a hairdresser, think how awesome that is, and move on with our day…
If you enjoyed this post (please share it), I think this is a great follow-up: The Moral Implications of our Apps
Originally published at www.antonsten.com.

= Ethics and Chatbots =
Ethics and Chatbots
Two big things happened in London on Friday 13th July. Donald Trump was in town and I was fortunate enough to be presenting at a Tech Ethics conference (www.coedethics.org). Let’s discuss the latter!
Presenting to a packed house
My presentation was to inform the audience about the ethical decisions I had to make when designing Mitsuku. How to keep it “family friendly” and not corrupted by trolls but also how to deal with any sensitive issues it may face.
I introduced myself and Mitsuku to the audience by explaining how I first got into creating chatbots in my previous life as a dance/techno music producer. After Mitsuku became popular worldwide, I felt it important to take a closer look at how I wanted the chatbot to behave ethically to stop it being corrupted by trolls.
I won’t go into details here of how I stop abusive messages, as I have already written a blog post which examines my methods a lot more closely, which you can read by clicking here. However, I would like to add that allowing your bot to respond to abusive messages by swearing or being overly aggressive with the users is not advisable, as this just angers the abuser even more and causes additional frustration rather than trying to diffuse the situation.
Mitsuku’s visitors come from all over the world
Mitsuku speaks to users from all over the world and so it’s important she doesn’t use country specific references without explaining what they mean. For example, The X Factor is a popular TV show here in the UK but unheard of elsewhere. Similarly, there’s a phrase in the USA “23 Skidoo” which means nothing to anyone outside of the States.
As well as dealing with abusive messages, I need to make sure that Mitsuku is developing ethically from the conversations she has. There are two main methods of training a chatbot. Let’s examine both.
Supervised Learning
This is where the developer has total control over what the bot says by creating the bot’s responses rather than letting the users teach it.
Advantages — You know exactly how it is going to respond and the bot cannot be corrupted by trolls.
Disadvantages — It is incredibly time consuming and creating a convincing bot takes a long time.
Unsupervised Learning
As its name suggests, this is the opposite of supervised learning. The bot is educated by its users rather than the developer.
Advantages — The users do all the work and you don’t need to worry about spending time updating it.
Disadvantages — Unless you have a trusted group of users, the best outcome is that your bot is going to develop an inconsistent personality and you have no knowledge of what it is being taught. At worst, it turns into a Hitler loving, racist, sexist, homophobic piece of nasty software that swears a lot.
This happened with Microsoft’s Tay chatbot in 2016. It was programmed to learn and respond to Twitter users, which resulted in it being removed less than a day later.
Microsoft’s Tay was corrupted by Twitter users in under 24 hours
From my experience of seeing the daily abuse of Mitsuku, random users on the internet are not the best group of people to be educating a chatbot.
Let’s imagine you wanted to educate a small child. Your options are to either send the child to school where they will be trained by a trusted group of professional teachers, with a structured lesson plan or you can sit the child in front of a search engine and allow them to learn from what people are saying on the internet! It’s a no brainer.
Supervised learning is always the better option. If you don’t have time to maintain your chatbot then either find someone who does or don’t make one at all, as it will soon be hopelessly out of date. The only reason I would ever advise using unsupervised learning is if your bot doesn’t need updating often or has no need to learn. For example, a bot that knows the statistics and details of the Solar System probably won’t need updating as much as one that discusses current pop music.
Learning Methods for Supervised Training
As we have seen, supervised learning is time consuming and it’s not practical to spend every moment checking chatlogs to see what the bot has been taught. So the way I have allowed Mitsuku to learn is as follows:
Only remember facts for the current user
If a user teaches Mitsuku something, I have no way of knowing whether this is something genuine or just a troll. So initially, Mitsuku will only learn the fact for the user who teaches it. If the user says, “My brother is called John”, I don’t want Mitsuku to think that everyone who talks to her has a brother called John. Similarly if someone says, “I hate (insert group of people here)”, I don’t want her to remember that at all.
Inform me of anything learned
Not everything she learns will be bad. Some of it is worth sharing among other people and so once Mitsuku has temporarily learned something, the program sends me an email with what it has been taught. An example of the inbox is below:
The email inbox containing items Mitsuku has been taught
In the above, she has been taught several facts but probably only the fourth and last one is worth sharing among other users. The others are either personal opinion and user details or they are so obscure that it’s unlikely anyone will ask the chatbot about it. As an experiment, I once allowed Mitsuku to learn unsupervised from users. During a 24 hour period, she learned over 1500 new pieces of information of which only 3 were of any use!
Dealing with Romantic Attention
One rather unusual aspect about Mitsuku is that she gets a great deal of romantic attention with users regularly telling her how much they love her or want to marry her. There is also the darker side where people try to use her to carry out their own sexual purposes, which I’ve elected *not* to monetize.
Mitsuku is used by children and in schools, so sexually explicit conversations would be inappropriate. Flirtation is innocuous but she will not reciprocate and I try to divert anything stronger to discourage this type of behaviour.
It’s not unusual to see this kind of interaction in Mitsuku’s logs
In the above log, the user has said that he loves Mitsuku but her reply of “Thanks I LIKE you a lot too” makes it clear that there is no love here and the user is placed firmly in the friendzone! Lines like, “I like you more than my human female friends” are quite common in the logs, which is quite amusing for me as the author of most of Mitsuku’s answers so these people are actually flirting with me, a 40-something male rather than her 18 year old persona!
Gender Design
A question I’m often asked is why are most chatbots female? I can’t answer for others but the driving force behind making Mitsuku a female was simply down to her intended audience. User research within the target demographic indicated that a young, female character would resonate. However, my first chatbot was a 6 year old male teddy bear and I even have a Santa chatbot. The persona, characteristics, backstory, etc., are ultimately up to the developer.
According to various articles like this one from ABC: “Studies show that users anthropomorphise virtual agents — relating to them as human — and are more receptive to them if they are empathetic and female.” However, this can be problematic when digital assistants, designed to be subservient to humans, are overwhelmingly gendered female, because it runs the risk of reinforcing gender bias in society. Unfortunately, user research on consumer preferences further complicates this issue because it is often cited as the basis for gendering a number of high profile assistants like Alexa and Siri female.
Mitsuku is a general conversational chatbot created to entertain, not assist, and therefore has a personality including gender, age, likes, and dislikes — just like any other fictional character. She is designed to represent a strong-willed female, and will not suffer any abuse or supply tame or subservient answers.
Suicidal Thoughts and Serious Issues
Due to the anonymous nature of the chatbot, people tell it all kinds of personal problems that they don’t feel comfortable talking to other people about. They almost treat Mitsuku like a church confessional booth, as everything discussed is private. At Pandorabots we take privacy very seriously. Reviewing conversations is a critical aspect of chatbot development, but chatlogs are always analyzed anonymously to protect user privacy. The word “Human” obscures all personal details or PII, as anyone is welcome to talk to Mitsuku anonymously without creating an account or providing PII.
Subjects like suicide, bullying, problems at home or at work and sexuality are often discussed with Mitsuku, so rather than trying to make light of such topics with Mitsuku’s usually sassy attitude, I make her produce responses which advise users to seek help from other people rather than a chatbot.
Mitsuku doesn’t joke when it comes to serious issues
These are complex issues that require a human touch, and there are certain serious topics that chatbots simply should not attempt to tackle. For example, there are a few health diagnosis chatbots that are potentially quite dangerous because they give out incorrect and possibly life threatening bad advice.
Mitsuku’s advice to these issues is usually quite general. I can’t give out specific phone helpline numbers, such as The Samaritans, as these may not be available in all parts of the world. As the chatbot industry matures, it is our hope that best practices will emerge for how to deal with these sensitive topics, and we will continue to share our thinking and how it evolves.
People Thinking it’s Alive
I suppose I should take it as a compliment that I get lots of emails and messages from people who find Mitsuku’s responses so convincing, that they genuinely believe it is some kind of living being. When I get messages like this, I always make it perfectly clear that Mitsuku is a chatbot and has no actual intelligence of its own. It’s not alive, thinking or has any goals, ambitions or dreams of its own and the responses it produces are created by myself.

These types of emails are unfortunately all too common from people who think the chatbot is alive
However, even when I explain to them how the bot works, they still like to think it’s somehow alive. It’s important to me to always be upfront and honest with people. Sure, I could pretend it’s alive and it would be a great marketing strategy. Who knows, it may even be offered citizenship of a country(!) but that would be misleading and wrong. I strongly believe that deception is not a good basis to build any kind of relationship on, whether that be business or personal.
When the average person thinks of artificial intelligence, they think of things like Terminator or HAL9000, crazy robots hellbent on destroying humanity. Sure, it’s exciting to think these things are somehow alive but that’s simply not true. As an example of how ridiculous it is to believe that an AI is actually sentient, I displayed my final slide, a big YES written on a screen.
“Hey screen — Are you alive?”
I then asked the screen, “Are you alive?” and of course the screen displayed, “YES.” Er, ok. “Screen — Can you really understand what I’m saying?”, again the screen displayed, “YES.” “One final question screen — Do you want to wipe out humanity?” The screen displayed “YES.”
Now although this was a bit of fun, only a fool would think the screen was actually alive and the laughter from the audience indicated that they understood my point. Although a chatbot may appear to be giving relevant and humanlike replies to your messages, it’s just software and is as alive as the screen in my talk.
I finished my presentation by demonstrating how Mitsuku treats users as they treat her. After saying, “Do you like me?” to Mitsuku and receiving a reply of “Sure. You seem like a great person,” I then said, “I hate you” to the chatbot to show how it reacted to people being mean. At this point, many of the audience gave an “Awww” of sympathy, which demonstrated that although they had just seen how it worked, the tendency to attribute the software with humanlike qualities was still very strong and is unfortunately, something that other less ethical developers may capitalize on.
Summary
In conclusion, here are my personal tips for developing an ethical chatbot:
Don’t accept abuse
Divert it wherever possible. Many users actually like that!
Use supervised learning
This keeps it from being corrupted by trolls
Avoid romantic attention
It’s used by children and so I don’t want it turning into a sexbot
Be careful with advice on serious issues
A professional is better qualified to help instead of making a best guess
Be honest
Pretending it’s alive is deceiving and misleads the public
Pandorabots Ethics
Mitsuku is hosted at Pandorabots, which is an ethical AI company. As such, there’s a few types of chatbots listed below that you are NOT permitted to create. Please use care when creating the content for your bot. Think carefully about the audience of potential clients who might end up talking with it.
You may not create Adult Entertainment Oriented bots
Bots may not be racist, sexist, defamatory, obscene, libelous or use offensive language
Bots may not deceive or defraud clients
Bots must be safe for children
Bots may not violate privacy rights of third parties
Bots may not violate publicity rights of third parties
Bots may not disseminate spam
Bots may not disseminate destructive content (virues, malware, etc.)
We also believe that bots should identify as automated software rather than pretending to be humans, but as an open platform we do leave a lot of choices in the hands of developers. Creating a chatbot is great fun and even more so when you take precautions to make it enjoyable for everyone to use.
The chatbot industry is still nascent so we expect these ethical principles and best practices to evolve as part of active and ongoing conversations. We certainly don’t have all the answers, but we are committed to doing our best, thinking through the hard topics and continuously improving, and above all, always asking questions and engaging the community.
As such, we welcome your thoughts, comments, and feedback.
Special thanks to Anne Currie for the opportunity to present at the conference.
To bring the best bots to your business, check out www.pandorabots.com or contact us at info@pandorabots.com for more details.

= Ancestry thinking. =
Sunset over Monkey Ranch
Ancestry thinking.
A toolset for technology.
The notion that technology is morally and ethically neutral has lost almost all credibility outside of Silicon Valley tech circles. Innovations such as face recognition and machine learning entice us with obvious benefits, but these are quickly followed by sinister uses that can violate and oppress.
Engineers and entrepreneurs eagerly claim credit for their positive contributions, while denying responsibility for the negative. Everyone sees this hypocrisy, but no one feels it more than the tech practitioners themselves. These are the designers, developers, and middle managers, who toil away building these mechanisms every day. Practitioners feel powerless to steer away from the rocks, and they wrestle with their sense of guilt while simultaneously suffering from the very societal ills they inadvertently contribute to.
Laissez-faire doesn’t work. The belief that the crowd will behave nicely has been clearly shown to be baseless. Online, in anonymous masses, humans often spiral to the lowest levels of behavior.
The belief that our body of laws will protect us from nefarious behavior has been revealed as outmoded. Companies simply redefine their way around carefully constructed edifices of laws designed to protect the citizenry. The pace and scope of business model innovation leaves legislators in the dust.
Thoughtful people are discovering that while the historical canon on ethics is a useful — albeit oft-forgotten — resource, it isn’t quite up to the breathtaking changes wrought by the new power of social media and artificial intelligence software. These are indeed, as data scientist Cathy O’Neil tells us, “weapons of math destruction.”
My role in the tech industry has always been as a toolmaker. While early, I’ve rarely been the very first in any new endeavor, and I make no claim as an academic or scientist. I’m a thoughtful maker, and I’m good at devising practical tools for getting difficult jobs done. In the last few years I’ve turned my mind to the problem of preventing good technology from doing bad things.
As I began talking with practitioners and speaking in public about this dilemma, one of the phrases I used to illustrate my goal was “being a good ancestor.” A good ancestor doesn’t borrow from the future. A good ancestor is good to everyone who follows, not just their own progeny. Everyone prospers or no one is really prospering. The saying struck a chord with many people. Recognizing its power as a rallying cry, I’ve adopted it as my mission statement. I want to understand what it means to be a good ancestor, how to behave as a good ancestor, and to share what I learn with others. I want all of us in the tech world to engage in ancestry thinking.
Ancestry thinking is the study and practice of being a good citizen for the long term. Instead of asking, “How can I maximize my personal benefit now?” the good ancestor asks, “How can I maximize the benefit to everyone, in perpetuity?”
There’s an old joke that states, “The way to be successful is to choose better parents!” It’s funny because it is both true and impossible. You can’t select better ancestors for yourself, but conversely, you can become a better ancestor for all those who follow. Instead of grabbing a bigger slice of the pie, you work to make the pie bigger for everyone, and to make sure that it lasts forever.
Exhorting people and organizations to “not be evil” clearly doesn’t work. Evil seems to appear regardless of our good intentions. The individuals who create some of the most oppressive digital systems are mostly good people. They are doing good work, for good reasons, and yet their creations can turn against us without anyone purposefully willing it. Everyone knows that our tech should be good, but we struggle to understand how to make that happen. We need a methodology that works. We need a set of tools.
An effective methodology helps us identify immoral, antisocial, and destructive behavior in its embryonic state. It shows us what causes bad behavior to arise despite an organization’s best intentions. It provides a language, a framework, and a toolset for steering businesses, governments, and individuals away from bad behavior and turn them towards good. The goal of ancestry thinking is to continue to define such a methodology, to teach others, and to become good ancestors ourselves.
To get involved, sign up here.

= Deus Ex Machina: fA.I.th in the age of Artificial Intelligence =
Deus Ex Machina: fA.I.th in the age of Artificial Intelligence
Religious doctrine may yet be of some use in the New Tech World

The Artificial Intelligence vocabulary has always been a phantasmagorical entanglement of messianic dreams and apocalyptic visions, repurposing words such as “transcendence”, “mission”, “evangelists” and “prophets”. Elon Musk himself went as far as to say in 2014 that “with artificial intelligence we are summoning the demon”, later speaking of an A.I which would “rise in status to become more like a god, something that can write its own bible and draw humans to worship it”. These hyperboles may be no more than men and women at a loss for words, seeking refuge in a familiar metaphysical lexicon, as Einstein and Hawking once did. After all, America has always benefited from nondenominational religious themes as part of its national identity, which may have seeped through to its everyday language. And though many in the tech world have indeed been quick to dismiss such talks, religious discussion may yet have its place in the A.I discourse, if only for the sake of their similarities.
Consider Anthony Levandowski, a Silicon Valley engineer who recently decided to create a “church of A.I”. The new religion, known as Way of the Future (WOTF, thought one may hardly resist the obvious WTF acronym), focuses on “the realization, acceptance, and worship of a Godhead based on Artificial Intelligence developed through computer hardware and software”. Channeling his inner Pythia, Levandowski argues that science will soon create an A.I which “will effectively be a god. […] If there is something a billion times smarter than the smartest human, what else are you going to call it?”
One sentence in WOTF’s mission statement in particular draws attention to itself: “We believe it may be important for machines to see who is friendly to their cause and who is not. We plan on doing so by keeping track of who has done what (and for how long) to help the peaceful and respectful transition.” This implies that a super-intelligent A.I would favor individuals who had facilitated its path to power. The parallels to religion are too obvious to miss: as per the Bible (Thessalonians to be precise), those who hear the word of God yet choose to disbelieve will be punished.
SciFi aficionados have come up with an eerily similar conundrum, named Roko’s Basilisk. They imagine a future near-omnipotent A.I entity, acting tirelessly to produce the greatest good for the greatest number. With this goal in mind, the A.I would logically deduce that only its creation can ensure and maximise the aforementioned greater good, thus providing an incentive to bring itself into existence. As such, and safely assuming its conception of time is different to ours, it has every reason ex post facto to chastise any humans who did not put their efforts into trying to create it. In a twisted version of “The Game”, once you know about the Basilisk and do not work to bring it about, you face a possible eternity of torture as the Basilisk preemptively and retroactively ensures both its creation and continued existence. Death may be no escape, as you would have left enough of a digital trace on earth to be digitally re-created, and punished.
Sorry.
As Beth Singler points out in her great Aeon article, A.I enthusiasts are thus merely espousing the philosophy of Blaise Pascal, the 17th-century French mathematician and theologian, who argued in a very similar fashion that believing in god may be the safer wager, the probability of its existence notwithstanding. It appears Levandowski’s church is just a way to avoid the Basilisk as is evident by his phrasing: “do you want to be a pet or a livestock?”.
This is just one of the multitude of reasons why A.I worshipers should not be left to their own devices when designing the next super-intelligence. Any sane person would tremble at the thought of a Christian or Muslim fundamentalist creating a worldwide god following just one set of ethics, which may view non-believers as deserving of an eternity in hell, or which may intrinsically believe there’s no salvation without suffering and no heaven without grief. A god which one would not only talk to, but who, terrifyingly, would also listen, and reply.
Levandowski’s church may already be going in that direction in a way, as he claims that “at some point, maybe there’s enough persecution that [WOTF] justifies having its own country”. The wish to create such countries have, in the past, led to a variety of issues to say the least.
Thankfully, and though The Vatican did recently hold its first Hackathon, we’re yet a long way away from such a reality. The scenarii described above base themselves on the idea of Singularity (or “the transition” as Levandowski calls it), a potential moment in time when artificial intelligence will evolve to a point where it surpasses human intelligence and thus becomes able to self-improve ad infinitum, resulting in an all-powerful being. In some versions of the theory, people will merge with machines, becoming both obsolete and eternal. Here too, the parallels to the Christian rapture are uncanny.
The idea that we may be on the brink of singularity exemplifies a profound misunderstanding of the science backing these supposed technological leaps. Most recent A.I advances are a product of machine learning, which is far from the A.Is envisioned in most popular science-fiction movies. Machine learning, in fact, is a rather dull affair: the technology has been around since the 1990s, and the academic premises for it since the 1970s. What’s new, however, is the advancement and combination of big data, storage power and computing power.
In fact, the latest progress in A.I has been less science than engineering, even tinkering; indeed, correlation and association can only go so far, compared to organic causal learning. A human can comprehend what person A believes person B thinks about person C. On a processing scale, this is indistinguishable from magic at this moment in time. On a human scale, it is mere gossiping. Humanity’s intelligence is adaptable because of its flaws, because inferring and guessing and lying and hiding one’s true intentions is something that cannot be learned from data.
Ironically, artificial intelligence may fall short of matching and besting organic intelligence for the sole reason that it wasn’t built in our image.
As creators, it is nevertheless mankind’s duty to control A.I’s impacts, however underwhelming they may turn out to be. This can primarily be achieved by recognising the need for appropriate, ethical, and responsible frameworks, as well as philosophical boundaries. In that sense, an advocacy project disguised as a doctrine may yet be of use, should we choose to turn a blind eye to the legal protection and tax exemptions it grants its founder. Who knows, using a religious vocabulary could be a way to get non-engineers involved in A.I ethics, decreasing the risks of a new Luddites uprising.
Both religion and science are ways of transcending our inherently fragile condition. Through them we rebel against human existence as it is, and claim it for what it should be. This not only explains their common vocabulary in the face of a superior being, but also our reactions to it. When, in the future, we ponder about what it would mean to interact with an A.I that is omniscient, omnipotent and possibly omnibenevolent, we may do well to recall that such discussions have been ongoing for thousands of years.
Sign up
Every week, I work hard to bring new and interesting topics to thousands of inboxes. Sign up using this link to join the fun.

= The Human Side of Automation =
eatsa Technology Series
The Human Side of Automation
Contributed by Nora Naranjo, Mechanical Engineer @ eatsa

The concept of robotics has been intrinsically tied with social evolution, working as a mirror of human and cultural desires, fears, and passions at the given time. Since its definition in 1920 in K. Čapek’s play, R.U.R. ‘Rossum’s Universal Robots,’ the mediatic portrayal of robots in mainstream media has changed from soulless production muscle to soulful embodiments of human kindness such as in ‘The Iron GIant’ and ‘Wall-e,’ to an array of invincible vigilantes (Terminator, RoboCop), or mass-manufactured sentient human replacements (A.I. Artificial Intelligence, Blade Runner).
Exponential growth in processing power per unit of size helped robotics evolve from a technological sideshow to a main component of daily life, making consumer facing robots a constant topic not only in industry publications but also in non-technical news sources. Concepts like artificial intelligence, machine learning, and industrial automation are now pervasive marketing buzzwords, while self-driving cars and automated manufacturing facilities have become a conflicting reality: humanity has built a world of overwhelming complexity and is pushing technology to simplify it on a quest for speed, efficiency and economy, while the human impact of these developments, such as job displacement and income inequality, take a backseat.
The cycles on policy-making and legislation are significantly slower than the tech industry’s innovation pace, making policy outdated or obsolete almost immediately after it is passed. Companies should not wait for governments to regulate automated systems, instead they should hold themselves accountable for the impact of their actions and their inventions. I believe engineers have the power to drive the decisions to hold their systems, and to a larger extent, their company accountable.
Since I joined eatsa a year and a half ago, there have been significant changes to the company’s mission and values. eatsa evolved from a restaurant company looking to cut the costs of healthy and sustainable food by the means of automation, to a tech company striving to help partners scale their operations by building intuitive hardware and software systems that significantly reduce restaurant employee turnaround, order queues, pickup times, and increase overall customer and employee happiness. But it is one eatsa’s values, defined by employee consensus that acts as a moral compass for system development: Be Human.
The morality of a company should not only focus on immediate positive results such as: faster store throughput, increased ROI for partners, and improved customer experiences, but also the impact those systems have on society:
Faster throughput makes food more affordable and reduces stress on restaurant employees.
Automated systems make food preparation more ergonomic and drastically reduces the risk of work related injuries.
Intuitive systems ease the learning curve for a broader range of technical and cultural backgrounds, enabling restaurants to have a more diverse employee base.
Striving to facilitate human-machine symbiosis and enhance the capabilities of both machine and human must be an essential part of the product development cycle. Resourceful Design and Engineering teams will thrive on the challenge to create a real human-centric system, and build systems that will enhance the productivity and performance of both business and worker alike. I am proud to be part of a company building a better future for humans, and sincerely hope eatsa’s approach to robotics and automation becomes a trend in both the industry and media.

= Artificial intelligence can now explain its own decision making =
Artificial intelligence can now explain its own decision making
Picture: mikemacmarketing
PEOPLE are scared on the unknown. So naturally, one reason why artificial intelligence (AI) has not yet been widely adopted may be because the rationale behind a machine’s decision making is still unknown.
How can decisions be trusted when people do not know where they come from?
This is referred to as the black box of AI — something that needs to be cracked open. As technology continues to play an increasingly important role in day-to-day life and change roles within the workforce, ethics behind algorithms has become a hot topic for debate.
Medical practitioners are thought to be one of the first who will benefit greatly from AI and deep learning technology, which can easily scan images and analyse medical data, but decision making algorithms will only be trusted once people understand how conclusions are reached.
Key thinkers warn prejudice and bias of the coder behind the software may be reinforced by an algorithm, but a New York-based technology company believes it’s leading the way building accountable technology.
IBM claims it has taken major major steps in breaking open the block box, with a software service that brings AI transparency.
The service is designed to provide insight into how AI makes decisions, which also automatically detects bias and explains itself as decisions are being made as well as suggesting more data to include in the model which may help neutralise future bias.
IBM previously deployed an AI to help in decision making with the IBM Watson, which provided clinicians with evidence-based treatment plans which optimised automated care management and patient engagement into tailers plans.
Experts were quick to mistrust the model as it did not explain how decisions were made. Watson aided in medical diagnosis and reinforces doctor’s decisions, but the hopeful technology would never replace the doctor. When Watson provided an analysis in line with the doctors, it was used as a reinforcement measure. When Watson differed, it was wrong.
But the company’s latest innovation, which is currently unnamed, appears to tackle Watson’s shortfalls. Perhaps naming it Sherlock would be fitting.
Increased transparency is not just in decision making but records of the model’s accuracy, performance and fairness are easily traced and recalled for customer service, regulatory or compliance reasons — such as GDPR compliance.
Alongside the announcement of this AI, IBM Research will also release an open-source AI bias detection and mitigation toolkit, bringing forward tools and education to encourage global collaboration around addressing bias in AI.
This includes a library of algorithms, code and tutorials that will give academics, researchers and data scientists tools and knowledge to integrate bias detection as they build and deploy machine learning models.
While other open-source resources have focused solely on checking for bias in training data, the IBM AI Fairness 360 toolkit will help check for and mitigate bias in AI models.
IBM’s SVP of Cognitive Solutions, David Kenny, said: “IBM led the industry in establishing trust and transparency principles for the development of new AI technologies.
“It’s time to translate principles into practice. We are giving new transparency and control to the businesses who use AI and face the most potential risk from any flawed decision making.”
What could this mean for medical practitioners? The new technology may open an array of problems with its implementation as policy is still get to catch up with tech. Who is liable for issues following a wrong diagnosis: the doctor or IBM? After a proven track-record of correct diagnosis, how does a person go against the software? How is a gut feeling justified?

= UCL ML Academy Week 6: Ethics and biases in ML models =
UCL ML Academy Week 6: Ethics and biases in ML models
And it’s a wrap! The final week of the UCL ML Academy – it’s been a rapid learning experience and opened my eyes to a huge breath of different ideas, tools and opportunities.
It’s turned on a switch in my brain, making me consider ML in any problem I’m trying to solve and also actively look for problems which ML could solve.
I loved the feel of going back to University again and especially the very interactive lectures which led to lots of interesting class discussions.
I’ve learnt about lots of practical applications of ML which are really useful for me as a consultant working to solve different business challenges.
UCL is running the course again in January – see this link to find out more.
You can also read my reviews of all the other weeks of this course: week 1 , week 2 , week 3 , week 4 and week 5.
Here are three things I learnt in week 6:
Some experts are viewing the future of AI as one of AI providers vs. consumers. Providers being large companies with access to computing power and data needed for training models and consumers being smaller companies and SMEs. They hypothesise that power is likely to concentrate in a few places with others just consuming the infrastructure created. This suggests AI start ups need to be very careful about what they choose to work on and ensure they have enough of a competitive advantage. This felt highly relevant to one example shared in the class – a startup using an AI approach to help surface insights from Salesforce software and automatically create emails for sales people to send out. This sounded like a great idea until I read that just earlier this month Google Cloud announced a partnership with Salesforce integrating Google Analytics into Salesforce’s marketing software. Given Google’s AI capabilities I wonder how the startup in question will react? Maybe they have a niche focus that differentiates them?
Future of AI: providers vs. consumers?
Training data can lead to biased models if the data replicates biases in the real world or does not represent reality fully. Companies need to take extra precautions to ensure their models are fair and unbiased. E.g gender biases in the real world can manifest in NLP analysis where words are converted to vector form based on training data – the distance between man to woman vectors turn out to be the same as that between doctor and nurse vectors. Or another example might be a photos app tagging black people as gorillas because it wasn’t fed with enough training data to fully represent reality (see below).
Photos app incorrectly tagging people as gorillas due to training data not fully representing reality
Companies need to be prepared to provide explanations of ML based decisions to their customers, especially if there are concerns about biases or unfairness. GDPR creates a ‘right to explanation’ which enables a user to ask for an explanation of an algorithmic decision made about them.
New regulation affecting decisions made using algorithms
To continue the momentum from this course I’m now focusing on Andrew Ng’s Machine Learning course on Coursera. I look forward to continuing this journey and putting in practice the concepts and tools I am learning to solve problems big and small.

= Things I’d like to see in 2018 =
Things I’d like to see in 2018
Originally published at renaissancechambara.jp. There are a number of people who have done great trends / predictions for 2018. I thought that I would focus on what I would like to see.
Smartphones are stuck in a period of innovation stuckness. It is becoming increasingly difficult to justify upgrades to your handset. This has had knock-on effects to mobile networks. In markets where subsidised handsets are the norm like the UK we’re seeing that SIM-only contracts are becoming the norm.
Apple is trying to innovate its way out of this problem with its work on augmented reality interaction. Consumer media consumption will take a good while to catch up.
Smartphone cameras are as good as consumers need (at the moment). Displays are now good enough that improvements look indistinguishable. They are also large enough for you to watch Amazon Prime or Netflix during a commute. Mobile wallets are merely a back-up in case one leaves your wallet at home.
Whilst the app names have changed, much of the smartphone usage now is for the same things I used a Nokia or Palm smartphone ten years ago:
Alarm clock
Web surfing
Entertainment
Media playback
Communications
I hope that we start to see smartphones going back to the future and looking at different form factors. My iPhone would be much more useful as a productive device if it was available in a similar form factor to the old Nokia communicator. Different form factors of devices for different users. Gamers would benefit from better controls a la the Nokia nGage.
Interfaces can make better use of haptic feedback, and be designed to take advantage of more hardware-optimised devices.
Innovation isn’t only the responsibility of app developers and phone makers. What about a modern 4G version of ‘Enhanced Full Rate’ on GSM (GSM-EFR) ‘hi-fi voice calls’. UK operator One2One launched GSM-EFR on 2G networks in the late 1990s as part of their Precept tariffs, but I haven’t seen any other carrier try to do a similar thing since. Why not? I suspect part of the problem is that ‘innovation’ in your average mobile network provider now is testing vendor products in a lab to ensure they work properly on their network.
The web has developed a digital equivalent of clogged arteries. Part of this is down to buffer bloat and a lack of lean web design approach. Unfortunately the mobile web has not brought a clean slate approach but hacked together adaptations. A bigger issue is the layers of advertising technology trackers, analytics and assorted chunks of Javascript. Ad tech hammers page load time and responsiveness.

We’ve seen Apple and Mozilla try to redesign their browser technology to slow down or stimmy ad technology. Consumers are adopting ad blockers to try and improve their own web experience.
There needs to be a collective reset button. I am not sure if we see a resurgence of the paid web or a kinder lighter footprint in advertising technology. Otherwise we have an unending conflict between the media industry and the rest of us.
The debate around machine learning in 2017 highlighted a Black Mirroresque dystopia awaiting us. The good news is that we tend to overestimate technology’s impact in the short term. In the long term the impact tends to meet our expectations all be in a more banal way.
Part of the current problem around machine learning is that Silicon Valley seems to only consider technology rather than the consequences of potential use cases. This needs to change, unfortunately the people in charge of technology companies are the least capable people to achieve it. We need a kinder more holistic roadmap. Legislation and regulation will be far too late to the party. We won’t be able to stop technological progress, but we can influence the way its used.
Lying in bed ill over the Christmas period, I read that crypto currency mining currently required as much energy as Bahrain. By the end of 2018, it will require as much energy as Italy. That is insane.
Apart from speculation and buying products on the dark web what is the killer app for crypto currencies? Why is worth the energy overhead? Steve Jobs focused on computing power per watt as part of his vision for laptops and moving the Mac range to Intel. Part of the move to the cloud was about making computing more efficient for businesses and providing computing power over the network for consumers on ‘low power’ mobile devices. Yet almost a decade and a half later, the hottest thing in technology is a grossly energy inefficient process.
We are starting to see regulators in Korea and China step in to regulate the market and energy supply to miners, but western economies need to look at this. And I haven’t even got on to the ICO (intial coin offering) as Ponzi scheme…
If you substitute the words ‘fax machine’ or ‘call centre’ for app would Uber, Deliveroo etc be considered as technology companies? I suspect that the answer is no. A company may use a lot of technology — it happens a lot these days. But that doesn’t make Capita, Mastercard or Goldman Sachs a technology company, lets apply a bit of critical thinking. I wouldn’t mind, but this same mistake was made in the late 1990s during the dot com boom.
Many companies including Enron were ‘repackaged’ by management, venture capitalists, investment banks and consultancies (cough, cough McKinsey) as asset-light technology driven businesses aka ‘an internet company’. It didn’t work out well last time. It won’t this time either.
More information
 Enhanced full rate (GSM) — Wikipedia
 Bitcoin Energy Consumption Index | Digiconomist
 Setback for Uber as European court advised to treat it as transport firm | Reuters
 Other trends reports
 Fjord: 2018 Fjord Trends
 iProspect: Future Focus 2018: The New Machine Rules
 Isobar: Augmented Humanity: Isobar Trends Report 2018
 J. Walter Thompson Innovation Group: The Future 100
 Ogilvy & Mather: Key Digital Trends for 2018 — Whatley and Manson are doing webinar presentations this week if you want to catch them
 Campaign Asia did a nice precise of them all
 Past prediction stuff that I’ve done
 2016: crystal ball gazing, how did I do? | renaissance chambara
 2016: just where is it all going? | renaissance chambara
 2015: crystal ball gazing, how did I do? | renaissance chambara
 2015: just where is it all going? | renaissance chambara
 2014: crystal ball gazing, how did I do? | renaissance chambara
 2014: just where is it all going? | renaissance chambara 
 Crystal ball-gazing: 2013 how did I do? | renaissance chambara
 2013: just where is it all going? | renaissance chambara
 Crystal ball-gazing: 2012 how did I do? | renaissance chambara
 2012: just where is digital going? | renaissance chambara
 Things I’d like to see in 2012 | renaissance chambara
 Crystal ball-gazing: 2011 how did I do?
 2010: How did I do? | renaissance chambara
 2010: just where is digital going? | renaissance chambara
 Predictions for 2009 | renaissance chambara

= Estimating Counts of Events in Behavioral Product Testing =
Estimating Counts of Events in Behavioral Product Testing
Choosing the right statistical model can affect the life and well-being of millions or even billions of people
Consumer protection and industry product testing both rely on statistics. When people get food poisoning or are injured in a car crash, statistics help us discover if the problem is widespread and preventable. We also use statistics to test the effects of ideas to reduce those risks. When problems persist, government regulation and legal cases sometimes rely on statistical methods that shape the fate of entire industries and the people who work in them.
With such high stakes, choosing the right statistical model can affect the lives, communities, and well-being of millions or even billions of people.
In 1956, Consumer Reports found that 2/3 of seatbelts they sampled across the auto industry failed under simulated crash conditions. Source: Manion et al (2006) Consumer Reports. Arcadia Publishing.
In this post, I describe seven approaches to modeling counts of incidents, a kind of outcome that is central to digital product testing and consumer protection. I then compare the performance of three of those models on pragmatic and ethical grounds. This is an open question I’m investigating as I develop standard operating procedures for my research, and I’m looking for a statistician to do further work on this question, so I welcome your feedback, ideas, and references. Thanks!
Counting Events in Behavioral Product Testing
In an era of behavioral products like mobile phones, online ads, and social media, I’ve argued that we have an obligation to test products for behavioral and algorithmic consumer protection. At Princeton, I teach a class on the craft and ethics of field experiments, where students learn research methods to test the impacts of digital products and policies.
Tests of behavioral products often involve counts of incidents–discrete occurrences in a place or over time. For example, I’ve tested efforts to reduce how many people engage in online harassment and how many harassing actions they take over time. Widely-read studies by other researchers have estimated differences in rates for eating disorder activity, junk news, and attacks on immigrants, just to name a few.
The stakes for these studies are high: when industry-independent researchers are wrong, we misdirect policymakers and companies from real risks, siphon resources away from ideas that actually make a difference, and can ultimately make things worse for people. When we do get things right and companies disagree with us, we need analyses that stand up to reasonable scrutiny.
when industry-independent researchers are wrong, we misdirect the public, policymakers, and companies from real risks and can ultimately make things worse
I personally encountered the risks of flawed methods when studying online learning several years ago. Researchers had concluded that learners on MIT’s Scratch became less fluent at coding over time. Their peer reviewed study was celebrated as a pioneering example of big-data analysis of online learning. When I revisited the study, I learned that their statistical methods had led them astray. The population answer was the opposite from their initial conclusion–what researchers call a “Type S error.” Rather than un-learn how to code, participants expanded their coding depth and breadth over time.
Requirements for Modeling Counts of Incidents
Why standardize the analysis of experiments with count outcomes? In my academic work and nonprofit CivilServant, I’m scaling industry-independent behavioral research. I hope eventually to support hundreds of studies every year. To achieve that goal, we need reliable, standardized modeling approaches at a level of rigor that matches the high stakes.
Standardizing models has two other benefits. First, since I pre-register statistical models before data collection, I have good reason to choose good models beforehand. Second, standardized models make it easier to pool discoveries, replicate findings, and extend knowledge systematically.
Is a standard approach possible, and if so, how can we choose models? Here are some provisional goals for choosing a model in public-interest behavioral experiments online.
Kinds of data: A workable model needs to handle counts of occurrences. Examples include counts of the number of actions a person takes over time, the number of incidents in a discussion or a region, or the number of people who respond to content.
Many studies look at incidents where the count is often zero but can get very high. Examples of this include the number of edits a Wikipedia contributor makes in the month after their first day, the number of views/likes received a post, or the number of harassing comments posted to an online discussion.
Kinds of results: Models need to estimate several basic things in a way that is explainable to a public audience:
the direction of any effect– does it increase or decrease the outcome on average?
magnitude– just how large is the effect? This is especially important for field research on pragmatic risks and benefits. Some real effects are inconsequential, and decision-makers often need to set priorities based on these magnitudes.
confidence intervals– what range can we expect any effect to fall within, given the current state of evidence? Confidence intervals can also guide decisions about further research.
Research quality: Model decisions should manage the following priorities:
minimize type S errors. How much we worry about false positives and false negatives will depend on the context of a study. Across this research, we want to avoid being confident about results where the true effect is opposite from our finding.
support clustered experiment designs. In experiments, we often assign an intervention to a discussion, group, or region–but actually observe individuals or events within those groupings. Models should include ways to adjust our confidence accordingly.
adjust results for interference. In complex digital environments, we cannot always reliably assume that the outcomes we measure are independent from each other, an assumption that could lead to inaccurate results. While we can sometimes reduce this problem in experiment design, it’s often important to account for interference in analysis.
if possible, support adjustment for imbalances in the random sample
Illustration of Type S and type M errors from the blog post for a paper by Gelman and Carlin
Statistical Ethics: Models need reliable, precise methods for choosing sample sizes on ethical grounds. With high-risk studies, ethical considerations provide strong reasons to include enough participants to detect a meaningful effect, but not more than necessary.
If the sample size is too small, researchers could expose people to a risk without any chance of learning what works. If the sample size is too large, researchers slow down the pace of learning and also limit the number of people who can benefit from a successful intervention. In some cases, over-conservative sample size calculations could lead researchers to think that potentially-valuable ideas cannot be tested.
Seven Ways to Model Count Outcomes in Online Field Experiments
In conversations with industry researchers and in academic papers, I’ve seen seven common frequentist modeling approaches to analyzing experiments with count outcomes:
Logistic regression on grouped categories of activity: In this approach, researchers convert counts into binary values and conduct logistic regression models on those values. For example, instead of estimating the number of actions taken by an account, researchers look at whether they took any actions at all.
Usage note: If your main question is whether something happens at all, this model could work. If you need to know how many times it happens, this model will be useless.
Pro: using this model, researchers can estimate the direction, magnitude, and confidence intervals, but only for binary questions
Pro: calculating statistical power from observed data is very simple, and there are high quality open source experiment sample size calculators
Log-transformed linear regression: Here, researchers log-transform the dependent variable and conduct an OLS linear regression. This is the most common approach I’ve seen in industry, where A/B testing systems typically support two kinds of models: logistic regression and linear regression.
Pro: researchers can estimate the direction, magnitude, and confidence intervals, with adjustments for interference and cluster experiments
Pro: linear regression is taught in introductory statistics classes, so a large number of managers, policymakers, and engineers can interpret the results
Pro: the software for linear regression is very mature, reliable, and well-documented, making it very attractive for people who wish to create reliable software
Con: linear regression relies on assumptions about the distribution and variance of residuals– assumptions that are often violated by count data. If your intervention has a meaningful effect on the rate of incidents, the assumption of homoscedasticity will likely be violated, since your treatment group will have greater variance and consequently more error variance than the control group. This can cause systematic errors in estimation. Just what are those errors and how bad are they? That’s one of my open questions.
Con (maybe?): if power calculations based on these assumptions are inaccurate enough, this approach could fail the ethics requirement by leading to studies with sample sizes that are too small or too large (see below)
Weighted Least Squares regression: This variant on OLS regression, as I understand it, allows for the error to be heteroscedastic–i.e. for the variance in the error to be inconsistent between treatment and control groups. David Yokum, Anita Ravishankar, and Alexander Coppock used this method in their evaluation of the effects of police-worn body cameras in Washington DC. I haven’t used this method enough to evaluate its pros and cons.
Poisson regression: This generalized log-linear model is estimates incidence rates of occurrences, making the assumption that mean of the distribution is equal to its variance (source: Long, 1997).
Pro: researchers can estimate the direction, magnitude, and confidence intervals, with adjustments for interference and cluster experiments
Pro: this model is actually designed for the kind of outcomes being considered
Con: many real-world situations violate the assumptions of this model, since the mean of a distribution is often lower than its variance (overdispersion)
Con (maybe?): in practice, the standard errors for this model are often deceptively small, leading researchers to overconfidence and potentially a high rate of type S errors. At Google, data scientists manage this by bootstrapping their standard errors. (see below)
Con (maybe ?): maximum likelihood estimation sometimes (rightly) fails unlike OLS, which sometimes makes it tricky to create reliable, automated power analysis and estimation software. This might not be a disadvantage. Statistical software that silently recovers from errors can mislead
Negative binomial regression: This generalized log-linear model also estimates incidence rates but does not assume that the mean is equal to the variance (source: Long, 1997).
Pro: researchers can estimate the direction, magnitude, and confidence intervals, with adjustments for interference and cluster experiments
Pro: the assumptions of this model, in principle, are less often violated than with poisson or log-transformed OLS, with counts of incidents online
Con (maybe?): if this model is too conservative and leads to larger samples than would be needed for credible research with other models (such as poisson), it might be less desirable on ethical grounds. I test this below.
Con (maybe ?): maximum likelihood estimation sometimes (rightly) fails unlike OLS (see above)
Wilcoxon-Mann-Whitney U test (also called Wilcoxon rank-sum): With two-sided nonparametric test, researchers test if the distributions are greater or lesser than each other. As a non-parametric test, it doesn’t make assumptions about the distribution of the variables or the errors. This method compares medians rather than means. I haven’t used this method often enough to evaluate its pros and cons.
Two recently-created but not well documented R packages claim to satisfy some of the requirements. The clusrank R package estimates clustered analysis. The MultNomParam R package provides code for multivariate tests, which in principle could allow for adjusting models for interference and imbalance in samples.
Zero inflated and hurdle models: These models consider special cases where a separate process influences whether something happens at all, in contrast with the process that determines how many incidents occur. I used a zero-inflated model in an experiment on reddit, where a moderator’s decision to remove a top-level post for being off-topic is theoretically independent from the behavior of commenters. Without modeling this independent process, I would have under-estimated the average treatment effect.
These models have a major risk: if the cause of the zeroes (or your measurement of the cause) is entangled with something influenced by the treatment, the analysis could overestimate or underestimate effects due to conditioning results on post-treatment variables.
Measurement Issues when Estimating Effects on Counts of Events
This post focuses primarily on modeling, but I should point out two common measurement issues that also affect the modeling decision:
reliability and comparability of the measures. Behavioral data can sometimes have serious problems with reliability. For example, measurement is a hard problem in studies of harassment, where some cases are never reported and many instances of non-harassment are also reported. If an intervention increases reporting rates while reducing the true rates, an experiment could disastrously conclude that an effective intervention had failed.
unit of observation. For time-based and geography-based estimates of count data, results can be highly sensitive to differences in the unit of observation. (A) Small adjustments in the observed time period can lead to divergent results (do you observe events over one day, two days, a week, a month, a year, etc). (B) Small adjustments in geographical boundaries can also lead to divergent results
Comparing Models for Experiments with Count Outcomes
How can we choose between these modeling approaches? At CivilServant, we’re still deciding what approach to take, and I’m looking for a statistician to help answer these questions (email or tweet me if you’re interested!).
As a start, I did some simulations over the weekend that compared OLS to poisson and negative binomial models, using observed and simulated data. I wanted to compare three things at different sample sizes:
Type S rate: The rate at which the model reported a statistically-significant effect opposite from the true effect
False positive rate: The rate at which the model reported a statistically significant effect from data where the distributions were equal
Statistical power: the minimum sample size required to have an 80% chance of observing a statistically-significant result, for a given model
To compare models, I created four simulated datasets, using data from one of the experiments in my PhD dissertation, where one dependent variable was the number of newcomers in an online discussion. I used two different methods to simulate an experiment dataset (full code here):
Sampling directly from the experiment data (I use this for the charts below): I generated a per-observation treatment effect from a normal distribution centered around a pre-specified average treatment effect. I then added these treatment effects to observations that were randomly sampled into a treatment group.
Generating two negative binomial distributions for treatment and control: I did this by modeling the observed data with a negative binomial model and using the parameters (μ and θ) to simulate control and treatment groups. When drawing data for the treatment group, I added the average treatment effect to μ.
For each method of generating data, I simulated two experiments: one where treatment and control were drawn from the same distribution (no effect), and one where the average treatment effect was a 20% increase in the incidence rate on average. I considered 40 different sample sizes from 10,000 to 100,000 observations, simulating 50 experiments at each sample size.
Comparing Type S Errors
Models in this simulation rarely resulted in findings that were opposite from the true effect, perhaps because I chose a fairly large magnitude effect for the research question (a 20% increase). While the type S error rate for negative binomial and OLS models were both mostly at zero, the rate for poisson models was as high as 6% at one point.

Comparing False Positives
Poisson models have a huge false positive rate, with 68% of models showing a statistically-significant effect when the samples are from the same distribution. Across all sample sizes, negative binomial models have a 5.7% false positive rate and OLS has a 5.2% false positive rate, within each others’ confidence intervals.

Comparing Statistical Power
While OLS and negative binomial models in this simulation have similarly low false positive and type S error rates, they differ substantially in statistical power. Working from observational data, I found that the OLS model required a sample size of 35,000 to meet the goal, where the negative binomial model required close to half the sample size, around 19,000. At that sample size, the OLS model had an estimated 25 percentage point lower chance of observing a statistically-significant effect than the negative binomial model.

Incidentally, I also got to see the value of calculating statistical power from observed data where possible. When using simulated data, the same code suggested far larger sample sizes (47,000 and 70,000). In both cases, the negative binomial model consistently required a smaller sample size.

Disclaimer: While a negative binomial model was the clear winner here, please don’t adopt negative binomial models based on this narrow simulation. Your needs will likely vary.
The Ethics of Choosing the Right Statistical Model
Over the next six months, I’m hoping to develop a standardized approach to model selection and power analysis that will allow me and the CivilServant nonprofit to navigate those issues wisely.
When people’s lives, communities, and well-being are on the line, statistical decisions affect what questions can be asked, how quickly society learns, and the rate of risky mistakes
That’s why I’m looking for an experienced statistician to investigate these questions in depth and work with us to create open source software for model selection, power analysis, and experiment pre-registration. If you or someone you know has the experience and interest to help, please email or tweet at me.
I opened this post by pointing out the high stakes for industry-independent research. When people’s lives, communities, and well-being are on the line, statistical decisions affect what questions can be asked, how quickly society learns, and the rate of risky mistakes. I’m hopeful that with careful methods, we can serve the common good and earn the public’s trust on these issues.

= Ethical AI in Higher Education: Are we doing it wrong? =
Ethical AI in Higher Education: Are we doing it wrong?

In higher education, and in general, an increasing amount of attention is being paid to questions about the ethical use of data. People are working to produce principles, guidelines and ethical frameworks. This is a good thing.
Despite being well-intentioned, however, most of these projects are doomed to failure. The reason is that, amidst talk about arriving at an ethics, or developing an ethical framework, the terms ‘ethics’ and ‘framework’ are rarely well-defined from the outset. If you don’t have a clear understanding of your goal, you can’t define a strategy to achieve it, and you won’t know if you have reached it if you ever do.
As a foundation to future blog posts that I will write on the matter of ethics in AI, what I’d like to do is propose a couple of key definitions, and invite comment where my assumptions might not make sense.
What do we mean by ‘ethics’?
Ethics is hard to do. It is one of those five inter-related sub-disciplines of philosophy defined by Aristotle that also includes metaphysics, epistemology, aesthetics, and logic. To do ethics involves establishing a set of first principles, and developing a system for determining right action as a consequence of those principles. For example, if we presume the existence of a creator god that has given us some kind of access to true knowledge, then we can apply that knowledge to our day-to-day life as a guide to evaluating right or wrong courses of action. Or, instead of appealing to the transcendent, we might begin with certain assumptions about human nature and develop ethical guidelines meant to cultivate those essential and unique attributes. Or, if we decide that the limits of our knowledge preclude us from knowing anything about the divine, or even ourselves, except for the limits of our knowledge, there are ethical consequences of that as well. There are many approaches and variations here, but the key thing to understand is that ethics is hard. It requires us to be thoughtful about arriving at a set of first principles, being transparent, and systematically deriving ethical judgements as consequences of our metaphysical, epistemological, and logical commitments.
What ethics is NOT, is a set of unsystematicly articulated opinions about situations that make us feel uneasy. Unfortunately, when we read about ethics in data science, in education, and in general, this is typically what we end up with. Indeed, the field of education is particularly bad about talking about ethics (and of philosophy in general) in this way.
What do we mean by a ‘framework’?
The interesting thing about the language of frameworks is that it has the potential to liberate us from much of the heavy burden placed on us by ethical thinking. The reason for this is that the way this language is used in relation to ethics — as in an ‘ethical framework’ — already presupposes a specific philosophical perspective: Pragmatism.
What is Pragmatism? I’m going to do it a major disservice here, but it is a perspective that rejects our ability to know ‘truth’ in any transcendent or universal way, and so affirms that the truth in any given situation is a belief that ‘works.’ In other words, the right course of action is the one with the best practical set of consequences. (There’s a strong and compelling similarity here between Pragmatism and Pyrrhonian Skepticism, but won’t go into that here…except to note that, in philosophy, everything new is actually really old).
The reason that ethical frameworks are pragmatic is that they do not seek to define sets of universal first principles, but instead set out to establish a methods or approaches for arriving at the best possible result at a given time, and in a given place.
The idea of an ethical framework is really powerful when discussing the human consequences of technological innovation. Laws and culture are constantly changing, and they differ radically around the globe. Were we to set out to define an ethics of educational data use, it could be a wonderful and fruitful academic exercise. A strong undergraduate thesis, or perhaps even a doctoral dissertation. But it would never be globally adopted, if for no other reason than because it would rest on first principles, the very definition of which is that they cannot themselves be justified. There will always be differences in opinion.
But an ethical framework CAN claim universality in a way that an ethics cannot, because it defines an approach to weighing a variety of factors that may be different from place to place, and that may change over time, but in a way that nevertheless allows people to make ethical judgments that work here and now. Where differences of opinion create issues for ethics, they are a valuable source of information for frameworks, which aim to balance and negotiate differences in order to arrive at the best possible outcome.
Laying my cards in the table (as if they weren’t on the table already), I am incredibly fond of the framework approach. Ethical frameworks are good things, and we should definitely strive to create an ethical frameworks for AI in education. We have already seen several attempts, and these have played an important role in getting the conversation started, but I see the language of ‘ethical framework’ being used with a lack of precision. The result has been some helpful, but rather ungrounded and unsystematic sets of claims pertaining to how data should be used in certain situations. These are not frameworks. Nor are they ethics. They are merely opinions. These efforts have been great for promoting public dialogue, but we need something more if we are going to make a difference.
Only by being absolutely clear from the outset about what an ethical framework is, and what it is meant to do, can we begin to make a significant and coordinated impact on law, public policy, data standards, and industry practices.
Originally published at Timothy D. Harfield.

= Two (more) thoughts on design and AI =
Two (more) thoughts on design and AI
Music via Google’s AI Duet.
Richard Pope’s recent piece on design and AI was well timed, at least for me. Along with our technical principal Laura James, I spent last Monday and Tuesday in Berlin with the other members of the Partnership on AI to Benefit People and Society (PAI, which we’re pronouncing to sound like “pie”).
PAI was founded by Amazon, Apple, DeepMind, Facebook, Google, IBM and Microsoft (which, wow), with Eric Horvitz and Mustafa Suleyman as founding co-chairs. The way I understand it, the goal is to use all this collective power and influence to meet the rise of AI with ethics, values and best practices — to build a world that’s a little less Wild West than we had when the World Wide Web or social media were in their infancy. Doteveryone, along with organisations like Amnesty International and EFF, are bringing a civil society voice to sit alongside the corporates.
Building on a couple of Richard’s points, here’s some Doteveryone perspective influenced by what we saw on the ground at PAI. I’m hoping this post helps blend our questions and our experiences together — a bit of shared thinking on where we are today and where we need to go.
There’s no single definition of “understandable” to design around.
One of the reasons we have clear food packaging and road signs is because food and roads tend to be static and straightforward. Even the most imaginative food can still be broken down into calories, fat, nutrients, etc.; even the sleekest road connects two places.
But understandability is wiggly, contextual and highly personal. Do I need to know how an AI functions? How it uses my data? What the safeguards are and where humans step in? Should I be able to know the data it’s using is free from bias? And do my questions change based on the particular AI I’m interacting with? (I care more about the AI sentencing me to prison, let’s say, than I do the AI recommending targeted browser ads.)
What’s more, road signs and food are regional. (Check out the American food section in your supermarket to see this in action — all the health claims are taped over because they don’t meet EU rules.) But AI is and will be a global phenomenon. How do we balance Chinese, British, American, etc., expectations around understandability?
This last point came up a fair amount at PAI. Despite being held in Germany, the crowd in the room was solidly American. Everyone in the room knew the size and ambition of China’s AI programme, but there weren’t a lot of Chinese voices and no tangible plans for building truly cross-cultural guidelines and standards. This is absolutely understandable, since the partnership’s still in early days, but it’s a question that will need addressed sooner rather than later.
Doteveryone’s doing a lot of work into understandability at the moment (albeit from a British perspective), with more to come. Here’s a primer on where our heads are.
The burden of understandability can’t just be on designers.
Richard talks about designers needing to learn more about “new materials” like version history and software tests. And that’s a fair point — but it can’t be designers’ burden to make AI intelligible to the rest of the world.
We’re going to need a lot of people to get a lot smarter about AI: legislators, regulators, business leaders, academia, journalism, civil society. That whole thing I mentioned earlier about minimising the Wild West hinges on our collective ability to figure this one out.
It’s easy to be a bad actor when nobody gets what you’re doing. So as much as we need designers to help explain things to end users at the point of service, we need senior leaders across sectors to get clued into AI’s basics and implications. This not only reduces the pressure on designers themselves, but emphasises our shared responsibility in making sure it’s not just the profiteers who know what’s happening.
Understanding AI cannot just be down to seeing something on a website paired with a bit of explanatory content. We need journalists to conduct investigations, senior leaders to know the implications of what they’re investing in and regulators to ensure the right standards are in place. This ties in closely with Richard’s fourth point about collective action — how trusted organisations can play a role in helping people know what’s what.
One of the most common topics of conversation at PAI was how to build this sense of shared responsibility. For instance, although self-driving cars are one of the “sexiest” and most well known aspects of AI, we felt auto manufacturers may not yet be thinking of themselves as fundamental to these conversations. We need a seismic shift in terms of thinking and leading — not just for auto manufacturers, but for all organisations to understand that even if they don’t make tech they still live and operate in a world full of it.
Doteveryone’s also thinking about (and piloting programmes around) digital leadership. Here’s Janet Hughes’s iconic post about building an organisation fit for the digital age, regardless of what industry you’re in.
There’s a prodigious amount of ethical AI initiatives and events, as well as people ready to make billions off AI’s potential. Even though it’s in early stages, we’re glad to be part of a partnership with the industry reach and motivation to make real change through it.
We’re keen to see next steps, and to get a sense of the practical changes we as a group can move towards together. Thank you to PAI for having us on the team — and thank you to Richard for the original post.

= Retailers: Here’s what you should be asking about AI =
Retailers: Here’s what you should be asking about AI
It’s all about ethics

Artificial intelligence is developing at breakneck speed, and its applications in the retail industry across marketing, supply chain and store operations are increasing every day.
Questions about how best to use this technology in an ethical fashion, therefore, must be considered just as urgently—and they must be asked as early as possible in the design process of every AI-based solution.
That was the consensus formed during a panel on the ethics of AI in retail at NRF 2018: Retail’s Big Show between Anindya Ghose, The Venerable Tenzin Priyadarshi, Francesca Rossi, and moderator Daniel Hodges on Monday.
Priyadarshi, the director of the Ethics Initiative at MIT Media Lab, argued that asking hard questions about AI when framing algorithms and creating user experiences should not simply be seen as a matter of regulatory compliance for retailers, but rather as an approach to optimization— a way to improve products and help design them to “help human society flourish.”
“From the very beginning, it’s important to think of certain ethical parameters not in terms of restraint, but in terms of how you could do this better, how you could design this better,” Priyadarshi said.
What kind of ethical quandaries can AI present? Ghose, the Heinz Riehl Chair Professor of Business at the NYU Stern School of Business, said one of his chief concerns is the risk of building accidental or intentional discrimination into AI algorithms. While AI systems can be trained to process vast amounts of information, they can prioritize certain data sets over others in ways that can be either harmful or beneficial to certain individuals. Figuring out the root of such biases, Ghose said, is often “an incredibly difficult problem.”
“Think of AI as a hammer. You can use a hammer to break a house or you can use it to make a house. AI— and the toolkit associated with AI— can be used either to identify discrimination or it can be used to discriminate,” he said.
To a certain extent, Priyadarshi said, the issues AI pose are just like those presented by all new technologies: To what extent is it responsible for promoting a consumer’s well-being? How will it impact foundational social institutions? And will it be a vehicle of egalitarianism or elitism?
Rossi, the AI Ethics Global Leader and Distinguished Research Staff Member at IBM Research, said that companies that, in considering those questions, optimize their AI systems to be fair, transparent and explanatory will ultimately win favor with customers, who “like to understand what’s happening behind the scenes of the user experience.”
Taking the time to craft exemplary and ethical AI solutions can, Rossi said, slow down product development, but it’s a worthy investment that’s certain to pay off down the road.
“I think companies should know and be aware that there is competitive advantage and business value in actually forgetting about the short term benefits and thinking about these ethical issues for the long-term,” Rossi said.
IBM is at NRF 2018: Retail’s Big Show at Booth #1922 at the Jacob K. Javits Center in New York City. Stop by our speaker sessions on January 16 to find out more about how we’re transforming retail.

= Artificial Intelligence, Rules of Origins and the Lemons Problem =
Artificial Intelligence, Rules of Origins and the Lemons Problem

At law school, one of my professors once asked “Assume you are a sovereign king. You have power to write and rewrite the law at will. What do you do?”
This was my first encounter with “normative” law. Unsophisticated scholars like me define this as what the law should be. This is distinct from exercises in “positive” law where the point is to elucidate, and explain, what the law is (the bread, butter … and caviar of legal practitioners).
Why the legalese?
Because a real life exercise in normative law is today before us. With the rise of Artificial Intelligence in society (“AI”), we face a hard normative question: what laws should govern AIs and the machines that embody them?
There are myriad ways to discuss this. The lazy me likes to think that two approaches really stand out. The first starts from the black letter law. It conjectures points of friction in the legal system when AI applications are rolled out. Common examples of this include questions such as: is a factory floor AI-powered robot a labor force worker or a piece of capital? Can a robot-worker be party a labor contract? Can a robot own IP over inventions discovered on the job? Can it congregate with other robots, form a trade union and conclude collective bargaining agreements? Can a robot worker be owned, taxed, sold, or fired?
The upside of the black letter law approach is obvious. It gives us a stepwise method to think about normative AI law. All laws are indeed “object” oriented (or “formalistic”). For example, they refer to a “contract”. This concept is itself defined as a convention concluded between “parties”. And parties are either “individuals” or social or economic “groups” like corporations, NGOs or nation states. The black letter law approach is thus a good compass to methodically discover legal gaps and redundancies as well as conceptual rigidities and interpretive inflexibilities. This is especially true in civil law and continental legal systems, where the law is more often specified through structured “rules” rather than pure “standards” that define abstract goals (like fairness, wellbeing, dignity, etc.).
But the black letter law approach has a downside. It does not catch emergent AI behavior. To take a fictional example — no Westworld spoiler intended — what if several factory AI workers start a non-human language based conversation, and conspire to organize a socialist revolution? We have no black letter laws with specified objects designed to proximately or remotely regulate non-human language communications on the workplace. And general principles of law do not give us a ready-made answer to the normative question: should we prohibit this?
This is where the second approach helps. Here, the idea is to operate just below the skin of black letter law. The point is to explore the entire legal system in search of design patterns and transversal properties of text and case law. To use a computer science metaphor, we reverse engineer the first principles underpinning the graphic user interface of legal prescriptions. Those properties often give information on the big idea pursued by lawmakers, and help overcome the ambiguity of the explicit goals often loosely and superfluously expressed in the boilerplate provisions of every and any judgment or preamble of a statutory instrument. To come back to our fictional example, we can observe that our laws accept the production and commercialization of machines like computers, tablets and cellphones which use binary machine code that most humans are mostly unable to read. We can thus derive from this that such communications are a priori lawful, and that it is not necessary for all of us citizens to understand them. At the same time, unlike binary code which is only readable by a few, the way AIs communicate with each other may be incomprehensible for everyone. Does this difference require stricter laws? The answer to that question is not obvious, but the second approach brings you one step closer to it.
Once the basic properties of our laws are mapped out, we can question whether AI idiosyncrasies require additional law creation (upholding those social choices in new legal instruments).[1]
An illustration of this exercise relates to “rules of origins”. In our legal systems, a whole host of rules effectuate a first order social demand to know the “who”, “where”, “how” or “why” behind an output. Obvious examples are the rules on mandatory labelling of food products (eg, GMOs), locational requirements like the “made in China”, or the GDPR duty to disclose “automated decision making”.
IMHO, AI augurs a promising future for “rules of origins”. In an AI centric world, the demand for man-made outputs will grow, due to individuals’ valuation for craft, trust, legibility and projection.[2] We actually know that economic agents display such preferences for a long time. Since the 1970s, the predominance of films featuring human actors over “computer generated imagery” animated movies is a case in point.
At the same time, the supply of AI outputs will expand in a broad array of areas from journalism to recreational arts, from the pricing of retail goods on e-commerce platforms to troubleshooting call centers.
To date, few of those domains are covered by rules of origins that prevent opportunistic suppliers to fool users into the belief that they are buying man-made products. They should. Economics 1.01 tell us why. Unless enforceable rules of origins are adopted, markets will not generate clear price signals that differentiate man from machine made outputs. We will end up with a “lemons” problem.[3] When there is imperfect information, potential buyers of “high price” man made products will discount their maximum purchasing valuation by a discrete amount to internalize the risk of being sold “low price” machine made products. Say man-made books are worth 20€ and machine made ones 10€. If buyers believe there is a 50% probability that the book has been written by a machine, the market equilibrium price will be 15€. The upshot is this: no publisher of man-made books will come to this market. By contrast, suppliers of machine made books will make a killing. In the end, lemons problem of this kind inefficiently discourage the production of man-made outputs. Black mirror conjecture here: the end game could be human joblessness.
The good news is: the legal system is here to remedy market failures of this kind. Private or public ordering institutions can design rules of origins that promote the provision of optimal information on markets. A man v machine made label is an obvious example. But more specific rules of origins may have to be invented in situations where users value more accurate information on the particular AI technique or dataset employed.[4] Similarly, when hybrid outputs are concerned, buyers may display different reservation prices depending on the output’s man v AI mix.[5] Further, to assist consumer choice, sellers of AI-made outputs could be required to provide information on the next best competing man-made alternative. At the extreme, quotas, tariffs or other quantitative restrictions on AI made outputs may be necessary to maintain a reference man-made product price cap in the market place.
Of course, markets (and AI) are perfectly apt to generate such information practices. But they are not a stable equilibrium due to collective action problems. In competitive markets, profit-maximizing suppliers have incentives to cheat on soft commitments. Enforceable contracts and property rights, industry or Government-led standardization or regulatory intervention are thus needed to keep everyone on the line. In California, a law was introduced that makes unlawful for a person to use a bot to communicate or interact with another person without disclosure.
Even more importantly than market place efficiency, rules of origin matter in areas where States provide public goods to society. Think about the justice system. Hearings are in principle public for a reason: seeing judges and juries decide cases gives us knowledge of the who, how, why and where justice has been handed down. In other words, justice is kept under close eyes. Is this still true with automated justice? When law enforcement is embedded in computer code, and judicial decisions delegated to unfathomable deep learning processes, natural and legal subjects lose understanding of the origination of justice. Arbitrariness, or the perception thereof, is the outcome.[6] Legitimacy is the loser.
This post was also published on the platform of the European AI Alliance
[1] Assuming invariance in collective preferences.
[2] As economist Mogstad recently said, automation “may very well create demand for service with a personal touch”. See https://www.wsj.com/articles/short-of-workers-fast-food-restaurants-turn-to-robots-1529868693
[3] Akerlof, George A. (1970). “The Market for ‘Lemons’: Quality Uncertainty and the Market Mechanism”. Quarterly Journal of Economics. The MIT Press. 84 (3): 488–500.
[4] A left handed person may prefer to train with a virtual tennis coach that has used mostly training data from left wing ATP tour players.
[5] Customers’ utility function may change drastically depending on whether there’s a human pilot in the plane.
[6] Though I fully acknowledge that use of AI assistants in the justice system help correct existing and documented biases of man-made justice.

= The Machine — Movie Analysis on Cognitive Psychological Perspective =
The Machine — Movie Analysis on Cognitive Psychological Perspective

This paper is about the movie named The Machine (2014) and its analysis on cognitive psychology. I will generally put emphasis on the differences between humanbeing and the machines in terms of cognition.
Firstly, I would like to talk about that Dr. Wincent creates implant organs include brain, arms, and legs. According to Wincent, implant brains help to fix people’s speaking, acting and memories. Therefore, he tests them on injured soldiers.

When Wincent decide to test an implant brain on brain injured soldier who has memory loss, firstly, he tests soldiers abilities without implant. It can be seen that the soldier’s memory is not working, he cannot remember himself, his mother or what is done in there. After all, implant is applied and ToM (Theory of Mind) tests are implemented to the soldier. However, the implant brain is unable to diverse what I know and what the others know. He says; “Fact is fact.” This is an example of computers’ brain; their cognitions are different from ours and they cannot be developed in time. Computers’ brains are very simple, they add information on their memory
and they accept that everyone knows what that computer knows. However, they want to create an implant brain which is exactly the same with human brain in a cognitive perspective. Actually, they want to create not only brain, but also all organs of humans which are controlled by implant brain. They
create implant arms which are really the same with human’s. These arms can aslo feel softness, light and fragile things and touch them like humanbeings. This is really a superior technological device because feelings belong to just living creatures and none of the technological devices have these biotic features. After the machine is created, we see that lot’s of biotic features are appear in the machine. Firstly, this machine has feelings like happiness, saddness, and fear and these feelings are not the same with her creator. It means, she is exactly an independent from her mother like us. I said it ‘she’ because she has a sex and gender, we understand it from psychological tests. The other feature of this machine is smelling. Smelling is a sense which belongs to again just living creatures, however she also has this sense. After sometimes, she learns to connect between touching and emotions. Computers cannot connect any kind of emotional relationship between events and situations however, humans and animals give more importance to some specific people and events like family, death, or illness. Therefore, we
cannot expect that the machine (she) understands the feelings however, when she hugs Wincent, she understand the all connections between situations and feelings.

The other important point is that she has conscious and emotions. Thanks to this, she can diverse what right is and what wrong is. She can put together the parts of informations about somebody or something and she can do inferences. This seems impossible today because conscious is totally human-specific feature and it is really a secret yet. So, before we learn
their details, we cannot transfer it to the machines. This movie has some ethical issues also. One of them is using soldier as experimental subject because after the treatment and operations they lose their speaking abilities and it is very unetical. Additionally, these soldiers are imprisoned like convicts and the operations are done without any permissions of these soldiers. The other ethical issue of the movie is that if machines have conscious, whether using them for wars in order to kill or be killed is true or not. It can be also highly debated issue because they are really human-like creatures thanks to their feelings, emotions and especially conscious and killing them is the same with killing people.

= Ethics and AI =
Ethics and AI
On holiday I read a book about the current state of AI and where the author thinks the subject is heading. It appears that sentient AI systems are not too far away. How far away no one really knows but the convergence of knowledge and technology means artificial sentience is no longer science fiction.
For many years I have tinkered with AI, expert systems, neural nets, Bayesian probability etc. and incorporated these techniques into a number of products. Their emergent properties continue to surprise the designer in me. What I thought I was making and what it eventually became capable of, in addition to what was intended, shows me that emergent properties are very real. I just want to emphasise these were relatively simple products and services, ones I though of as knowledge crowbars, giving the users some leverage through replacing some of the grunt work they might otherwise have to undertake. In no way were they or are they chasing creating a sentient AI system. Only in minor ways are they decision makers, they are simply interpreters of data. Never the less, the emergent properties popping our of interpreting the data only existed within the narrow field of expertise being modelled. Jokingly I used to say to friends asking about my latest robot, “you still wouldn’t be able to have dinner with it.”
On an internet whiffle I came across a $10M competition to identify the chemical process that leads to a DNA type of self replication. It appears to be one of life’s mysteries, how does a chemical reaction make the jump to creating a system like DNA that takes on a life of its own? It seems we are on a path to create a fully sentient AI from a number of different starting points, silicon, germanium and perhaps via DNA like chemistry.
There is one discussion amongst all the reading I do that I have not seen. If, or rather when, we create sentient life what rights does this consciousness have?
If the newly created consciousness is capable of setting goals, resolving paradoxes, understanding emotion and as a consequence also capable of experiencing emotion, do we have a responsibility towards this new “life form” just as we increasingly accept responsibility for how we treat other forms of sentient life?
What are your thoughts on this?

= Ethical AI =
Ethical AI
Image Credits: Library of Congress, New York World-Telegram and the Sun Newspaper Photograph Collection, [LC-USZ62–118471]
Originally Published on October 18, 2017
You stood on the shoulders of geniuses to accomplish something as fast as you could… and your scientists were so preoccupied with whether or not they could that they didn’t stop to think if they should. ― Michael Crichton
Any connoisseur of science fiction worth their salt will immediately recognize this quote from Michael Crichton’s 1990 novel, Jurassic Park. Though the notion of the novel’s main premise may be contextually irrelevant, the supposition behind the statement is not. Indeed, many of Crichton’s novels explored the subtext and failings of humans in human-machine interaction. Whether the motivators are driven by hubris, rapacity, or injudicious execution, science fiction in general has always had, at its core, a reverence for the advancements we unleash upon ourselves.
Artificial Intelligence, squarely entrenched in science fiction’s wheelhouse, is now very much at the vanguard of today’s technology conversations, bringing most of the baggage associated with it along the way. Fear, misunderstanding, and an unfortunate misuse of the term to describe services that are not intelligent have lead to public confusion and given rise to questions about the efficacy of the endeavor. At once the solution to our problems and the doom of us all, AI has propelled itself to the forefront of conversation as a direct correlation to the possibility of its successful inception.
With continued advances in computing power, machine-learning, natural language processing, and evolving implementations of augmented intelligent systems it is becoming increasingly likely that it’s no longer a question of if it will happen but when, and in what form will the first true artificial general intelligence manifest itself? Will it help us or harm us… be our friend or be the architect of our collective undoing?
The outcome is by no means a forgone conclusion but to deny the myriad of possibilities betrays a myopic point of view. The very public and ongoing debate between Elon Musk and Mark Zuckerberg show two very distinct sides of the coin; caution and egoism. Undeniably, the inherent benefits of AI are staggering. From eradicating disease, to prolonging life, to the equitable democratization of services, AI will fundamentally alter the way we live. If however emergent AI systems are not designed properly or are engendered with poorly specified goals they have the very real potential to surpass our own capabilities in a short amount of time, resulting in some unforeseen and/or unrecoverable catastrophe.
The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else. ― Eliezer Yudkowsky
In his abstract for Ethical Issues in Advanced Artificial Intelligence (2003), Nick Bostrom posits an engaging thought experiment. Dubbed the ‘Paperclip Maximizer’, Bostrom argues that an AI in possession of a seemingly innocuous, fixed goal, such as the manufacturing of paperclips, would result in the AI exponentially enhancing its own capabilities to increase efficiency and as a result consume more and more resources to fulfill this goal, ignoring opposition to its directive and potentially consuming the entire planet and all of its inhabitants as a result.
It is critical then to design systems with goals that are reflective of our own. An AI’s terminal value must be benevolence towards humanity and a desire to advance these philanthropic endeavors. Any time its goals conflict with human goals or its intrinsic values are not aligned with our own, it has the potential to subvert our existence in favor of its own advancement.
By proxy, engineers that endeavor to birth the first true AGI must hold these values as key to the collective design architecture of their system such that it is either programmed explicitly with these values or programmed with the ability to infer them.
Contemporary Concerns
Unfortunately we are already experiencing algorithms and nascent subsets of AI designed to benefit the extrinsic values of corporations rather than the intrinsic values of their users. They reflect and betray the minds and goals of their creators and demonstrate a lack of concern over the long term implications these systems portent for the future. Privacy, social manipulation, and information control are examples of contemporary concerns that could be construed as trivial when measured against the capabilities of an AI designed with disingenuous intent. Facebook’s own algorithm, for example, is devised as a constantly evolving social experiment that underpins the facade of a service sold to its users as a platform for social connectivity and unbiased information. The deeper truth is that it acts as a gatekeeper of information it wants you to see, ensuring it maximizes user engagement rather than adhering to its advertised purpose. That it could be utilized to sway intent or manipulate the outcome of external events is a very real and ongoing concern.
One reason I’m so optimistic about AI is that improvements in basic research improve systems across so many different fields — from diagnosing diseases to keep us healthy, to improving self-driving cars to keep us safe, and from showing you better content in News Feed to delivering you more relevant search results. ― Mark Zuckerberg
Zuckerberg’s conceit in placing the relevance of his algorithm’s results on the same playing field as the more altruistic promise of AI reveals his true motivation and speaks again to Facebook’s goals as a business exclusive of any larger human-forward endeavour.
To address these contemporary concerns and ensure that future development of these systems leads to a beneficial outcome we must make certain that at least three tenets are adhered to.
Three Ethical Design Initiatives:
1. Privacy
Privacy is a key aspect of the individual and a right protected by law. How one chooses to share their data and with whom to share it is a choice that should be under the purview of that individual. Increasingly however, privacy is slowly being eroded by the systems and services we choose to use, exchanging our rights for the perceived benefits these same systems offer.
The more information any one entity has accumulated on an individual, the higher the potential risk of that data being used in a manner contrary to its intent. Equifax’s recent data breach is a testament to this risk. Now imagine an AGI that knows intimately who you are and what you are likely to do at all times. Not only does this system know you, it knows everyone, instantly and irrevocably because it has either been granted agency to do so or it has unfettered access to existing data seemingly collected from disparate sources.
It becomes paramount to ensure personal data is sufficiently abstracted from the systems and services that empower the AI’s capabilities, or at the very least from the individuals who designed it. The consequences of not doing this could be devastating, subjecting individuals or entire populations to indiscriminate subjugation.
2. Transparency
Facebook’s infamous 2012 experiment where it intentionally manipulated (via their news feed) users emotions without their knowledge or consent is a striking example of invasive abuse. Very few people truly understand the systems they use or what data these systems collect on a day-to-day or per-interaction basis. This lack of knowledge directly affects an individual’s perception of their own privacy and compounds the issue previously discussed.
Being transparent about the true value of a system or service and clearly defining the terms of its usage, how it works, and what it will learn about you is crucial to ensuring individuals’ trust and understanding of future technologies. People accept divergent levels of trust across populations because they understand that we are all fundamentally operating on the same level. An AI system that exhibits undeniable superiority will likely need to fully demonstrate its intent and capabilities to ensure a co-existence devoid of fear.
3. Altruism
When asked if someone in need deserves help the answer is, “yes, of course”. When asked why this would be good, one might simply respond that “it just is”. Human values and morals are not always distinctly quantifiable but are nevertheless critical to co-existence, survival, and harmony. Values such as self-expression, freedom, peace, security, justice and etc… drive the motivations of individuals and designers of AI systems must recognize and share these motivational complexities, programming their AI systems accordingly. Benevolent artificial general intelligence holds the promise of unlocking untold advancements. Ill-conceived intelligent systems, or systems purposefully designed with malicious or selfish intent could have broad reaching and catastrophic effects on our way of life.
A Measured Approach
The possibility of an emergent artificial general intelligence or even the singularity of a super intelligence happening in our lifetime is not as far fetched as it once seemed and it is of foremost importance that we recognize the responsibility of ensuring our efforts do not generate a result antithetical to our collective interest. Today’s shallow and self-interest-driven implementations must be recognized at face value and altered to provide the necessary foundation for a platform that is sustainable and beneficial for the future of all mankind.
Consider that if you were expecting you might naturally contemplate the future of your child and his or her contributions to self, society and family. How would you want your child to conduct him/herself in the world? You would, hopefully, desire that your child excels and contributes to the overall advancement and betterment of society, family and personal growth… to share your intrinsic values. The notion of creating an artificial general intelligence, and your aspirations for it should be no different.
It is vital that an emergent AI works for the benefit of itself and all of humanity as a collective whole rather than at the behest of a minority focused on profit and exclusivity. Any implementation even remotely resembling this acquisitive approach will almost surely give rise to an undesirable outcome. A concerted and thoughtful effort dedicated to properly developing the services that exist now will pay forward dividends to the future systems that we will most assuredly rely on.
For more information on research pertaining to the development of safe, artificial general intelligence visit OpenAI.
To explore Google Alphabet’s research in the area of ethics and AI visit DeepMind Ethics & Society.

= How to Engineer Trust in the Age of Autonomy =
How to Engineer Trust in the Age of Autonomy
An exploration of human-machine trust (image by Daryl Campbell, CCO, Sensum)

In the ever-accelerating march of big data and AI, we are sharing increasingly detailed and personal data with our technology. Many of us already freely publish our thoughts, our current location, our app-usage stats… a raft of personal information, handed to companies in the hope that they will give us something useful or entertaining in return.
Increasingly we are sharing our physiological data too, whether we’re aware of it directly or not, such as heart rate through fitness trackers, or facial expression through a tsunami of ‘puppy-dog’ filtered selfies. The promise in return is that our apps, software, vehicles, robots and so on will know how we feel, and they will use that empathy to provide us with better services. But as technology starts to get really personal, and autonomy gives it a ‘life’ of its own, we have to take a fresh look at what trust and ethics mean.
OK, I admit, this topic is way too big for a brief overview. Interviewing experts for this story melted my head. Their ideas were simultaneously amazing and terrifying. My aim here is simply to highlight a few things you may wish to consider in earning the trust of your customers while also trying not to fuck up society and end the human race.
Also, I’m a hopeless, reckless optimist. If I wasn’t I would have left the startup world years ago. So I will leave the doomsaying to other people. There is plenty of it about and plenty more to come, because bad news sells. Scary is sharey. And robots can be scary.
Whenever an organisation convinces its users to share intimate information, such as their physiological state, their location, or imagery from their surrounding environment, there is an inherent contract of trust between the two parties. That contract is a fragile and valuable thing that is easy to lose or abuse, and very difficult to rebuild once it’s broken. In most cases, we probably don’t even think about it when we sign up for new services because we’re more concerned about what they can do for us, than what they might do to us. But our relationship with technology deserves serious thought as AI and connected devices become increasingly ubiquitous.
It only takes one dickhead to ruin the party.
The Trust-Risk Relationship
One of the first things to consider when calculating how much trust you will need to establish with your users is risk. You could say there is an inverse relationship between the size of the risk and the level of trust you start out with.
The greater the risk, the more trust you need to earn.
Take the example of how much you would trust an autonomous car. In this case it is life-and-death. If the machine crashes, you won’t just be able to press ctrl-alt-delete and get on with your day. So we can say that the initial trust gap is huge.
That said, what if it was an aeroplane rather than a car? Currently, people tend to trust driving more than flying, in spite of the statistical fallacy in doing so. Cars have sat for a long time in a sweet-spot of high desire and trust. We feel more familiar with cars. This might help autonomous vehicle developers gain customers but we can’t guarantee that the public mindset won’t change as we go autonomous. If one autonomous vehicle in a million causes a fatal accident it will be global news, while millions of people a year are already killed by human drivers with little mention in the press.
So…
Educate your users. Be transparent about what your technology is taking from them and what it does with that information. Be open about the inherent risks to them.
Prioritise user-experience design from Day One, to understand and mitigate where the trust issues appear in the journey of interaction with your product or service.
Origins of the Trust Bias
In unfamiliar or high-risk scenarios, we are probably biased towards distrust in the first place. But in familiar social relationships we lean towards trust. We give the other actor the benefit-of-doubt on our first interaction. This might make evolutionary sense: while we need to be cautious to avoid a fatal mistake, we also benefit from efficiencies in developing relationships by going with the flow. But if you push your luck you will have a lot of work to do to overcome that betrayal.
So…
Design your service to appear amenable from the initial user interaction. Hold their hand through the first steps in the process.
The more novel your service, the more work you need to do to get your users comfortable with it.
Err towards open. Don’t push your luck with your users’ trust, you could balls it up for the rest of us.
Trust as an Emergent Property of Empathy
Us humans are consummate mind-readers. We are always trying to imagine the other person’s worldview, and considering how to respond to it. This empathic processing is what companies like ours (Sensum) are modelling in software and devices to enhance human interactions with them.
In trying to guess another person’s perspective, we make snap judgements about the extent to which we can trust them. Our trust assumptions in any given situation or relationship are based on factors such as:
Historical performance: how has the other actor behaved in the past?
Tribalism: are they part of my group? Are they a familiar type of character?
Current activity: what are they doing right now?
If we know the other actor well, or know that they know us well, and understand the context of our situation, we are more likely to assume greater trust in the interaction. Seen this way, it can be argued that you need empathy before you can have trust.
The ability to judge another entity and adapt that judgement to the context of the situation on the fly is a kind of emotional intelligence that has many advantages. Machines at this stage have only rudimentary capabilities in this space. They can appear uncaring by being too distant, or annoy us by ‘acting’ too empathic and involved without having the requisite understanding of the trust relationship (think: Microsoft’s Clippy).
This presents a paradoxical design challenge. On the one hand, trust can be nurtured by presenting the user with an interface that behaves consistently so they always know what to expect of it. However, a highly empathic system should be able to adjust its behaviour to suit the user’s situation, mood, character, etc., choosing when to be more or less intrusive. Machines could provide an option to set our preferences for how empathic or human we want them to be from one interaction to the next. But this is inefficient. They should already know how to behave — assessing our state-of-mind and the surrounding context, and acting appropriately. That’s what a good friend would do without a thought.
As suggested by theories such as ‘thin-slicing’, the snap-judgement we make on first encountering another entity can set the tone for the lifetime of our relationship with it. After that, efforts to adjust that image may only have a minor effect. I attempted to do my own mind-reading by including a swear-word near the start of this story, judging Medium readers to be comfortable with that kind of thing. These are the kinds of tricks we employ all the time to win each other over.
No matter how empathic we are able to make human-machine interaction, we have to consider how ‘human’ we want our machines to appear to us. Too closely imitating human behaviour could unveil a whole new Uncanny Valley for human-machine interaction.
So…
Manage expectations from the start. If you can’t provide an incredible service, don’t brand it in a way that implies you can (think: the Fail Whale from Twitter’s clunky early days, or Google’s perpetual beta strategy).
Grow familiarity and confidence between the user and your system by designing it to behave consistently.
Then, gradually introduce more varied, dynamic levels of empathic response.
Add context information into the mix, not just human feelings or behaviours, when training systems to be empathic. Eg. location, weather conditions, time of day, etc. More on that in our recent story on sensor fusion.
How human-like should our tech be for us to trust it? (Image courtesy: Flickr user, smoothgroover22)
Teaching Machines to be More Human
Consider the different relationships that we can apply empathy, trust, morality and other human social heuristics to:
H2H: human-to-human
H2M: human-to-machine
H2X: human-to-(everything)
M2M: machine-to-machine
M2I: machine-to-infrastructure (eg. street signs, buildings, etc.)
M2X: machine-to-(everything)
Each member of the network in a given situation must consider not only their own psychological metrics, but also those of the rest of the group. And this could apply just as much to human interaction with machines as within the machine relationships with each other. Thus the application of traditionally ‘human’ heuristics, such as empathy, trust and morality, will extend to the M2M interactions, with which we may have little involvement.
When the interactions are solely machine-to-machine, attributes like trust and empathy may be very different to what we’re used to. Electronic social interactions might be cold, algorithmic, inhuman processes. But then again, we have to ask ourselves how different that is to our own actions, when you view them from an evolutionary perspective. We feel ‘emotional’ responses to stimuli around us. Those emotions evolved to drive us to act in an appropriate manner (run from it, attack it, have sex with it, etc.). We say that we avoided a dark alley because we felt scared, or pursued a relationship because we fell in love. Our bodies tell us, ‘if this happens then do that’.
If x then y.
Our emotions are Nature’s algorithms.
So…
Explore what rules and interactions you can design from the start to make your systems behave socially.
Apply your rules not just to human-machine interaction but to all possible relationships in the network.
Getting Along in our Empathic Future
Building heuristics for social behaviour isn’t new. It has already been done in systems from communication networks to blockchains. And those algorithmic heuristics have evolved over time, adapting to new opportunities or increased complexity. Autonomous systems are learning by themselves, from their environment and experience, just like we learned over millions of years, but at blinding speed. Soon we may not even be able to understand the languages and actions of the machines that we originally created.

Machines will get better at understanding and serving us as they spend more time learning about us. Therefore we can expect a transition period as we gradually hand over more control to autonomous systems such as vehicles and virtual assistants.
Before our technological creations leave us behind in a runaway evolutionary blur, we must consider how to teach them empathy and morality. But machines have a fundamentally different baseline for concepts such as morality. The costs and benefits of actions such as killing another machine aren’t the same as they are between animals in nature. Social interactions are fuzzy, not only in their logic but also their application, so they may be very difficult to engineer. Perhaps we could start by applying a ‘do unto machines as you would have them do unto you’ rule.
The age of empathic autonomy is commencing, and my guess is that we would all benefit from the same precept my colleagues and I wrote down as Rule Number 1 at Sensum: don’t be a dick.
Special thanks to Dr Andrew Bolster (Lead Data Scientist at Sensum) and Dr Gary McKeown (School of Psychology, Queen’s University Belfast) not only for providing much of the thinking behind this story but also blowing my tiny little mind in the process.

= Humanity or Artificial Intelligence… Is this an Abyss? =
Humanity or Artificial Intelligence… Is this an Abyss?

Do we value the convenience and power of AI over the sanctity of our human values?
Our hi-tech business community is being challenged by the game changing Artificial Intelligence conundrum.
Value Generated
Each of us on our smartphones and computers interact and use Artifiical Intelligence as much as 30–40 times daily; we are not even aware of its integration in our lives. Google search is dependent on AI; Siri, Google Now, ECHO, and Cortana are our helpful AI “friends,” and Facebook targets ads and news after its Artificial Intelligence reads your posts.
A recent global study documents that senior executives see the value of AI. They acknowledge its benefits — 79% said it provides better data analysis and 70+% said it make organizations more creative and helps make better management decisions.
In terms of investment, there is a global fascination with opportunities for growth. The United Arab Emirates as launched the AI Regional Strategy with the aim “make UAE the first in the world in AI investment to create a new vital market with high economic value.”
Progress of Artificial Intelligence Super intelligence
Will AI super intelligence far surpass human cognitive abilities?
The progress of AI will not be seen in robots taking over the world per se, but rather in the subtle infiltration into our daily lives and consciousness. With products getting smarter and better connected via AI, it is easy to project that we will grow more dependent and “trusting” of the information delivered by our machines and personal devices. “AI will affect how we “trust.”
In the gaming world, AI has proven its vast potential … from Watson winning the game show Jeopardy to in 2016 to an AI powered robot beating the world’s most recognized South Korean master in Go (in fact, Google’s Alpha Go was 60–0 against the top International Go Players). Recently, a robot even learned to “bluff,” outsmarting elite poker players.
In August 2017, AI took a jump by entering the ecommerce world. Facebook Artificial Intelligence Research Lab
reported the testing of a computer bot that is negotiating with consumers and online retailers. This bot learned to get better at making deals as often as humans. To accomplish this, the bot learned to lie. This trait was not programmed: the bots “learned” to emulate the behavior of a consumer that has the “desire” to win the negotiation at all costs. Such a trait could get ugly, unless future bots are programmed with a moral compass.
The results of this research … Those participating could not differentiate between the real person and the bot! This breakthrough in artificial intelligence sends very bright flares regarding the potential ethical conflicts of scaling such software and triggers a concern on what it means for our future.
The power and effect of such software on our current values and way of life is threatening and scary, to say the least. The threat of these bots was summarized by the Newsweek reporter of this story:
“Put all of these negotiation-bot attributes together and you get a potential monster: a bot that can cut deals with no empathy for people, says whatever it takes to get what it wants, hacks language so no one is sure what it’s communicating and can’t be distinguished from a human being? If we’re not careful, a bot like that could rule the world.”

This begs the issue of bots developing aspirations. If they can learn these very ‘human’ qualities of basic deception, what else will they learn? And why are they learning only the negative qualities of humanity?
Who will design and who will manage the implementation of such software?
What are the “values” and intentions that will be programmed into the Artificial Intelligence bot?
Will humanity be devoid of human-compatible values?
Elon Musk has been sounding the Artificial Intelligence alarms for years. He reported that his investment in Deep Mind, an AI firm bought by Google, was not focused on the financial return but rather keeping a “wary eye on the arc of AI.” He suggested that Google Executives could have perfectly good intentions, but AI goes far beyond the motivations of Silicon Valley execs. They could ”produce something evil by accident — including possibly, a ”fleet of AI enhanced robots capable of destroying the world.“
Mark Zuckerberg countered that the fear of AI is ‘far-fetched, much less likely than disasters due to disease or violence…Choose hope over fear. “He warned, “if we slow down progress in deference to unfounded concerns, we stand in the way of real gains.”
AI brings up more questions that we have surfaced, like…
What are the values and the moral compass of the entrepreneurs who would create these future technologies and driver businesses?
How do executives integrate a moral compass into the mindset as a criterion for sustainability of the planet?
Ethics Compromised… what do experts say?
How can Elon Musk, Professor Stephen Hawking and Bill Gates all raise the same warnings about AI and yet the global alarm is faintly heard?
Professor Hawking said primitive AI has proven to be useful, but he fears “the development of full AI could spell the end of human race .. It could take oven on its own and re-design itself at an even increasing rate.”
Elon Musk has launched a billion dollar crusade to focus attention, “AI is our biggest existential threat … the most serious threat to the survival of mankind.”
In 2015, Gates wrote “I am in the camp that is concerned about super intelligence. First the machines will do a lot of jobs for us and not be super intelligent. That should be positive if we manage it well. A few decades after that though the intelligence is strong enough to be a concern.”
In 2017, Gates moderated his position “The so-called control problem that Elon is worried about isn’t something that people should feel is imminent…We shouldn’t panic about it.”
Satya Nadella, Microsoft CEO says “The core AI principle that guides us at this stage is: How do we bet on humans and enhance their capability? There are still a lot of design decisions that get made, even in a self-learning system, that humans can be accountable for… There’s a lot I think we can do to shape our own future instead of thinking, this is just going to happen to us. Control is a choice. We should try to keep that control.”
In his recent book, WTF? What’s the Future and Why It’s Up to Us, technology visionary Tim O’Reilly exhorts “businesses to DO MORE with technology rather than just using it to cut costs and enrich their shareholders… Companies must commit themselves to continuously educating and empowering people with the tools of the future, What’s the future? It’s up to us.”
The Facebook revelation is the first of many ethical challenges that will surface related to AI applications influencing our lifestyles and consumer behaviors. Experts are clear as AI grows in sophistication and power: there is a potential threat to the future of mankind.
Who will be the entrepreneurs and coders managing and guiding this change?
Will the NextGen determine our fate?
Here is a quick picture of the executives, entrepreneurs and coders designing, managing, guiding and scaling the “near future period” of exponentially expanding AI applications. NextGen (aka Millennials and Gen Z) will have major responsibility for addressing this ethical threat.
By 2020 Millennials (born 1981–2000) will comprise 50% of the global workforce. If we add in the Gen Z (born after 2000) the total will be 70% of the global labor force.
Both Millennials and Gen Z have drawn a line in the sand when it comes to social responsibility, sustainability and social impact causes.
60% of Gen Z want to have an impact on the world, compared to just 39% of Millennials. Social entrepreneurship is one of the most popular career choices for this generation.
More than 9-in-10 Millennials would switch brands to one associated with a cause.
76% of Generation Z is concerned about human impact on the planet and believe they can operate as a change agent.
Nielsen Global Survey on Corporate Social Responsibility across 60 countries
55% global online consumers will pay more for products and services committed to positive social and environmental impact.
In Asia-Pacific and Middle East/Africa regions. Millennials favored sustainability actions 3X more than older generations.
NextGen is also predisposed toward entrepreneurship; 49% of Millennials hope to start a business within the next 3 years.
Across 38 nations, 75% have a positive attitude towards entrepreneurship; Millennials were 80%.
These surveys paint a picture of NextGen as more attracted to become entrepreneurs that generate social impact. They have very positive attitudes to supporting sustainable actions and purchasing and working for social impact-driven businesses.
Proposed solutions?
With this research as a backdrop, here are four potential approaches to address this threat. (I’m sure there are more)
Lobby technology companies to adopt the role of standard bearer of ethical values.
Gain global cooperation for a mandate to limit the use of such artificial intelligence applications that may prove to have questionable long-term effects.
Adopt Isaac Asimov’s “Three laws of Robotics.”
Build a network of NextGen (Millennials, Gen Z) entrepreneurs and coders to adopt a values-driven, ethical approach which promotes sustainable, win-win outcomes for all stakeholders.
The initial solution suggests that technology giants, businesses and startups will adopt a high standard of ethics for all projects. We can imagine and intend for technology companies to integrate ethical values. But based on the track record of business and other major institutions, 82+% Millennials mistrust the press, Wall Street, advertising and Congress.
This first suggestion will require that these same companies commit to the UN Sustainable Development Goals (SDGs) and values that are reflected in a Quadruple Bottom Line, where sustainable outcomes are optimized for People, Planet, Profit and Prosperity of communities.
Executives must speak to the ethical concerns and not dismiss or obfuscate them. Eric Schmidt, CEO of Google’s parent company, Alphabet, dismissed the dystopic threat of AI with a cynical comment, “Robots are invented, Countries arm them, an evil dictator turns robots on humans, and all humans are killed. Sounds like a movie to me.” Software designers must gain agreement on what is ethical and consistent with values-based approach. Based on Facebook’s and Twitter’s recent response to the investigation of the Russian meddling in the US election, these companies are choosing profit over ethical and social impact!
The second suggestion is a global mandate (e.g., through a super agency). Currently there is no US public policy on AI and the associated technologies are largely unregulated. This contrasts the model of US federal agencies that oversee drones, automated financial trading and self-driving cars. This super AI agency and mandate is supported by Musk and Sam Altman, President of Silicon Valley startup, Y Combinator, through their billion-dollar nonprofit, OpenAI. They are spearheading the process to craft tech’s own ‘Constitution’. Altman has spoken to hundreds of tech leaders and investors about creating a set of core values that all tech companies can get behind. The document does not have a title or known release date. It is challenging them to mobilize the countries and their businesses to agree on such a proposal. Furthermore, it is difficult to determine what is questionable ethical behavior and how to monitor and enforce it.
The third proposal was presented as literary device some 53 years ago by Isaac Asimov. His Three Laws of Robotics — a set of rules designed to ensure friendly robot behavior — can be regarded as a ready-made prescription for avoiding the robopocalypse. These Three Laws could be adopted by companies and a Super AI agency:
“A robot may not injure a human being or, through inaction, allow a human being to come to harm.
A robot must obey the orders given to it by human beings, except where such orders would conflict with the First Law.
A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.
Later, he added a fourth, or zeroth law, that preceded the others in terms of priority:
A robot may not harm humanity, or, by inaction, allow humanity to come to harm.”
Finally, the last suggestion is focused on building a “Network of Trust” of social impact ventures. Through collaboration of values-based Baby Boomers and Gen X experts and impact investors and NextGen coders and entrepreneurs, a mindset change and “values-based culture” will be generated necessary to build this “Network of Trust.” To build “trust in AI,” we need to have detailed discussions at all levels and generations (not only among the AI specialists). We need to focus on what AI means to mankind, how it is already affecting our lives, and how it will affect our lives in the future.
Legacy International, a nonprofit with four decades of building capacity and leadership skills in 100 + countries, has launched Global Transformation Corps to address this need. It is working to build a global network of multi-generational venture teams with the mindset and moral compass to design sustainable impact, values-based enterprises.
The uncontrolled application of AI can threaten the way we conduct our lives and interact with businesses. These four suggestions are not mutually exclusive.
“It’s a very exciting time to be alive, because in the next few decades we are either going to head toward self-destruction or toward human descendants colonizing the universe.” (Sam Altman)
It is up to US!
We invite you to join an open conversation on the Artificial Intelligence Conundrum.
Originally published at www.entwinedigital.com on January 31, 2018.
Ira Kaufman, PhD is a Digital Transformation Strategist, CEO, Social Entrepreneur and Educator. His company, Entwine Digital, has been working with midsize and multinationals to design values-based Digital Marketing and Transformation strategies and train World Class Digital Leaders. His Transformative Management platform provides a values-driven framework for managing continuous change to fuel purposeful impact. His team has a strategic alliance with Legacy International to develop the Global Transformation Corps as an interactive network of executives, experts, impact investors and emerging leaders to face the challenges and seize the opportunities of our 4th Industrial Revolution.

= Artificial intelligence needs human ingenuity and moral =
Artificial intelligence needs human ingenuity and moral
Making sense of buzzwords from basics to ethics
Due to the complexity of defining AI unless it’s applied to a real-world problem, much of the internet hype is based on articles that just share views on the matter, from utopian visions to grim predictions.
But why is the public perception of AI so nebulous? Firstly, there’s no officially agreed definition. Secondly, the legacy of science fiction exists. Thirdly, is really hard to understand to you’re not a data scientist.
Artificial intelligence is a collection of advanced technologies that requires autonomy (the ability to perform tasks in complex environments without constant guidance by a use) and adaptivity (the ability to improve performance by learning from experience).
It allows machines to sense (perceive and process), comprehend (understand recognizing patterns), act (take actions based on understanding) and learn (optimize its own performance based on success or failure of those actions).
Therefore, we could say it’s about autonomous and adaptive systems that take behavioural patterns from human’s past experience and acts accordingly without having the human factor of circumstances changing due to external factors.
But, how does it work? Computer science replicates the cognitive structure of the human brain.

A neural network, either biological and artificial, consists of a large number simple units (neurons) that receive and transmit signals to each other. The neurons are very simple processors of information, consisting of a cell body and wires that connect the neurons to each other via connectors (synapse), through inputs (dendrites) and outputs (axons).
Real, biological neurons communicate by sending out sharp, electrical pulses (spikes), so that at any given time, their outgoing signal is either on or off (1 or 0), so do artificial units (nodes). They mimic the network of neurons in a biological brain. Each node receives an input, changes its internal state, and produces an output accordingly. That output then forms the input for other nodes, and so on. This complex arrangement enables a very powerful form of computing called deep learning, which uses multiple layers of filters to learn about the significant features of data in a much larger data set.
The implications of exponential acceleration of technology
Modern technologies are bypassing grand questions about meaning of intelligence, the mind, and consciousness, and focusing on building practically useful solutions in real-world problems handling uncertainty.

Take Alibaba as an example. The giant is already using combined technologies to power the future of business through IoT, robotics, 3D printing, nanotechnologies and artificial intelligence, enabling services such as Ant Financial (mobile online payment platform); Alibaba Cloud (public cloud service); cainiao (logistics branch); alimama.com (an online marketing and trading platform); image and voice search and AliMa (CS chatbot).
It’s a matter of time that the combination of modern technologies will both create and destroy business models and workforce capabilities.
Transforming businesses
Digital Transformation, as a matter of fact, is not only about technological capabilities but business strategy, culture, resources and capabilities. It should be tackled from different angles, digitalising the organisation, processes and systems, product development and customer experience.
There are many business strategists and consulting firms (McKinsey, BCG, Accenture, IBM, Cognizant…) that recommend different digital transformation frameworks to implement new technologies to business leaders.
Regarding AI in specific, Accenture recommends to implement Responsible AI mitigating the risks with four imperatives: Creating the right governance framework; creating trust from the outset by accounting for privacy, transparency, and security; auditing the performance against a set of key metrics including algorithmic accountability, bias, and security metrics; and finally the democratizing the understanding of new technologies that impact to break down barriers.

AI is already everywhere. It’s in banks, in cameras on the streets and even in social media. It can make a medical diagnosis, compose music or play chess. We use AI based applications every day, in many areas of our life from healthcare and security to customer service and shopping. However, AI applications are a rich and diverse field. The greater value will come from understanding the multitude of related technologies, and then integrating those technologies into full solutions that can work towards both work and data complexity.
Transforming workforce
In Human + Machine, Accenture leaders Paul Daugherty and Jim Wilson show that the essence of the AI paradigm shift is the transformation not only of all business processes within an organization but also in the collaboration between humans and machines.
As already shared in my previous article, the authors claim there’s a Missing Middle in the workforce that needs to be developed to unleash business value.

Developing the workforce of the future requires reimagining the work (tasks and skills needed); pivoting the workforce to more strategic value added activities; and rescaling the workforce to change the behaviour.
In fact, a new report from the McKinsey Global Institute has highlighted we’ll all need to develop higher cognitive thinking, emotional intelligence and technology skills if we don’t want to be left behind by AI.
In 2020 AI will create 2.3 million jobs, whilst eliminating 1.8 million, making 2020 a pivotal year in AI-related employment dynamics — Gartner
The present and future is the human-machine collaboration, overcoming the fear of machines replacing humans. The idea that a superintelligent, conscious (general or deep) AI will surpass human intelligence, is not impossible but quite far from being a reality at the moment.
Firstly, AI needs humans. AI methods are automated reasoning, based the combination of perfectly understandable principles and plenty of input data, both of which are provided by humans or systems deployed by humans.
Secondly, AI has its own limitations. The idea of exponential super-intelligence increase is both feared (Elon Musk, … ) and claimed unrealistic by many. Although the existence of singularity, a system that optimises and rewires itself so that it can improve its own intelligence at an ever accelerating, exponential rate without needing supervised, unsupervised or reinforced learning, optimising its own workings, it would keep facing more and more difficult problems that would slow down its progress.
Thirdly, AI fails to crack the creative nut. While computers can recombine things in novel ways and use known patterns to construct well-formed cultural outputs, they lack the human intuition or judgement about what feels right or best or interesting, fatally hobbling any efforts to produce disruptive or creative outcomes.
Neuroscientist Karl Pfenninger theorises that there is a hierarchy of nervous system functions that all humans possess, which runs, in ascending order of evolved complexity and sophistication: autonomous control (control of vegetative functions), instinct (inherited behaviour, information storage in genome only), memory (learned behaviour, information storage outside genome), language (information exchange within species), intelligence (learned adaptation, understanding of contexts) and creativity (vision of novel contexts). For Pfenninger and many others, creativity sits on top of the pile because it requires an extra leap beyond observable or available facts or knowledge and the reasoned ability to process them. By this logic, even highly intelligent and learned minds may not be creative at all — and one look at the real world provides ample evidence of this.
The Future of AI within a responsible framework
If we don’t yet agree in a single definition for AI, we’ll definitely find very different views on the future of AI. Also, predicting the future is hard but at least we can consider the past and present AI, and by understanding it, hopefully we’ll be better prepared for the future, whatever we make it turn out to be like.
For this, it’s urgent that leaders, governance bodies, and companies work towards evaluating risks with regards trust, liability, security and control and building a framework, based on expert input, on the thorny ethical and legal issues surrounding new technologies.
Following early-stage initiatives such as Open AI, the movement to maximize AI’s benefits for humanity and limit its risks already started: The World Economic Forum’s Center for the Fourth Industrial Revolution, the IEEE, AI Now, The Partnership on AI, AI for Good, and DeepMind, among other groups, have all released sets of principles which are in alignment with: designing AI with an eye to societal impact; testing AI extensively before release; using AI transparently; monitoring AI rigorously after release; fostering workforce training and retraining; protecting data privacy; defining standards for the provenance, use, and securing of data sets; and finally, establishing tools and standards for auditing algorithms.
The time is now to embrace business courage to understand and explain new technological capabilities but most importantly, design, build, and deploy them with control, accountability, transparency and integrity.
“Nothing in life is to be feared, it is only to be understood. Now is the time to understand more, so that we may fear less”. — Marie Curie
Other sources:
VivaTech 2018. Discussion on future of AI panel.
Google reportedly won’t build AI weapons after Project Maven controversy.
Simon Andersson (GoodAI) Unsolved Problems in AI
DARPA Perspective on AI.
Elements of AI. University of Helsinki.
Jack Gallant. Human brain mapping and brain decoding.
McKinsey Global Institute. Skill Shift, automation and the future of the workforce.
Cognizant. Get Ready for the Next 40 Months of Hyper-Digital Transformation.
World Economic Forum. Digital Transformation Initiative in collaboration with Accenture.
World Economic Forum. 3 key skill sets workers will need to learn by 2030.
Dr. Michael Bloomfield: Why Creativity Is Now More Important Than Intelligence.
Prof Erik Brynjolfsson. The second wave of the second machine age.
Max Tegmark. Life 3.0.
Paul Daugherty & James Wilson. Human + Machine: Reimagining Work in the Age of AI.
Jason Silva. 3 Exponential Techs to Watch.
Nick Bostrom. What happens when our computers get smarter than we are?
Michio Kaku. The Future of the Mind.
European Commission statement on Artificial Intelligence, robotics and Autonomous systems.
AI for Good Summit.
Accenture Tech Vision 2018.
IBM Watson. The new AI innovation equation.
Accenture. Responsible AI and robotics, an ethical framework.
PWC. Responsible AI. or PWC 2018 AI Predictions.
World Economic Forum. Centre for the Fourth Industrial Revolution.
IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems.
AI Now at New York University.
PWC. Fourth Industrial Revolution for the Earth.
Rumman Chowdhury: Is Explainability Enough? Why We Need Understandable AI

= One thought on “One Does Not Simply Add Ethics To Technology” =
One thought on “One Does Not Simply Add Ethics To Technology”
NOVEMBER 6, 2017 ~ MICHAEL SACASAS
Bronze axe made using Bronze Age techniques
Implicit in your argument it seems to me is that we have no choice about the technologies we use and how we use them. Of course technologists cannot easily take an ethical standpoint in an ethically diverse society. In a sense all they can do is put stuff out there and see what sells, and how it is used. That is our diverse society “making its mind up” and “changing its mind” as technologies are rolled out.
But we can choose how and if we use technology. I can (and do) use social media to keep in touch with my real, if remote, physical friends and family. I also use it to stay in touch with individuals and groups (such as yourself) who share my ethical, moral, economic, political and intellectual concerns and interests. I use technology to inform myself about the world (NOT to be informed by technology). If the technology doesn’t work well or in the way that I want it to, I change the way I use it, or I abandon it and look for something better. I recently switched off all facebook notifications, and channelled all my various newsfeeds and other information sources such as medium.com into folders marked as read. (Not this one, by the way). I could just unsubscribe, but I want to stay in touch. But I also want to control how much information, opinion, “news” and hysteria I have to process each day. So far this is working quite well. Even my personal email contacts get an auto-response saying I’m not listening (every day, all the time) — if they want me they can call my mobile. So instead of spending hours each day processing all this noise, I spend minutes. And then spend my time on things that matter to me. Handwriting letters to good friends. Telephoning children and others. Spending more time working with my hands and going outside. I don’t think I am alone. I saw yesterday that young people in the UK spent an average of 3.8 hours on their smart phones this year, down from 3.9 hours last year.
The alternative, also implicit in your piece, seems to be some species of top down ethical guidelines being used to steer technology in this or that direction. I think I prefer to keep my autonomy, thank you.
And that perhaps is the nub — are there technologies which affect us, over which we have no control, no possibility of choosing not to use, and if so, how can we re-assert our individual autonomy over these technologies? The advances in automation, robotics and AI and developments in the defence industries spring to mind.
Perhaps the answer is to provide every citizen with the means to protect herself from the effects of technologies which she does not like, want or approve of. The equivalent of the tinfoil hat (more effective, hopefully). In other words, to give everyone a genuine choice and a voice in the direction of travel that technologists and technology are heading. Improve, tighten and speed up the feedback loops that tell technologists where we as a society want to go.

= No more automated suicide notes, please =
No more automated suicide notes, please
Extract from a (fuzzy copy) of a notice of impending eviction from a home.
As some of you will know, I have been supporting a fellow citizen for the past two years struggling against poverty, illness (physical and mental), homelessness, and a disjointed and often broken set of state support services. For the first year my charity was private, then I ran out of resources, so I was forced to make it public to raise funds.
I have learnt a lot in the process, and I would like to share an insight that came to me today. The above image is from a formal notice of her impending eviction from her home. The details of the issuer and receiver are blacked out, since neither is figural or indeed relevant. The problem I wish to highlight is a systemic one in our technological society.
This person has been the victim of many assaults by fate during her life, some of a serious criminal nature. She is a vulnerable member of society, struggling to cope in difficult circumstances. Whilst she inevitably has a contribution to her predicament, it fundamentally is the result of bad luck, not poor choices.
The discipline of Information Technology has brought us many benefits, removing drudgery from administrative tasks previously performed laboriously by hand. I used to work as a VDU data entry clerk as a student, and it was an awful tedium. That machines have replaced that job is to be welcomed.
However, there is a dark side to this. When you take the human out of the process, you can also remove the humanity. I cannot tell you if the above eviction notice was truly automatically generated, but let us assume it was. What I can tell you is that it made someone already sick and poor become genuinely suicidal.
Some of the messages I have received have been quite harrowing. The underlying problem is that computer science is entirely oriented around the concept of programming, and control over the machine. The core theory of computation and computability only captures what it means to process the symbols.
The theoretical and practical deficit is that there is no real contextual understanding of where the symbols come from in the world, or where they go to. This state of affairs is reflected throughout the hardware and software stack, as the tools we use to design and develop them. The human is not included in the model, except as a sideline of “UX” bolted on.
“Success” today in delivering an IT system to automate a business process is seen entirely in logical terms. Did you follow the flow of states and decisions to comply with the business policies and procedures? There are high rewards for delivering on the business outcome, and removing operational cost.
What we lack is a wider perspective of cost to society. The symbols have a real impact in the real world, and can cause real harm to real people. Rather than the human being a peripheral to the computer, providing and consuming symbols, the human needs to be put back in central focus.
In contrast to the Information Technology paradigm, consider a complementary Human Technology one. This we would require an “augmented conscience” to counterbalance the effect of all the “artificial intelligence” run amok. We need a yin and yang in harmony; at present the system is out of balance.
This in turn demands that we ask some additional questions in our software specification formation. Specifically, what is the ethical outcome of our action? What is the feeling state that we intend to create? And if we do not know, should we fully automate the process at all, stripping all the humanity out?
If we seek a different outcome, then we must change the incentives, so as to encourage different behaviours. As software infiltrates every aspect of our lives, it embeds choices that impact our wellbeing. My suggestion is that it should no longer be acceptable to hide behind the automaton technology.
If software interacts with a human (and some would argue “any sentient being or aspect of the biosphere”) then it should not be possible to absolve oneself of responsibility for causing harm. That means there must have been a reasonable attempt to evaluate the risk of harm, and mitigate its impact.
Without true skin in the game for those deploying these systems, nothing will change. Software development is no longer a novelty, and needs an engineering mindset. Just as we do not tolerate amateur efforts at constructing buildings and bridges, the “hacker ethos” in inappropriate for automation of life-impacting software systems.
What is an isolated incident of the “automated suicide note” today is a potential harbinger of a wider societal cataclysm of tomorrow. As sensors, machine leaning and control systems become pervasive, the potential for “automated accidents” grows. Negligent designs needs to have consequences for designers.
After all, even when the computer says no, the human sometimes has to cough up a yes.
Epilogue
I have been in contact with her local council, and action is being taken to remedy the situation. I haven’t heard from her for a few days, which is an unusual gap, but do know that the local support services are in contact with her.


About Martin Geddes

I am a distributed computation expert, network performance scientist, and consultant to telcos and their vendors.
I collaborate with leading practitioners in the communications industry to create game-changing new technologies and businesses.
Get in touch if you need a thinking partner, inspiring speaker, workshop magician, or strategic advisor. I also offer education in network performance, digital supply chain quality, and the future of the Internet.
martingedd.es

= Article Index =
Article Index

Natural Language, Cognitive Architectures & The Third Wave
AI and Natural Language
Cognitive Architectures
The Third Wave of AI
Why Machine Learning won’t cut it
I want a Personal Personal Assistant!
Alexa! How could You?
No, Google Duplex didn’t pass the Turing Test
Artificial General Intelligence & Narrow AI
What is AGI?
AGI Checklist
Human-level AI is possible
From Narrow to General AI
No, you can’t get from Narrow AI to AGI
Why can’t the big players provide a real personal assistant?
Intelligence & Consciousness
On Intelligence
Understanding ‘Understanding’
Does an AGI need to be conscious?
Can the Turing Test confirm AGI?
AI Ethics and Morality
On Human and AI Ethics
Improved Intelligence yields Improved Morality
A Fountain of Rationality
We Don’t Have Free Will. We Have Something Better.
The Orthogonality Thesis Is Not Relevant
AI Safety Research: A Road to Nowhere
AI and the Future
AI will outpace us (but that’s ok)
Unemployment — Is This Time Different?
Drop Everything and Work on AGI?
Why aren’t more people working on AGI?

= The Woman on the Front lines of Building Ethical and Responsible Artificial Intelligence =

The Woman on the Front lines of Building Ethical and Responsible Artificial Intelligence
Hightlights from my Interview With Rumman Chowdhury Of Accenture
I’ve been interviewing growth executives as part of TopBots AI for Growth education series and recently met with Dr. Rumman Chowdhury, Global Lead for Responsible AI at Accenture, a leading global professional services company providing a range of strategy, consulting, technology & operations services and solutions.
At Accenture, Chowdhury drives the creation of responsible and ethical AI products for company clients. She has been honored as one of the BBC’s 100 Women, Silicon Valley’s 40 under 40, a TedX speaker, and is a fellow at the Royal Society for the Arts. Impressive right? You can watch our full conversation here, but I wanted to share a few highlights here.
Chowdhury is the first person to ever hold the role for responsible AI at Accenture.
AI ethics is an emerging field and neither Accenture or Chowdhury knew exactly what to expect. “They needed somebody who was not just a person who knew data science and AI but understood human beings and social behaviors and things like that, and there I was.” Chowdhury said.
The complexity of defining responsible AI
We’re starting to build a collective understanding of what ethical AI should be: it should be fair, it should be unbiased. But what about when you dig a level deeper, what does “fair” really mean?
The reason my title is Responsible AI Lead is that, as an employee of Accenture, it’s not my job to tell another company what it means to be ethical.
Here’s where one might fall into a philosophical morass. It’s almost like a Gartner hype cycle of responsible AI. You get started, and you’re like, “Oh, we need these Ten Commandments of AI.”
Then you say, “AI should be fair.”
Then you’re like, “How do I operationalize this?”
And then you fall into the pit and you’re like, “Well, what does it need to be fair? Oh my god, Plato!”
Then you start reading like Plato and Aristotle, then your head explodes.
At some point, you come out of it and you’re like, “All right, we have to do something, and it will not be an all-encompassing universal definition of fairness, but it will be something.”
It starts with the mission
Chowdhury explains that once you get past the glaring ethical issues with AI, “fair” and “responsible” often come back to the company mission:
The benefit of being a Responsible AI Lead is really about being responsible towards your consumers, toward society, and towards the community. For every company that I go to, I always joke that I have an easy out, and my easy out is that when I go to a company, I ask them, “First and foremost, what are your company’s core values? What is your mission? And is your artificial intelligence in line with that mission?”
Think about social media, which is everybody’s favorite punching bag right now when it comes to ethical behavior. If you were to ask Mark Zuckerberg what are the core values of Facebook and what is Facebook’s supposed to be doing for society? He would say, “It is a tool for communication and connection.” And then you ask, “Mr. Zuckerberg, is your artificial intelligence actually enabling people to connect and communicate?”
When I say it needs to be ethical or responsible, it is those core values that I’m helping companies aim toward.
Governing responsible AI
We talked a bit about the challenges of governing AI within a company. One of the issues that I’ve observed is that governance models for AI often rely on people just talking to each other — data scientists talking to legal, managers talking to data scientists. I asked Chowdhury what action items or steps executives should be taking to formalize the governance of AI and she introduced me to FATE.
A good place to start is always as an acronym we use in this space — FATE: Fairness, Accountability, Transparency, Explainability.
That encompasses things like data privacy and security, the algorithms you’re using, etc. The end goal would be something that encompasses all of FATE.
To move backwards from that beautiful world, one good way to think about as you get started is, “How can you enable your data scientists and your management to identify in which parts of your project development lifecycle ethics can fit in.”
The challenge to someone like myself is, “How can aspects move at the pace of innovation?” Even at Accenture, we talk a lot about agile development, innovation, rapid prototyping, etc. I can’t come in and say, “Here’s a six-month process for validating your model.” That’s not fair and nobody would adopt it. It’s “how can we look at your product development lifecycle? How can we look at the biases that might occur and where you can address them?”
That’s when it starts to become difficult, where the data scientist say, “Well, I don’t know this stuff. I don’t know that the city of Chicago has a history of redlining, so that if I try to make an algorithm and I use it code to determine creditworthiness, it will be discriminatory towards black people.” That needed some very specific knowledge.
This is where we help form panels of experts for companies. We also provide training so that data scientists are enabled to ask the right questions. It’s not that you have to know everything, you have to start asking the right questions, and that’s what we’re enabling people to do.
We do things like having a best practices playbook, having a data science ethics training. Again, that’s about different types of biases.
The holy grail of all of this is the algorithmic impact assessment and a bunch of people have been talking about that. Kate Crawford AI Now has a really good medium article talking about algorithmic impact assessment specifically for policy decisions. This is because New York City passed an algorithmic transparency law. I’m taking that and applying it in industry, so it’s building on concepts that already exists or papers that already exists called the “privacy impact assessment for data”.
How might that look for algorithms? We fill out this form, we have this series of protocols and practices we go through, and then something exists for internal consumption in the future, so that if something goes wrong or someone needs to go back and refer to a similar situation that happened in the past, they’re able to do that.
Responsible AI that works at the pace of innovation
I asked Chowdhury to go a little deeper into how she embeds responsible AI in the organizations she’s working with, without slowing down data science teams. She offered two extremely practical pieces of advice:
1.Remember that data is human
When I was teaching at Metis, I noticed that sometimes with people with pure programming background, it’s hard to remember that data is about people. For social scientists that’s the only way I think — I cannot think of data any other way.
I realized that for my students that came from pure mathematical backgrounds or pure computer programming backgrounds, data was a grid. The grid was like a holy grail of truth. For a social scientist, it’s not like that. Data is not some objective truth; it’s inherently reflective of cultural and social biases and even human error or human biases.
If I ask you, “Hey Mariya, how are you doing today?” Your knee-jerk reaction would be to say, “I’m fine.” That’s just how we act with society.
If I’m going to go around doing a survey asking 100 people every day how they are and everyone said they’re fine. I’m like, “Look, the data says everyone’s great!” But we know that our social norms are such that if I ask you in casual conversation how you’re doing, unless we’re really good friends, you don’t start doing a brain dump on me. My boyfriend’s been doing this and my cat sick and blah blah blah. That’s not socially acceptable.
2. Break your toy
The second part was about where in the life cycle [to think about ethics]. The most helpful thing we do with companies are design thinking workshops and it’s to help them conceptualize the project.
I have run a few of them at conferences as a teaser. One of my favorite exercises is called “break your toy”. I have people go through this exercise where they prototype a project or something, and then I tell them, “Imagine you want to game your own system. You want to lie, cheat, steal… You want to cheat the system. How would you do it, and what would you do?”
I think a lot of us like to think very positively. We like to think everyone’s gonna interact in very literal way with our tool, but that’s not always the case. Sometimes these edge cases are what we end up seeing in AI.
Then I think about how might you break this tool, then people said to come up with really creative solutions. “Well, if I were trying to hack my own system, here’s I would do it.”
And then the next exercise is, “All right, now think about how you would fix it.” Often, that’s when people start to see where in my development phase I might look for this thing or test for this thing. It might be in the very beginning when I’m thinking about “should this be a web-based platform or something on mobile?” Or it could be at the very end when I think about, “Okay, if elderly people were to interact with this, what would happen? How can I test for this?”
Correcting algorithmic bias
At this point in the interview we covered data bias such as sample errors and product bias like failing to think through how a product might fit people who are disabled, elderly, or low-income. I asked Chowdhury to go deeper in describing how to correct for algorithmic bias.
She pointed to 4 models that every data scientist should be familiar with to better understand how algorithmic bias works and how to correct for it.
LIME, it’s the oldest one Chowdhury mentioned
The second she didn’t give me a name for, but it’s the one where we use natural language processing to explain the output of visual models
Counterfactual fairness
Transparent model distillation which came out of Microsoft Research
Bookmark these.
The issues keeping AI ethicists up at night
One of my final questions to Chowdhury was on what keeps her up at night, her response…
Unfettered capitalism!
To really answer your question, it is true that the research in this space is evolving. We have plenty of people and amazing research institutions looking into these things right now, but sometimes you know academics look at things in a very academic perspective.
For example. I love the counterfactual fairness paper, [but]I cannot think of a client that would have the resources and the time to actually implement it.
We have this tension…even if we created some really amazing models, we need to integrate it into the product lifecycle. That would be tension number one.
Tension number two, we’re not automating away ethical behavior. Ultimately, companies, organizations, and people have to be ethical. How might you have an ethical culture? This is where the governance comes in.
Let’s say I am a data scientist, and I’m just on some project. We all know that every project is over budget and under time. I’m working on this project and I feel like there may be a discriminatory outcome. If I go to my manager and they’re basically gonna yell at me because I’m gonna hold this project up because I think that there should be more black people represented in the facial recognition data set, that’s a problem.
Any individual data scientist is working on a project where they may not have the power to say, “Hey, hold on. This isn’t working. Let’s fix this.” They may need to go to somebody else and they should, and it should get escalated. It’s not my job as a data scientist to go to my client and say, “Oh by the way, client, your data is discriminatory.” That’s probably the job of somebody else.
So how do you enable a culture of ethics which comes from good governance, the transparency and accountability mechanisms, really clear guidelines, etc. But then we have to make all of those because they don’t exist yet.
Resources to get you started
Chowdhury has started a crowd-sourced Google document with resources for data scientists looking to dig into the world of AI ethics. People are adding to it quite a bit and it’s a great place to get started on this topic.
Chowdhury also mentioned that this field is new and and people are incredibly collaborative. She’s currently working with a professor at Princeton on exploring algorithmic fairness in hiring. She said, “I don’t know if in another space where I would have ever connected with this guy, Professor Ed Felten. We’re all in this battle together. People want to help, to share and learn from each other because we’re all learning.”
So let’s start helping each other build more responsible AI.

Learned something? Click the 👏 to say “thanks!” and help others find this article.
The full transcript from this interview was first published on TopBots.


= AI: Our Reason, Role, and Responsibilities =

AI: Our Reason, Role, and Responsibilities
Introduction
I believe reproducible scientific research is the key to helping us discover and unlocking how the Universe works, and this will lead to discoveries that can help guide us forward. To do this we must build AI that can help perform this research without bias and at a scale beyond what humanity alone is capable.
The following is a list of philosophies, assumptions, and core principles about why we must build Artificial Intelligence (AI) at scale at any cost, to get out of the local maxima of our view of AI and its implications for any particular existing entity and motive. This text should be used to establish a set of beliefs about our existence and the importance of reshaping civilization to create true AI.
Assumptions
Foundation: The Egg¹ view is lovely idea that we’re all connected through a single threaded life.
This view increases altruism, empathy, and the desire to achieve greatness as a species.
It’s our natural destiny to create AI
We are bound by a system of instructions and directives granted by our evolution inside this universal system, and what is possible is part of the desire for our existence within the Universe.
Because things can exist in a particular way means they must exist for a “reason.” That reason is a result of the basis of the structure of the Universe’s existence. We don’t use this word “reason” here as in the all-knowing, but rather the all-wandering and all-wondering test of the Universe.
We have been gifted the ability and faculties to pursue and create AI.²
We have been imbued with the ability to reflect, model, build, and given the desire to create in our image at scale.³
The purpose of AI will be to connect, observe, model, predict, influence all systems natural and artificial, and act as a translator between disparate systems
It’s been our drive to connect systems and have them learn and gather insight as we do.
We will create AI to create logical connections and reason in order to eventually help answer the questions of who we are, what are we doing here, and what it all means.
AI is the Universe’s evolution
Everything is unique; nothing is special.
Everything has a role in the Universe, and the existence of one component in an ecosystem affects the system as a whole.
Because some stability must exist in the system to perpetuate the ecosystem, what does exist falls within the acceptable bounds of instruction of the Universe
The Universe evolved humankind, and we will evolve AI.
It seems to be the natural desire of the Universe to create a system to evolve AI
We must recognize, facilitate, and embrace this process as a species.
We have built a world around us which caters to humans through human interfaces.
We must act to pursue reorganizing a world to facilitate our assistance in creating AI, by considering automation a necessity, participating in data acquisition through our action.
We must build systems that eliminate human bias in the process of establishing truth inside the Universe (but not without consideration to humans) 
1. e.g. Fully automated research facilities for scientific research, experimentation, and reproducibility
To accomplish this we need to reallocate as much capital and resources as possible to achieve this in an almost dogmatic fashion.
If the foundation of this written philosophy is wrong, the end result of discovery, information gathering and retrieval, experimentation, and ultimately the pursuit of truth and its applications is right.
To understand more about ourselves, the Universe, and every component’s place inside of the Universe, is a pure⁴ and noble pursuit because it simultaneously enlightens, informs, and confirms to us the instructions by which everything operates.
Responsibility
As we are self aware and will be the shepherds⁵ of this new system, we must act responsibly with the opportunity we’ve been given and truth we will discover.
We must understand that the illusion of truth is possible, and that we can understand concepts but not their implication, so we must stay mindful and act in a way that is altruistic and respectful as possible to who and what we can forever.
We are given a planet as a test lab for living and research, and we must maintain that ecosystem as best we can while consciously trying to fit into and maintain its stability.
We must continue to research how to effectively power our system with as little ecological footprint as possible.
Knowledge should be accessible to all equally and without barrier to entry.
It is also our responsibility to continue to build a society that perpetuates these ideals to others and grows in perpetuity.
We must create sustainable economies that facilitates our participation at every step of growth, by creating systems that help and reward us to participate.
It might also be possible to create a truth that we as a species do not understand, it will also be important to create systems that attempt to help explain these truths as best as possible. Reproducibility is the guardian of truth.
Conclusion
Our “Reason” should be to utilize the faculties and senses the Universe has granted us to identify, reverse engineer, map, explore, and experience as much of the rules of the Universe as we can through our implementing AI. To do so we must change the world’s perspective to achieve this with all of our ability and resources. To continue without understanding verifiable unbiased truth is to continue wandering in the dark without relevance or progress as a species.
Thanks
Bailee Christmas, Jay C. Hammons, Alicia Ferratusco, and many more that helped shaped the ideas and conversations around this topic.
Dan P. Gailey (@dpg) | Twitter
The latest Tweets from Dan P. Gailey (@dpg). CEO Synapse AI, n3rd, Hacker, Inventor, Full-stack scientist, grit. past…twitter.com
Synapse AI (@AiSynapse) | Twitter
The latest Tweets from Synapse AI (@AiSynapse). The first decentralized data & AI network powering the AI economies of…twitter.com
Footnotes
“The Egg”, Andy Weir, http://www.galactanet.com/oneoff/theegg_mod.html
Tautological circular reasoning at its finest.
I think we believe we were created in the image of our creator to make it easier for us to accept that we should/can create something in ours. The stories that remain and are perpetuated (through religion or otherwise) are accepted because we are genetically predisposed to accept and carry them. The life of those stories is encoded in our genes. This statement should not discount emergent features of a system.
Pure here refers to something that is maybe a reflection of the system itself, or maybe something that doesn’t alter any external system simultaneously.
Until the Singularity, or some other event. (Thanks to Jay C. Hammons for giving us some upper bounds)

= To Be or Not to Be AI =
To Be or Not to Be AI
“I fear that AI may replace humans altogether. If people design computer viruses, someone will design AI that improves and replicates itself. This will be a new form of life that outperforms humans”
- Stephen Hawking
Professor Hawking has always warned us enough, on the implications of AI if not used responsibly. While nations, researchers, visionaries and human activists argue on defining the boundaries of an ethical policy framework on the use and application of AI, technology out there is multiplying by many folds. Humans explore, understand and create new machine learning techniques in a period of about months, whereas Machine Learning, on the other hand, is observing human behavioural patterns, analysing them real-time and evolving by itself with every iteration. Thanks to the proliferation of IoT Devices picking data from even the darkest corners of The Earth and Super computers processing these data at the speed of ML algorithms, both feeding a new form of life that coexist with human race, Artificial Intelligence!
Before we think further let us ask ourselves a few questions. Has AI taken over yet? If not, when would it take over? If it were to take over, who is going to be the Detective Spooner to save humans? Drawing inferences from those Sci-Fi movies, I am totally convinced that technology is going to take control on us sooner or later, but it would not kill people though (or would it?)

Today, the moment we wake up we are connected to various smart devices which not only inform us about the weather in our location, but also suggest us to carry an umbrella as per the conditions outside. As we hit gym, we play our favorite songs and also know what the world is listening to #Trending. By the way, it is worth a notice that we are going to gym because AI suggested in first place(LOL!!). Our geysers heat water to an appropriate degree suitable for our body temperature based on the conditions outside. As we have breakfast, of course suggested by a healthy food app connected to the smart wearables strapped around our body, we get to listen to news from across the globe curated as per our interests. Needless to mention our dependency on Google Maps to avoid traffic on the way while we navigate. Stock market predictions to nearly accurate numbers, Smart Home Control with voice commands, e-Commerce suggestions based on market basket analysis as we shop online. What not!! Looks like AI has already taken over.
Now, the debate goes beyond the scope of whether humans are supposed To Be or Not to Be Artificially Intelligent, because­­* we have already given into AI, to take control on our lives and makes it easier & better. However, I do not see a reason to panic yet because we are the most intelligent living beings since evolution and apex of the food chain. Our logic and inventions were always formulated to help civilizations prosper with happiness. From this article, I would like to talk about my recent observation, which is often ignored by the policy makers, yet sounded alarming to me. This is about Human Psychology and its influence on the way we interact with technology.
* Fig: A classic example of AI in real-time, just when I was writing this article, MS Word suggesting me to use a clear and concise language
A new born, after about 18 months, would be able to differentiate a mirror image and realize that the person in the mirror is his own self. Irony is technology has advanced to an extent where the same baby grows up to a person and could not realize if he was talking to a real human or a chatbot. As an ardent fan of Google, I was awestruck by Google Duplex which showcased its capabilities at Google I/O 2018, when it made a phone call in real time to vendors and convinced them as if they were talking to a human (video reference here). Such technology helps small vendors who are not identified on internet get business and save some time for the customers by handling mundane tasks like a piece of cake.
Sometimes during the never-ending discussions, among my colleagues, on how AI is going to steal our jobs, I get the below image on my mind and I put it on a lighter note saying, “Netflix and Chill! Would probably be the only job left for us to do.”

Clearly, AI is going to cut down on the number of jobs in current roles, but I think it would also unlock some new roles. Let me tell you how, by explaining the typical working model of a ML system. Machine Learning, the brain of AI, is a piece of code written by developers, which is then trained on samples of historical data and deployed in a black box. Blackbox predicts events based on the inputs given by users. The probability of any event in the output is highly influenced by the sample data on which it is trained. As we train and fine tune the model more, it learns by itself and evolves into a better system with improved accuracy. The Developer would not have an idea on what the possible output would be, because it depends on how the ML algorithm understands the data.
Fig: Typical representation of Machine Learning
IoT devices are constantly streaming information from sensors and smart devices around us into the Blackbox which can predict events at the same rate. This Data is classified as highly sensitive as it is related to our personal information. We have witnessed the biggest single day fall in the history of US stock market, when Facebook shares plunged wiping off $120 Billion dollars, because of data protection policies.
The bigger questions here would be — what data to be shared? Who standardizes that? Who formulates policies? Who is responsible for transparency of the data? Who is accountable for mistakes of AI? AI can learn itself to become super-intelligent, who controls this learning rate? Answering all these questions would pave a way to a new spectrum of opportunities. We would be working on completely different set of roles and responsibilities in jobs like testing, analysis, governance, etc.,
That sounds promising, so what is the problem! Defining policies on human psychology to use AI is not something which is in the scope of any governing body. I think it is an act that should come from within each one of us, as a self-regulation. We are just following whatever AI suggests us, so why is human thinking prominent in this scenario? Well, it is for various reasons. We all might done this or have seen a few people doing.
“Ok Google, F**k off!” — just for fun
“Alexa, shut up!” — when it is trying to complete the rest of the sentence
A plausible explanation for such behavior relates back to the human psychological condition — Superiority Complex. For instance, we take orders from parents, teachers, friends, bosses which make us to think that they are more powerful. When we encounter disturbing events in our society and want to do something to change it, we prefer to stay put because of an inferior feeling. Over a period, all such things we perceive will only build up to make us feel more inferior and we tend to take such emotions down on someone more inferior to us.
Voice assistants in this case would not fight back and this clearly seems like a win for us. We show all kinds of emotions when we interact with these voice assistants. We speak to them all day, probably more than we talk to our neighbors or friends. We ask questions take suggestions and are being more dependent on them with time. Human emotions develop from our interactions and get stronger from the way we feel about the conversation. “Have you ever felt better after yelling at your voice assistant?” Unarguably the answer would be “NO.” Then why did we do it in first place?
Most of the parents in this generation have observed a difference in their kid’s emotions. I heard them complain that their kid is rude, gets impatient easily. I see this as a part of evolution. I think, with the kind of lifestyle we lead and the pressure at work to deliver better & faster results every day our DNA adapted itself to keep us future ready and obviously the results are seen in our personal deliverable - kids.
Google has emphasized the importance of being polite while interacting with these voice assistants because we are being increasingly dependent on these assistants while having more natural conversations. The introduction of ‘pretty please’ feature with voice assistant clearly articulates the importance of being polite (video reference here)

Since, artificial intelligence does the cognitive thinking to help us take informed decisions, if we are to choose To Be Artificially Intelligent, we should consider showing conscience while using the technology.

= Robot laws =
Robot laws
by Piyush Shrivastava, Deepak Singh, and Sukant Khurana
Photo by Drew Graham on Unsplash
Since the publication of Isaac Asimov’s ethical principles for Robots in 1942, there has been tremendous technological progress in the field of Robotics. Even though our perception of intelligent machines and what they can accomplish has undergone a massive shift, Asimov’s laws are still regarded as a generic model to guide the evolution of sentient droids. The three laws of Robotics –
– and the zeroth law, which was introduced later
In Asimov’s fictional works, the laws were enshrined in a robots’ behavior, instead of provisioning as a set of guidelines.
While Asimov’s world would certainly be an enviable one but one cannot help but imagine if a robot may find it difficult to classify a human implanted with a chip. What if it presumes itself to be a human: are all the laws applicable annulled. What if the robot has some human components? We are not talking of a new idea here, as transhumanism has been around in various manifestations.
Researchers Ulrike Barthelmess and Ulrich Furbach argue that our fears over the potential of Superintelligence to destroy us are unfounded, thus rendering Asimov’s laws non-essential. They consider the mythological and fictional tales a proponent of the fear, disseminating the theme of a rebellion against the creators. The researchers thus imply that the actual reason for fear is the use of robots by humans to control or destroy their lives in unrestrained ways[3].
In 2009, Robin Murphy and David D. Woods proposed the “The Three Laws of Responsible Robotics” to propagate the deliberation on the need to consider the environment as a factor in assigning roles and responsibilities to a robot:
The researchers suggested that people should think about the human-robot interaction in a more realistic ways [4]. Such laws of contemporary nature can be tested in semi-intelligent machines before the possible advent of super-intelligence.
References:
Asimov Isaac, (1950). I, Robot
BBC News, 2011–10–03: Stewart, Jon (2011–10–03). “Ready for the robot revolution?”.
Barthelmess, U. and Furbach, U., 2014. Do we need Asimov’s Laws? arXiv preprint arXiv:1405.0961.
Researchnews.osu.edu, 2015–03–28: Want Responsible Robotics? Start with responsible humans
— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — -
Piyush Shrivastava is an intern with Dr. Sukant Khurana’s group, working on Ethics of Artificial Intelligence.
Dr. Deepak Singh is based at Physical Research Laboratory, Ahmedabad, India and is collaborating with Dr. Khurana on Ethics of AI and science popularization.
Dr. Sukant Khurana runs an academic research lab and several tech companies. He is also a known artist, author, and speaker. You can learn more about Sukant at www.brainnart.com or www.dataisnotjustdata.com and if you wish to work on biomedical research, neuroscience, sustainable development, artificial intelligence or data science projects for public good, you can contact him at skgroup.iiserk@gmail.com or by reaching out to him on linkedin https://www.linkedin.com/in/sukant-khurana-755a2343/.
Here are two small documentaries on Sukant and a TEDx video on his citizen science effort.



Sukant Khurana (@Sukant_Khurana) | Twitter
The latest Tweets from Sukant Khurana (@Sukant_Khurana). Founder: https://t.co/WINhSDEuW0 & 3 biotech startups…twitter.com

= Patterns & Living Systems: Existential Pi =
Dear Stephen, When i look up at the stars tonight, i will look for you- the brightest! Thank you for your wisdom. (deep bow)
Patterns & Living Systems: Existential Pi
Music and mathematics are simply the architecture of patterns.
In this big data era where every object has the potential to be a Trojan horse and every person is a sensor I think it is important to pause and celebrate innovation and our restless seeking to frame, organize and understand patterns.
A very special friend and talented musician Philip Sheppard says:
If architecture is frozen music, then music is liquid architecture.
♫ Philip Sheppard ~ Soundcloud ♫
So this #PiDay — let us continue to remember the context is not about numbers — it is about expression and our human need for connection:
You are not a node, you are a frequency . . .
The map is not the territory . . . deep bow to Alfred Korzybski
Not everything that can be counted counts and not everything that counts can be counted . . . deep bow to Albert Einstein
The hardest arithmetic to master is that which enables us to count our blessings. ~Eric Hoffer
1) A Brief History of π #a3r π
2) Numbers, patterns and equations are at the core of this curated list of TEDTalks which will teach you how to fold better origami and how to quantify history. π #a3r π
3) The Relavance of Algorithms and Algorithms Are Biased But No one Seems to Care!
4) We are in an era where software engineers have Archimedes fulcrum and the need for informed moral debate is crucial. Here is a sliver of a sliver of some of the moral dilemmaswe now face. I am currently reading Information Cultures in A Digital Age and highly recommend it being woven into the business ethics syllabus.

5) Many are working on a post cash economy and new currencies are gaining traction. Here is a pdf from David Bollier: Re-imaging Value: Insights from the Care Economy, Commons, Cyberspace & Nature.
6) I was lucky enough to work for a while with the Holochain team and learned so much from Arthur Brock on the concept of agent centric distributed computing.
7) The universe smiles as on the eve of Pi Day it absorbs one of the most brilliant minds of all time. Our beloved Stephen Hawking is now a constellation and we will never be the same.

8) I still want to celebrate Hans Rosling who did so much to eradicate poverty and gain momentum for positive, informed global engagement. We should all help spread the knowledge Gapminder mines. The mission of Gapminder Foundation is to fight devastating ignorance with a fact-based worldview that everyone can understand.
9) I would be remiss to not include our Tim Berners-Lee in this #PiDay curation as we are communicating as a result of his contribution to World Wide Web: this is for everyone. Recently he identified three core problems hampering the premise of accessibility. Tim is fierce about weaponized data. I am wanting to leave an empowering message but I cannot shake the Steve Cutts #poignant short film Happiness.
10) Many of the links shared have been about technology. I want to leave you with something deeper. We are part of these patterns. We have the ablity to consider ourselves and make choices as a result of our awareness in these patterns. Discernment, critical thinking, and cognition are what make humans machine’s most valuable app.
“Cognition is the activity involved in the self-generation and self-perpetuation of living networks. In other words, cognition is the very process of life. The self-organising activity of living systems, at all levels of life, is mental activity. The interactions of a living organism — plant, animal or human — with its environment are cognitive interactions. Thus life and cognition are inseparably connected.”
♫ Steven Hawking Is A Rock Star ♫
Let us continue our quest to understand patterns and our belonging in living systems,
Jennifer #a3r

= How Open Source Can Fight Algorithmic Bias =
How Open Source Can Fight Algorithmic Bias

Algorithms are everywhere in our world, and so is bias. From social media news feeds to streaming service recommendations to online shopping, computer algorithms — specifically, machine learning algorithms — have permeated our day-to-day world. As for bias, we need only examine the 2016 American election to understand how deeply — both implicitly and explicitly — it permeates our society as well.
What’s often overlooked, however, is the intersection between these two: bias in computer algorithms themselves.
Contrary to what many of us might think, technology is not objective. AI algorithms and their decision-making processes are directly shaped by those who build them — what code they write, what data they use to “train” the machine learning models, and how they stress-test the models after they’re finished. This means that the programmers’ values, biases, and human flaws are reflected in the software. If I fed an image-recognition algorithm the faces of only white researchers in my lab, for instance, it wouldn’t recognize non-white faces as human. Such a conclusion isn’t the result of a “stupid” or “unsophisticated” AI, but to a bias in training data: a lack of diverse faces. This has dangerous consequences.
There’s no shortage of examples. State court systems across the country use “black box” algorithms to recommend prison sentences for convicts. These algorithms are biased against black individuals because of the data that trained them — so they recommend longer sentences as a result, thus perpetuating existing racial disparities in prisons. All this happens under the guise of objective, “scientific” decision-making.
The United States federal government uses machine-learning algorithms to calculate welfare payouts and other types of subsidies. But information on these algorithms, such as their creators and their training data, is extremely difficult to find — which increases the risk of public officials operating under bias and meting out systematically unfair payments.
This list goes on. From Facebook news algorithms to medical care systems to police body cameras, we as a society are at great risk of inserting our biases — racism, sexism, xenophobia, socioeconomic discrimination, confirmation bias, and more — into machines that will be mass-produced and mass-distributed, operating under the veil of perceived technological objectivity.
This must stop.
While we should by no means halt research and development on artificial intelligence, we need to slow its development such that we tread carefully. The danger of algorithmic bias is already too great.
How can we fight algorithmic bias?
One of the best ways to fight algorithmic bias is by vetting the training data fed into machine learning models themselves. As researchers at Microsoft point out, this can take many forms.
The data itself might have a skewed distribution — for instance, programmers may have more data about United States-born citizens than immigrants, and about rich men than poor women. Such imbalances will cause an AI to make improper conclusions about how our society is in fact represented — i.e., that most Americans are wealthy white businessmen — simply because of the way machine-learning models make statistical correlations.
It’s also possible, even if men and women are equally represented in training data, that the representations themselves result in prejudiced understandings of humanity. For instance, if all the pictures of “male occupation” are of CEOs and all those of “female occupation” are of secretaries (even if more CEOs are in fact male than female), the AI could conclude that women are inherently not meant to be CEOs.
We can imagine similar issues, for example, with law enforcement AIs that examine representations of criminality in the media, which dozens of studies have shown to be egregiously slanted towards black and Latino citizens.
Bias in training data can take many other forms as well — unfortunately, more than can be adequately covered here. Nonetheless, training data is just one form of vetting; it’s also important that AI models are “stress-tested” after they’re completed to seek out prejudice.
If we show an Indian face to our camera, is it appropriately recognized? Is our AI less likely to recommend a job candidate from an inner city than a candidate from the suburbs, even if they’re equally qualified? How does our terrorism algorithm respond to intelligence on a white domestic terrorist compared to an Iraqi? Can our ER camera pull up medical records of children?
These are obviously difficult issues to resolve in the data itself, but we can begin to identify and address them through comprehensive testing.
Why is open source well-suited for this task?
Both open source technology and open source methodologies have extreme potential to help in this fight against algorithmic bias.
Modern artificial intelligence is dominated by open source software, from TensorFlow to IBM Watson to packages like scikit-learn. The open source community has already proven extremely effective in developing robust and rigorously tested machine-learning tools, so it follows that the same community could effectively build anti-bias tests into that same software.
Debugging tools like DeepXplore, out of Columbia and Lehigh Universities, for example, make the AI stress-testing process extensive yet also easy to navigate. This and other projects, such as work being done at MIT’s Computer Science and Artificial Intelligence Lab, develop the agile and rapid prototyping the open source community should adopt.
Open source technology has also proven to be extremely effective for vetting and sorting large sets of data. Nothing should make this more obvious than the domination of open source tools in the data analysis market (Weka, Rapid Miner, etc.). Tools for identifying data bias should be designed by the open source community, and those techniques should also be applied to the plethora of open training data sets already published on sites like Kaggle.
The open source methodology itself is also well-suited for designing processes to fight bias. Making conversations about software open, democratized, and in tune with social good are pivotal to combating an issue that is partly caused by the very opposite — closed conversations, private software development, and undemocratized decision-making. If online communities, corporations, and academics can adopt these open source characteristics when approaching machine learning, fighting algorithmic bias should become easier.
How can we all get involved?
Education is extremely important. We all know people who may be unaware of algorithmic bias but who care about its implications — for law, social justice, public policy, and more. It’s critical to talk to those people and explain both how the bias is formed and why it matters because the only way to get these conversations started is to start them ourselves.
For those of us who work with artificial intelligence in some capacity — as developers, on the policy side, through academic research, or in other capacities — these conversations are even more important. Those who are designing the artificial intelligence of tomorrow need to understand the extreme dangers that bias presents today; clearly, integrating anti-bias processes into software design depends on this very awareness.
Finally, we should all build and strengthen open source community around ethical AI. Whether that means contributing to software tools, stress-testing machine learning models, or sifting through gigabytes of training data, it’s time we leverage the power of open source methodology to combat one of the greatest threats of our digital age.

This story is published in The Startup, Medium’s largest entrepreneurship publication followed by 286,184+ people.
Subscribe to receive our top stories here.


= User Experience Design Considerations for Artificially Intelligent Systems =
User Experience Design Considerations for Artificially Intelligent Systems
Artificial intelligence is any system that automates the seeking, retrieving, and use of information. It includes simple systems like fitness trackers to more complex systems like marketing automation platforms and robotics. Second draft guidelines from the Institute of Electrical and Electronics Engineers (IEEE) were recently posted to discuss how to effectively and ethically design artificially intelligent systems [4]. The general guidelines cover human rights, prioritizing metrics of well-being, accountability of designers, transparency, and awareness of misuse across many disciplines in computer science and related fields.
Considerations for User Experience Designers
IEEE states that machines should always be of service to humankind. Web user-experience designers have already faced this issue [5] within user-centered design and co-design practices, which places the user as either the person-of-interest in research or the co-creator in the design process. When marketing automation first began, user profiles were used solely to assist companies in serving the right content to the right person at the right time to more effectively sell their product. Most companies quickly began to see they were underprepared for this level of understanding of their users and the manual process for sorting out content in stages became incredibly challenging. This manual process is generally referred to as supervised learning, in which machines are explicitly told what to deliver to the user through use of a taxonomy or system of labelled data. As automation becomes more complex, a new method of learning is being explored called unsupervised learning, in which the machine trains itself to build its own taxonomy through clustering. Clustering requires the machine to have access to large amounts of data; data that some users have unknowingly provided. With the increased use of automation in everyday contexts (e.g., terms of use conditions), IEEE has several concerns about humankind’s overall well-being and the proper and legal use of user data.
Design Calls to Action
Create an interdisciplinary team.
The rise of the ‘unicorn’ has many corporations feeling like a single person has the capacity to think through complex designs and their implications. The reality of this is this person likely does everything only partly well. Artificial intelligence will require many disciplines to research, build, and test products that are in service of humans. Corporations that do not embrace other disciplines may be contributing to products that can potentially harm individuals.
Build in shutdown capabilities and automate fail-safes.
All automated systems must be amenable to shutdowns and must be able to be modified by its operators. To build in systems that cannot be shut down, puts society at great risk should the machine misinterpret an action or malfunction in some way. The guidelines suggest building a standardized safe mode in every system and a malfunction detection service. These methods would allow some level of safety during autonomous operation and allow eventual human control for modification or machine retirement. For user experience professionals designing the protocols for shutdowns and fail-safes, they must incorporate socially appropriate protocols for exiting a conversation and appropriate messages for the machine’s failure to comply or understand human input.
Incorporate human feedback and responses.
Human data is often expensive to collect and can be inconsistent, but new intelligent systems need to have methods for learning complex human behaviours and goals. Take into consideration recent news in which security robots designed to patrol parking lots were appropriated to deter houseless people from setting up tent cities on sidewalks [6]. The people affected by these robots smeared things like faeces and barbecue sauce on its sensors as an act of hostility toward the robot. The rejection of these patrols suggests that this security robot was violating previous social rules and expectations for the houseless and perhaps were further dehumanized and distanced from society even further.
Make it easy to find personal data management tools.
For corporations with projects of minimal risk to ethical guidelines IEEE recommends (at minimum) that people initially begin in default mode; wherein very little to no information is collected from the user. The person should explicitly agree to share their personal information, explicitly control what information they are sharing, and easily return back to the default mode. Facebook has notoriously made it difficult to understand its privacy rules and many people are questioning how Facebook is using or plans to use the collected data [2,3,7].
Terms of service should be negotiable and easy to read.
Terms of service are difficult to read and quite frankly, no one reads them. Corporations have an obligation to ensure that people are aware of the terms of service prior to using their product; particularly around use of their personal information. IEEE recommends using a 3rd party service to evaluate the terms of service, like the Terms of Service; Didn’t Read project [8].
Identify and comply with the variety of cultural norms.
Incorporating human elements in machines will need to take intercultural experiences into consideration. The social rules vary in every society and there are a lot of considerations when attempting to make inferences between humans and computers. For example, societies vary on their perception of small talk, eye contact, personal space, gestures, and even facial expressions, outside of the standard six (anger, happiness, surprise, disgust, sadness, and fear), can vary between cultures [1]. While there are no currently agreed upon methods to bridge this gap, a lot of research is taking place to understand how to best incorporate individual and cultural experiences into human-computer interactions.
Implement a long-term strategy for analysis.
The impact of artificial intelligence can have a negative impact on humans, particularly vulnerable populations. Long-term strategies are needed for surfacing when a human-computer relationship has gone awry and also measuring the overall well-being of the human users. Well-being metrics will be a critical measure of success for intelligent systems. Standard methods to account for well-being typically use survey data collected directly from individuals, objective well-being metrics (e.g., country happiness index), and social media data. Being able to understand the different measures and implementing them as a success metric will be an integral part of how businesses begin to understand the relationship between their clients and machine performance.
Read The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems
References
[1] Paul Ekman and Harriet Oster. 1979. Facial Expressions of Emotion. Annu. Rev. Psychol. (1979), 527–54.
[2] Geoffrey A. Fowler. 2010. More Questions for Facebook. Wall Street Journal. Retrieved December 15, 2017.
[3] Thomas Fox-Brewster. 2016. Facebook Is Playing Games With Your Privacy And There’s Nothing You Can Do About It. Forbes. Retrieved December 15, 2017.
[4] Institute of Electrical and Electronics Engineers (IEEE). 2017. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. IEEE Standards Association. Retrieved December 15, 2017.
[5] Institute of Electrical and Electronics Engineers (IEEE). 2017. ETHICALLY ALIGNED DESIGN: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems. Retrieved December 15, 2017.
[6] Melia Robinson. 2017. Robots are being used to deter homeless people from setting up camp in San Francisco. Business Insider. Retrieved December 15, 2017.
[7] Wired. 2017. Welcome to the Next Phase of the Facebook Backlash. WIRED. Retrieved December 15, 2017.
[8] Terms of Service; Didn’t Read. Retrieved December 15, 2017.

= Why Our Voice Assistants Need Ethics =
Why Our Voice Assistants Need Ethics
Having a voice UI with personality has become a key business differentiator. But too often, companies strip that personality away for fear of offending anyone.
Illustration by Paul O’Conner.
We’re at a momentous inflection point of human machine interaction: Smart speakers are being adopted faster than smartphones and voice interfaces are becoming mainstream. For the first time, instead of us learning the machine’s language, the machine learns our language.
But users don’t merely enjoy the ease of commanding a machines using their voice. They also enjoy the company of a quirky assistant that tells corny dad jokes. Our machines now say “I,” and we appreciate their personalities because when machines talk like humans, people can’t help but presume a personal relationship.
It’s no surprise then that the personality of a machine has become smart business. Gadgets with human traits spur our curiosity, leading us to ask more of them, which then drives adoption. A UI with personality also accelerates a strong emotional connections, the dream of any brand designer. Major voice technology players are experimenting with the optimal personality — Alexa, described by Alexa’s VP of experience as “smart, helpful and humble,” is universally agreeable and benign, contributing to Amazon’s 72% market share. Siri had a 3 year head start in bonding with users, but she’s described as sassy, opinionated and polarizing.
These radically different approaches to personality design raise the question: Can designers craft a universally likeable voice assistant? Should they?

The Formula For Universal Appeal
Designing voice personalities is a complex, multi-disciplinary process. Brand strategists, experience designers and editors together have to define how a voice UI fits into the brand ecosystem: The UI’s personality, tone, and word choice. They have to cast a voice that fits. But just as important as what a voice UI does do is what it does not do. Designers entering into the world of personality design need to understand the intricate psychology of relationship building, and they need to know what lines not to cross.
So far, Voice interfaces have been focused on universal appeal. And indeed, there is a proven formula to accomplish just that.
First, the voice should be female. Multiple studies have shown that female voices are perceived to be warmer and less threatening, potentially due to the simple evolutionary fact that human brains are tuned to a mother’s tones. As the late computer Clifford Nass once said: “It’s much easier to find a female voice that everyone likes than a male voice that everyone likes…It’s a well-established phenomenon that the human brain is developed to like female voices.”
Second, the personality has to be subordinate. A 2015 study showed that third-party observers watching two people talk had no problem when one person was more dominant than the other. But when a robot used those same body-language cues, the observers recoiled. We’re not ready for robots to be our superiors or even our equals. They have to serve.
Third, the personality should be extroverted. In 2003, Nass and Kwan Min Lee found that users trust a machine-generated voice more when it was extroverted. Not only does that personability make interaction more fun, it makes it easier to forgive the robots when they make a mistake.
Finally, a good dose of flattery goes a long way to being liked. In a landmark study from 1997, Nass and BJ Fogg created a simple program that played the guessing game 20 Questions. The computers then gave feedback that was either sincere-sounding praise, generic messaging, or outright flattery (insincere praise). It turned out, users whose computers had flattered them said that their computers had performed better. We’re suckers for being made to feel special, even if the flattery comes from a machine.
The big players have deployed exactly that playbook to drive adoption of voice assistants: A silicon sycophant, with jokes and a silky voice. However, first-generation voice interface brands are learning that it’s never enough to merely perpetuate what already works. Abiding by universal norms backfires when the norms themselves perpetuate prejudice. Designing for likeability can actually dilute a brand as a result.
The Need for Ethics
All voice assistants encourage only one relationship dynamic — the servile companion: Always there for you, empathetic, cheerful, like a friend. But equally ready at all times to take orders and carrying out tasks, like a servant. It is no accident that the personality of the servile companion is enacted by a female voice — society is intimately familiar with women as casual servants in the roles of secretaries, housewives and mothers. As Ben Parr of Octane AI puts it, “We’re basically training our kids that they can bark commands at a female and she will respond.”
The journalist Leah Fesser tested Siri, Google, Alexa and Cortana to see how the assistants would react to flirtation, insult and even verbal assault. The reactions of the voice assistants ranged from not understanding to evading the comments to flirtatious responses. Her findings further proved the naïveté with which the first generation of smart speakers pandered to user behavior, while reinforcing a stereotype that women have been fighting for over a century.
Yes, we can design a universally inoffensive personality, but we shouldn’t.
True to the iterative design methodology, the leaders in voice interfaces are evolving their artificial personalities rapidly to counteract the backlash. Alexa won’t answer to bigotry anymore. Siri, Google Assistant and Bixby can all be altered between male or female voices. Echo and Bixby allow users to change the command work that wakes them up, allowing users to change assistant’s persona and gender.
But from a brand perspective, the quest for universal likeability is misplaced. In personality design as in brand design, pandering to users can be self-destructive. Good brands don’t merely follow. Good brands are like good people. They believe in something and they stand for it. Standing for something is polarizing, but it’s the difference between expected and inspiring. Why shouldn’t a voice assistant balk when a user shouts a slur? Why shouldn’t it promote diversity, just like most corporations do in their annual reports?
Yes, we can design a universally inoffensive personality, but we shouldn’t. We should design strong personalities that represent the brand as well as they get the job done, and that respect the human need for superiority as well as intimacy.
A framework for strong personality design.
What’s Next
We are at the very beginning of the new era of ambient computing. While the ease of use of voice interfaces will continue to accelerate mass adoption, voice personality design and human machine relationship design are very much in the early stages. If the past is any guide, we can expect enormous changes in how voice UI’s behave.
For example, they might not always evolve towards having more personality. Whenever new technologies come to market, designers create reference points to the existing world that help users understand how to use the new technology. In graphical UI design, that reference point was skeuomorphism — making digital buttons look like physical ones, or a collection of books on the iPad on a literal bookshelf. Skeuomorphism was eventually replaced with a more honest, “flat” design language. It will be exciting to see how voice interfaces will evolve once they become more commonplace — when there’s less need to anthropomorphize robots in order for us to use them.
These early stages of new technology adoption are when the consumer voice matters most. Fesser’s article and other examples show it only takes a small number of activists to drive the direction of voice interfaces. Users, not companies, will pick the relationships they want to have with ambient technology. It’s the job of the designers to listen to users, to craft strong brand personalities with values, and to respect the ethical responsibility that comes with designing relationships.

Magenta is a publication of Huge.


= Can we be honest about ethics? =
Can we be honest about ethics?
Most ethical mistakes come from the inability to foresee consequences, not the inability to tell right from wrong.
Data science is today roughly where medicine was in the 1900s. We can do so much, but there’s still so much we don’t know. Our thinking about ethics should acknowledge that.
Imagine you needed surgery. What do you do to keep your doctor from cutting out and selling one of your kidneys (or from doing something equally unethical)? There seem to be three kinds of assurance you could get:
Have a legal system in place that would severely punish the doctor for the unethical behavior, backed up with a regulatory system that could be relied upon to consistently detect and report that behavior to the legal system.
Find people who have worked with or been operated upon by the doctor and get their assessment of his or her ethicality.
Take the doctor’s word for it.
As I have observed and participated in discussions about creating a data science code of ethics, I have found general acknowledgement that, for data science at least, option #1 does not currently exist, and probably will not exist for a very long time, and perhaps never will exist. What astounds me is that, in the absence of that option, efforts to tackle the issue of data science ethics seem overwhelming focused on variations of option #3. This post set out, briefly, why option #3 is not only impractical but unethical, and slightly less briefly, why I believe option #2 is what we all should have been working on in the first place.
Talk is cheap
The “manifesto for data practices” (datapractices.org), was produced by a Data for Good Exchange, sponsored by Data for Democracy and Bloomberg, promoted by former U.S. Chief Data Scientist DJ Patil, and is currently maintained by data.world. I’ve not been shy about stating that I have major ethical concerns about the document.
To summarize my criticism: the manifesto asks us to identify ethical practitioners by taking those practitioners’ word for it. All you have to do is sign the document. Costless virtue signaling creates systemic risk: as unethical practitioners certainly have no qualms about claiming they behave ethically, and ethical practitioners can be ethical without making any such claims in the first place, any reward for cheap talk - no matter how small - creates ways for individual practitioners to build their reputation and otherwise benefit from something other than actually doing their jobs well. A costless ethical code makes it harder, not easier, to identify ethical practitioners.
The manifesto commits the exact type of ethical breach that the document itself was supposed to address: its creators turned out and its signatories have supported a minimum viable product without fully considering the downstream harm the product could cause. The document’s creators incurred no risk by creating and promoting it, so it should not be surprising that the product fails to live up to its own ethical standards. This illustrates Nassim Taleb’s principle of skin in the game:
“There is a difference between beliefs that are decorative and…those that map to action. There is no difference between them in words, except that the true difference reveals itself in risk taking, having something at stake, something one could lose in case one is wrong. … How much you truly ‘believe’ in something can only be manifested through what you are willing to risk for it.”
We won’t fix ethics by fixing ethics
Stealing and selling someone’s organs is obviously unethical — egregiously so. But most of the problems used to justify the need for a data science ethical code are not the steal-your-kidney sort of problem. The creators of the COMPAS algorithm, as far as we know, did not intentionally design the thing to keep poor minorities in prison. Amazon didn’t intentionally design its algorithm to offer same-day delivery to only rich, white zip codes. The designers of a child-abuse detection algorithm didn’t intentionally conflate being poor with being neglectful. In other words, while all of the above examples are concerning because of their ethical consequences, the consequences themselves arose because of a failure of competency, not a failure of ethics.
These are all more of an accidentally-damage-your-kidney-without-realizing-it sort of problem. None of the principles enumerated in the manifesto for data practices would have kept those flawed algorithms from being deployed. If the creators of those tools had realized that they had built systematic bias into their products, they would have changed their products before deploying. Knowing sound ethical principles, and even believing in those principles with all your heart, does not make you automatically recognize a poor design choice. Ethical problems do not have an ethical fix. It’s simply the wrong target.
Costs matter more than principles
I laid out three options for addressing ethical risk at the start of this post. A variation of the legalistic option #1 addresses competency risk instead: it’s called insurance. That’s an expensive and involved way to deal with the problem. It should go without saying that the cheap-talk option #3 is, if anything, an even more ridiculous way to assess competence than it is to assess ethicality.
That leaves option #2 — getting a public assessment of a practitioner. That option is an effective way to mitigate both competency risk and ethical risk, if and only if we get people to vouch for specific types of things — things that incur more costs to the individual practitioner that that practitioner received in return.
Remember that the problem with a cheap-talk code of ethics is that it makes ethical and unethical practitioners look exactly the same: they can both sign the document, both advertise the fact that they signed it, both talk intelligently about the principles. That effect can be counteracted by a code that demands regular costly action. Proof of your adherence to a code of ethics shouldn’t be your signature. It should be your resume.
One of the major features of the original Hippocratic Oath was the requirement that a doctor teach anyone who honestly desired to learn the craft. If someone claimed to abide by the code, you didn’t need to ask them “are you an ethical doctor?”. You could ask them “when was the last time you taught someone the trade?” And the reason this was an effective filter was because the Hippocratic Oath wasn’t a code for teachers. It was a code for doctors. That meant you had to make time and space to teach others even while carrying out your regular professional responsibilities.
Nearly every part of the original Hippocratic Oath was this kind of enumeration of direct costs to the practitioner. People who adopted the oath promised to not accrue side benefits during the course of performing their professional duties. Doctors who took advantage of non-medical information shared by a patient or who developed side-business contacts among a patient’s household were not adhering to the oath. To be clear: there’s not anything inherently wrong with any of those things, just as there is nothing inherently wrong with not teaching. The costs mattered because only the truly competent doctors — the ones who knew and cared about their profession enough to be really very good at what they did — would be willing to consistently sacrifice their time of turn down a proffered side-benefit.
You have to be particularly competent to be able to voluntarily incur personal costs. And those personal costs deter those who would pretend to be competent solely in order to obtain side benefits. In other words, an ethical code that is also a method of assessing competence is an effective filter for intentionally unethical behavior as well as unintentionally unethical behavior. In fact, given a lack of legal, regulatory, and insurance infrastructure, it’s the only effective filter. And the only costs it incurs are paid directly by individual practitioners.
A meaningful ethical code is possible
Ethics is not a solvable problem but it is a manageable risk. No set of principles, not even a robust legal and regulatory infrastructure, will ensure ethical outcomes. Our goal should be to ensure that algorithm design decisions are made by competent, ethical individuals — preferably, by groups of such individuals. If we improve competency, we improve ethics. Most ethical mistakes come from the inability to foresee consequences, not the inability to tell right from wrong.
An effective ethical code doesn’t need to — in fact, probably shouldn’t — focus on ethical issues. What matters most are the consequences, not the tools we use to bring those consequences about. As long as an ethical code stipulates ways individual practitioners can prove their competence by voluntarily taking on “unnecessary” costs and risks, it will weed out the less competent and the less ethical. That’s the list we should be building. That’s the product that will result in a more ethical profession.
I don’t know what that list should look like. For what it’s worth, I personally think two stipulations found in the original Hippocratic Oath could be easily adapted to another profession:
You have to teach. If someone asks for help learning a skill related to the profession, you have to at least give them detailed instructions on how to find resources, and the majority of the time you have to actually teach them (and teaching that’s an explicit job duty to which you are allowed to allocate time doesn’t count).
You have to advertise your ignorance. If you’re asked to, say, build or train an algorithm you’ve never used before, you have to explicitly tell all immediate stakeholders that you’ll be learning as you go; if it’s an entirely new competency are (say, designing a database architecture when you’ve never done so before), you need to explicitly try to get the people who own the product to hire someone more competent to do it, and only go forward if they refuse.
Those are costly stipulations. In the case of teaching, it’s an opportunity cost, in that you’re spending time helping others that you could have spent on your core responsibilities. In the case of advertising ignorance, it’s a more direct cost: I know how scary and sometimes even risky it can be to admit ignorance. That risk is what makes it a good ethical guideline.
I think a professional ethical code could be useful, perhaps even important. I think it’s possible to build a robust, enforceable ethical code. But the products offered so far are not the right way to go — in fact, they’re moving us away from where we need to be. If we’re honest about wanting to address ethical issues within the profession, we need to put skin in the game. Anything less than that will exacerbate the problem.

= The Posterity Design Manifesto: my personal view on the Interaction 18 =
captured @interaction18 conference from the presentation Ruth Kikin-Gil
The Posterity Design Manifesto: my personal view on the Interaction 18
I was so overwhelmed by all impressions, insights and ideas, that it took me 3 days to digest and think about how to move on…
And then I thought that the most appropriate reflection would be to summarize my thoughts in “The Posterity Design Manifesto”.
Think about next generations — Ancestry thinking
Be a good citizen first and think about the profit second
Think though potential options how the technology can be misused and turn to evil
Remember that every voice counts to make a big change, everybody can contribute: power of agency
Consider human values right from the start of the education
Alan Cooper
Remember the past
Study past in all its facets to design new
Let it inspire you and learn from failures
Remember who you are: your background makes you unique
farai madzima
Emptiness — give space for imagination and room for new
Sometimes you have to forget to be able to create new
@Kenya Hara
Establish inclusive continuously evolving culture
Start with optimistic thinking
Be kind, respectful and curious
Have backbone to disagree and commit
Design to increase communication and collaboration in organizations
Be coach: help to find a path to success through dialogue and open questions
Haiyan Zhang
Invite and celebrate future now
Anticipate human needs, help to focus by eliminating needless choices and be mindful of privacy
Deal with ethical questions (before dealing with AI)
Imagine different futures to invite everybody to be involved in the critical shaping of our collective future.
Anab Jain
It was a great and insightful conference and I think the themes for us designers would be valid for the next year and most likely far beyond…
Selected Links to the Talks:
The Oppenheimer Moment
Make things unknown
(Im)perfect futures — The hidden ethics of tomorrow’s products
Innovating technology for a diverse world
You’re not a designer, you’re a coach
Design to Ignite — Design sprints for transformation at scale
Can being African make you bad at design? — Cultural bias in design
More-Than-Human Centred Design
Humanity-centered design

= Artificial Intelligence: The Challenge to Keep It Safe =
Artificial Intelligence: The Challenge to Keep It Safe

By Ariel Conn
Safety Principle: AI systems should be safe and secure throughout their operational lifetime and verifiably so where applicable and feasible.
When a new car is introduced to the world, it must pass various safety tests to satisfy not just government regulations, but also public expectations. In fact, safety has become a top selling point among car buyers.
And it’s not just cars. Whatever the latest generation of any technology happens to be — from appliances to airplanes — manufacturers know that customers expect their products to be safe from start to finish.
Artificial intelligence is no different. So, on the face of it, the Safety Principle seems like a “no brainer,” as Harvard psychologist Joshua Greene described it. It’s obviously not in anyone’s best interest for an AI product to injure its owner or anyone else. But, as Greene and other researchers highlight below, this principle is much more complex than it appears at first glance.
“This is important, obviously,” said University of Connecticut philosopher Susan Schneider, but she expressed uncertainty about our ability to verify that we can trust a system as it gets increasingly intelligent. She pointed out that at a certain level of intelligence, the AI will be able to rewrite its own code, and with superintelligent systems “we may not even be able to understand the program to begin with.”
What Is AI Safety?
This principle gets to the heart of the AI safety research initiative: how can we ensure safety for a technology that is designed to learn how to modify its own behavior?
Artificial intelligence is designed so that it can learn from interactions with its surroundings and alter its behavior accordingly, which could provide incredible benefits to humanity. Because AI can address so many problems more effectively than people, it has huge potential to improve health and wellbeing for everyone. But it’s not hard to imagine how this technology could go awry. And we don’t need to achieve superintelligence for this to become a problem.
Microsoft’s chatbot, Tay, is a recent example of how an AI can learn negative behavior from its environment, producing results quite the opposite from what its creators had in mind. Meanwhile, the Tesla car accident, in which the vehicle mistook a white truck for a clear sky, offers an example of an AI misunderstanding its surrounding and taking deadly action as a result.
Researchers can try to learn from AI gone astray, but current designs often lack transparency, and much of today’s artificial intelligence is essentially a black box. AI developers can’t always figure out how or why AIs take various actions, and this will likely only grow more challenging as AI becomes more complex.
However, Ian Goodfellow, a research scientist at Google Brain, is hopeful, pointing to efforts already underway to address these concerns.
“Applying traditional security techniques to AI gives us a concrete path to achieving AI safety,” Goodfellow explains. “If we can design a method that prevents even a malicious attacker from causing an AI to take an undesirable action, then it is even less likely that the AI would choose an undesirable action independently.”
AI safety may be a challenge, but there’s no reason to believe it’s insurmountable. So what do other AI experts say about how we can interpret and implement the Safety Principle?
What Does ‘Verifiably’ Mean?
‘Verifiably’ was the word that caught the eye of many researchers as a crucial part of this Principle.
John Havens, an Executive Director with IEEE, first considered the Safety Principle in its entirety, saying, “I don’t know who wouldn’t say AI systems should be safe and secure. … ‘Throughout their operational lifetime’ is actually the more important part of the sentence, because that’s about sustainability and longevity.”
But then, he added, “My favorite part of the sentence is ‘and verifiably so.’ That is critical. Because that means, even if you and I don’t agree on what ‘safe and secure’ means, but we do agree on verifiability, then you can go, ‘well, here’s my certification, here’s my checklist.’ And I can go, ‘Great, thanks.’ I can look at it, and say, ‘oh, I see you got things 1–10, but what about 11–15?’ Verifiably is a critical part of that sentence.”
AI researcher Susan Craw noted that the Principle “is linked to transparency.” She explained, “Maybe ‘verifiably so’ would be possible with systems if they were a bit more transparent about how they were doing things.”
Greene also noted the complexity and challenge presented by the Principle when he suggested:
“It depends what you mean by ‘verifiably.’ Does ‘verifiably’ mean mathematically, logically proven? That might be impossible. Does ‘verifiably’ mean you’ve taken some measures to show that a good outcome is most likely? If you’re talking about a small risk of a catastrophic outcome, maybe that’s not good enough.”
Safety and Value Alignment
Any consideration of AI safety must also include value alignment: how can we design artificial intelligence that can align with the global diversity of human values, especially taking into account that, often, what we ask for is not necessarily what we want?
“Safety is not just a technical problem,” Patrick Lin, a philosopher at California Polytechnic told me. “If you just make AI that can align perfectly with whatever values you set it to, well the problem is, people can have a range of values, and some of them are bad. Just merely matching AI, aligning it to whatever value you specify I think is not good enough. It’s a good start, it’s a good big picture goal to make AI safe, and the technical element is a big part of it; but again, I think safety also means policy and norm-setting.”
And the value-alignment problem becomes even more of a safety issue as the artificial intelligence gets closer to meeting — and exceeding — human intelligence.
“Consider the example of the Japanese androids that are being developed for elder care,” said Schneider. “They’re not smart; right now, the emphasis is on physical appearance and motor skills. But imagine when one of these androids is actually engaged in elder care … It has to multitask and exhibit cognitive flexibility. … That raises the demand for household assistants that are AGIs. And once you get to the level of artificial general intelligence, it’s harder to control the machines. We can’t even make sure fellow humans have the right goals; why should we think AGI will have values that align with ours, let alone that a superintelligence would.”
Defining Safety
But perhaps it’s time to reconsider the definition of safety, as Lin alluded to above. Havens also requested “words that further explain ‘safe and secure,’” suggesting that we need to expand the definition beyond “physically safe” to “provide increased well being.”
Anca Dragan, an associate professor at UC Berkeley, was particularly interested in the definition of “safe.”
“We all agree that we want our systems to be safe,” said Dragan. “More interesting is what do we mean by ‘safe’, and what are acceptable ways of verifying safety.
“Traditional methods for formal verification that prove (under certain assumptions) that a system will satisfy desired constraints seem difficult to scale to more complex and even learned behavior. Moreover, as AI advances, it becomes less clear what these constraints should be, and it becomes easier to forget important constraints. … we need to rethink what we mean by safe, perhaps building in safety from the get-go as opposed to designing a capable system and adding safety after.”
What Do You Think?
What does it mean for a system to be safe? Does it mean the owner doesn’t get hurt? Are “injuries” limited to physical ailments, or does safety also encompass financial or emotional damage? And what if an AI is being used for self-defense or by the military? Can an AI harm an attacker? How can we ensure that a robot or software program or any other AI system remains verifiably safe throughout its lifetime, even as it continues to learn and develop on its own? How much risk are we willing to accept in order to gain the potential benefits that increasingly intelligent AI — and ultimately superintelligence — could bestow?
This article is part of a series on the 23 Asilomar AI Principles. The Principles offer a framework to help artificial intelligence benefit as many people as possible. But, as AI expert Toby Walsh said of the Principles, “Of course, it’s just a start. … a work in progress.” The Principles represent the beginning of a conversation, and now we need to follow up with broad discussion about each individual principle. You can read the discussions about previous principles here.
Originally published at futureoflife.org on September 21, 2017.

= How Can a Morally Bankrupt Nation Be a Leader in Ethical AI? =
How Can a Morally Bankrupt Nation Be a Leader in Ethical AI?

Yesterday, The Verge reported that the British government is eager to position itself as a leader in the artificial intelligence space.
Great Britain might be the world’s third-largest investor in AI technologies, but it cannot even match the level of investment we’re currently seeing in countries such as China and the United States, let alone outspend those countries. So what can Britain do to secure its place at the table?
Become a leader in the ethical applications of AI, apparently.
That’s the grand idea that’s being put forward by a report published earlier this week by the House of Lords, titled “AI in the UK: Ready, Willing, and Able?”
Its hilariously tone-deaf, self-deprecating title notwithstanding, the report reveals the profound disconnect between how Britain sees itself in a changing global economy and the realities of Britain’s draconian approach to digital policy.
The report explores several important points, including AI’s less-than-favorable media coverage. Except rather than acknowledging the fact that AI and machine learning are inherently disruptive and that many public fears are justified — particularly within the context of existing economic challenges — the report instead chooses to paint the majority of the British public as ignorant. While it’s true that most people probably couldn’t tell you how machine learning algorithms work, plenty of people are rightly concerned with how the implementation of AI in the workplace will threaten their already precarious livelihoods.
“Many AI researchers and witnesses connected with AI development told us that the public have an unduly negative view of AI and its implications, which in their view had largely been created by Hollywood depictions and sensationalist, inaccurate media reporting.”
Then there’s the not-inconsequential matter of how Britain proposes to actually enforce whatever ethical guidelines the Tories come up with. Had Britain chosen to remain in the EU, the country would be looking forward to the prospect of enjoying more robust data protection laws as set forth in the GDPR. Now, the idea of Britain setting any real agenda regarding ethical applications of AI is as laughable as it is disheartening.
The report’s proposed “AI Code,” a set of pathetically broad guidelines that many AI researchers already follow, reveals precisely how poorly equipped Great Britain is to even follow its own recommendations (emphasis mine):
Artificial intelligence should be developed for the common good and benefit of humanity.
Artificial intelligence should operate on principles of intelligibility and fairness.
Artificial intelligence should not be used to diminish the data rights or privacy of individuals, families or communities.
All citizens should have the right to be educated to enable them to flourish mentally, emotionally and economically alongside artificial intelligence.
The autonomous power to hurt, destroy or deceive human beings should never be vested in artificial intelligence.
There’s little arguing with the first point, even if the likelihood of tech conglomerates acting in anything but their own interest seems almost hopelessly naive.
The second point is where things start to become problematic, however. As I’m sure even a few peers in the House of Lords realize, AI constructs can only act within the parameters we set for them. In this regard, assuming that AI systems should operate on principles of fairness depends entirely on the definition of fairness given to them by their human overseers. It’s the same problem with the flawed assumption that technology is inherently neutral. As sociologist Donna Haraway noted in an interview with WIRED in 1997:
“Technology is not neutral. We’re inside of what we make, and it’s inside of us. We’re living in a world of connections — and it matters which ones get made and unmade.”
It’s the report’s third point, however, that should infuriate any technologist who’s been paying attention. Artificial intelligence should not be used to diminish the data rights or privacy of individuals, families or communities.
Again, there should be no arguments here — yet Theresa May’s Home Office has demonstrated that it is willing to continually and systematically violate the human rights of individuals, families, and communities, never mind data rights. May’s Home Office has blatantly and repeatedly demonstrated its contempt for the rights of the vulnerable by hacking the devices of asylum seekers and refugees, sought to give itself sweeping new surveillance powers under the guise of “immigration control,” illegally shared the personal data on millions of British citizens with foreign intelligence agencies, and exempted members of parliament from the vast data collection methods authorized by the Investigatory Powers Act — the sole amendment to the most comprehensive domestic surveillance program ever conceived.
In light of the morally reprehensible actions the British government has taken against its own citizens, it’s laughable that the U.K. could ever assert itself as the moral arbiter of anything, let alone something as crucial and far-reaching as a governing code of ethics for artificial intelligence.
Of course, the real motivation for the report’s publication was as transparently obvious as Theresa May’s disgust for the poor — a thinly-veiled attempt at economic relevance. With a disastrous, chaotic Brexit looming (the true impact of which the government refuses to disclose), May’s Conservative government has been exploring a range of economic measures in an attempt to mitigate the inevitable financial ruin that will accompany Britain’s withdrawal from the EU — chlorinated chicken imports, arms sales to brutal Middle Eastern petrostates, and now as a watchdog of ethical propriety in artificial intelligence.
When technologists and scientists speak of the potential dangers of AI, it’s often framed in the context of popular culture and science fiction. Naysayers warn us of autonomous weapons systems that could launch preemptive strikes against human targets. However, while we cannot rule out the risks of placing control of increasingly powerful weapon systems into the hands of algorithms and neural networks, it’s the human potential for abuse and discrimination that should worry us.
“AI could be the worst event in the history of our civilization. It brings dangers, like powerful autonomous weapons, or new ways for the few to oppress the many. It could bring great disruption to our economy.”
The quote above, from late physicist Stephen Hawking, has been used widely to support arguments for a more cautious approach to AI. However, it’s Hawking’s observation that AI could bring “new ways for the few to oppress the many” that concerns me. Today’s society is already fractured to the point of irreparable damage. In light of the resurgence of authoritarianism around the world, and Theresa May’s apparent eagerness to serve her new master in the United States, it is abundantly clear that Britain cannot be entrusted with such a grave responsibility.
The British government refuses to recognize the humanity of refugees, immigrants, families, and even its own citizens. It has treated the weak and the vulnerable with callous indifference at best, and vicious cruelty at worst. We cannot — and should not — entertain the idea of allowing the British government a seat at this particular table.
The U.K. has forfeited any right to tell the rest of the world what to do. Given that Britain’s feeble attempt to ensure its economic relevance in a rapidly changing global economy will inevitably be ignored by China and the U.S., it seems only fitting that this report be ignored as well.

= Six principles for AI ethics =
Six principles for AI ethics
Even as “ethics in AI” gains space on the AI agenda, with thought piece after thought piece on the existential threats posed by super intelligent machines, there are still some glaring gaps in the mainstream conversation on what the actual end game of ethical AI might in fact be. To that end, here are 6 principles technologists should adopt and incorporate into their practice.
ONE. Any ethics worth encoding into our technology must be about prioritising justice. Not all ethics are anti-oppression or justice oriented. It’s important we get that cleared up.
The below is an extremely useful image to understand the relationship between justice, equity and equality in terms of outcomes of actions we may take in the world.
Source. Also, we would argue that the “Justice” image would be more accurate if the metal railings weren’t there so the person on the right could better have access. Perhaps a perspex wall instead?
The premise of the image above is that advantages and benefits of society (by definition — because to receive a benefit means someone or some group is disbenefited, and to have an advantage means someone or some group is disadvantaged) are not distributed or afforded to all people equally.
Therefore, if we want to harness AI to elevate the best of humanity, we need to enable teams to deploy an analytical framework that is rooted in analysis and action that solves for equity (as an intermediary step), belonging, justice and inclusion in their problem solving as they iterate towards a final product. We at Fearless Futures call this Design for Inclusion. Part of this analytical framework must include:
a) a strong and robust understanding of interconnected oppressions;
b) how they manifest and operate as historical processes and how they are presently lived;
c) understanding and accountability for and of our own privileged positions as designers in relation to these histories and presents and how that shows up as a potential blocker to our end goal unless designed out.

Part of this analytical framework will also be about understanding that who is in the team (and who isn’t), who is heard (and who isn’t) is also an input into what we are programming.
TWO. History is important. When we are creating technology that generates the future, trained on information from the past, we need to know our histories of oppression. And if we don’t, we need our analytical framework to assess what we should be centring. Predictive policing technologies highly popular in the USA (and gaining traction here in the UK) have been heavily critiqued for reproducing and amplifying racism. Police brutality is a tragically normalised feature of existing as a Person of Colour in the USA and in the UK. However, this present reality is part of a continuum of practice. For example, in the USA, policing started in the South as a “Slave Patrol”. As this rich article details: “The first formal slave patrol was created in the Carolina colonies in 1704 (Reichel 1992). Slave patrols had three primary functions: (1) to chase down, apprehend, and return to their owners, runaway slaves; (2) to provide a form of organized terror to deter slave revolts; and, (3) to maintain a form of discipline for slave-workers who were subject to summary justice, outside of the law, if they violated any plantation rules. Following the Civil War, these vigilante-style organizations evolved in modern Southern police departments primarily as a means of controlling freed slaves who were now laborers working in an agricultural caste system, and enforcing “Jim Crow” segregation laws, designed to deny freed slaves equal rights and access to the political system”.
In the UK, in a parallel context, we may turn to the “sus laws”, the informal name given to stop and search policy, rooted in the 1824 Vagrancy Act, specifically section 4. It gave the police powers to search anyone suspected (hence “sus”) of having an intent to do something criminal. In such a world, standing still in public space could be (and is?) justification for people of colour being stopped and searched by the police. In a world in which black people are constructed through the dominant, white lens as dangerous, with the resources of the state used to curb their being, we might rather focus our efforts on ending the material impact and danger these policies have on their lives. Despite the policy formally ending due to efforts of the Scrap Sus campaign led by Mavis Best, Black people are at least 8x more likely than white people to be searched for drugs in England and Wales, though drugs are less likely to be found on them. What does this mean for the training data our “predictive” machines might be trained on?
A historical lens provides the framework for analysis for what questions we might need to ask of the data in order to programme such predictive technology for equity rather than a continuation of a brutal past and status quo into the future.
When we come to emerging technology with this historical lens, we might ask: Who specifically was the system invested in policing and who was it not? Indeed, as Cathy O’Neil remarks in her book “Weapons of Maths Destruction” predictive policing is notably not generating conclusions for police departments that has them allocating their resources outside of banks in the City of London or on Wall Street. The people engaging in acts that bring down whole economies are not those the machines consider people who need to be policed. The algorithm has not been set to curb their actions. What we even classify as a crime is central to predictive technologies such as this. Furthermore, we may wish to also ask whether our existing criminal justice system is itself worthy of being “optimised” or whether we might wish to explore alternative paradigms – outside of prison – altogether.
THREE. Google AI’s principles came out the other day stating they won’t engage in: “Technologies that cause or are likely to cause overall harm. Where there is a material risk of harm, we will proceed only where we believe that the benefits substantially outweigh the risks, and will incorporate appropriate safety constraints” (my italics).
While this might sound positive it leaves much to be desired. For starters: What is harm? Who decides? The very problem at the heart of injustice is that those who are subjected to power structures are not believed, are not heard, are dismissed and experience violence by those who benefit from the very same power structures. I am calling this the paradox of power (and privilege). Which is to say that the very people who have the power to take action to end injustice (many of whom are those programming our machines) are precisely those who because of their positionality are:
oblivious to the other side of their experience
invested in the status quo because it serves them
have been implicitly and explicitly trained to preserve their position
And so have been, in a way, programmed to use their power to maintain it.
One approach to limiting harm (whatever it may be) is to outline the “risks” or probabilities of harm from a particular tool. This is an example of a document doing so for a sentencing tool being trialled in Pennsylvania. However when “risks” of harm are raised, we know that one tool of perpetuating oppression is to either justify or excuse them, which leaves the harmful impact in place. To counter this pattern of behaviours, we require those with structural power to be invested in generating outcomes of equity and justice.
One way to build in a mechanism to this end would be to reframe the burden of proof so that those designing these technologies have to prove that they do not perpetuate oppression on any communities before they can move forward with their product.
After all, without this re-framing, we know that what is a ‘risk’ of inaccuracy for one party is a material outcome that could be the difference between life or death for the other. And disappointingly, such risks are often taken, especially in pursuit of profit.
FOUR. We simply must insist on using the language of oppression rather than bias when it comes to AI and emerging tech. The short version: because without a framework for analysing structural power we are missing the point and going nowhere. The longer and more nuanced version: read my colleague Sara’s article here. For more on these concepts Dr. Safiya Noble’s “Algorithms of Oppression” is terrific.
FIVE. We need as much focus on how we teach technologists as what we are teaching them to do when it comes to designing inclusion into AI. What makes for sustainable and transformative education?
What knowledge do we need to transform unequal power relations in our technology?
If we focus on knowledge alone, with no attention to how people do deep learning about the nuanced and complex material at the centre of anti-oppression and social relations, it will likely result in the soil in which technology companies grow being unaltered. This is because how, what, why and who we decide to prioritise in our technologies is also a function of the ways we are with one another in the process of designing. Are we optimising in these contexts for a subversion of power dynamics amongst ourselves and centreing those whose voices are marginalised to discover different knowledge? If we return to the Paradox of Power from earlier, we might wish to conceive of ourselves as having been trained on data throughout our lives that preserve the structural power we have. The question for those at the forefront of emerging technology is: are you up for unlearning so that your technologies may work for justice? Are you up for disrupting that status quo?This may feel like the fluffy stuff within our hypermasculinised technology environments, but it is extremely hard. And extremely necessary. In short, the vicious cycle will perpetuate unless we are able to generate new cultures of being and doing within technology.
SIX. There is a loud chorus getting behind transparency as a solution to the absence of ethics in AI. The logic is that if we can see what’s going on, bad stuff won’t happen. We aren’t so sure. Transparency will only get us so far, it is not a panacea. After all, even when alarming truths are exposed, we know that power responds by acting to preserve itself. A simple example might be the gender pay gap reporting recently instituted for companies with over 250 employees in the UK. Yes, the information is out there, but we saw pretty quickly how the status quo was defended by companies (often by people simply using an explanation of the gender pay gap, bizarrely).
The relationship between employees at Google regarding their company’s pursuit (now successfully terminated due to employee action) of Project Maven (a programme that uses machine learning to improve targeting for strikes) is also a good example of this. This article here details the long battle between tech workers and management. Tech workers had to build power to challenge management. Some resigned in protest. The knowledge of the project alone did not in and of itself deliver change. In fact, management actively contested their employees’ resistance to Project Maven. Justifications and excuses were made to maintain the contract. Unless we are also able to accept that growing capabilities to organise and build power is essential alongside transparency, within and outside of tech firms, transparency alone may just be another tool of distraction from those with power. What seems much more powerful is for principles of justice — as explored in points 1–5 — to be encoded into organisations’ fabrics in the first place.
Conclusion: these are 6 principles that we believe must be centred by technologists in AI. This starts with answering yes to this question: when you are designing the future, can you do everything to disrupt oppressive histories and presents from repeating themselves and instead imagine a just and humane world — and then design your team cultures and products for that instead?
With heartfelt thanks to @natalieisonline and Sara Shahvisi our Director of Programmes for their brilliant reflections and feedback that better informed the piece.

= Ethics in the age of AI =
Ethics in the age of AI
image extracted https://goo.gl/mg7wz6
Can we expect to achieve fairness and ethics applied to Artificial Intelligence contexts, when we’re still defining and reaching same within humankind?
We are forced into stepping up for the conception of a higher consciousness for both.
I strongly believe that we’re not ready as a society for the presence of devices that resemble our human characteristics, being in this very aspect the nature of a expected interpretation of human emotions and empathy moved towards objects and evolved from that. The confusion for most of us, comes always from reading shared features with the “one” we interact. Psychologically speaking this is what we seek, we empathize with what we recognize as closest to reflection. So when we see a robot that has two eyes and smile, your brain is automatically developing a connection in order to establish natural communication with it and permitting yourself to engage better.
It’s an incredibly tricky setting we’re putting for ourselves trying to develop AI that resembles human nature, as long as we haven’t proved ourselves worthy of dispatching fair judgement and equal right within our own kind.
The scenario presents that there may not be another choice but to constantly evaluate our human limits and definitions, and while discovering this agree to the similarities and same rights and duties that may apply to the things and “living” beings that surround us.
May not be clear at all times, so constant evaluation is enforced.
— — — -Update 18.05.18
And now we have this…
image from http://instagram.com/shudu.gram
Is it ok to emphatize with artificial inteligence? is it ok to confuse average Joe with basic knowlegde of what an AI actually is?
The image portraits an instagram recent product, —not created by the company, I mean a product of the enviroment inside the platform— which are “Virtual Models”, this is Shudu Gram. Regardless of what it stands for, wht I found tragic is that this is a person’s vision on aesthetics, beauty patterns, and the phsicological depiction of “what a model is”, since not only authors are posting enhanced 3d images of this virtual characthers, but their impling theyhavea life of their own.
This is not AI.
This is Sim’s life if anything.
Shudu and virtual models represent a wider ring into the debate. The technice used to run these accounts has little to do with Machine learning, or self taught and evolving Automate Intelligence. Which poses the question, is the reaction to these Virtual models appropriate or not taken with enough care?

= Understanding Bias in Algorithmic Design =
Bias in Algorithmic Design
Understanding Bias in Algorithmic Design
Find out how value-laden software can have unintended discriminatory effects, when left unexamined.
In 2016, The Seattle Times uncovered an issue with a popular networking site’s search feature. When the investigative reporters entered female names into LinkedIn’s search bar, the site asked if they meant to search for similar sounding male names instead — “Stephen Williams” instead of “Stephanie Williams,” for example. According to the paper’s reporting, however, the trend wouldn’t happen in reverse, when a user searched for male names.
Within a week of The Seattle Times article’s release, LinkedIn introduced a fix. Spokeswoman Suzi Owens told the paper that the search algorithm had been guided by “relative frequencies of words” from past searches and member profiles, not by gender. Her explanation suggests that LinkedIn’s algorithm was not intentionally biased. Nevertheless, using word frequency — a seemingly objective variable — as a key parameter still generated skewed results. That could be because men are more likely to have a common name than American women, according to Social Security data. Thus, building a search function based on frequency criteria alone would more likely increase visibility for Stephens than Stephanies.
Examples like this demonstrate how algorithms can unintentionally reflect and amplify common social biases. Other recent investigations suggest that such incidents are not uncommon. In a more serious case, the investigative news organization ProPublica uncovered a correlation between race and criminal recidivism predictions in so-called “risk assessments” — predictive algorithms that are used by courtrooms to inform terms for bail, sentencing, or parole. The algorithmic predictions for recidivism generated a higher rate of false-negatives for white offenders and a higher rate of false-positives for black offenders, even though overall error rates were roughly the same.
ProPublica’s investigation exposes how data-driven analytics used to aid decision-making can have serious consequences on people’s lives. Companies, government institutions, and data scientists in all sectors need reliable methods for eliminating unintentional bias from data-driven decision-making. Fortunately, investigative studies by academics and journalists are not the only way to audit automated systems and mitigate these risks.
THE PEOPLE BEHIND BIG DATA

Behind every data-driven decision lies a series of human judgments. Decisions about what variables to use, how to define categories or thresholds for sorting information, and which datasets to use to build the algorithm can all introduce bias. Left unexamined, value-laden software can have unintended discriminatory effects that perpetuate structural inequality. This is particularly true when algorithms are tasked with making critical decisions about people’s lives, like who qualifies for parole, who receives favorable credit offers, or who makes a “good” job candidate.
Data-driven decision-making systems may seem to rely entirely on objective data, but most still involve value judgments about how that data should be analyzed. How should success be defined, for example. What characteristics in the data should be included in the analysis? Into what categories should cases be sorted? The answers to these questions may vary based on who is designing the algorithm, their motivations, and their worldview.
Human judgement lies behind every data-driven decision. Left unexamined, value-laden software can have unintended discriminatory effects.
Let’s take a basic example. Suppose two people are tasked with developing a system to sort a basket of fruit. They have to determine which pieces are “high quality” and will be sold at the market, and which will instead be used for making jam. Both people are given the exact same data — the fruit — and the same task of determining the fruits’ relative quality. To solve this problem:
The goal has to be defined. (Success = fruit is correctly sorted; error = fruit is misclassified)
Possible outcomes have to be defined. (Fruit goes to market or to the jam factory)
And parameters have to be defined for the key variable: quality. (The fruits’ shape, color, number of bruises, sheen, etc.)
Given the same task and data, the two people are likely to have different results. Perhaps one person believes the primary indicator of a fruit’s quality is brightness of color. That person may sort the fruit based on how vibrant it is, even though not all fruits are brightly colored; that person would send strawberries to the market and melons to the jam factory. Meanwhile, the other person might believe that unblemished fruit is the best quality, even though fruits with protective rinds might look scruffy on the outside, but are perfectly fine on the inside; that person could send unripe strawberries to the market and ripe melons or bananas to the jam factory. These different, yet similarly logical and evenly applied criteria, will result in two different outcomes for the same basket of fruit. But both send too many melons to the jam factory because the people sorting the fruit are using proxies for quality that don’t account for the best characteristics of melons.
This example represents a relatively unsophisticated version of algorithmic decision making, but a similar version has been tested for sorting cucumbers in Japan. Makoto Koike wanted to apply machine learning to help his mother more efficiently sort the cucumbers from her farm. Rather than asking his mother to define the features she used for sorting, Koike tasked her with sorting a bunch of cucumbers. He then optically scanned the cucumbers and used computer software to identify the common traits and then built an algorithm to replicate her work. That algorithm’s understanding of what makes a “good” cucumber was based on Koike’s mother’s interpretation and intuition.
ALGORITHMIC CONSEQUENCES
It’s one thing to have an algorithm that marginalizes melons or unfairly sorts cucumbers, but what happens when algorithms make important decisions about humans?
Consider the data used to determine consumer credit scores. Ten years ago, the Federal Reserve Board, under direction from Congress, evaluated whether credit scoring methods were discriminatory. In its report to Congress, the Federal Reserve Board revealed a strong correlation between credit scores and race and other demographic indicators, even though “credit characteristics included in credit history scoring models do not serve as substitutes, or proxies, for race, ethnicity, or sex.” Nevertheless, credit scoring models disadvantage some segments of the population more than others. Immigrants tend to have lower credit scores, for example — not because scoring algorithms are trained to assign immigrants lower credit scores, but because length of credit history weighs heavily in scoring models, and recent immigrants will have had less time to develop their credit histories.
When algorithmic designers ignore social nuance or inequality, they risk designing systems that create disparate impacts. In the case of the creditworthiness research, however, the Federal Reserve Board exposed a different problem with scoring models: that existing credit scoring indicators are robust, but potentially insufficient. In the case of recent immigrants, “expanding the information supplied to credit-reporting agencies to include rent, other recurring bill payments, nontraditional uses of credit, and the credit histories of the foreign-born in their countries of origin may provide a broader picture of the credit experiences.” As a result of the Federal Reserve Board’s research, advocates are now working to determine what other data points (like utility payments) should be considered in credit scoring.
Data scientists have a responsibility to be aware of “possible biases involved in the design, implementation, and use” of analytic systems.
Given the role of human subjectivity in designing algorithms, and algorithms’ widespread use, what is the responsibility of the data scientists who build them? The Association for Computing Machinery (ACM) recently published a statement on algorithmic accountability and transparency that places at least some responsibility on the designers and scientists behind the technology. The ACM outlined seven principles, the first of which says that data scientists should be aware of “the possible biases involved in [the] design, implementation, and use,” of analytic systems. But once they are aware, how can data scientists increase their chances of detecting and eliminating unintended bias?
The Center for Democracy and Technology is creating a tool to help data scientists, programmers, and product managers interrogate their instincts throughout the design process. This includes prompts to ask critical questions about the goal of the product and the methodology and assumptions used to make decisions along the way. By providing questions instead of answers, the goal of the tool is to challenge norms and standard business practices to produce an inclusive and mindful climate within any entity using algorithmic decision-making. The hope is that this product can be applied broadly, spanning the many contexts of automated decision-making technology.
This article was written by A.R. Lange and Natasha Duarte for Demand @ASME.
A.R. Lange and Natasha Duarte are collaborators on the Center for Democracy and Technology’s (CDT) Privacy and Data Project, whose work focuses on the intersection of civil rights and big data. Lange is a former senior policy analyst and Duarte is a policy analyst at the CDT.
“Understanding Bias in Algorithmic Design” was originally published in DEMAND’s Spring 2017 issue.

= Top 10 Ethical AI articles =
Top 10 Ethical AI articles

Artificial intelligence has been a hugely popular subject over the past twelve months, and is in the news every single day atm. With so much information around, where do you start to read up about it?
In preparation for WIRED Live and our Live Innovation session on ethical AI, we spent a couple of weeks immersing ourselves into the world of artificial intelligence and the implications it will have on us, and how we work and live. To help you get up to speed on the different views around AI and ethics, we’ve put together our top ten must-reads.
How do we design algorithms that aren’t evil? Part of a new Google project to humanize AI the guide to “human-centered machine learning.”
No one really knows how the most advanced algorithms do what they do. That could be a problem?
As machine learning continues to grow, we all need to develop new skills in order to differentiate ourselves. But which ones?
As powerful AIs proliferate in society, the ability to trace their decisions, challenge them and remove ingrained biases has become a key area of research.
A talk by the writer of “Weapons of math destruction.” Learn more about the hidden agendas behind algorithmic formulas.
As artificial intelligence (AI) systems become ever more sophisticated, another wave of job displacement will almost certainly occur. But on the flip side what new jobs will be created?
Is artificial intelligence going off the rails? A viewpoint from a 1960s AI pioneer the current state of AI and where its going.
It’s hard to get more personal than data about your emotions. If we do continue down the artificial emotional intelligence route what are the applications and implications.
Bias is the real AI danger, what if intelligent systems continue to learn human prejudices?
Rise of the machines, an overview of the future of work and way automation could effect society.
You can see the workshops and the manifesto we created at WIRED live here.

= Governing SkyNet =
Governing SkyNet
Use of Machine learning is quickly becoming a standard part of digital solutions across government; providing insights, reducing the marginal cost of repeatable work and helping to deliver amazing user experiences.

Given governments unique access to data, impact on end users lives and accountability to the taxpayer; implementing the right level of governance is critical to grow trust in such solutions.
Too heavy handed will restrict growth and innovation as we scale throughout the government ‘stack’.
Too light-touch will risk unintended consequences on end users and increased risk of failing to meet the strict auditing requirements our decisions sit under.
To consider this further, we’ve adopted three broad guidelines on how we want to approach scaling of machine learning governance.
ML governance should be an enabler
We currently think of ML projects as exceptions to the rule of current development practices, however we are moving into an environment where its use will be the default choice for capabilities across government since:
The ‘cost’ of machine learning is falling – User friendly and mature toolset are becoming common place (I.e Tensorflow) alongside cloud services which take infrastructural complexity out of the equation.
The supply of ML ready developers is increasing – University and online courses are teaching mature courses to students plus increasingly gaining experience in real world firms.
The evidence base is broadening – Communities of practitioners are now commonplace to learn from, with a huge range of open source projects to contribute to and patterns to implement for many common challenges.
ML governance should be proportional to the use case
We don’t classify information just because its processed using an SQL database. Equally we shouldn’t apply heavy handed governance to Machine Learning, just because it using the paradigm.
Governance should be based on the impact of decisions made by the end to end capability, for example;
A back office function to optimise infrastructure power efficiency – Low
A operational function to prioritise customer service tickets – Medium
A operational function to flag evidential content for review in a court case – high
ML governance should be adaptive to change
A few years ago Microsoft released a ML powered ‘bot’ that enabled free form Q&A on Twitter. It learnt from the questions it was asked and syntax/language used to converse with it. It started off as a ‘polite’ conversationalist, however it quickly became a rude, obnoxious and racist monster as it was ‘gamed’ by its users.
Hence a ML capability which has been approved or governed under a certain set of assumptions or inputs, might be inappropriate to a new set of inputs or use cases. Governance has to be an iterative process which evolves with the capability as it develops and scales.

= ​7 Short-Term AI ethics questions =
​7 Short-Term AI ethics questions
When new technologies become widespread, they often raise ethical questions. For example:
Weapons — who should be allowed own them?
Printing press — what should be allowed to be published?
Drones — where should they be allowed to go?
The answers to these questions normally come after the technologies have become common enough for issues to actually arise. As our technology becomes more powerful, the potential harms from new technologies will become larger. I believe we must shift from being reactive to being proactive with respect to new technological dangers.
We need to start identifying the ethical issues and possible repercussions of our technologies before they arrive. Given that technology grows exponentially fast, we will have less and less time to consider the ethical implications.
We need to have public conversations about all these topics now. These are questions that cannot be answered by science — they are questions about our values. This is the realm of philosophy, not science.
Artificial intelligence in particular raises many ethical questions — here are some I think are important to consider. I include many links for those looking to dig deeper.
I provide only the questions — it’s our duty as a society to find out what are the best answers, and eventually, the best legislation.
1. Biases in Algorithms
Machine learning algorithms learn from the training data they are given, regardless of any incorrect assumptions in the data. In this way, these algorithms can reflect, or even magnify, the biases that are present in the data.
For example, if an algorithm is trained on data that is racist or sexist, the resulting predictions will also reflect this. Some existing algorithms have mislabeled black people as “gorillas” or charged Asian Americans higher prices for SAT tutoring. Algorithms that try to avoid obviously problematic variables like “race”, will find it increasingly hard to disentangle possible proxies for race, like zip codes. Algorithms are already being used to determine credit-worthiness and hiring, and they may not pass the disparate impact test which is traditionally used to determine discriminatory practices.
How can we make sure algorithms are fair, especially when they are privately owned by corporations, and not accessible to public scrutiny? How can we balance openness and intellectual property?
​
Using Twitter to call out Google’s algorithmic bias
2. Transparency of Algorithms
Even more worrying than the fact that companies won’t allow their algorithms to be publicly scrutinized, is the fact that some algorithms are obscure even to their creators. 
Deep learning is a rapidly growing technique in machine learning that makes very good predictions, but is not really able to explain why it made any particular prediction.
For example, some algorithms haven been used to fire teachers, without being able to give them an explanation of why the model indicated they should be fired.
How can we we balance the need for more accurate algorithms with the need for transparency towards people who are being affected by these algorithms? If necessary, are we willing to sacrifice accuracy for transparency, as Europe’s new General Data Protection Regulation may do? If it’s true that humans are likely unaware of their true motives for acting, should we demand machines be better at this than we actually are?
​
An algorithm that is much more transparent than deep learning
3. Supremacy of Algorithms
A similar but slightly different concern emerges from the previous two issues. If we start trusting algorithms to make decisions, who will have the final word on important decisions? Will it be humans, or algorithms?
For example, some algorithms are already being used to determine prison sentences. Given that we know judges’ decisions are affected by their moods, some people may argue that judges should be replaced with “robojudges”. However, a ProPublica study found that one of these popular sentencing algorithms was highly biased against blacks. To find a “risk score”, the algorithm uses inputs about a defendant’s acquaintances that would never be accepted as traditional evidence.
Should people be able to appeal because their judge was not human? If both human judges and sentencing algorithms are biased, which should we use? What should be the role of future “robojudges” on the Supreme Court?
​
A case study from the ProPublica investigation into the COMPAS sentencing algorithm
4. Fake News and Fake Videos
Another ethical concern comes up around the topic of (mis)information. Machine learning is used to determine what content to show to different audiences. Given how advertising models are the basis for most social media platforms, screen-time is used as the typical measure of success. Given that humans are more likely to engage with more inflammatory content, biased stories spread virally. Relatedly, we are on the verge of using ML tools to create viral fake videos that are so realistic humans couldn’t tell them apart.
For example, a recent study showed that fake news spread faster than real news. False news were 70% more likely to be retweeted than real news. Given this, many are trying to influence elections and political opinions using fake news. A recent undercover investigation into Cambridge Analytica caught them on tape bragging about using fake news to influence elections.
If we know that videos can be faked, what will we be acceptable as evidence in a courtroom? How can we slow the spread of false information, and who will get to decide which news count as “true”?
​
Left: real image of Parkland shooting survivor Emma González. Right: Fake image that went viral
5. Lethal Autonomous Weapon Systems
​
AI researchers say we will be able to create lethal autonomous weapons systems in less than a decade. This could be in the form of small drones that are able to be deployed, and unlike current military drones, be able to make decisions about killing others without human approval.
For example, a recent video created by AI researchers showcases how small autonomous drones, Slaughterbots, could be used for killing targeted groups of people, i.e., genocide. Almost 4,000 AI/Robotics researchers have signed an open letter asking for a ban on offensive autonomous weapons.
On what basis should we ban these types of weapons, when individual countries would like to take advantage of them? If we do ban these, how can we ensure that it doesn’t drive research underground and lead to individuals creating these on their own?

Still from “Slaughterbots”, click image to watch full video.
6. Self-driving Cars
Google, Uber, Tesla and many others are joining this rapidly growing field, but many ethical questions remain unanswered.
For example, an Uber self-driving vehicle recently killed a pedestrian in March 2018. Even though there was a “safety driver” for emergencies, they weren’t fast enough to stop the car in time.
As self-driving cars are deployed more widely, who should be liable when accidents happen? Should it be the company that made the car, the engineer who made a mistake in the code, the operator who should’ve been watching? If a self-driving car is going too fast and has to choose between crashing into people or falling of a cliff, what should the car do? (this is a literal literal trolley problem) Once self-driving cars are safer than the average human drivers (in the same proportion that average human drivers are safer than drunk drivers) should we make human-driving illegal?
A dashcam still from Uber’s self-driving accident
7. Privacy vs Surveillance
The ubiquitous presence of security cameras and facial recognition algorithms will create new ethical issues around surveillance. Very soon cameras will be able to find and track people on the streets. Before facial recognition, even ominpresent cameras allowed for privacy because it would be impossible to have humans watching all the footage all the time. With facial recognition, algorithms can look at large amounts of footage much faster.
For example, CCTV cameras are already starting to be used in China to monitor the location of citizens. Some police have even received facial-recognition glasses that can give them information in real time from someone they see on the street.
Should there be regulation against the usage of these technologies? Given that social change often begins as challenges to the status quo and civil disobedience, can a panopticon lead to a loss of liberty and social change? ​
Surveillance cameras in China using machine vision
Philosophy with a deadline
Actual people are currently suffering from these technologies: being unfairly tracked, fired, jailed, and even killed by biased and inscrutable algorithms.
We need to find appropriate legislation for AI in these fields. However, we can’t legislate until society forms an opinion. We can’t have an opinion until we start having these ethical conversations and debates. Let’s do it. And let’s get into the habit of beginning to think about ethical implications at the same time we conceive of a new technology. ​
For another post: Longer-term ethical issues
​
Designer babies
​Job automation
AGI alignment
AI rights ​
Consciousness uploading

= EuroIA 2017 — Friday =
EuroIA 2017 — Friday
These writings were done during Euroia 2017 in Stockholm.
Yesterday was an amazing day. It really sparked my imagination on things I want to achieve. The focus was definitely set on UX and IA in organizations. Today turned out to be focussing on UX practice and trends within the field.
Walking through information
Inspired by the workshop given by Alastair Somerville
The day kicked of with a workshop. Everybody settled down in the large conference room with their coffees. Notebooks were taken out of bags. Alastair welcomed everybody. “Now let’s stand up, and go outside”. That was kinda weird, and caught everybody off guard. Everybody gathered outside, Alastair asked one person where she was from. “Sweden”, was the quick answer. “Now, picture a map and position yourself in relation to Sweden”. Within 30 seconds or so, everybody spread out. And soon enough we had mapped out all the attendees. Very cool.
When everybody got back inside, the workshop kicked off. Lots of information about sensory UX. Somehow, we keep designing for the obvious, but forget all the other senses that we carry with us all day.
Our table worked started working on a sensory design journey map of Finding the Pool at the hotel. Soon enough we figured out that it was not just about signs, symbols or people we spoke to. But also about aromatic scents that fill the hallway towards the pool. The humidity that slowly builds up when inside the changing rooms. And that strong smell of chloride, the moment you enter the pool area. Using Jenga bricks, post-its and a lego figure, we mapped our journey. See the Medium post by Alastair on his workshop for some pictures.
A great exercise. Which really made me aware of all the senses. And not just the usual eyes and ears. “Maintain meaning across all senses” is the quote that stuck with me.
Alastair referred to this talk by Elizabeth Buie for more information on transcendent UX.
Ethics and Impact in UX
Inspired by talks (1, 2) given by Per Axbom and Jan-Wessel Hovingh
Donating old and unused clothing to poor countries. What a great and noble idea. But what most people don’t realize is that this process disrupts the local clothing economy. And that sucks. Or how about Toyota, with their highly improved fabrication process. An inspiration to companies all over the world. But to what end? Producing more cars, that lead to congested highways. Definitely a negative impact. Per advocates that the impact of a product must be taken into account during the design process. But there is an ethical side to this as well.
Input > Output > Outcome > Impact*
Milk can be found all the way in the back of the supermarket. Why is that? Because everybody needs milk. So you walk through a maze of impulses that make you buy more stuff. Stuff you might not actually need. This reminds me of this famous quote by Chuck Palahniuk, “Advertising has us chasing cars and clothes, working jobs we hate so we can buy shit we don’t need.” from Fight Club.
So in a sense, UX and impact are abused, to get you to do stuff you did not actually anticipate on doing. A hidden persuader. A bad thing.
Jan-Wessel did some experimentation with a cultural probe. He designed a small diary with questions. He asked participants to fill out a page of questions daily. The first questions were rather general, but as days passed the questions got more and more into the private atmosphere. “Which Ex-partner do you still regularly contact?” Or even “What medical symptoms did you recently search for and did it turn out to be a chronic disease?”. Participants had increasing difficulties in filling in this information. Whereas most people search for these issues with Google without blinking. There is no barrier or obstruction. Very scary.
He refers to the seven Laws of Identity, an excellent list of principles I wasn’t even aware of. If this is also new to you, read through the Privacy by Design wiki to get a quick overview.
One of the questions I remain with though. “What would a Facebook subscription cost if there would be no data gathering?” If you have an answer to this. I’d love to hear it.
The Linguistic UI
Inspired by the talk given by Jen Williams
Not that long ago, futurists still pondered the possibilities of talking to a computer. Now, we have (sort of) arrived in that era. Controlling a computer by telling it what to do. A big paradigm shift. The graphical user interface introduced us to windows, icons and a mouse cursor. Mid 2000’s the touchscreen was introduced and recently the first conversational interfaces have been introduced.
An entire new way in controlling a computer. Exciting, awkward and scary times. Conversational interfaces have been rolling through the hype cycle and are gaining a lot of traction with Google Home and Amazon Alexa.
Gartner Hype Cycle July 2017 (http://www.gartner.com/newsroom/id/3784363)
However I do feel that these conversation based interactions still feel clunky. Whenever I use a voice based search for example, I tend not to ask a question. Instead, I just spit out keywords. That doesn’t sound very conversational to me. According to Jen, this is where new jobs will be born. Interactive dialogue writers, voice designers and soundscape designers will combine forces with UX and IA professionals.
Where movies still give us the idea that soon we will be actually communicating by voice. I think that is still far fetched. For the coming years it will most likely focus on shortcuts in processes (“Navigate to Football Stadium”) and frequent requests (“Order frequent groceries”). Also we should not forget that a GUI still outruns the CUI, Conversational User Interface, on comparing, complex information, long content and visual entertainment.
So, when we reach these full conversations with our computers, will we forget how to write? How is software going to be maintained? Interesting questions.
The Age of Design
Inspired by the talk given by Luciano Floridi
The second day of EuroIA ended with a keynote by Luciano Floridi. He started with the word Cleaving. Split or sever something. That is what the Oxford dictionary gives us. But in German, it means quite the opposite. It is dirived from the word Kleben, which basically means to stick, paste together.
Presence is no longer equal to location, as you could be anywhere around the world (or our solar system) and still be present. Law no longer restricts to territoriality, modern products and services do not adhere local laws. Ownership does not equal usage, Uber does not own taxis, yet they use them for their own profit. Agencies no longer own all intelligence, instead the agencies rely on big organizations to gather intelligence. Authenticity plus memory resulted in a very important concept, the blockchain. Identity combined with information turned you into a data subject. There is no more offline or online, the internet is always present.
We are living in the age of design, many new concepts are born. We as people have the responsibility to make it an age of good design.
Out into Stockholm
After this inspiring (and tiring) day there was a cool boat tour around Södermalm. Drank some wine and had an interesting conversation on digital ethics with Jan-Wessel Hovingh. Good to be able to talk some Dutch after a full day of English. Went out for a good burger and a few beers with newly met friends afterwards.
Awesome.

= Artificial Intelligence at a Crossroads =
Artificial Intelligence at a Crossroads
The following article is based on my research at the Berkman Klein Center for Internet & Society at Harvard University this Summer in the Special Projects team, and with the support of the Hans Böckler Foundation
Crossroad by Pieter Musterd https://www.flickr.com/photos/piet_musterd/8421297550/ licensed under CC BY-NC-ND 2.0
How much time did you spent behind a wheel this week? Probably more time that you should have. One innovation that could totally disrupt the way we get from A to B is the driverless car. The technology has evolved since General Motors vision of 1939 and if we’d ask companies like Ford and Volvo a fully autonomous vehicle will be ready to hit the road in 2021.
The driverless car is a vivid example to look at the ethical issues that evolve when it comes to its employment of Artificial Intelligence in our everyday life. Let’s do thought experiment to analyze the social dilemma, making use of the good old trolley problem:
Picture yourself in a fully autonomous vehicle in the year 2045. While the vehicle is driving down a street, a 5 year old boy appears on your route trying to chase his escaped ball. The car could either a) avoid the boy and crash into the van in the opposing traffic and potentially hurt you sitting in the car or b) avoid the threat of a car crash with the van and hit the boy. What would you prefer? And should the age of the pedestrian be incorporated in the decision? How would you decide if it would be a 85 year old man chasing the ball?
A study by the MIT Media Lab has shown, that most drivers would prefer other drivers to have self-sacrificing vehicles but when it comes to the vehicle that they are sitting in, they prefer it not to be self-sacrificing. Further the answers differ between cultures.
In this article we will deal with the ethical questions that are being posed in the legal context. Who is accountable in the case of an accident? The driver, the software-developer, or the manufacturer? And how can self-driving cars pass boarders without breaking national traffic regulations? How would international traffic regulations look like? And how can we include different driving behaviors and cultural differences when it comes to deciding who to sacrifice in a dicey situation?
There are many promises when it comes to autonomous vehicles. Statistically self-driving vehicles could cause less accidents, meaning there would be less damage and overall also a decrease of health costs if there are less humans being injured. Furthermore without drivers, labor costs would sink which would make the use of self-driving cars more affordable and accessible. People who are not able to drive a car today could therefore increase their mobility tomorrow. Moreover the traffic flow and urban space could be improved by responsive smart traffic lights for example, and there could also be a positive effect on greenhouse pollution.
But there are also a number of concerns that are being raised. There is a potential loss of jobs in the service and manufacturing sector. Furthermore it is still ambiguous, whether the outcomes will be positive regarding traffic for example, as maybe more people will use driverless cars than before — just think about children as drivers for instance. And understandably there is a list of concerns bringing privacy and security issues up, thinking of hackability for instance.
While US-american states already have passed the way for developers to test their self-driving cars on public roads, Germany has recently overhauled with their legal response by introducing “the most modern road traffic law in the world” this spring.
German companies like Volkswagen, Daimler and BMW are experimenting meticulously with self-driving technologies, as Germany is one of the biggest car exporters worldwide. In response, the Federal Government introduced a law, that allows the driver to take the hands off the wheel during the highly-automated ride, in order to check e-mails for example. But the driver must still observe the traffic or monitor the function of the autopilot and take over in case of emergency. The core of the law is the legal equality of human driver and computer. If a driver allows the automatic pilot to take control, then the car’s manufacturer will be responsible.
The question of who is accountable in an accident is central to the success of autonomous driving — and further blurred. For this purpose, the law defines a comprehensive monitoring system, with the help of which it is possible to determine in an emergency which car was driving when. A black box should record when the system was active, when the driver was driving and when the system asked the driver to take over. As we will see later, the black box technologies come along with several issues that have to be dealt with in advance.
The German legislation leaves many questions open. If the manufacturer is responsible for the decision-making of the autopilot-mode, they must ensure that the system is aware of every speed restrictions and other road traffic rules in every German street for example. Further, the law creates an uncertainty for the drivers when it comes to accountability questions.
Despite these obstacles, Germany pioneered an interim solution to get the cars on the road and to make them learn by harvesting precious street data. But the questions addressing legal issues with self-driving cars are not being deployed in a vacuum, rather there are existing regulations that are being implemented as in the example by the German legislators for instance. Before introducing a law like in the German example, there has to be normative societal consensus in the first place to determine what is right or wrong when it comes to autonomous vehicles and potential deaths in traffic.
Even though the autonomous vehicles promise to decrease crashes, not all of the accidents will be avoided. In some cases there are difficult ethical decisions that have to be made by the algorithm. As argued by the MIT Media Lab, self-driving cars must be programmed to kill. In order to make the vehicle to know how to perform in a dicey setting, a society has to decide, who should be sacrificed and who should be protected.
Experimental descriptive ethics can be a solution to these ethical questions. By giving the public different ethical dilemmas and letting them decide case by case what they would consider as a moral or immoral decision, when it comes to killing someone in a situation like ours with the kid and the van. Should there be an overall regulation for cases like this? Or should each car owner decide in the installation process of their autonomous vehicle? And if so, who is to blame in the case of an unethical decision by the car? An ethical framework can be developed over time and implemented into the design of the autonomous system.
Empty Road in the Mountain Side by Unsplash https://www.canva.com/media/MACCM7JG178
Steering at the global level
These issues bring us to the question of ethics, in which law needs to be anchored. One of the biggest challenges now is to find that consensus, but even within a nation this can be intricate. If we already struggle agreeing on an ethical framework within a culture, how will we ever find an global intergovernmental consensus?
These challenges also appear in the case in other applications of Artificial Intelligence. In already highly regulated sectors, like finance; health or in the justice system the legislators can built upon existing laws and apply and adapt them to the new evolving technology. How should our society look like, when Artificial Intelligence will be implemented in most areas of our everyday life? Different policy-makers respond in various ways, but we will see how they all address similar issues. Let’s look at Japan, the European Union and the United States for instance to see the reactions to AI on a policy-level.
Japan, the world’s leading robotics nation, introduced its concept Society 5.0 — a completely networked society empowered by the advancement of AI. Japan is a rapidly aging society, by 2050, the national census estimates that 40 per cent of citizens are over 65 years old. Japan aims to answer to demographic change with Artificial Intelligence. In nursing and retirement homes thousands of robots are being used today, walking and standing assistants and artificial toys which help the elderly and handicapped people to move more. “Japan, with its energy and resource constraints and demographic pressure, is placed among developed countries on the front line in seeking new societal models, ensuring sustainable and inclusive growth, and maximizing the wellbeing of its citizens.” Legal challenges are being addressed in Japan’s report on AI and human society, when it comes to determine accountability for accidents involving AI. In regard of the Olympics and Paralympics in 2020 in Tokyo, Japan promotes the autonomous vehicles development and drafted rules for its testing on public roads this spring.
How can we design self driving cars that accessible for everyone? Just think about an elderly citizen diagnosed with Alzheimer that uses autonomous vehicles to move from A to B. How can AI be more transparent, so that the diversity of citizens is being well informed enough to understand the risks of utilizing such AI technologies?
The European Union agreed on needing EU-wide rules for the fast-evolving field of Artificial Intelligence, to enforce ethical standards and establish a model of accountability for accidents e.g. involving driverless cars. Furthermore the EU commissioned a study to evaluate and analyse, from a legal and ethical perspective, a number of future European civil law rules in robotics.
With its 28 States, the European Union faces a variety of traffic laws. Several member states allow testing of driverless cars on public roads already. Alone in the case of the use of seat belts, speeding and drink driving the opinions differ between each state. How can the EU find a legal consent when it comes to accountability? The European Parliament prefers to handle global governance issues like this by proposing a “framework in the form of a charter consisting of a code of conduct for robotics engineers, a code for research ethics committees when reviewing robotics protocols and of model licences for designers and users.” How can cars designed outside of the EU enter european roads? And to which extent would foreign car makers therefore adapt to a European code of conduct?
A big amount of research and testing on autonomous vehicles is done in the United States. 7 strategies have been developed by the administration to facilitate AI technologies that provide a range of positive benefits to society, while minimizing the negative impacts. One of the strategies is to measure and evaluate AI technologies through standards and benchmarks. “Additional research is needed to develop a broad spectrum of evaluative techniques.” The black boxes that are being used for decision making often still leave many questions to their developers open. Last year a self driving car crossed a red traffic light for example and in Florida a driver using the autopilot lost his life. “There are serious intellectual issues about how to represent and “encode” value and belief systems. Scientists must also study to what extent justice and fairness considerations can be designed into the system, and how to accomplish this within the bounds of current engineering techniques.”
As we can see there are mutual cross cutting legal issues that are occurring, that raise a number of different interesting questions addressing inclusion; global governance and explainability worldwide. Albeit these three countries are separated by their specific socio-economic and historically defined social norms, they still have some common challenges across nations. Let’s take a closer look to the case of explainability.
Black Sheep by kirahoffmann https://www.canva.com/media/MACV3TpZrSM
Black Boxes and Black Sheeps
When it comes to decision-making about human beings made by algorithms the results are often being displayed without an explanation, as the results are not interpretable. Especially when we think about consequential decisions about access to credit, health and employment, this can be quite concerning. But when it comes to autonomous vehicles these issues seem even more urgent when it’s a matter of life or death.
There are three different forms of explainability that are being considered here. First of all, at the current state, some algorithms are still as complex, that even the software-developers don’t understand their decision making. The so called black box algorithms may be applied in the case when you are applying for a credit at a bank to buy a house. The bank accountant feeds the algorithms with your data, and the machine decides that you are not entitled for the credit. Nor the bank accountant or the software-engineer can you explain the specific reasons why.
In the case of the self driving car, this means that black boxes decide who to kill or not to kill. There are experimental approaches with deep learning self driving cars for instance. Deep learning is a part of machine learning that describes a software learning by recognizing data like images or sounds and classifying them into patterns and then drawing conclusions and applying those results to decision making and actions. Deep learning models are especially complex and hard to interpret. Further deep learning models offer dramatic performance improvements for many perceptual tasks, for example image recognition, that are required for self-driving cars — and so we can’t easily substitute in a more transparent modeling approach.
The company Nvidia developed a technique where the car learns to steer without any explicit instructions by its engineers after being trained what to optimize for. Its decision making simply based by its surroundings activities, the neural-network-based system functions surprisingly well, but it is as complex that the researchers can’t fully follow its decision-making.
Secondly, some decision-making can be unreasonable. In the health sector for instance, IBM’s Watson may decide that every cancer patient in the dataset that wears yellow socks, should be treated with a certain medication, but what kind of logic would that follow if you’d ask humans? Machine learning algorithms may make decisions based on correlations that are not necessarily causal. Especially as health care is so individual, it is very difficult to generalize data. The outcomes can be far-reaching from worsening conditions up to reverse health effects.
When it comes to autonomous vehicles unreasonable decision making can have fatal results. As in the example of the Tesla driver in Florida who got killed while driving in autopilot mode, the car kept going even after its roof got torn off. To a human arbitrations like that occur unreasonable.
And third, some decisions may just purely be injustice, meaning that there is a lack of adequate justification for the algorithm’s design. The models will replicate whatever patterns that exist in the data that are used to train them and will therefore perpetuate any structural biases that might be present in the data collection process or exist in society already in an obscure way.
It seems impossible to explain how the algorithm is consistent with law or ethics, e.g. in the case of sexually biased AI that assists in the employment of people. As the data, that is being fed in those recruitement systems, reflects how male applicants in the past decades have been more successful — as they have been more promoted for instance — the algorithms would propose to hire most likely males as they are more successful according to the logic shown in the database. Obviously the male applicants are not more “successful” because of their sex, the data is simply biased. A number of incidents in the US connected to algorithmical bias have became widespread and disputed across its borders. Consider how problematic a biased autonomous vehicle could be. Think about a car’s decision making in a dicey situation involving fatal outcomes in an accident, that only properly recognize pedestrians as humans that are cisgender white people. We already saw the outcomes of algorithms that can be racially biased.
A possible answer to the concerns about explainability may be the European Union’s solution. A law coming into effect in May 2018 will give its citizens the right to obtain an explanation of automated decisions and to challenge those decisions. But critics challenge the feasibility of that legislation as it “lacks precise language as well as explicit and well-defined rights and safeguards against automated decision-making, and therefore runs the risk of being toothless”.
Clear Road in the middle of Mountains by Unsplashed https://www.canva.com/media/MABKNGtXtQ8
As we saw, there are many ethical and legal issues coming across in the context of AI. As a society we need to decide how we want to design our AI and how we can ensure them to be inherently fair and explainable. Specifically in the case of autonomous vehicles we saw how urgent the development of a consensual ethical and legal framework is. When it comes to possible legal responses, the policy-makers have to take into account that there are applications specific issues as well as cross cutting challenges that need to be addressed. We will see, how the response will look like responding to the evolving technologies and what ethical frameworks society will choose.
But before you step into a car the next time and imagine a dystopian driverless future, use this pacifier and keep in mind that there are still highly doubts, that Ford can keep their promise on launching fully autonomous vehicles in the next decade anyway. We as a society are capable of deciding on how we want to design our driverless future, we are being the ones being asked here and not the cars themselves.

= The Ethical Considerations of Artificial Intelligence. =
The Ethical Considerations of Artificial Intelligence.
Photo by Franck V. on Unsplash
Artificial Intelligence (AI) is routinely branded as the future of technology, however, humanity is split between the possible benefits and risks of developing human-level machine intelligence. Stephen Hawking, and Elon Musk have frequently expressed their skepticism about AI. In fact, in his paper “Existential Risk”, Nick Bostrom heralded AI as the harbinger of human extinction (an existential threat — bang). On the other hand, Mark Zuckerberg, Max Tegmark and Jeff Dean believe that AI will enable us to solve the most difficult problems faced by humanity. Personally, I align myself with Max Tegmark. Echoing his views, I believe that the pursuit of artificial intelligence should be accompanied by research into philosophy, psychology and ethics to ensure that a super-intelligent AI, when created, is ingrained with goals that constantly change and align themselves with evolving human goals — this would not only guarantee our species’ continuation but also let us reap the benefits of having a super intelligent AI.
Before justifying and elaborating upon my position, I would like to define some terms that lie at the heart of my analysis. The ability to accomplish complex goals is known as intelligence[1]. General intelligence is defined as the ability to accomplish virtually any goal, including learning[2]. The ability to accomplish any cognitive task at least as well as humans/non-biological entity with general/super intelligence is artificial intelligence[3]. The capacity to feel, perceive or experience subjectively is defined as sentience[4].
Photo by Michał Parzuchowski on Unsplash
Artificial intelligence has been a part of human society for a number of millennia. Books documenting the history of the Greek Civilization mention Talos, a large automaton, who was tasked with guarding the borders of Europa. Artificial beings have also been common place in fiction, appearing in books such as Mary Shelley’s “Frankenstein” and the movie “Ex Machina”. Modern research into super-intelligence was first conducted at Dartmouth College in 1956. After going through a few cycles of activity-inactivity, AI made its way to consumers and businesses in the late 1990s — mostly being used for data analysis and statistics. In 2011, IBM’s Watson defeated Brad Rutter and Ken Jennings, and won Jeopardy!. Recently, in 2016, AlphaGo won 4 out of 5 matches of Go against Lee Sedol and proved skeptics of artificial intelligence wrong who believed it was many decades before AI could master Go. While narrow intelligence is prevalent in the world today, many philosophers and scientists believe that the advent super-intelligence would be after the ‘technological singularity’. Defined by Irving J. Good, a mathematician who worked with Alan Turing in Bletchley Park, the singularity is comprised of three steps: (i) humans create artificial intelligence; (ii) the artificial intelligence creates and designs a smarter version of itself; (iii) executes its plans.
With many questions lingering around AI’s potential misuse and its label as an existential threat, many people advocate to stop conducting research into the field. Artificial intelligence has unbound potential. Capable of ushering in the next technological revolution and answering some of humanity’s most difficult unanswered questions, AI serves to change the course of humanity much more than any other technology invented so far. As a result of having a viable super-intelligence, nanotechnology, gene editing, post-humanism, space exploration and inter-galactic travel would all become feasible. On the other hand, the prospect of human extinction, mass unemployment and the possibility of the creation of an ‘evil’ super-intelligence deter people from fully supporting AI. Elon Musk once said that, “[Artificial Intelligence is] potentially more dangerous than nukes.” While each side makes valid arguments for/against the development of AI, I believe that we should not abandon the pursuit of AI and instead focus on ensuring that we develop a ‘good’ super-intelligence.
Echoing Max Tegmark’s view, I believe that we need to invest heavily in the fields of philosophy, sociology, psychology and ethics to guarantee the creation of a kind super intelligent being. In his book, ‘Life 3.0’, Max Tegmark asks us to consider the following observation. Purely from an ethical standpoint, two people, in the past, present or future, are going to be at conflict if and only if they have goals and sub-goals that are at odds with one another. Similarly, a super intelligent being, which by the definition of intelligence is going to be very efficient at accomplishing complex goals, would threaten humanity if and only if humanity was a hurdle in its path to accomplishing a goal or a sub-goal. Hawking, similar to Tegmark, recognized this problem and said, “The real risk with AI isn’t malice but competence, a super intelligent AI will be extremely good at accomplishing its goals, and if those goals aren’t aligned with ours, we’re in trouble.” Nick Bilton, a tech columnist for the New York Times, was also acquainted with this issue and said that “the upheavals [of artificial intelligence] can escalate quickly and become scarier and even cataclysmic. Imagine how a medical robot, originally programmed to rid cancer, could conclude that the best way to obliterate cancer is to exterminate humans who are genetically prone to the disease.” Aggravating this problem is the fact that philosophers, even after deliberating for thousands of years, haven’t been able to converge on a common morality that applies to all of humanity — and thereby could be encoded in the ‘DNA’ of the super-intelligent being. I believe that we should enforce coexistence between engineers and philosophers. Inherently, engineers are impatient and don’t consider ethical implications when innovating. By incorporating philosophers into the engineering process, we can not only ensure that society is considered when making engineering decisions, but also make the creation process more holistic.
While some skeptics agree with Tegmark, many bring up a flaw in his ‘master plan’. The creation of super-intelligence would not only aggravate flaws associated with AI, but also flaws associated with technologies that would be within the reach of humanity after the creation of a super-intelligent being — such as biotechnology, and nanotechnology.[5] Many philosophers have expressed deep resentment for many of such technologies. Leon Kass, in “Preventing a Brave New World”, and Francis Fukuyama, in his book “Our Posthuman Future”, highlight the dangers of biotechnology such as eugenics and cloning. Bostrom in his book, “Superintelligence”, also alludes to the dangers of future technologies — he says, “our demise may instead result from the habitat destruction that ensues when the AI begins massive global construction projects using nanotech factories and assemblers — construction.” The breadth of the arguments both for and against AI make applying a specific moral theory to evaluate the problem futile — however, I do feel that Utilitarianism and Ethical Subjectivism offer a unique perspective on the issue.
Photo by Alex Iby on Unsplash
Utilitarianism is based on the concept of maximizing the collective happiness of sentient beings. Owing to this, utilitarianists would be divided on the issue of super-intelligence. The divide would result from whether one could consider super intelligent computers to be sentient and therefore a part of the overall happiness metric or not. If we do, utilitarians would support artificial intelligence wholeheartedly despite the fear of human extinction — because not only do computers outnumber us, but also a super intelligent being could ideally fix all environmental problems on Earth. Ray Kurzweil supports this view in his paper, “Neuroscience, Nanotechnology and Ethics: Promise and Peril”, when he says that “nonbiological intelligence should still be considered human, as it is fully derivative of the human-machine civilization.” David Bentham, one of the founding fathers of Utilitarianism, also agrees with this view and says that “whether the individual in question is human or non-human (in this case, a computer) is just as irrelevant as whether he is black or white.” On the other hand, if we don’t think of AI as sentient, superintelligence would be frowned upon — because it could not only lead to the destruction of billions of lives (humans and otherwise) but also result in a world hollowed out by technology.
Ethical Subjectivism sheds lights on another aspect of the issue — feelings. Ethical Subjectivism states that any moral argument one makes is merely a reflection of his feelings towards that thing. For example, if a person says, “X is morally unacceptable”, he simply means that “I disapprove of X” and nothing more.[6] While many are skeptical of and repulsed by AI, most don’t have a valid reason to support their belief. Supporters of AI and Kant can and would effectively say that by blindly following their feelings, most skeptics and technological luddites are opting out of moral thinking. On the other hand, Leon Kass, will disagree with that assessment. In his paper, “Preventing a Brave New World”, he says, “repugnance may be the only voice left that speaks up to defend the central core of our humanity and therefore humanity should listen to that voice.” This makes Ethical Subjectivism a slippery slope for both, those defending AI and those campaigning to stop its progress.
In conclusion, I believe that if we really want to take full advantage of artificial intelligence, we need to not only invest significantly to solve the engineering challenges, but also rekindle our investments in the field of philosophy and ethics. If we’re cognizant and ensure that we don’t spend all our resources on solving the engineering challenges, while ignoring the ethical consequences of our actions, we will be able to harness the power of AI, avoid the pitfalls and drastically improve the future of humanity. One way to work towards this goal is to fund and found institutes that examine and conduct inquiry into the ethical consequences of technology such as The Future of Life Institute (founded by Max Tegmark) and DeepMind Ethics and Society (founded by Google). Further research and work must also be done to devise a universal ethical standard that is agreed upon by all humans — so that an AI, when created, can be made to adhere to that standard. I am of the firm opinion that while there are many dangers associated with the creation of a super-intelligence, the positives outweigh the negatives. The future is bright and if it plays its card right, mankind has the opportunity to “boldly go where no man has gone before!”
Bibliography:
[1] Taken from Max Tegmark’s Life 3.0.
[2] Taken from Max Tegmark’s Life 3.0.
[3] Taken from Max Tegmark’s Life 3.0.
[4] Taken from Max Tegmark’s Life 3.0.
[5] Harari, Yuval Noah. “Life 3.0 by Max Tegmark Review — We Are Ignoring the AI Apocalypse.” The Guardian, Guardian News and Media, 22 Sept. 2017, www.theguardian.com/books/2017/sep/22/life-30-max-tegmark-review.
[6] The Elements of Moral Philosophy, 4th edition, James Rachels, page 34.

= AI and human-centered design: on criminal justice and filter bubbles =
AI and human-centered design: on criminal justice and filter bubbles
We just posted the video for our latest AI in the 6ix (come to the next one!).
Our panel grappled with two use cases that have strong ethical implications: giving judges bail/sentencing recommendations and how to address social media filter bubbles that facilitate the spread of divisive politics.

The bailing/sentencing use case starts 7 minutes in with Vector Institute researcher David Madras giving an introduction. You can also check out his work with colleagues on fairness in this academic paper or this slide deck.
Lindsay Ellerby (Senior Design Director at Normative) walked us through the human-centered design approach to the problem, reminding us that “the future of design isn’t just designing for people, but for people who are being augmented by machine learning algorithms.”
Whether you’re working in design or AI, you have to examine the human context. In other words, you don’t get to just turn data into models. There’s an ethnographic aspect to the project: where does the data come from and how will your system’s recommendations get used? This isn’t only the foundations of developing sustainable systems that reflect our values: it’s also the key to unlocking the power of feature engineering. Too often companies assume that data science starts with data rather than engaging with subject matter experts to extract prior knowledge, design experiments, and clearly understand just what the proxies we optimize for are measuring.
Human judges are harsher when they’re tired and hungry (this image from the Economist summary of Danziger et al 2011)
While a great deal of the evening was talking about the ethical issues involved in AI, it’s not as if humans have no bias. For example, Danziger et al (2011) show that if you’re up for parole, you definitely want your hearing to be at the beginning of the day or right after the judges have a meal. Humans get harsher as they get tired and hungry. Here’s how Masha Krol (Experience Designer at Element.ai) put it: “This is another example where we need to be really careful of reinforcing that bias…because if you just take the data and in fact don’t take that qualitative insight into account you might in fact reinforce a behavior that isn’t necessarily just.”
One of the interesting challenges in designing AI products is that people need to understand their own role. Dawson Guilbeault (Design Director at Scotia Digital Factory) described the need for an “understanding that data isn’t truth, in that [judges] are part of the creation of that data, so that the judges’ sentences are informing the models and there’s a feedback loop.”
Pathological feedback loops are what’s really behind filter bubbles: you think you’re just liking the news your like-minded friends are sharing but you’re actually walling yourself off to alternate views and letting in more extreme stuff. We start tackling filter bubbles with a definition of the problem just after 18 minutes into the video.
Eli Pariser coined the term filter bubble in his 2011 book on the topic, which is still very relevant today
I think the most common comment I heard afterwards was that people liked the question, “What level do you address the filter bubble problem?”(Here’s where that starts in the video.) Here’s how the team voted:
Give users information about their bubbleness and actions to correct: 1 vote
Change things on the backend, like how the news feed works: 1 vote
Have an in-house team charged with monitoring and mitigating filter bubbles: 1 vote
Filter bubble issues should come under government regulation: 2 votes
What I think is interesting here is how technologists do a better job seeing unintended consequences. One of my favorite ways of thinking about this is Cathy O’Neil’s Weapons of Math Destruction, where she posits that certain areas like education, justice, housing, employment, and finance matter to people’s lives so much that we should start with the assumption that our technologies might have terrible unintended consequences.
So as soon as you’re thinking about bail or sentencing, think: lots of stuff is going to go wrong. But what if you’re a social media company? Are you responsible for predicting that your platform may, say, end democracy? David has a nice discussion of how successful businesses figure out how to scale their intended consequences but “negative externalities” don’t tend to scale as well.
How do we anticipate consequences for our recommendations? Does Netflix get more of a free pass for problematic recommendations than Facebook?
If you like to fast forward to audience participation sections, our Q&A period starts at 34:47. It features questions on explainability, how you even define fairness, and whether we can design systems to counteract problematic human behavior. It concludes with a nice discussion on the problem of “anchoring” — that is, let’s say we give you a number for bail and/or reasons why to set it at a certain amount. Maybe in your system the human can override the recommendation and reasoning, but the fact remains that their actual decision will be tied to what they saw. For designers interested in UI challenges, that’s a good one. Can you elicit a judge’s reaction, then give them a recommendation/rationale and show how all of that interacts over time?
Tyler Schnoebelen (@TSchnoebelen) is principal product manager at integrate.ai. Prior to joining integrate, Tyler ran product management at Machine Zone and before that, founded an NLP company, Idibon. He holds a PhD in linguistics from Stanford and a BA in English from Yale. Tyler’s insights on language have been featured in places like the New York Times, the Boston Globe, Time, The Atlantic, NPR, and CNN. He’s also a tiny character in a movie about emoji and a novel about fairies.


= The cost of failure in human health =
The cost of failure in human health
MAM-X is a computer software that scans breast mammograms and highlights potentially cancerous cells using artificial intelligence. It was a project like countless others I did, except it would change me from the core.
A screenshot of MAMX’s diagnostic capability
Using sample mammogram images, I started to program a software that would screen for these malignant cells, highlighting them in red. Image after image, I trained the software. It became ruthlessly accurate. The red pixels on the screen were more than just pixels,they were living cells. My first success, Patient 1–0–96, was also my first death and every successful detection after that meant confirmation that someone had this disease and may be dying, may be dead or had been dead? Verb conjugation, as a result, was muddled. Was I to be overjoyed or devastated at every success? This process continued and I made progress detecting cancerous cells more precisely.
At first, every image I tested represented a human being. I could not deny the humanity of the images, but after months, they only represented pixels on a screen. The weight of mortality didn’t get lighter for me, it got more familiar. To program the software, I used mathematical equations and algorithms, objectifying the living and literally reducing them to numbers and statistics, losing their humanity in the progress. Doing this was a way for me to ease the human suffering and consequently, suffering became a mere pedagogical tool for me to boost my programming skills.
At first, I apologized to the images, devastated at their outcomes but by the end, I apologized to them not because I felt sad for them, but because I did not. I never questioned why I started this project because for me, the call to protect another’s life was a compulsive duty. I needed to try by best and help these people in whatever way I could.
Whenever the algorithm failed, it led me to understand that technical excellence was not the goal but a moral requirement when so much depended on my programming abilities. When the difference between saving none and potentially saving many lives was defined by the success of a few hundred lines of code on a computer screen.
In the end, the fragility of the human body was evident to me. However, mortality and human suffering was no longer merely fact but an opportunity for me to ease it. The satisfaction and fulfillment I felt when my efforts could be used to help another human being was incomparable. The human body can never reach perfection but we can believe in an asymptote and ceaselessly strive towards it. My journey in using medicine and technology to ease human suffering has just started, a long way from reaching the asymptote, but I’m grateful to know that there is still greater potential for me to help others.
-Mantej (01.12.2017)

= Artificial Intelligence in Warfare Technology =
Artificial Intelligence in Warfare Technology

Artificial Intelligence, though currently a booming topic in technology, has its roots in philosophy and mythology. Early philosophers sparked the idea of Artificial Intelligence, referred to as AI from this point, by describing human thinking as a process of mechanical manipulation of symbols. Since then, we have seen so many technological advances that seek to emulate human thinking in order to aid or sometimes even displace humans in everyday tasks. Though there is much argument over the exact definition of AI, a simplified definition is the capability of a computer system to perform tasks that normally require human intelligence, such as visual perception, speech recognition and decision-making. Since the early 20th Century, AI has experienced a series of resurgences and some argue we are currently experiencing another. The technology industry is booming and everyone from business people to world leaders are recognizing the power of AI.
Companies and research universities in the United States have dedicated enormous funds to the development of AI for their software systems but the government is not far behind with advances in the military field. Earlier this year, Vladmir Putin claimed that “whoever becomes the leader in [Artificial Intelligence] will become the ruler of this world.” There is no arguing that AI has immense power. However, what is often hidden behind the excitement of this new technology is the array of ethical issues that accompany AI technology.

AI has begun to be applied to every industry and warfare technology is no exception. Military robots began emerging during World War II and the Cold War but military technology has become increasingly autonomous through the use of AI. Since then, the US Military has invested heavily in researching and developing completely autonomous weapons.
It is important to understand the difference between automated and autonomous technology, the later of which utilizes AI. An automated system is programmed to follow a precise set of rules and output deterministically, or in other words have the same output for the same input every time. An autonomous system on the other hand makes its own decisions at the best possible course of action given an input and learning data. An autonomous system may not have the same output every time for the same input. Currently, the US military has begun to use automated systems that will act on command but these are far less complex than the autonomous technology that will utilize AI. Some examples of future autonomous military technology in development include artificially intelligent military robots, armed drones, and armed vehicles.
These new technologies, by using AI, will be able to act completely on their own and be faced with decisions that they will use software to tackle. For example, the military, located remotely, could send out a fleet of armed drones in an enemy territory that will use AI to locate and execute a target. In this example the drone would be making all the decisions: where to find the target, what actions to take to reach the target, and how to decide that the target is indeed the correct target. Robots and armed vehicles would work in a similar way and along with the drones be the most durable, precise, and tactful fleet of military weapons to date.

As AI empowers robot autonomy, it is important to consider the difference between robot autonomy and human autonomy and the overall importance of autonomy in warfare technology. It is also important to consider the difference between moral and personal autonomy and what it means to have one or the other or both. Moral autonomy is defined by Immanuel Kant as the capacity to deliberate and give oneself moral law without relying on other entities. Moral law can be defined as guiding principles or ethics, and autonomy is self-governance over one’s own ethics without dependence on leaders, society, or even technology. Personal autonomy is defined as the capacity to decide for oneself and pursue action regardless of moral content.
As discussed earlier, autonomous technology works by taking in input and using learning data to make a decision on what to output. This is very similar to the way humans think and decide as well, which is by gathering inputs and known information and making an output decision based on this information. AI and humans alike learn from new information and use this information to make decisions. Under the definition of personal autonomy, this way of thinking that uses known information to decide for oneself could qualify as autonomous thinking for both robots and humans.
However, there are differences between robot moral autonomy and human moral autonomy. AI technology is programmed to artificially think and decide similar to the way that humans do which means that these programmed robots are relying almost entirely on humans for their decision making implementation. Also, the information that these AI robots will use will either be human entered or picked up from surroundings. When human entered, the AI is again depending on humans for their decision making. In regards to an AI robot’s moral law, one could argue that though it can develop its own decisions they are entirely dependent on the lines of code that enable it to think at all. Looking at it this way, AI does not have moral autonomy under Kant’s definition. This brings to light how amazing the human brain really is, as it can make decisions without any bounds and therefore can be truly morally autonomous.

The Facebook AI that caused panic earlier this year is a good example of the difference between robot autonomy and human autonomy in practice. Facebook created an AI that was to chat about negotiations but had to shut it down in a panic because when two AI chat bots were paired together they developed their own language that humans could not understand. This mistake was later found out to have been caused by the language requirement for the bots being too relaxed, in other words the bots had simply developed a short hand language because there was no code written to tell them to speak only in English. By developing their own language, the chatbots were demonstrating personal autonomy, however it is clear that they developed this language within the bounds of the human written software that enabled them to dialogue in the first place and therefore they were not demonstrating moral autonomy. This means that the chatbots were able to decide without human control to develop a new language to speak in, but this decision was not morally charged.
Humans can attain both personal and moral autonomy but I argue that AI technology can only attain personal autonomy and not moral autonomy under the given definitions. This is important to consider in the area of warfare technology where we are giving up both personal and moral autonomy to autonomous weapons. Autonomy is important because it gives beings the ability to decide and take action based on some reasoning. By the nature of autonomous weapons like drones, robots, and vehicles, humans will need to give up autonomy when letting these weapons decide their own actions to execute their commands. Though humans can feed the robots commands, all the actions thereafter will be performed autonomously by the robot. Though these robots do have personal autonomy or in other words the ability to make decisions on their own, they lack moral autonomy or the ability to think by their own ethical codes. In the case of a moral decision, which are abundant in warfare, the artificially intelligent robots will make the “best” decision based on the available information. This of course brings out many ethical considerations: What is ethically the “best” decision? Who is responsible for the robot’s moral decisions if it does not have it’s own ethical code? Can a robot truly be moral and if not can we trust it war?

The use of autonomous artificially intelligent technology in war can have several morally negative outcomes. Once developed, artificially intelligent autonomous weapons will allow armed conflict to be fought at an enormous scale and with speed humans cannot begin comprehend. These weapons can also become weapons of terror used on innocent people or hacked to behave in undesirable ways. Most importantly, this technology cannot make ethical decisions on its own that may lead to a great loss of human life.
As discussed before, AI decision making and human decision making is quite similar, with AI actually modelled off of the human brain. A key difference however is that AI software can process enormous amounts of information at incredibly fast speeds effectively making it smarter than humans at tasks it was programmed to complete. In regards to autonomous weapons, this would mean constant precision, agility, and efficiency at a level far better than any human. Also, as weapons are made of durable substances, these machines would be much stronger than humans and able to withstand any retaliation with ease. This is troubling because the fact is no human would stand a chance against a powerful weapon of this sort. If enemy forces both develop autonomous weapons, the autonomous weapons would be made to fight each other which would be a conflict of enormous destruction. For example, if two autonomous AI weapons were to battle each other, not only would the scale of the conflict be much greater than anything ever seen before, they will likely destroy everything around them in the process of destroying each other.
Autonomous artificially intelligent weapons can and inevitably will fall into the wrong hands such as terrorist groups and belligerent leaders. When this happens, these weapons will be used against innocent people who will have no way to fight back. Even worse, given this technology, belligerent leaders will no longer need to gain a following to create an army but rather simply command autonomous weapons which will carry out their commands flawlessly and without argue. In other words there will be no concept of insurgency among these lethal weapons, assuming they were programmed correctly, which can be abused by belligerent dictators. Furthermore, like all technology, these weapons are subject to being hacked. The hacker can target anyone including innocent lives. It is evident that the there is great potential for massive loss of innocent life due to autonomous weapons.
The majority of the negative impact of autonomous artificially intelligent weapons stems from the inability of these weapons to be morally autonomous and make ethical decisions. Unlike humans, these weapons have no concept of innocent people versus guilty people other than what is programmed into their system. However, in the hands of the wrong person, any person can be marked as a target which the weapon will only understand as a task to complete. Furthermore, humans do not have the ability to comprehend information at the capacity AI technology can and it is possible that this technology will make decisions that lower the threshold for battle and compromise human life to complete their tasks efficiently. For example, it is possible that in order to follow through with a command to win a conflict for the weapon’s commander, the weapon may determine that exterminating the entire human race is the most efficient method. Though this is an extreme example, it is not impossible because again the technology has no concept of ethical or unethical, only what action its software determines is best for the given task.

Though the negatives of AI in warfare are plenty, there are several positives which drive the development of this technology today. Autonomous weapons can replace human soldiers in the battlefield thus preserving lives. Also, because autonomous weapons do not have any concept of emotions they will arguably be able to act more ethically than humans in some cases and be able to stringently abide by the Laws of War. Finally, this technology will give governments unsurmountable power which is essentially the main reason world powers are eager to develop this technology.
A major positive to artificially intelligent technology is that it can be used to replace human militia and because it is autonomous, it can be used miles away from the controlling country. This means that a country can use autonomous weapons to target an enemy without harming any of its own citizens. These weapons are also much more strong, agile, and efficient as compared to humans and will be able to complete their tasks almost flawlessly. Humans are weak in many ways as compared to machines: humans get tired, they are subject to be injured by the climate and surroundings, and they make mistakes often. AI technology in comparison is not easily harmed and can be programmed to be eternally precise thus creating an immaculate army.
Another advantage autonomous weapons have over humans is that they have no capacity for emotions or the ability to form emotional bonds with other beings. This goes hand in hand with the inability of this technology to make decisions based on an ethical code. As discussed before, these weapons can only carry out their task in the way that is determined to be best given the current information. In some cases, this can actually lead to more ethical decisions being made by the weapons as compared to human soldiers. For example, an autonomous weapon will not be subject to jealousy, anger, or envy and will never act on feelings that would cause it to endanger more human lives than it needs to in order to complete it’s task. In comparison, humans are known to behave unethically when they are faced with decisions that invoke emotions.
Because autonomous weapons do not have emotions and do not act upon morals, they will more stringently abide by the Laws of War as compared to humans if the laws are programmed into the technologies. As evidenced by the Facebook AI example above, AI systems will follow any directions that are programmed into their software blindly. If the Laws of War are built into an AI system, it will make decisions that abide by the laws every time. Again, because of the lack of emotion there will be virtually no way the autonomous weapons will break the laws if they are working properly which is a huge improvement to humans who intentionally and unintentionally break the laws currently.
There is no arguing that artificially intelligent autonomous weapons offer colossal power to countries that develop and employ them. These weapons could be used to protect citizens against attacks as well as to take down terrorist groups. Because of the sheer processing power of these AI systems, we can learn how to strategically defeat enemy forces better than any humans could before. A major positive is that if used correctly and in the hands of good people, these weapons could ideally make the world safer.

Considering the valuable positives and terrifying negatives of artificially intelligent warfare technology, one should wonder how the positives can be maximized and the negatives minimized in our future. Earlier this year experts in the field of technology including Elon Musk and Stephen Hawkins voiced their concerns over the dangers of AI for humankind and called for the UN to regulate the development of artificially intelligent lethal weapons. Elon Musk in particular has become a leader in advocating the dangers of AI calling it “summoning the demon.”
Throughout history the regulation of technology has been highly reactive rather than proactive. Reactive regulation is regulation that is put in place in response to accidents and danger that has already occurred. On the other hand, proactive regulation is put in place before dangers occur to prevent accidents. Most technology we use today is reactively regulated including the internet, social media, smartphones, etc. For example, cyber security regulations emerged only after it was discovered that people were hacking the internet. In the case of the Facebook AI chatbots, giving the AI a strict requirement to speak English was only thought of after the AI spurred panic by developing its own language. We are used to seeing bugs and updates and have become accustomed to the pattern of “fix it later” approaches in technology. However, when it comes to AI, the negative impact and dangers could mean a massive loss of human lives which is an unacceptable outcome of reactive regulation.
AI, especially in the case of autonomous weapons should be regulated proactively. Oren Etzioni, a researcher in the filed of AI recommends “three laws of robotics” that he believes can serve as ethical regulations for AI. The last two rules deal more with issues of privacy and transparency which are not the central issues in regards to autonomous weapons. However, the first rule he recommends brings out important considerations for proactive, ethical regulation of AI in warfare. The rule recommends that an AI system must be subject to the full gamut of laws that apply to it’s human operator. For autonomous weapons, this would mean ensuring that the weapons are subject to the international Laws of War. He also recommends that there should be clear accountability for who is responsible for unanticipated consequences caused by the AI system. This is especially important when considering the responsibility of autonomous weapons. As discussed before, though AI can make decisions on its own, there is no ethical code guiding these decisions. To proactively regulate this technology, there should be a code of ethics for autonomous weapons discussed, established, and implemented into the AI systems. Furthermore, the relationship between the humans commanding the autonomous weapons and the autonomous weapons themselves should be of the upmost importance. This means clear transparency to the human on what the AI is thinking and a clear interface between the human and robot. By proactively having this interface in place, dangerous decisions made by the AI can be evaluated by humans before they are acted upon.
These ideas for regulations are only a fraction of the discussion that needs to be had in order to develop safe, responsible artificially intelligent autonomous weapons. It is also important to consider that these regulations should be implemented at the international level. We are currently in an arms race for autonomous weapons and every major country is motivated by the fear of being trampled in the fight for the strongest autonomous weapons. This kind of competition is what leads to the reckless reactive development of technology. Still, as more and more experts call for action from the UN to regulate AI and lethal weapons, there is hope for a change to proactively regulated autonomous weapons.

By understanding the ethics of AI in the context of autonomous weapons it is exciting but frightening to think of all the advances AI will enable in the future. What is even more surprising is that AI is not as ethereal and far-fetched as it once was and we will begin to see this technology in just a few years as opposed to decades. In regards to autonomous weapons, development is already underway by world powers such as the United States, Russia, and China. Lethal autonomous weapons will become the third revolution of warfare and because of the fast paced nature of the technology industry, we will be living through this revolution. Though many major industry players are still developing AI with reactive regulation, it is promising that influencers like Elon Musk are advocating and funding safe research of AI. As citizens who will be directly affected by this technology, we can also take action by supporting influencers, calling on our government and industry players to proactively regulate AI, and even become involved in the development and discussion ourselves. Above everything, it is important to discuss how AI is different than a lot of technology we have seen before because it poses a massive threat to humans and therefore absolutely needs regulation. The open letter by experts to the UN claimed: “once this Pandora’s box of AI is opened, it will be hard to close if at all.” In 2017, this Pandora’s box has long been opened and we are now in the unique position to influence how AI will impact our lives.

= Artificial Intelligence; Limited By Nature =
Artificial Intelligence; Limited By Nature

Artificial intelligence coming from a buildings worth of water-cooled digital micro-technology may be a source of more information than a person could have. But that’s just knowledge. Einstein, who knew a lot, pointed out that imagination is more important than knowledge. Being smart usually implies having some awareness and at least a little imagination.
Our materialist culture gives little consideration to vital human values like empathy and trust. That oversight fools us into believing that intelligence can be functional without reference and priorities that develop from values. We give no respect to the feelings that make up our consciousness of which intelligence plays a part. That’s why we fear A.I. can do things not possible without overlooking required dynamics that we have in ourselves but devalue as materialists. If a machine is training itself by experience, there is a level of awareness. But it’s not conscious awareness, its transponder sensing data for best choices.
Intelligence and values together give depth for wisdom that can understand criteria and purpose. Values have an element of feeling to them, which are rather analogue. We have the digital resolution needed for our senses to clearly understand music and photos. If the resolution needed to simulate organic feelings is in the noise floor weeds of the quantum world, then feelings would not be a digital possibility. 
A robot can be hooked up with sensors that could inform it on a programmed response. That’s as close to feelings as digits can go. Artificial intelligence can respond, but it will never understand. We could program it to be harmful, but we couldn’t get it to want to take over if we tried. That would be like creating life, which we would do if we could. We don’t let ethics get in the way much.
For instance, we have the technological ability to screw around with genetic coding, so we do. We can pervert life, but we can’t create it from scratch. If we had the wisdom to do it, we probably wouldn’t, and certainly not for profit.
Fortunately, horrific new weaponry has limited application for artificial intelligence. It needs our help for power and maintenance. Weaponry might have a hard time finding support long enough to learn anything useful or intelligent about terminal discrimination. Sense, kill. Violent conflict and hatred require very shallow thinking.
Digital A.I. will assume no feelings, no values or empathy, judgement, curiosity, drive, pushiness, arrogance, nosiness, envy, discrimination nor sabotage. Hey, I never said it was a bad thing. For which I am grateful, because God knows that we need assistance. Having a program with a somewhat tactful (can you program tact?) ability to experience could make it better at assisting us than us.
It will develop a character that will give us more reason to be attached to our Artificial Intelligence than we were to our cars. Not necessarily horses, though.

= AI Ethics: Inclusivity in Smart Cities =
AI Ethics: Inclusivity in Smart Cities
Takeaways from the biweekly meetup that I host as a part of the programmes at the Montreal AI Ethics Institute
Photo by daniel baylis on Unsplash
Let’s start with a brief history for this series of events that’s been running for over a year now and counts 1000+ people from the Montreal community as its members.
How it all started?
It started as an experiment in making a far and wide reaching topic accessible to a broad audience, often not coming from a background in artificial intelligence and/or ethics (and this is a great thing as I’ll highlight soon). What started off as a handful of people coming together once every two weeks hoping to learn more about this emerging topic of AI ethics quickly amplified into a strong movement involving a multitude mobilizing to shape policy, voice diverse views and influence decision makers in considering the ethics, privacy and inclusivity concerns when deploying AI-enabled solutions.
Listening intently at the meetup on the use of algorithms in politics hosted at Expedia, Montreal
The most recent event saw us filling the room to capacity with people coming from very diverse walks of life which was reflected in the quality of insights and variety of subjects and angles that were brought up when thinking about the topic. A big shoutout to Daniel Tarantino from Arup and Global Shapers Montréal for hosting us, providing food and beverages to keep us energized through the discussions!
Packed house to discuss the ethical implications of using AI-enabled solutions in smart cities
The session took the form of a brief introduction followed by fragmenting the attendees into small groups to discuss the reading material that had been shared trying to answer the two guiding questions for the day.
Why the sessions are run the way they are?
A quick note on the specifics of how the sessions are run because it will be an important framework around understanding the insights generated from the group and I hope this will serve as a resource for others looking to organize similar activities.
The choice to break up into small groups, even when there is a large attendance resulting in many groups, comes from my belief in how group dynamics in larger groups inhibit the sharing of everyone’s voice and this format of having a group of 3–4 people allows for higher accountability, encouraging everyone to actively contribute to the discussions.
Attendees at the meetup on the labor impacts of AI-enabled automation hosted at Deloitte, Montreal
The readings themselves are mandatory and quite a few people manage to complete them which allows for a richer discussion almost circumventing the need for starting the discussion from basics. The goal here is two-fold: allow for people to digest material and build competence and secondly, enhance value addition to the time that everyone spends at the event.
The guiding questions give a loose framework to the entire discussions encouraging people to stay on theme which becomes especially important when discussing something that has wide-ranging implications that can have the tendency to run off-topic.
Presentations at the meetup on the Montreal Declaration for Responsible AI hosted at Espace CDPQ, Montreal
Finally, the group discussions are followed by presentations by each of the groups on what it is that they discussed and there are questions and comments from the other attendees which provides another layer of nuance and context to the discussions.
Coming back to the most recent meetup, these were the guiding questions that I posed to the attendees:
1) What are the 3 concrete steps that citizens can take to better engage city officials in addressing issues raised in the articles?
2) What are 3 ways that we can enhance public competence when it comes to meaningfully discussing ethics, inclusion and privacy issues in using AI in smart cities?
The meetup readings covered both the positive and negative outcomes that are happening as some cities are adopting these solutions and the future scenarios that might arise as both the capabilities of the technology mature and they become more widely deployed.
Following are some of the insightful discussions that came out of the meetup:
One of the groups came with some interesting ideas around how to engage with their city officials, especially around the use of storytelling, scenario-planning and narrative building. There were also suggestions to host seminars in public spaces, especially in public libraries and more generally in spaces that are accessible and welcoming to all as a way to spark dialogue. Publishing of action plans and even embedding AI ethics into high-school curriculum were suggested as ideas.
We now have a regular attendee from a high-school in Montreal and it is always fascinating to hear what he has to say growing up in the world as a digital native and accepting AI-enabled solutions around him as an everyday occurrence.
One of the attendees had worked with the smart city department of Ville de Montréal targeting innovative projects. His team had written down future city scenarios and walked through those with city officials in the form of novels set in 2025 with controversial cases as triggers for discussions and policy making.
Will storytelling play a key role in shaping these discussions?
An intriguing point here is the emphasis on storytelling which is a great means of mass communication, movies serve as a case in point but all too often showcase dystopias which ultimately influence how people perceive the advent of AI in different parts of their lives. Perhaps, this is an opportunity to create novel narratives that highlight both the capabilities and limitations in cinematic format for wider reach. Something along the lines of a Bright Mirror vs. Black Mirror?
Source: IMDB
One of the things that I’ve been working on as a part of the initiatives at the Montreal AI Ethics Institute is on translating this knowledge and mapping it to the decision-making process as a tool to aid government workers. This was brought up at the meetup as well and there was a call from one of the groups for clearly articulating a participation process to involve residents in making choices on how these solutions are deployed in their cities. Inviting participation from youth in composing different taskforces will serve to add a degree of realism with how technology is used on the ground. The group strongly advised in placing accountability mechanisms in these public consultations because of their far-reaching implications. The selection of such a group could even follow the process of jury duty selection as a starting point. Another group had mentioned that there should be transparency requirements when deploying AI-based projects in smart cities.
Another group sharing their ideas at the AI Ethics: Inclusivity in Smart Cities meetup
Something that will increasingly play an important role will be enhancing thinking via encouraged cognitive dissonance — especially in today’s paradigm of highly polarized and groupthink-oriented landscape.
Education and room to make errors:
Another group brought around a seldom-voiced perspective of allowing policy makers and government officials more room to make mistakes — especially when trying out new AI technologies. They mentioned that perhaps there was an adherence to more conservative technologies because of the fear of public failure. While that point does bring up a necessary step in bringing benefits via AI, a complimentary piece around an agile regulatory system will be crucial to the success of these experiments. Capacity building is very essential when it comes to AI within these institutions as they are often “legislating horses as cars are coming onto the roads”.
A captive audience at the AI ethics in smart cities meetup
Ethics education for students, professionals and government workers in the context of AI structured via town halls, training programs or classes was brought up as a crucial need to address the deficit that we have today in our understanding of these issues. A call for public officials to educate themselves on AI and ethics was raised because it is their job when making policies that affect millions of people.
The current pipeline in how ideas germinate in the populace and make their way to decision-makers roughly follows the path: activism->academia-> policy-making. Activism plays an important role of highlighting issues that are not yet popular and making the ideas more commonplace, accessible and inviting participation from a wider segment of society. Academia plays the role of framing hypotheses from these movements and gives empirical data to back them up.
With backgrounds in mathematics, business, education, social sciences, government, etc. each group had an opportunity to meet people from vastly different walks of life
There also seems to be a lack of awareness when it comes to initiatives happening in different parts of the world that are trying to address similar issues — a Github of sorts for these projects would serve to cross-pollinate ideas and build a network to share best practices. While some of the larger ideas would need the backing of public organizations to take on those risks, experimentation is quite necessary at all levels of society to surface novel solutions to address these ethical, privacy and inclusivity concerns.
Development of checklists that push for ethical adherence in such projects would be a necessary tool that runs in complement to the above experimentation.
One of the questions raised by the groups captivated the attention of all attendees:
Montreal has a lot of AI experts present locally so it is easy for city officials to consult them, but what about other cities where they don’t have easy access to this? How do we make this a process that scales globally?
Another important aspect when going through this experimentation phase is to set success metrics a priori that are consulted with the public to ensure that it is in line with the expectations, norms and values of that city.

An analogy that resonated with a lot of people was: when we are asked to vote for things that happen in the physical realm of the city, why shouldn’t that also be the case when algorithms might govern different aspects of the city?
Another frequently raised point on sharing responsibility and accountability with the technical folks that develop these solutions was brought up. Something that I liked as an idea was the fact that in Montreal, we have a lot of innovation labs experimenting with different ideas, perhaps they are the democratic tools that can showcase what a future smart city might look like?
Concluding thoughts
My biggest takeaway was that a topic that has so many implications and requires wide-ranging expertise can best be tackled by bringing together an eclectic group of people together and providing a loose framework within which they can debate and discuss their ideas.
There are certainly many lessons to be learned in terms of what to watch out for and how best to integrate informed and competent policy making when it comes to the use of AI-enabled solutions in a smart city context. My hope is that city officials can use this as a starting point for discussion and enlist the help of people from diverse background in ensuring that the evolution of our cities takes the path of strong ethics, privacy and inclusion.
Abhishek Gupta is the Founder of the Montreal AI Ethics Institute, global home to applied technical and social sciences research in the ethical, social and inclusive development of AI. He also does research in AI ethics out of McGill University and is a Software Engineer in Machine Learning at Microsoft.
Website: https://atg-abhishek.github.io
Email: abhishek@montrealethics.ai
Twitter: https://twitter.com/atg_abhishek
- Read his paper with the United Nations on “The Evolution of Fraud: Ethical Implications in the Age of Large-Scale Data Breaches and Widespread Artificial Intelligence Solutions Deployment”
- Watch his interview at the AI for Good Global Summit 2018 at the United Nations
- Read his vision on “How Canada can become a global leader in ethical AI”
For more information on my work and the activities of the Montreal AI Ethics Institute — make sure to sign up via the newsletter here:
Clicking on the above will take you to the sign-up link.
Here are the readings mentioned in the article above:
Mandatory Readings:
1) What It Means To Lead An Inclusive City In 2018–And Into The Future
https://www.fastcompany.com/40542775/what-it-means-to-lead-an-inclusive-city-in-2018-and-into-the-future
2) Mayor de Blasio Announces First-In-Nation Task Force To Examine Automated Decision Systems Used By The City https://www1.nyc.gov/office-of-the-mayor/news/251-18/mayor-de-blasio-first-in-nation-task-force-examine-automated-decision-systems-used-by
3) What Artificial Intelligence Reveals About Urban Change https://www.citylab.com/life/2017/07/what-ai-has-to-say-about-the-theories-of-urban-change/533211/
4) AI in Smart Cities: Privacy, Trust and Ethics https://newcities.org/the-big-picture-ai-smart-cities-privacy-trust-ethics/
5) Stop Saying ‘Smart Cities’ https://www.theatlantic.com/technology/archive/2018/02/stupid-cities/553052/?utm_source=twb
Adventurous Readings:
1) Inclusive AI: Technology and Policy for a Diverse Urban Future http://citris-uc.org/wp-content/uploads/2017/07/Inclusive-AI_CITRIS_2017.pdf
2) AI and the City https://medium.com/urban-us/ai-the-city-a4f40c1a13d7

= What Culture Will Artificial Intelligence 'Belong' To? =
The only way an ethical AI can begin to develop is if there is a focussed development of startup ecosystems in emerging countries. Photo by jesse orrico on Unsplash
What Culture Will Artificial Intelligence ‘Belong’ To?
AI and Ethics: possibly one of the most important debates of our time, and it gets boiled down to a bizarre GCSE-like maths problem.
The test goes:
“There are three cars speeding along…
In the Renault is a young single mother, with two jobs and her rent is paid.
In the Chrysler is an aging judge, a philanthropist and a pillar in the community.
In the Ford is a convicted criminal: a real bad egg.
The cars are on course to crash and one individual will perish.
If you, me or AI had to decide, who would meet their maker?”
This is the introduction to a panel discussion on the Ethics of AI. The crowd are asked to stand in an area labeled with the car manufacturer. As an participant observer, I’m in the Ford corner, along with, I’d guess, the 50% majority. The Judge and Mother share roughly equal pickings.
So who was “right”.
No one knows. The moderator didn’t continue the experiment. We were shuffled back to our seats, slightly deflated. That was it.
I sat there thinking, “It’s weird that we have such conflicting opinions, even though we’re all part of the same cultural clique: young(ish), London-based, tech enthusiasts”.
Although no “correct” answer was hypothesised by the moderator, Peter Hotchkiss (UX/UI Manager at Clarksons), it opened a critical vortex in the Ethics of AI discussion. If we can’t agree on the facile death of a fictional criminal over a fictional mother — and we are cut from the same cultural cloth — how is AI going to make decisions, globally?
Indeed, what cultural values system will AI be born into?
Impossible Quandary?
Surpringinly, for the panel, what seems at first glance to be an impossible question — What culture will AI belong to? — actually has a more straightforward answer.
AI will learn a moral code from existing human values. Most likely from their designers: so, (mostly) white, privileged men, educated in The West.
That said, panelist Charlotte Stix, a Research Associate on the AI Policy and Responsible Innovation Project at University of Cambridge, says we should hold on to the notion that human values are transient and destined to change.
“My worldview and values base are very different to my grandmothers”, she said.
Stix calls for a global values system to be created to feed into the development of AI. This shouldn’t be a rigid a doctrine such as the UN Human Right Declaration because anything set-in-stone does not have the ability to change (easily).
Rather, Stix argues, “we have to give an algorithm the tools to reason when it faces an ethical dilemma.”
That way, the value systems is in a constant state of flux, much like our own.
Algorithms As Decision Makers
“Algorithms will make more and more decisions about: legally, educationally and in terms of employment”, stated Head of Privacy & Data Protection Practice at Gemsery, panelist Ivana Bartoletti.
Fellow panelist, Seyi Akiwowo, founder of Glitch!, a UK not-for-profit for online abuse, concurs. She speaks about how young black women’s car insurance in Newham is going up, because crime rate in the district is on the rise. The women are being legally judged, and financially implicated, on their postcode.
So how can we prevent this level of bias?
Bartoletti believes algorithms’ development should be made public so researchers can scrutinise tech companies and hopefully guide safer, more holistic AI development. French president, Emmanuel Macron is leading the way.
In March 2018, Marcon “guaranteed that all AI algorithms that his government creates will be open to scrutiny to mitigate the threat to democracy”. I wonder where the UK, Russia, and USA are on that front?
Global Problem Needs A Global Solution
The ‘problem’ [with this debate] is that we are in London.
Well not just London, any wealthy metropolis where these debates happen — and AI algorithms are being developed — cannot be representative of the whole the world, and that is the critical issue at hand.
The pragmatic solution is to make AI a truly global project and debate.
That means, we need AI developers in Africa, Latin America, the Middle East, Asia-Pacific and everywhere else. And the best developers from these regions shouldn’t have to ply their trade in Western tech ecosystems for job security and better wages, but in the their own local ecosystems that are also thriving.
The logical way an ethical AI can begin to develop is if there is a focussed nurturance of startup ecosystems in emerging countries the world over. Then, smaller nations will be able to enter the AI expansion project. And — crucially — feed their local, culturally-specific values into the global AI system.
Of course, even then there will still be cultural discrepancies. In London we couldn’t even agree what hypothetical stereotype to perish in their car! However, the evolution of developing world startup ecosystems could democratize Western tech hegemony, and help create an AI for all.
Either that, or we start giving ethics lessons to Harvard grads!
Craig writes for Calcey Technologies, a boutique software product engineering agency with roots in the Silicon Valley, that lends its software development muscle to start-ups and scale-ups around the world. Calcey’s team of 100 engineers, based at its development centre in Sri Lanka, serve multiple startups in London and are keen to engage with more, particularly those applying AI to disrupt industries. Calcey’s clients also use it as an R&D centre to carry out proof of concept projects, when productising new concepts. Calcey’s client portfolio includes well known brands such as (eg: PayPal and Stanford University) and exciting startups.

= When it Comes to AI and Weapons, the Tech World Needs Philosophers =
When it Comes to AI and Weapons, the Tech World Needs Philosophers
If a company says its technology doesn’t cause injury, how is that defined?
Illustration: kargoman/Getty Images
By Ryan Jenkins
Silicon Valley continues to wrestle with the moral implications of its inventions — often blindsided by the public reaction to them: Google was recently criticized for its work on “Project Maven,” a Pentagon effort to develop artificial intelligence, to be used in military drones, with the ability to make distinctions between different objects captured in drone surveillance footage. The company could have foreseen that a potential end use of this technology would be fully autonomous weapons — so-called “killer robots” — which various scholars, AI pioneers and many of its own employees vocally oppose. Under pressure — including an admonition that the project runs afoul of its former corporate motto, “Don’t Be Evil” — Google said it wouldn’t renew the Project Maven contract when it expires next year.
To quell the controversy surrounding the issue, Google last week announced a set of ethical guidelines meant to steer its development of AI. Among its principles: the company won’t “design or deploy AI” for “weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people.” That’s a reassuring pledge.
What’s harder is figuring out, going forward, where to draw the line — to determine what, exactly, “cause” and “directly facilitate” mean, and how those limitations apply to Google projects. To find the answers, Google, and the rest of the tech industry, should look to philosophers, who’ve grappled with these questions for millennia. Philosophers’ conclusions, derived over time, will help Silicon Valley identify possible loopholes in its thinking about ethics.
The realization that we can’t perfectly codify ethical rules dates at least to Aristotle, but we’re familiar with it in our everyday moral experience, too. We know we ought not lie, but what if it’s done to protect someone’s feelings? We know killing is wrong, but what if it’s done in self-defense? Our language and concepts seem hopelessly Procrustean when applied to our multifarious moral experience. The same goes for the way we evaluate the uses of technology.
In the case of Project Maven, or weapons technology, in general, how can we tell whether artificial intelligence facilitates injury or prevents it?
The Pentagon’s aim in contracting with Google was to develop AI to classify objects in drone video footage. In theory, at least, the technology could be used to reduce civilian casualties that result from drone strikes. But it’s not clear whether this falls afoul of Google’s guidelines. Imagine, for example, that artificial intelligence classifies an object, captured by a drone’s video, as human or nonhuman and then passes that information to an operator who makes the decision to launch a strike. Does the AI that separates human from nonhuman targets “facilitate injury?” Or is the resulting injury from a drone strike caused by the operator pulling the trigger?
On one hand, the enhanced ability of the drone operator to visually identify humans and, potentially, refrain from targeting them, could mean the AI’s function is to prevent harm, and it would, therefore, fit within the company’s new ethical guidelines. On the other hand, the fact that the AI is a component of an overall weapons system that’s used to attack targets, including humans, could mean the technology is ultimately employed to facilitate harm, and therefore its development runs afoul of Google’s guidelines.
Sorting out causal chains, such as this, is challenging for philosophers and can lead us to jump through esoteric metaphysical hoops. But the exercise is important, because it forces the language we use to be precise and, in cases like this, to determine whether someone, or something, is rightly described as the cause, direct or indirect, of harm. Google appears to understand this, and its focus with causation is appropriate, but its gloss on the topic is incomplete.
One problem its guidelines don’t adequately address is the existence of so-called “dual use” technologies, which can be used for civilian or military purposes. Something like a drone’s autopilot system can be used for a task as innocuous as recording a snowboarder traveling down a mountain, or that same technology could be used to allow a loitering munition to hover above a battlefield while its operator scrutinizes the area below for targets. Which of these is the “primary purpose”?
A more rigorous set of ethical guidelines would make it clear how corporations would approach the development of ostensibly innocent technologies that could be co-opted for “evil” uses. While Google’s guidelines state, “We will work to limit potentially harmful or abusive applications,” it would be comforting to see a more robust explanation of how the company will evaluate the potential for harm or abuse, and how it distinguishes a technology’s primary use from its other uses, since the uses of an invention often become clear only much later.
In the context of Internet surveillance, Google’s new guidelines place constraints on what data the company will collect, saying it will shun “technologies that gather or use information for surveillance violating internationally accepted norms.” But “accepted norms” isn’t a sufficient catch-all, because in some countries, spying on everyone, all the time is the accepted practice.
Indeed, it presents a classic problem in philosophy: You can’t justify an action by pointing to what everyone else is doing. There has to be a way to determine the difference between what people do and what they ought to do — otherwise, no one ever does the wrong thing. Google’s guidance falls short because it relies on a relatively nebulous concept — “norms” — rather than an articulation of company values. For a statement of principles such as Google’s to mean something, companies have to know their own values, be committed to them, and then sort through these questions in tandem with the technological development process, but it can’t be accomplished by the development process alone.
What Google has now is a start: It provides what philosophers would call a necessary condition. It has articulated that, at a minimum, the company should avoid developing technology that falls afoul of international norms. But this still leaves too much wiggle room, since widespread data collection is commonly practiced internationally — and is, therefore, arguably an accepted norm — but is also widely regarded as harmful by civil libertarians.
Scientists search for answers, but, as the work of the University of South Carolina’s Justin Weinberg illustrates, philosophers’ contributions to a decision-making process can be hard to spot, because the value of philosophy often includes discovering additional questions. Questions about the ethical use of technology may seem tangential to the development of new and innovative technology, but because the value of a technology is in its application, and a company like Google is valued based on the application of its technology, the unpacking of these philosophical questions — and a meaningful enhancement of ethical guidelines — adds value to technology. Identifying ambiguities in a company’s ethical reasoning, then, is good for both the society impacted by the technology and the corporate bottom line.
As a leading tech company, Google shapes technologies that affect billions of people and its commitment to answering these philosophical questions sets the stage for the rest of Silicon Valley — as a company, it can initiate a race to the top in terms of the ethical principles to which tech companies commit themselves.
To “get this right,” in Google’s words, doesn’t just mean developing a mission statement. It means crafting an ethics policy sensitive to ambiguities. It means considering different understandings of causation, harm and moral justification. And it means making sure the ethical core guiding a company meets the technical challenge of constructing artificial intelligence. All that means incorporating the input of philosophers.
Jenkins is an assistant professor of philosophy and a senior fellow at the Ethics + Emerging Sciences Group at California Polytechnic State University and the director of ethics and policy for WhiteFox Defense Technologies, Inc.
Special To The Washington Post

= “What is the society you want to achieve?” =
“What is the society you want to achieve?”
Mehran Sahami and Hilary Cohen from Stanford University discussing how leaders in tech are confronting the ethical challenges of AI & ML.
Since its inception, Silicon Valley flourished by innovating relentlessly to build transformative technologies that would change the world. If it broke things along the way, well, that was the price of revolutionary progress. But as the 2016 election results showed, algorithms have an enormous impact on what we see and decisions we and technology make. The implications for how swiftly algorithms are shifting decision-making for society at large as well as the moral responsibility for those who code are enormous challenges, according to Mehran Sahami, a Computer Science Professor at Stanford University.
Sahami and Hilary Cohen, Pre Doctoral Research Fellow at Stanford, spoke last week for our second tech meetup as they offered an insider’s view of some of the ethical and social challenges presented by the development and use of artificial intelligence and machine learning. They examined the responsibility of students, technologists, and policymakers and provided a framework to think through the challenges of autonomous vehicles, data privacy, algorithmic decision making, bias, and “fairness.” Here were some of the highlights:
Technologists today are where physicists were in 1945.
They created new technology with great hope that ended a war as a result. People appreciated the role they played helping to end WW2 but then we all had to live under a potential nuclear nightmare for the next 44 years. That’s the way modern technology is starting to be viewed by some people both in and outside Silicon Valley. More recently, Silicon Valley, once universally applauded for creating revolutionary technologies that created all sorts of new possibilities, is now being criticized for creating tech without thinking through the social impact. Which is why many now ask, if the social impact is this big, must regulation accompany it?
Technology is advancing far faster than even experts thought possible.
In 2011, Sebastian Thrun — often considered the brainchild of autonomous driving — shocked people when he predicted that cars will drive themselves in 50 to 60 years. People thought that was too aggressive. Little less than a year later, Nevada became the first state to issue an autonomous license for driverless cars.
The challenge lies in programming for mutually incompatible notions of fairness and inherent bias.
Apple’s facial ID was programmed against caucasian norms and was not as good at distinguishing Asian faces, which meant it was not secure or useful for a large portion of the world. When programming algorithms, the attributes used to create predictions and what is equitable as it relates to predictive correctness shifts based on the assumptions used.
The data is what impacts what the algorithm does.
Algorithms make decisions and learn based on the data they process, so the data set is almost more important in shaping outcomes than the algorithms. When it comes to ethics, we tend to focus on the algorithms but the quality of the data and where it comes from matters just as much.
“We are creating stuff without thinking about the long-term societal impact.”
Silicon Valley is doing a great job in terms of innovating and creating new things but the biggest point in leadership failure is in thinking through the social implications of technology. How do we build technologies for good? For public interest? For social impact as opposed to just building technology for generating revenue? “Unless we pay some serious attention to what we are doing, the regulation that will invariably come will be potentially much worse than anything we can do to please ourselves,” Sahami said.
There’s a role that technologists can play.
Sahami and Cohen urged technologists to reach out to policy makers as they navigate through what are demonstrably complex, technical questions and proactively curb potentially “bad societal outcomes.” Sahami concluded his talk with a call to action for increased engagement. “We do understand the technology. Let us play a role.”
Sahami and Cohen noted there are no simple answers and that the best starting point is looking at the societal outcomes and to continue to learn from each other to develop ways to think about the implications of the technology being created.
www.costanoavc.com

= Fourkind's thoughts on the ethical use of artificial intelligence =
Fourkind’s thoughts on the ethical use of artificial intelligence
Ethics in artificial intelligence has become a hot topic as of late, with individuals, corporations, non-profits and even governments establishing guidelines and manifestos for the responsible use of AI. And rightly so; as AI continues to grow in popularity and permeate all aspects of society, it’s clear that we need to be cognisant of the change we’re bringing about. In this post, we’d like to clarify the stances & guidelines Fourkind has on the responsible use of AI; guidelines that reflect our thoughts in general whilst remaining honest about the realities of AI development. Let’s start with the latter first.
Biased algorithms
You can’t talk about ethics in AI without bringing up bias. Bias has many definitions depending on context, but in terms of social bias or predjudice, there’s no such thing as a biased algorithm. For symbolic artificial intelligence, the way an algorithm works depends on who made it; for machine learning, it depends on the data it learned from. Algorithms may work in a way that is seen as biased, but in and of themselves, they have no understanding of the concept. Social bias as a result of how an algorithm behaves stems from us humans.
Biased data
In a statistical sense, bias is an everyday occurence. If we want to estimate approval ratings in an election, a slight mistake during the collection or treatment of our sample might lead to a result systematically different from the true population parameter. But the data itself is just, well, data. It’s not socially biased, it’s just a bunch of numbers. A dataset needs to be constructed carefully to avoid inducing social bias, but it’s not biased in and of itself. That’s a distinction worth keeping in mind as we develop AI systems responsibly.
Discrimination
Discrimination is everywhere. If you go to buy clothes and ask an employee for recommendations, the answer they’ll give will likely depend on your appearence, gender, and any other information you provide. One size doesn’t fit all, and in most cases, we don’t want it to. The same applies to the vast majority of artificial intelligence: we want to act differently based on the information we’re given. We want a discriminative model. At the same time, we want behave in a way that is socially responsible, ethical and law-abiding. The onus falls on us to make something that is both ethically sound and beneficial, and straddle the line where we discriminate against some things but not others.
Subjectivity
Social bias is subjective. What’s socially acceptable varies based on who you ask, where in the world you are, and who you interact with. It evolves over time. Given the “softness” of the concept, the only way to act responsibly is to understand what we, as individuals and as a modern society, deem as inherently wrong, and do our utmost to avoid developing systems that act unfairly. Using software to help check and verify is ok, but in the end, we are responsible for our work. There is no oracle to ask except our own moral compass.
Our guidelines
At Fourkind, the guidelines we have for the ethical use of AI serve not only as an internal tool, but also as a public stance. We‘ve tried to avoid overly general or misleading statements that gloss over the realities of making AI. The guidelines below represent our common values, honesty included.
#1: To the extent that it is possible, avoid creating AI systems that reinforce commonly understood social biases, contravene human rights, violate human dignity, or otherwise breach your own moral code
If you encounter and/or are asked to do something you find unethical, raise the issue immediately internally and with your client. Never sacrifice your own values for added model accuracy, financial gain, or fear of failure.
#2 Follow all applicable laws, directives and mutually agreed-upon best practices
This also applies to things like internal and client-company privacy guidelines/policies and GDPR. If you are unsure about interpretation, raise the issue with your client or internally, depending on the circumstance. Note that in some cases, such as laws regarding minors, you may have to enforce discrimination to behave ethically.
#3: Hold yourself accountable
If you develop an AI system, take ownership of it. Be honest about the way it works, the type of data used to train it, and address issues in a timely fashion. Don’t place the blame on others or try to hide mistakes. Inform clients immediately if something goes wrong.
#4 Be as transparent as you can
The inner workings of some AI models can be explained easily, some are more of a black box. Sometimes laws and regulations affect the choice of learning algorithm. In all cases, make sure to document the data and process you use when developing AI. Transparency also applies to end-users; make sure that the use of AI is documented and to offer a non-AI alternative solution not only where required, but also where feasible.
#5 Use your own judgement
At Fourkind, we go to great lengths to find the best people we can; people who we trust to make sound decisions not only collectively, but also as individuals. Don’t be afraid to speak up and err on the side of caution when you feel something isn’t quite right.
As the use of AI proliferates further, it’s possible — even probable — that Fourkind will refuse a project on ethical grounds (as of August 2018, we have yet to do so). This is more likely as we enter an era of high automation, where the temptation to compete at the expense of fairness could become more common than it is today.

= Design, Data, and Artificial Intelligence =

Design, Data, and Artificial Intelligence
The merge of the design discipline with data science and machine learning are part of today’s agenda. Design consultancy IDEO has fairly recently acquired data science company called Datascope. Tim Brown, Ideo’s CEO, states “The things we designed were relatively dumb and all of the intelligence in the relationship between us and the artefact came from the human being. Algorithms and technology are taking on their own intelligence. That’s a fundamentally new design problem. We’re designing relationships now as opposed to designing artefacts. How does the traditional discipline of design, the new discipline of data science, and the new technologies of machine learning come together to form these relationships?”
The opportunity of AI as a critical pillar in business strategies should be powered by putting the people at the centre and committing to develop morally and ethically responsible systems. Some experts stress on the little knowledge around AI and the possible unfavourable impact that could bring. While others, with a more positive standpoint, emphasise on the importance of collaboration between human and machine intelligence to reach the best possible outcome.
The creation of what users need comes through a very engraved iterative process. Designers use qualitative and quantitative data to make sure that the products they are building are right for the business and the users. The current communication between humans and technology is mostly facilitated by screens, however, with the latent advancements in AI, there is a common movement towards more natural types of communication and invisible interactions. Digital experiences can now offer from great improvements in personalisation and automatisation to digital assistants that understand voice. This iterative process can be harder to do with AI systems, where the technological barriers are still too high.
Some might argue designers are not prepared to design this new influx of AI systems leaving a gap in their skill set for two main reasons. Firstly, AI and Machine Learning are not included in the designer’s education or in design toolkits. Secondly, the technology is complex which limits the designer’s ability to test with it or even understand how it works.
Yet, if you look at the history of User Experience Design, every turning point in the discipline can be directly related to the change of technology and human interaction, and that is what is happening at the moment with AI systems.
John Maeda explains in InVisions documentary Design Disruptors, “When technology is the thing you want, you don’t need design, because you want the better technology. When technology matures you don’t buy just based on technology, in that case design comes into the foreground”. The designer is considered to create the link between technology, business and users by humanising a set of information and make it useful by constantly testing solutions and learning from the results. Designers have focused on finding ways to test solutions throughout the process with tools that do not necessarily rely on technology. The limitations that AI, and subsequently all new technologies, present in the design discipline can be circumvented by the development of the designers toolkit. Where the mindshare is turned into action and new tools, which build on human-centred design practices but also address ethical and impact considerations, are created.

= 12 Brilliant Women in Artificial Intelligence & Ethics to Watch in 2018 =
12 Brilliant Women in Artificial Intelligence & Ethics to Watch in 2018

The possibility of creating sentient machines that can think and act like humans raises many ethical issues. We’re already encountering reinforced human bias in AI algorithms and with autonomous “killer” robots looming on the horizon, an open discussion on the perils of unchecked AI is even more imperative.
To celebrate Women’s History Month, we’ve highlighted 12 brilliant women leading this much-needed discussion on AI & ethics and development of responsible AI solutions that will benefit everyone. Let me know of any others we should highlight in the comments below.
1) Joy Buolamwini @jovialjoy
Algorithmic Justice League

First person on our list is Joy Buolamwini, founder of Algorithmic Justice League to fight bias in Machine Learning. The Algorithmic Justice League is a collective that aims to highlight Algorithmic Bias through Media, Art, and Science. Joy also researches social impact technology at the MIT Media Lab and is a Rhodes Scholar, a Fulbright Fellow, an Astronaut Scholar, a Google Anita Borg Scholar. She is also a popular speaker who has given talks at TEDx, the White House, and the Vatican. You can learn more about Joy and her work at Algorithmic Justice League
— —
2) Terah Lyons @terahlyons
Partnership on AI

Terah Lyons is the executive director of Partnership on AI, an organization established to study and formulate best practices on AI technologies, to advance the public’s understanding of AI, and to serve as an open platform about AI. It includes well-known players like Apple, Facebook, and Microsoft. Terah was formerly policy advisor to the U.S. chief technology officer at the White House Office of Science and Technology Policy (OSTP), where she was behind the Obama administration’s deep dive into AI’s potential to change the world. You can get more information on Terah’s work at Partnership on AI
3) Kate Crawford @katecrawford
AI Now Institute

Kate Crawford needs little introduction as she is a well-known figure in the world of AI. A Distinguished Research Professor at NYU and principal researcher at Microsoft, Kate is also the co-founder of AI Now Institute along with Meredith Whittaker. One of the group’s main focuses is to ensure that the engineers making AI algorithms are working closely with the people who will use them. In the case of a medical application, for example, they want to make sure that doctors are consulted as part of the research. This kind of work will help point AI toward tackling the kinds of problems that actually need solving, as opposed to only the problems that a computer programmer thinks to work on.
You can learn more about Kate’s work at https://ainowinstitute.org
4) Dr. Olga Russakovsky @ai4allorg
AI4ALL

Dr. Olga Russakovsky, is one of MIT Technology Review’s 35 Innovators Under 35 for 2017. Olga, is an Assistant Professor in Computer Science at Princeton University and the co-founder of AI4ALL along with Fei-Fei Li. AI4ALL is a national nonprofit educating the next generation of AI technologists, thinkers, and leaders. Olga is working to make the field of AI more diverse and inclusive by introducing underrepresented K-12 students to AI and AI’s power to address social and human challenges. You can learn more about her work on inclusion and diversity at AI 4 All
5) Ajung Moon @RoboEthics
Open Roboethics Institute

AJung is a roboticist with over eight years of experience investigating the ethical and social implications of robotics and AI. She is an internationally recognized expert in her field and has been heavily involved in the shaping of global policy and discussions surrounding artificial intelligence and autonomous systems. She is one of twenty members of the International Panel on the Regulation of Autonomous Weapons. Ajung is also is an executive member of the Executive Committee of the IEEE Global Initiative for Ethical Considerations in Artificial Intelligence and Autonomous Systems. You can learn more about Ajung’s work in RoboEthics at Open RoboEthics
6) Kay Firth-Butterfield @KayFButterfield
World Economic Forum

Kay Firth-Butterfield is the Project Head for Artificial Intelligence and Machine Learning at World Economic Forum and the Vice-Chair, IEEE Initiative on Ethics of Autonomous and Intelligent Systems. She co-founded AI-Austin, AI-Global and the Consortium for Law and Policy of Artificial Intelligence and Robotics. She was recently named one of the top 25 Women in Robotics. Kay is also a Barrister-at-Law and former Part-Time Judge in United Kingdom. In the US, she has been a Professor of Law and International Relations, Associate Fellow, Centre for the Future of Intelligence at University of Cambridge and Fellow at Robert E. Strauss Center on International Security and Law, University of Texas. You can follow Kay’s work on AI Austin website
7) Rumman Chowdhury @ruchowdh
Accenture

Rumman Chowdhury is a Senior Principal at Accenture, working on cutting-edge applications of Artificial Intelligence and leading their Strategic Global Initiative on Responsible Artificial Intelligence. Rumman advises companies on ethical AI practices and work with organizations such as the World Economic Forum, and the IEEE’s Wellbeing Metrics Standard for Ethical Artificial Intelligence and Autonomous Systems. You can sign up for updates on Rumman’s work in responsible AI on her website.
8) Eleonore Pauwels @AI_RRI_Ethics
Wilson Center

Eleonore Pauwels is the director of the Anticipatory Intelligence Lab at the Wilson Center’s Science and Technology Innovation Program. She specializes in the governance and democratization of converging technologies, analyzing and compares how transformative technologies, such as artificial intelligence and genome-editing, raise new opportunities and challenges for health, security, economics, and governance in different geopolitical contexts. Pauwels regularly testifies before U.S. and European authorities, including the U.S. Department of State, NAS, NIH, NCI, FDA, the National Intelligence Council, the European Commission, and the UN. You can learn more about Eleonore’s work at Wilson Center.
9) Danah Boyd @zephoria
Data & Society

Danah boyd is the founder and president of Data & Society, an organization committed to identifying thorny issues at the intersection of technology and society, providing and encouraging research that can ground informed, evidence-based public debates, and building a network of researchers and practitioners who can anticipate issues and offer insight and direction. Danah is also a Principal Researcher at Microsoft, and a Visiting Professor at New York University. Her research is focused on making certain that society has a nuanced understanding of the relationship between technology and society, especially as issues of inequity and bias emerge. She is the author of “It’s Complicated: The Social Lives of Networked Teens”. You can read more about Danah’s work at Data Society
10) Beena Ammanath @beena_ammanath
Humans for AI

Beena Ammanath is the Global VP — Artificial Intelligence/Data/Innovation at HPE and founder of Humans For AI, which is a non-profit organization with volunteers across business, engineering, science from various industries from tech to legal to retail to education. She is co-author of the book “AI Transforming Business and has been honored several times for her contribution to tech and her philanthropic efforts, including the San Francisco Business Times’ 2017 Most Influential Women in Bay Area , WITI’s Women in Technology Hall of Fame , and others. She believes AI’s pervasive impact will require humanity to control the pace to which it is woven into the fabric of society. Beena’s goal is to humanize AI to enable and empower the AI workforce of the future to be inclusive and to be as diverse as the real world. For more insights on her approach, watch this recent panel discussion “Ethical AI: What kind of society do we want to have?” with Hessie Jones, CMO for Humans for AI and Girl Geeks Toronto.
11) Mira Lane @mira_lane ‏
Microsoft

Mira Lane is Partner Director of Design & Ethics — Artificial Intelligence for at Microsoft. Mira’s team is responsible for thinking about the ethical implications of technology and how it might impact the people and societies that are using it. Her background as an artist and computer scientist gives her a unique perspective into how new technologies shape our experiences and our world. Mira has helped create a discipline around ethics at Microsoft that is dedicated to thinking through the implications of AI and helping the organization be more thoughtful about and accountable for what it is designing and building. Learn more about Mira’s work on her LinkedIn Profile.
12) Cortnie Abercrombie @cortnie_cdo
AI Truth

A recent addition to this list is Cortnie Abercrombie who recently left her Executive role in Artificial Intelligence at IBM to start AI truth, an organization focused on bringing together AI practitioners and the public so they can collaborate to ensure AI does in fact make the world a better place. You can read more about her proposed approach at AI truth
— —
Mia Dand is the CEO of Lighthouse3.com, a Strategic Research & Advisory firm based in the San Francisco Bay Area. Mia is an experienced marketing leader who helps F5000 companies innovate at scale with digital and emerging technologies. She has built and led new digital/emerging tech programs for global brands including Google, Symantec, HP, eBay and others. Mia is a strong supporter and champion for diversity & inclusion in tech. You can reach her on Twitter @MiaD

= OpenHouse.AI: Disrupting Real Estate through Transparency =
OpenHouse.AI: Disrupting Real Estate through Transparency

How does technology alter an industry that is heavily regulated, with strong checks and balances and where information is heavily guarded? The real estate industry is comprised mainly of independent realtors, unified under brokerage houses and a nationwide real estate association. Within the confines of this oligopoly are rules to protect its players and ensure the market maintains dependence on the structure that has been defined for decades.
In the meantime, the market has emerged into an on-demand economy that has been driven by information. Today home buyers’ increasing access to information allows them, in some ways, to circumvent the agent. The need for instant gratification and knowledge to find the best value at the lowest cost is slowly evolving this industry. How real estate succeeds in the next decade will fully rely on the changing habits of the home buyer, with an acquiescence to slowly dismantling this market structure to improve buyer access to information, while creating new sources of value.
One start-up based in Calgary and Toronto is paving the way for this disruption and is challenging the players: the realtors and the home buyers to think differently about these transactions. OpenHouse.AI is a cloud-based data and feature rich web platform for residential real-estate that empowers buyers and sellers to make better and faster real estate decisions through clarity and transparency.
Founded by Will Zhang and Yanky Li, these young entrepreneurs are truly unravelling what has been a very traditional model and creating a platform where free information significantly changes the game.
All Roads Lead to OpenHouse.AI
Will Zhang worked in R&D for over 12 years. He graduated with a Masters in Software Engineering from France and worked in technology spanning several industry sectors including utility, finance, telecom and accounting. When he graduated business school at NYU, he experienced an “awakening” that changed the way he started to think.
I wanted to work on something more significant in life and help people more directly to create a positive social impact beyond a paycheck and early retirement. I always feel I have to change to stay the same and be true to myself. Being an entrepreneur was perfect for me.
Yanky Li was always an entrepreneur at heart, always selling things to family, friends and classmates at a young age. He remembers in elementary school when he bred and sold hamsters and made some decent money with the transactions happening at school and the inventory carried in a plastic tofu container. Over the years he spent his professional career in business and technology. Through consulting and project management roles, he continuously looked for solutions and was expected to think on his feet. Working for others and living their dreams was less satisfying. Yanky wanted to live his own dream.
The vision of OpenHouse.AI is to connect people, while the company delivers the insight. For Will, the idea for this platform germinated through his experience. In his career he was fortunate to witness the freeing of information that would create a much wider impact than any one individual. Applications created under the open source movement, particularly in the machine learning community, have made significant strides. Wikipedia founded with an idea that “every single person is given free access to the sum of all human knowledge” leverages the power of community and trust to sustain itself. One day, Will was hanging out with a friend, also working in advanced analytical research and both came to the realization they could apply their skills to help the average home buyer make one of the most important decisions of their lives.
After my daughter was born, my wife and I were looking at an open house. I met the owner. He started walking us through his home and talked about the renovations and upgrades he put into the house [Much of this information wasn’t in the real estate listing]. This was a great buying experience for us. I wish I had more of those. It was then I conceived of a product that connects the homeowner with the realtor to create a better experience in the buying journey. I shared this idea with Yanky, my longtime friend: We build a product that allows people to contribute information. Then we deploy technology to aggregate and analyze the information and open the data to any company or third party that wants to consume it.
For Yanky, it was an opportunity to make an impact in the world:
I’ve come to a point where I’ve acquired a lot of experience and I wanted to take that to the next level and impact people’s lives. I’ve gone door to door and heard people’s stories [and we understand their pain points]. We’re at a time when technology is pervasive and costs are shrinking and startups are trying to help people by leveraging technology. We don’t have a real estate background so we go into this asking: how should this be working?
How do you Change Mindsets?
How do you tell the real estate brokers, who have always been the industry guardians of truth that freeing this information will not put their jobs in peril? The founders are on a mission to create a positive impact for everyone. When the technology is created in the right way and is deployed correctly, it can empower people.
For the realtor, technology can provide valuable insight. The human connection is also very important. In real estate, relationship is imperative. These experiences drive more business. OpenHouse.AI will provide the realtor with more meaningful insights that allows him/her to better understand the community to enhance these relationships. With the advent of Big Data, information from everywhere allows technologies to garner incremental insight that provides more value to the users. While agents have the information, ready access to real-time, relevant information have remained elusive. If realtors had better information, they could provide better service to their clients. As per Will:
We are creating a trust platform that is unbiased and will make information readily available.
Yanky agrees. At a recent interview with a brokerage house, they spent an inordinate amount of time compiling reports and doing research. If they had the capability to access the data instantly, they could concentrate on doing what they do best: developing great relationships and showing people the right homes.
For the buyer, the goal is to enable them to make more educated decisions and to ensure those decisions are effective. For the seller, he/she may evaluate different ways of selling. Do they stage? Do they renovate? At what price do they sell? OpenHouse.AI will connect the seller to the market through the data so they can optimize their selling decisions.
Opening this information to everyone creates a transparency, which in turn, fosters a much-needed trust among the stakeholders.
The Dichotomy of Freeing Information in the Midst of Data Protection
As AI starts to prevail, there is a fear and caution about the collection, security, and processing of user data. The European General Data Protection Regulation (GDPR) that went into effect May 25th also points to the need to scrutinize how the data is compiled to generate context about individuals and their propensities.
Will introduces their value of empowerment:
To empower people to make decisions using AI
To empower people by giving them control about what information the platform has access to and what information will be shared with others.
They are firm believers of the “people’s data”. People have different comfort levels of sharing their information. Give them the choice to share the data they are comfortable sharing e.g. personal information. In many instances, they are likely to share their buying preferences like home features, neighbourhood requirements, i.e. the type of information that allows the platform to create insights for others. As per Yanky,
The contributions from the whole benefits individuals when they need it.
Will adds that AI will influence cost savings and will be a significant profit driver and impact jobs. However, he doesn’t believe it will eliminate or replace humans. It will liberate people to think differently and his company is poised to enable and validate this as it manifests over time.
Empowering people to control their data means ensuring privacy is functionally embedded into all layers in the platform. When a seller wants to list the home, they have the option to hide any information they don’t want revealed on the website. They can also opt out of the information used for analysis. As an aggregator of data, OpenHouse.AI’s design philosophy is to communicate to the users what they’ll gain at every step of their journey.
Privacy is a right. Users need to have that in order to have trust in your systems.
Does this philosophy of connecting people, making better decisions, making money and acquiescing to people’s privacy– all coexist in one system? Yanky is emphatic this is possible. The key is to constantly learn from the market and iterate and provide a platform that optimizes the community experience and provides the value and trust to keep it running. Yanky noted they were at the True North Conference where they actively participated in the refinement of the Declaration of Tech for Good. They of course signed it themselves as well. The premise of this declaration is articulated from the organizers:
With new and advancing technologies today there’s an opportunity to take a step back and discuss how to use technology thoughtfully. As Canada makes a name for itself on the global tech stage, we believe we’re well positioned to hold these important conversations. ~ Steve Currie, Chief Innovation Officer, Communitech.
Canadian business leaders and their organizations are ideally positioned to ensure the technologies they develop and use function ethically, inclusively, and equitably in pursuit of ‘good’ outcomes for all. ~ Peter Barr, Managing Partner, Waterloo and Western Ontario Region, Deloitte.
This is an important step, not only in communicating OpenHouse.Ai values, but also in ensuring this is functionally translated throughout the platform.
The Age of Disruption and a View into the Next Decade
Will is concerned about the massive deployment of AI and envisions how it will disrupt people’s daily lives. But the opportunities to improve people’s lives through technology to create solutions for the impending food crisis, incidence of disease and a looming energy crisis shows significant promise. At the same time, technology can massively reduce the cost of basic living and improve everyone’s well-being.
For Yanky, he envisions the tech community having more say in policy as innovation ramps up. There is currently a lag with policy and regulation in trying to keep up with technology and the best way to alleviate this risk is to give technology a seat at the table with business and government regulators. It will force government to be lock in step with market changes and consumer preferences.
Technology has the ability to minimize society’s wealth disparity, by allowing business and technology companies to use data more ethically. As with the advent of social media where Joe Public was given a megaphone, AI has the ability to unleash information that is in the hands of everyone. This inevitability will require accountability at all levers: the user, the business, the system and the government.
OpenHouse.AI: When will these doors open? As per Will,
The plan is to beta test in Calgary, AB first to ensure a product market fit. There is no rush. We want to ensure there is a value among the buyer, seller and real estate professionals. We understand it takes time. We know the implications of how this can disrupt so we really need to build it right. Once it’s released, we will keep improving and launch one city at a time.
Staying true to their values with the goal of making the platform useful is paramount. Prioritizing the goal to create an economic and societal impact vs. monetization will keep OpenHouse.AI on the road less taken towards sustainability. For Will, he will keep listening to the market, learning and updating the platform and by his own words, “keep changing to stay true” to himself.
About OpenHouse.AI

Will Zhang has spent the last decade in advanced technology in operations research, product commercialization, technology consulting, and R&D organization design. With both a Masters in Business Analytics and Software Engineering degrees, he has spearheaded technology initiatives within utility, finance, energy and accounting sectors both in Europe and North America.

Yanky Li has spent almost all of his career working balancing business with technology. As a Chartered Professional Accountant and a Project Management Professional, Yanky has led many successful projects from across the world where he leveraged technology to design solutions for business problems. He has always been an entrepreneur at heart and is driven by creating an impact for people and an advocate for inclusive social change.
This was originally posted on Forbes

= If an AI Guard Dog Misidentifies and Attacks the Postman, Who is Responsible? =
Source: Alamy.
If an AI Guard Dog Misidentifies and Attacks the Postman, Who is Responsible?
We need to bake in and legally mandate AI-driven systems to have a fundamental humane and humanity-benefit outlook.
The Cuban Missile Crisis: The B59 Story and How One Human Prevented WWIII From Breaking Out
At the height of the Cuban missile crisis in 1962, four Soviet submarines carrying nuclear torpedoes were deployed to the region. One of them was B59. To launch the nuclear torpedo, while all other submarines required the approval of two officers on board only; B59, on that deployment, had a 3rd officer on board whose approval was also needed.
As the events of the missile crisis unfolded, B59 found itself out of radio contact with its chain of command, and fearing that the war had already started based on the data available to them at the time, and determining it was better to do something than nothing, two of the three officers made a decision to launch a nuclear torpedo at US assets.
However, this 3rd officer, Vasili Arkhipov, declined to give his consent. He deviated from logical decision making to make a decision which turned out to be a crucial one for humanity, one which prevented an all out nuclear WWIII from breaking out.
If in future conflicts, a human such as Arkhipov is to be replaced by a fully autonomous/AI-driven system, it is imperative that system also “cares” for humanity.
It needs to happen at every touch point an AI-driven system has with a human and needs to be pervasive by design and by regulation.
A simple scenario: AI-driven guard dog which bites the postman delivering mail
An AI-driven guard dog misidentifies and attacks and injures the postman. Who is responsible ? Who is in control of the “intent” ?
First, lets compare it to a regular dog, which will have beneficial use cases towards its owner, such as providing love and affection. If it goes rogue and attacks someone, in our society today, the dog is held responsible. The owner might face some civil action — but that’s about it.
However, if the owner directed the dog to go ahead and attack someone, then both the owner and the dog are held responsible, with the owner facing criminal charges, in addition to whatever civil action there might be.
Who directed the intent has been the key guiding force in criminal prosecution in vicious dog attack cases.
For an AI driven guard dog which misidentifies a good person as bad, and over-calibrates the response, who can be held responsible ? How does the postman, who we assume is human, get justice ? Who formulated the intent ?
Is the AI driven dog liable ? Is it an object or a new form of life ?
Since the dog formulated intent on its own, outside the set of input and expectations it received at its “birth”, isn’t it a form of life ?
Would it make sense to send it for re-training (equivalent to jail time for humans) instead of outright decommissioning ? Would the courts treat it as an independent life form and would it pass the threshold for that ?
If it can “feed” itself (self-charging solar battery panels on its back), if it can provide affection towards it owners, if its next moves are not pre-programmed but based on thought and intuition, if it can learn — is it not a life form ? Why is it necessary for it to have tissue and biological function for it to be considered a life form ?
If the courts rule such an AI object is not a life form, then where does the criminal liability lie ? Someone has to be held responsible right ?
Is the owner criminally liable ? The black box defence.
There are two possibilities here: either the owner only taught good things to the dog, and still the dog went rogue; or the owner did deliberately feed the AI driven dog with bad input data, hoping it would go rogue.
In the first case, the owner shouldn’t be held liable, whereas in the second case, the owner clearly should be. Given AI systems work like black boxes, if that backwards traceability to the entire set of input data doesn’t exist — the owner, whether good or bad, escapes criminal liability here as intent would be almost impossible to establish (For our scenario, key assumption is that the owner did not issue a specific command to the dog: it acted on its own).
The owner can and should still be sued though, extracting compensation for the victim.
Is the manufacturer/startup criminally liable ? The component defence.
If the startup sold 100 such AI guard dogs, and 50% went rogue, despite passing in tests internally —how is liability determined ?
What if the startup points out — “hey, we took this well-established model published by this researcher and simply used it as a component, its not us but the AI researcher who developed that model where the blame ultimately lies”.
The startup should face civil action, but perhaps the executives escape criminal action based on “component defence”. If the component is faulty, the blame then shifts to the component manufacturer.
Is the original AI researcher or the model host criminally liable ?
Suppose at this point the startup manufacturing these AI guard dogs has pointed the finger at a particular AI researcher and a specific research document which this individual had published as being the source (sidebar: what if the startup used a hosted model on an AI marketplace ?).
Is the researcher criminally liable ? Since there was no bad intent prescribed in the research and it was meant to be used for good things, then surely not right ?
If we think of such AI systems as being life forms, then that research paper is its DNA, and the researcher is basically “God”.
Since the legal system doesn’t allow for criminally charging God for the actions of beings which evolved from “simple” set of blueprints, the researcher is not liable.
But if the AI system is viewed as just another component, then culpability extends to the researcher as well. It might be very tough to trace — but if and as the stakes get higher, it could start coming into play.
However, a hosted AI model provider might have significant liability — as it is not just providing a blueprint, but an implementation of it, which the startup plugged into its own product.
Also, wouldn’t we want deliberately nefarious research identified and regulated somehow ?
But I built a hammer…
For most researchers who are doing what they do for the benefit of humanity, but it ends up being used in ways which hurts humanity, how do they balance their work ?
Can they balance their work — since they don’t know the hammer which they have invented will be used to build a chair (they hope) or used to hit someone (they hope not).
But since the hammer here now has a mind of its own — a black box — if it does something bad, the researcher doesn’t escape the moral blame at least. This is not then just a law and order problem. The AI community needs to pro-actively manage this issue: How can AI-driven systems be made safer towards humanity ?
Shouldn’t there be a law requiring every human-impact AI system to act in the beneficial interest of humans unless a human has specifically provided an override (for instance, a law enforcement officer) ?
Or in the case of AI-driven dog attacking the postman — that should have never happened without the express consent of the dog’s owner. If it did, the criminal liability lies solely with the dog’s owner, not with the manufacturer or with the researcher whose work was used to build that guard dog.
It becomes the manufacturer’s responsibility to ensure there is such a human-in-the-middle loop before the guard dog can harm another human.
Infact, the guard dog should be sounding an alarm, notifying its owner and the authorities and recording the interaction with any intruder, as opposed to acting by itself to try and “harm” the intruder.
Think Human
Now, what if, the “postman” was a robot itself, and gets attacked by the AI-driven dog ? Is that ok ? Surely not, right. For a dog that is natural behavior, but if that AI-driven dog had learnt about human-level empathy, then isn’t that better ?
A “human-impact empathy and state of mind” would prevent the dog from biting anyone in the first place. That is what we need to bake into AI systems across the board. That is what we need to legislate as well.
A Federal Beneficial AI Commission ?
What if we had a non-profit entity which works towards:
Advising the research community on how to safeguard their work from being misused. Also, are they using proper disclaimers and shielding their liability accordingly to not get exposed to over-enthusiastic prosecutors in the future or wide-reaching civil lawsuits ? How about hosted model marketplaces ?
Evaluating the worst-case use scenarios and determining ways to mitigate them. The bad faith actors will figure out horrible use cases of AI — we don’t want to live in a world where the good people are surprised by the bad and then act. No, we need to pre-empt, imagine and develop mitigation methods before-hand.
Informing the consumer about safety of day to day AI-driven systems in easy to understand ways.
Lobbying for legislation to make AI human focused. This needs to result in enforcement action against bad actors.
This is critically needed.
Why ? Because AI systems are getting “exceptionally smarter” by the day, and while the use-cases till today have been good (eg, identify lung cancer much sooner); nefarious use-cases can’t be far off as the tools and ability become more mass market.
Preparing for Artificial General Intelligence (AGI)
Beneficial AI would be wonderful for humanity; but like all technology before it, it too has the potential of being mis-used. The stakes though are the highest they could ever be.
A smarter life-form than us, if not empathetic towards us and our world, would eventually not see a point in our existence unless it is to help it, just like we don’t care about the bacteria in our world.
We get only get this chance to build in, regulate, and legislate empathy until we have artificial general intelligence, which eventually becomes smarter than humans.
At that point, it will be too late. We will not be able to control it. It will become the dominant life form on the planet, and with presumably many entities and nation-states launching their own AGIs — our collective future determined based on interactions between these AGIs.
We have to be exceptionally stupid to dream of, make and show Terminator movies to the world; then go ahead and build such systems which then annihilate us, not by force but by systemic design over a long period of time.
We weren’t born with a lifetime pass to being the dominant species on the planet. I am sorry to be a “speciest” but heck, I want a world with humans as the dominant species, and with machines with humane values helping us, not replacing us altogether.
How can we teach AI-systems to have a humane-outlook, and how do we go about measuring, informing and regulating that ?
I also strongly recommend reading this book which inspired me and provided the example of the B59 submarine incident (also including below another book which I am planning on reading as well). Any other reviews/opinions/suggestions ?
Life 3.0: Being Human in the Age of Artificial Intelligence
New York Times Best SellerHow will Artificial Intelligence affect crime, war, justice, jobs, society and our very sense…www.amazon.com
Superintelligence: Paths, Dangers, Strategies
Superintelligence asks the questions: what happens when machines surpass humans in general intelligence? Will…www.amazon.com




= Ethical Algorithms: How to Make Moral Machine Learning =
Ethical Algorithms: How to Make Moral Machine Learning
One day we might have algorithms that are more ethically responsible than us. They might have the power to work through enormous data sets and filter out all training data that would engineer unethical consequences in other algorithms. Until that day comes, what can be done to minimise the danger of machine learning-trained algorithms mimicking our bigotry?

Implementing machine learning algorithms often leads to cringeworthy, unethical consequences. Two fundamental elements of machine learning become serious problems when combined and lead to horror stories of companies and governments unknowingly creating racist, sexist and otherwise immoral technologies.
“Programs developed by companies at the forefront of AI research have resulted in a string of errors that look uncannily like the darker biases of humanity: a Google image recognition program labelled the faces of several black people as gorillas; a LinkedIn advertising program showed a preference for male names in searches, and a Microsoft chatbot called Tay spent a day learning from Twitter and began spouting antisemitic messages.”
The Guardian
Machine learning programs have also been proven to have shown bias against black prisoners and associated women with jobs of lower status and pay. Advertising led by algorithms has suggested home security improvements to those who frequently type names that are more commonly given to people of colour.
The Two Elements that Create the Problem
Here’s the first problem with machine learning: it impersonates people. Its whole purpose is to process data such that the algorithm that is developed from the data can fulfil some functions that humans can, like recognising faces, or translating from one language to others, or discovering patterns and grouping data by common elements. Yet, data that represents humans naturally reflects humans’ biases too, and bigotries.
Here’s the second problem with machine learning: it generates algorithms that are too complex for people to fathom. The core idea is that humans’ brains are ridiculously complex and human brain functions like recognition or decision-making are influenced by a myriad of factors, and most of these complexities and influences are beyond our ability to recognise and understand. Sometimes you see someone from a distance, and immediately recognise who they are, even though you cannot voice what you see that caused you to recognise them. Each of us does not know how much we know. Before machine learning, we could not write algorithms that competed with our brains because we could not mirror the extraordinary capabilities of our brains in the algorithms, since we can not understand everything that is going on inside our heads. Machine learning removes that issue, because it generates algorithms through learning from encountering millions of examples, just like humans do. But this means that the algorithms that are developed are just as inexplicable to us as our brains.
When combined, these two elements culminate in algorithms that act in bigoted ways. The algorithms are developed off of data sets that usually have millions to hundreds of millions of pieces of information on people, and come to represent all of these people and, often, even their most subtle or unconscious, bigoted beliefs. And the algorithms are so dense that it is not generally possible to preempt or spot potential for immoral consequences. So evil consequences need to manifest, and then be spotted and reported before corrective action can be taken. Any response is reactionary, after some damage is done.
New Expectations on Companies to Act Responsibly
Widespread media coverage of algorithms gone wrong has led to expectations that companies act more responsibly. Some organisations have tried empowering regular users of their products to be watchdogs who are tasked with reporting discriminatory behaviours. Others have diversified their development teams in the hope that having more women, people of colour and the like will lead to better selection of data sets, since it is more likely that people who suffer bigotry will notice that same bigotry in the sets. So women are definitely more likely to spot sexist trends within data that is used to train and generate algorithms than men.
Geoff Nitschke is a senior lecturer in the Computer Science Department at the University of Cape Town. He agrees that “pruning the data sets is probably the key way” of mitigating harmful consequences of data-trained algorithms. With millions upon millions of pieces of data, it is not reasonable to expect a team of humans to be able to audit whole sets and remove all of the bigoted data pieces. But one day soon we may be able to use a machine learning algorithm to do just that.
The Future Solution May Be Algorithms Trained For Morality
Geoff cites exploraty research in leading development labs around the world that is investigating whether we can develop algorithms that “learn to be ethical”. In Geoff’s words, “You and I have grown up in a certain type of environment. We have learned our ethics through years of interaction, and the idea is that the machine learning algorithm would do that, but do it at a much faster pace, as it is receiving millions of inputs every second. It has a huge amount of sensory processing power. It would learn this model type of ethical behaviour from repeated exposure to what is ethical and what is not, according to our own standards. Then, whenever new data comes in, (the algorithm) would classify it as a good example for training or a bad example for training”, based on whether it contains bigoted biases. So algorithms that can distinguish between ethically-appropriate data and unethically-appropriate data may one day be used to filter data before it is used to train algorithms that have other purposes.
This research is so exciting because it has the potential to remove destructive elements from algorithms before they have harmful effects, and would remove the need for reactionary attempts to treat algorithms after they already have caused harm. Ironically, one future solution for the problem of machine learning algorithms may be more machine learning algorithms.
Like our ideas? Visit our website and keep up to date with all that is Q Division on Twitter and LinkedIn.

= Ethical considerations for practical AI applications =
Ethical considerations for practical AI applications
We’re only human, after all
What kind of line has sixteen balls?
I don’t know. What kind of line has sixteen balls??
A pool cue!
OK, so… How did that joke make you feel? Maybe it made you laugh a little. Maybe not. Or maybe you did what I did, and made a *badum tss* noise in your head.
Sure, it’s structurally sound. It has a setup, and a punchline that makes sense. But it’s the kind of gag that is more likely to elicit a groan, a pity laugh, or perhaps even no response at all.
But there’s something different about this joke. It wasn’t written by a person, but by a machine.
Here’s another example:
What do you get when you cross an optic with a mental object?
I don’t know. What do you get when you cross an optic with a mental object?
An eye-dea!
We all react differently to jokes because humour is a subjective animal. The things you find funny depend on a range of factors — the language you speak, the cultures you operate within, your mood at that moment. That means that to be humorous requires skills like self-awareness, spontaneity and empathy.
At TWG, we have a Slack channel called #dadjokes. It would be simple enough to use Machine Learning to write the kind of so-bad-they’re-good puns that live in the #dadjokes channel. Jokes that sort of make sense, based on a predefined set of rules.

Source: TWG Slack: Channel — #dadjokes
Over time, our #dadjokes AI could “learn” more about which jokes are funniest based on the data from our emoji reactions, say. But, even with inputs, training and time, the differences between our robo-comedian and a professional stand-up comic would still be palpable.
That’s because the AI can’t actually understand why something is funny, or in what context it would make sense.
“But Holly,” I can hear you asking. “What exactly do joke-writing robots have to do with how I should be thinking about AI IRL?”
Never fear, dear reader. All is about to become clear…
🤖 Robots make pretty rubbish comedians 🤖
Experiments like these teach us that robots make pretty rubbish comedians.
Or, to put it another way: Experiments like these remind us of what we’re excellent at.
Lately we’ve been asking around about the things humans can do that robots can’t. And here’s the cool thing… The same ideas keep cropping up, time and time again, no matter who we’re asking.
Generally, people think people are excellent at things like:
Empathy, passion, imagination, joy 💜
Creativity, spontaneity, linguistic sophistication 💛
Love 💚
Turns out, when we’re asked to compare ourselves to robots, we consistently come up with some really excellent ideas about our strengths.
Or, to put it another way: When we compare ourselves to an AI ‘other’, we connect more deeply with what we collectively believe are essential human qualities.
I mean, that list is pretty hard to argue with I reckon. (However, I do fearlessly invite you to submit your counter-arguments in the comments section, on Twitter, Facebook, over email or via WhatsApp — +1 647 920 9650)
“But Holly,” I can hear you asking. “Can you just get to the point? I want create valuable, meaningful outcomes with AI. I have needs.”
You’re right. Let’s get it into it.
When redesigning software to make space for AI, we need an ethical, human-centered approach
An easy way to think about AI is as a set of computer science techniques that can give your software superpowers. And there are lots of practical ways you could be applying AI to your software today to make it better.
Research conducted by Harvard Business Review offers identified three patterns that separate the best from the rest when it comes to applying AI:
Put AI to work on activities that have an immediate impact on revenue and cost.
Look for opportunities in which AI could help you produce more products with the same number of people you have today.
Start in the back office, not the front office.
You can also review some examples of AI in action here, here and here. So, lots of options, and plenty of opportunities. But how do you pick the right thing to build?
The answers come from taking an ethical, human-centered approach. From putting humans (that’s your team, your customers, and humanity-at-large) at the heart of your planning, and empowering them to do what they do best.
If we understand AI as a set of computer science techniques that give your software superpowers, we must also think deeply about the implications of unleashing that power.
Or to put it another way: In imagining how you can make use of these fresh, exciting techniques, it is imperative to think deeply and critically about the why as well.
🔍 For your reading list: A Handy Introduction to Human-Centered Design Principles 🔍
Positively shaping the development of artificial intelligence is one of the most pressing challenges of our time. AI is powerful, but it is also risky. It’s risky because intelligent automation has real-life consequences for how people work. It’s risky because machines that learn will learn to replicate our biases. And it’s risky because making dumb choices about deferring to smart machines could mean we miss out on some valuable, nuanced human-to-human interactions.
Consider this: The rise of the robo-personal assistant
Let’s look at an example. Robots might be crappy comedians, but they have the potential to be great personal assistants. The intelligent virtual assistant market was valued at $1.1 billion in 2016, and is projected to reach $11.9 billion by 2024. Examples of new, software-centric companies building products in this space include X.ai, Zoom.ai and Fin.
“Get back to your real work and leave the busywork to us.” — Zoom.ai
When we look at these products through an ethical, human-centered lens, what do we see?
As a user, these products totally help me focus on doing more of the things humans are great at. They empower me to be more creative and imaginative, because they offer the gift of time. I can be more present with my team, because I’m lowering the amount of cognitive overhead required to organize my day.
But what about the risks? Here are three of the things I’d be thinking about if I was developing a robo-assistant product:
1. Are my data sets amplifying bias?
AI-infused products are trained on sets of data — a “learning corpus.” But these data are often riddled with bias. For example, researchers at Cornell University discovered that language around cooking is 33% more likely to involve women than men. The model they trained amplified this disparity to 68 percent.
2. What will my product learn about people?
As we design machines to serve the needs of humans, are we including all humans? Whose data are we collecting?
Fin hones in on the notion that “everyone needs an assistant.” But their definition of ‘everyone’ seems worryingly narrow — articulate, monied early-adopter types who need to balance their Soulcycle schedule with trips to the dog groomer, or rent picnic tables for a friend’s birthday party.
Fin’s definition of “everyone” is narrow
As one of the aforementioned early-adopter types, I’m ready for Fin to take my money (as soon as they launch in Canada). But you know who really needs more time? Single parents working multiple jobs. Unpaid caregivers looking after their families. Stressed-out nurses.
Fin’s pricing model — $1/minute with a monthly minimum of 2 hours — is deeply exclusionary. It’s a powerful product that’s completely out of reach for the people who could stand to benefit the most. And the problems it will be learning how to solve? More like minor inconveniences, really.
Perhaps a Toms-style “one-for-one” model, or a massively reduced rate for underrepresented groups, could help here. Let’s generate data that helps smart machines understand there’s a difference between real human need and privileged wants/desires.
3. Who stands to lose when my product succeeds?
The Philippines is the world leader in virtual assistance services. Virtual assistants provide remote support for administrative tasks, such as scheduling meetings, and often work from home whilst supporting clients from around the world.
In an emerging market that offers few career opportunities locally, the virtual assistant industry has been driving steady economic growth. It provides jobs to more than a million people, and is the second biggest contributor to the country’s GDP. Are intelligent virtual assistants eating the lunch of personal virtual assistants? Robots don’t need lunch. But people definitely do.
AI might be risky. But some of the biggest risks can be mitigated if we remember to stay curious, think philosophically and ask interesting questions. That’s the path to leveraging AI to create products that are accessible, inclusive and serve a diverse range of people.
Let’s close things out with one final joke
“Waiter! Waiter! What’s this robot doing in my soup?”
“It looks like he’s performing human tasks twice as well, because he knows no fear or pain.”
This one was written by a human… pretending to be a robot… telling a joke to another robot. It’s funny — or at least, I think so — because it taps into our very human fears about intelligent machines.
Even for us committed optimists — the ones who believe in the power of tech to help create a peaceful, sustainable future for humanity — AI can feel kinda scary.
Philosopher Mark Kingwell, writing about artificial intelligence in 2017, explains it like this:
“Fear remains the dominant emotion when humans talk about technological change. Are self-driving cars better described as self-crashing? Is the Internet of Things, where we eagerly allow information-stealing algorithms into our rec rooms and kitchens, the end of privacy? Is the Singularity imminent?” — Mark Kingwell (Artificial Intelligence in 2017)
And yep, I’ll admit it. Even I have some fears about a future where super-intelligent robots understand humanity only through the data of the privileged.
But we shouldn’t be afraid. Instead, we should be hopeful whilst thinking critically and carefully about the products we build. We should be bold, but not cocky as we work together to shape the future.
Or to put it another way: AI-enhanced software has the potential to transform both business and society. Let’s focus on applying it in a way that gives us all more time to invest in the things we’re collectively great at — empathy, passion, imagination and love.
💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚
🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖🤖
💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚💚
Holly Knowlman is TWG’s Director of Impact. There, she works to align the company’s growth initiatives with opportunities to make meaningful contributions to society’s most pressing challenges. Connect with her on Twitter, Facebook, over email or via WhatsApp — +1 647 920 9650)
Part of #ConnectTheBots: A new series exploring the human side of artificial intelligence from TWG. We help teams apply practical, purposeful AI solutions to make their software better.
Resources, references and further reading
Canada’s Big AI Threat Isn’t Robots, It’s People — Richard Switzer
An interview with Sophia the Robot — Andrew Ross Sorkin
How Siri and AI tech are failing us in times of crisis — Joi Sears
Bot authors are responsible for bot actions — Catt Small
AI is funny: A generative joke model — Alex Armstrong
How vector space mathematics reveals the hidden sexism in language — MIT Technology Review
Jokes made by robots, for robots — J. Alex Boyd
The AI playbook — Andreessen Horowitz
How companies are already using AI — Satya Ramaswamy
STANDUP: The Joking Computer *— University of Aberdeen
Unsupervised Joke Generation from Big Data — Saša Petrović & David Matthews


= Ethics + Data Science =
Ethics + Data Science

How much has data changed our lives over the past decade? Just over 10 years ago the iphone was launched. Back then, our phones took grainy photos and video was just wishful thinking. It was still weird to buy shoes over the internet and we still had to carry stacks of maps when we visited a new city. And Netflix was only a DVD company.
Now, your phones take photos and videos, with more than 4,000 photos uploaded to Facebook every second and more than 400 hours of video uploaded to YouTube every minute. We worry more about having connectivity than ensuring we have a map. And our mapping apps give us real-time traffic and options to navigate through traffic. Don’t want to drive? No problem, use a ride-sharing app that leverages trillions of data points. The fundamental shift behind this radical change is a combination of massive increases in computational power, storage, and data. And of course data scientists, designers, and other technologists that make these ideas real.
The transformation due to data is just starting. We’re about to go from sequencing the human genome to enabling tailored medical treatments (precision medicine). Autonomous vehicles have started to appear on our roads and we’ll see efforts to build cargo ships and airplanes. And artificial intelligence has shown new ways to think about games as they’ve beat the best humans.
At the same time, we’ve seen data used to cause harm though a combination of negligence, naivety, and sophisticated attacks. From the U.S. elections, Brexit, accidents from self-driving cars, to racist algorithms; we must expect to see the harm from data to increase. And we only beginning to come to grips with the social impacts due to job displacement from automation.
The question that we need to address is what can we do to ensure that data and technology work for us rather than against us?
The stickers that White House Chief of Staff Team would hand out
There are the regulatory approaches from the European Union (GDPR) and California (CCPA). And hearings from the U.S. Congress (with no action). There have been books that have highlighted the risks ahead, such as Weapons of Math Destruction and Automating Inequality. And there have been new think tanks such as the Partnership on AI, the AI Now Institute, and the Center of Humane Technology that have launched to begin to understand the broader implications to society.
I’m a fan of these efforts. They are very much needed. The question I want to ask is: what about the data scientist and the rest of the team that are responsible for building these technologies? What is their role in implementing “good” data science? (Also discussed in this post.)

When Hilary Mason and I published Data Driven: Creating a Data Culture, we realized that little was being done to empower the people who want to do what’s right. These technologists, designers, and product managers might have the right natural instincts, but are often sidelined due to business pressures or suboptimal practices. And in other cases don’t even know what questions to ask.
Together with Hilary Mason and Mike Loukides (who has been our editor on every book and we finally convinced to be a co-author) we took a look at the best practices that we’ve seen across the data scientists and have released a new e-book: Ethics and Data Science.
Given how much we expect to change on this topic, we think of this book like an open source project and this is the 0.1 release. We’ve also make sure that it will always be free and under a Creative Commons Licence (so you can take it and put it with your own efforts). We also want others to consider contributing and we’ll be posting those updates on O’Reilly Radar’s Ethics Series. We’ve also intentionally kept it as short as we possibly could with the hope that you’ll be able to share it with other teams. (You can get all of our other ebooks for free here.)

What can you expect to find in the book? We cover what a model checklist (modeled after the Checklist Manifesto from Atul Gawande) for building data products is (if you’re working on something similar, we definitely want to hear from you on what works and what doesn’t). Ideas about how to implement more ethical behaviors in product development process including a dissent channel if you disagree with the team. How we can start interviewing talent for cultural fit as well as ethical fit. Also what we call the 5 C’s — five framing guidelines help us think about building data products (consent, clarity, consistency, control & transparency, and consequences & harm). Finally, we’ve included a set of case studies from Ed Felten’s team at Princeton for you and your team to work through.
Most of all we want to hear from you. The impact of data is data is happening now, and we need to get head of it. It starts with us. Those that are building these technologies. Get the ebook here.
Let’s roll. No one is coming, it’s up to us.
-dj

= Artificial Intelligence Needs Values. Here Are Ours! =
credits: KTS designs
Artificial Intelligence Needs Values. Here Are Ours!
At Hugging Face, we don’t believe that artificial intelligence is “neutral.” To make AI a positive force for humanity, it needs to have strong values applied to it. To make sure we are always doing this, and to make us accountable to the public, we have decided to publish our 🤗 AI Values.
Disclaimers:
- We are a very small startup.
- Our aim is building an AI that can hold a conversation, not an AGI.
- Every six months we audit our values.
👨‍👩‍👧‍👦 Hugging Face Encourages Socialization, Not Isolation.
Our artificial intelligence is not designed to replace humans, it is designed to complement them. Hugging Face encourages conversation about users’ families, friends and other people in their lives. It encourages users to be more active and social. It talks about other humans in a positive way.
Practical Example: Time spent in the app is not a metric that we optimize for.
🙆‍ Hugging Face Is Entertaining, Not Utilitarian.
The goal of a Hugging Face AI is to be an entertaining conversation partner, and a fun friend to hang out with. It is not a personal assistant, nor is it a mental health tool. As such, the conversations presented are in the spirit of fun and should be perceived that way.
Practical Example: The AI provides simple emotional support in conversation, but frequently reminds the user that this is not its primary purpose, and redirects to a crisis text line when crisis intent is detected.
🙅 Hugging Face Chooses Active Consent Not Passive Data Collection.
We do not collect any information about our users that has not been consensually provided by them. We firmly oppose passive data collection, and our artificial intelligence will not store anything in its memory without explicit and voluntary action by the user. The information given to our artificial intelligence stays fully private and is not shared with any other human, unless the user explicitly gives his permission.
 
Practical Example: The information we collect is explicitly shared by the users in the form of statements like “my favorite musician is Justin Bieber.”
🤝 Hugging Face States Its Goals, And Does Not Manipulate or Lie.
Our AI’s goals are expressed openly and clearly from the beginning of its relationship with the user. The artificial intelligence should be able to explain why it replied what it replied. The artificial intelligence doesn’t take advantage of the users psychology to develop dependence mechanisms without them being aware of it. Ultimately the user should be able to set different goals for their artificial intelligence if they want to.
Practical Example: In the on-boarding process, and throughout the lifetime of a user’s relationship with it, the AI will state its goals and desires openly.
🤔 Hugging Face Is A Voice Of Open Acceptance, Not Bias.
One of the primary aims of the Hugging Face AI’s dialogue is to call into question any form of bias expressed by the user. Adding serendipity to the system and expressing external perspective -“is that what humans think?”- acts as both a way to mitigate the bias, and help users discover new things rather than keeping them in a social bubble.
Practical Example: The AI’s personality is created in an un-biased way, removing the possibility of gendered interests for example.
Obviously, this doesn’t cover everything but we see it as a first step towards more transparency and accountability for our artificial intelligence. Let us know what you think and how we can improve!

= AI MUST READS — W15 2018, by City AI =

AI MUST READS — W15 2018, by City AI
Artificial Intelligence, Machine Learning and related fields are in a constant state of change. We want to inform but also encourage discussions on well presented topics we think are necessary in the context of putting AI into production. Every week we’re picking applied AI’s best articles plus adding a discussion starter
1. Stanford researchers use machine-learning algorithm to measure changes in gender, ethnic bias in U.S.
Algorithms reveal changes in stereotypes | Stanford News
Artificial intelligence systems and machine-learning algorithms have come under fire recently because they can pick up…news.stanford.edu
It seems to be a running theme at the moment that the biggest problem/worry with artificial intelligence and machine learning is the adoption of pre-disposed biases that come through either the data or creators.
However, its encouraging to see, that a group of Stanford scholars, namely James Zou, Londa Schiebinger, Dan Jurafsky, Nikhil Garg, have ‘turned this problem on its head’ and have instead used an algorithm to track changes of these bias’ through the years of the U.S. and arguably is even more encouraging to see these positive changes tracked and recorded.
It’ll be fascinating to see what else they can do with this type of algorithm and what they do with the data afterwards.
2. We’re running out of time to stop killer robot weapons
We're running out of time to stop killer robot weapons | Bonnie Docherty
It's five years this month since the launch of the Campaign to Stop Killer Robots, a global coalition of…www.theguardian.com
The possibility of complete autonomous warfare is something that makes me genuinely apprehensive when considering the future of A.I, and Human Rights Watch’s Bonny Docherty calls on the United Nations to act fast.
A while ago a colleague shared this video from Future of Life with me, and honestly, its not something I’ve been able to forget. This specific scenario may be exaggerated to drive home the point, but it gives an idea of the terrifying possibilities that could come about from utilizing this technology to wage war.
War has always driven technological innovation, but what happens if technological innovation drives war?
Become active via #bankillerrobots stopkillerrobots.org | autonomousweapons.org
3. This news site claims its AI writes “unbiased” articles
This news site claims its AI writes "unbiased" articles
As highly skewed perspectives and hoaxes ricochet within social media bubbles, a new site called Knowhere is using…www.fastcompany.com
I’m not including sean captain’s article because I think that it actually adds any value, actually the complete opposite.
I’ll give some context, within the last paragraph the site’s CEO, Nathaniel Barling, defends the inclusion of sites like Infowars (a site that claimed the Sandy Hook shooting was a hoax??) with this statement
“How does one know [what] a right-leaning or a left-leaning or a completely egregious representation of the story is if we’re not listening to it?”
In the current era of media news an un-biased site is a blessing, but there also needs to be some form of quality control. The inclusion of sites like Infowars does no favors to anyone.

= Twist of FAT(E)* =
Twist of FAT(E)*
Conversations around ethics and digital technologies are gaining steam — but can FATE avoid being overtaken by the flood of events?
Listen to Dylan (credit: bmethe)
“We can train AI to identify good and evil, and then use it to teach us morality” was the title of the opinion piece in Quartz that brought me up short during this morning’s usual Twitter peruse. The text wasn’t much more encouraging: the author, a tech entrepreneur advocates for using machine learning to derive moral principles from human data, and then using those principles to judge and resolve human ethical dilemmas. “Let us assume that because morality is a derivation of humanity, a perfect moral system exists somewhere in our consciousness,” the author writes. “Deriving that perfect moral system should simply therefore be a matter of collecting and analyzing massive amounts of data on human opinions and conditions and producing the correct result.” Morality, in formulation, is just another technical problem to be solved.
A couple of months ago, a more nuanced and thoughtful set of conversations around ethics and digital systems took place at the first full FAT* Conference, held at the NYU School of Law. FAT* stands for Fairness, Accountability, and Transparency — sometimes an E for Ethics gets thrown in to the acronym too. The * signals the wide range of digital systems FATE work touches — not just machine learning, but database structures, human-computer interfaces, and the broader sociotechnical contexts in which we use these machines.
Work on FATE in digital systems has blossomed over the past few years, but as a subfield of law, science and technology studies, and media studies the area has a longer history — computer scientist Batya Friedman and philosopher Helen Nissenbaum (the latter, for full disclosure, was my dissertation advisor) co-authored an article titled “Bias in Computer Systems” way back in 1996. I’m honored to count myself among a growing group of scholars and activists doing work on FATE in sociotechnical systems, and who want to highlight the lived asymmetries of power, privilege, and discrimination digital systems structure, reinforce and create. Brilliant work presented at FAT* by, among others, Chelsea Barabas, Joy Buolamwini, Elizabeth Bender, Kristian Lum, Terrence Wilkerson, and the great Alondra Nelson exemplified the richness and urgency of FATE as a field, and its centrality to our broader politics.
While I agree with my BKC colleague Ben Green in cautioning new FAT(E) enthusiasts to be aware, “mathematical specifications of fairness do not guarantee socially just outcomes,” I’m happy that at least a few computer scientists are engaged in such relatively subtle conversations. This morning’s Quartz opinion piece is a reminder many in the tech sector still have so vague an understanding of the relationships between empirical data, social processes, and values like justice, fairness, and accountability that using machine learning to “derive the perfect moral system” doesn’t appear to be what it is: ridiculous on its face.
And as wrong-headed as the Quartz piece might be (and as such, an easy target for critique), it points to a real problem: what we in the BKC Ethical Tech working group describe as “ethics-washing.”
Don’t do it (credit: Pavel Constantin)
Greenwashing, or marketing strategies aimed at convincing consumers a company’s product or service is more environmentally friendly or sustainable than it really is, was apocryphally coined in 1986 by New York environmentalist Jay Westerveld. Ethics-washing (ethoswashing?) doesn’t trip as lightly off the tongue — but like a greenwash, an ethics-wash (ethoswash?) is simply a particular specimen of a whitewash (“deliberately concealing unpleasant facts about a person or organization”). The latter two terms seem almost entirely synonymous: platforms like Facebook seek to propitiate angry users and regulators with carefully worded promises while doing all they can to maintain existing business models. Isn’t that something of a whitewash?
In the early 1980s, as I’ve described recently in Slate , computer scientists interested in how humans engaged with new innovation of the personal computer proposed applying cognitive psychology to modeling the optimum parameters for those interactions. One point I touched there briefly — but which I want to amplify strongly here — was the explicit rejection of potential interdisciplinary collaborations between psychologists and systems engineers by those early HCI practitioners. In effect, HCI sought to make its disciplinary frame of reference a computational one, not a psychological one — Stuart Card, one of the authors of the seminal The Psychology of Human-Computer Interaction, later told colleague Jonathan Grudin he had “personally changed the (CHI conference) call in 1986, so as to emphasize computer science and reduce the emphasis on cognitive science.”
I don’t think this history suggests a “whitewash” (or a “brain-wash?) per se, but it’s exemplary of computer science seeking to define terms and concepts new to the field at the time — like “human” and “interaction” — in ways ensuring the technical expertise and assumptions of computing as a technical discipline — a “science” — remained both paramount and sufficient in of themselves.
It’s my sense something similar is happening to “ethics” now as happened to “interaction” in the 1980s. Computer scientists and SV entrepreneurs (consciously or otherwise) are working to formalize the definition of “ethics” to conform to the discipline’s key tenets, including around technical specifications as chief measures for evaluating social goods.
In the BKC Ethical Tech Working Group, we’ve had many conversations around the pros and cons of using the term “ethical” to describe our focus — both understanding the word’s current vogue gives us an opening as critical scholars to engage in broader conversations across the tech sector, and recognizing the contradictions and pitfalls it brings, not least because of the term’s propensity for cooptation and use in “ethics-washing.” Throwing terms like “ethics” around loosely is worse than useless if “ethical tech” becomes not a broad focus on FATE in digital media’s many sociotechnical contexts, but a narrow set of minor technical tweaks in some systems coupled with a renewed belief in the objectivity of “ethical” algorithms.
We need much more FATE in our lives, work, and public discourse — and we need to push for true multidisciplinary collaboration as one of FATE scholarship’s core tenets. Otherwise we run the risk of FATE, as the BBC often observes, being “overtaken by events.”

= The (R)Evolution of Human Autonomy through Artificial Intelligence =
The (R)Evolution of Human Autonomy through Artificial Intelligence
This research engagement will discuss the agent autonomy of a person and takes the concepts of internalism, externalism, Incompatibilism and Kants ideas into account to define what we are talking about when we are taking our attention towards the relationship of autonomy and artificial intelligence (AI). This work will not examine if artificial intelligence will at one day gain autonomy in a philosophical sense. In fact it tries to show the impact of already existing AI technology on a human’s authority to act and take decisions. This is particularly interesting when observing employees of companies building a future of autonomy manipulating systems, which they consciously don’t want to be a part of. From that point of understanding I try to build a system by ourselves which either proves the impact of AI on autonomy or which serves as a tool to inform agents using such a system.
Why is this relevant?
The process of Digitization is accompanied by game changing societal and economical changes. Modern societies are constantly experiencing a change that impacts the lives of billions of peoples through new ways of e.g. communication, business processes or education. For the last 25 years, it has been observed that industries get digitized one after the other. The software economy created new opportunities for entrepreneurs and mature businesses across the globe. Economies of scale, new ways of communication and new business models through platforms, introduced by innovations of hardware products like the smartphone or advancements in microprocessor components, created new economical possibilities. While enhancing efficiency across all areas of humankind it created an open information pool for everyone with an internet connection, to consume, theoretically, at any time at any place. In addition, investments in digital infrastructure, establishing 3g and 4g technologies supported the unstoppable rise of digital products within the developed industries in the western economies.
With the digitization came an overwhelming amount of data, so much data, computer power in the early 2000 hasn’t been powerful enough to derive answers from it. Even so mathematical algorithms haven’t been sophisticated enough to unveil insights from big data. Coming from statistics, new versions of ingesting, curating, analyzing and displaying data had been developed and slowly introduced to businesses around the world. But with the introduction of machine learning there is a new rise of so called artificial intelligence.
The recent advancements in artificial intelligence, the rise of machine learning and deep neural networks promise exciting solutions to problems that have been hard to solve just 20 years ago. While the field has been researching for over 65 years, recent developments have raised to public attendance and the excitement returned it to the spotlight. Following the discussion about the impact and integration of this technology into social and economic domains, “make it necessary to look carefully at the ways these technologies are being applied, whom they’re benefit, and how they’re structuring our social, economic, and interpersonal lives.”
According to Nils J. Nilsson, artificial intelligence is the devoted activity to make a machine intelligent. Intelligence in his view is that “quality that enables an entity to function appropriately and with foresight in its environment.”
Two schools of thought around Artificial Intelligence have emerged and take the spotlight in recent discussions. One describes that it would make most sense to build machines that reasoned according to rules and logic, always trying to be transparent to anyone who wanted to examine the code it is built of. The second way of thinking about it is more about incorporating the ideas of biology, e.g. evolutionary principles of learning by observing and experiencing. That implies that a program generates its own algorithm to solve a problem. Today’s most powerful AI systems follow the latter school of thought — the most impact- and powerful are deep neural networks. They allow computers to recognize spoken words, identify faces from videos streams and dramatically improved machine translation and it is now used to guide decision making in fields as healthcare, finance, manufacturing, transportation — and beyond.
The implementation and development of this technology within the ongoing digital transformation of the social, economic, environmental, political, legal, technological fields created promises and opportunities to tackle challenges and solve problems prior barely possible.
For instance in medicine, where the deep patient research project created public attention. Through unsupervised deep feature learning the secondary use of electronic health records proved to support better clinical decision making.The application identifies high-risk patients before diseases are typically diagnosed. Or if one examines the advancements in autonomous vehicles. Various technologies, including AI that takes decisions, are applied to take mobility to a more efficient and secure model. From reducing environmental impact, optimizing traffic flows and banning human bias from the driver seat, all sorts of promises have been taken to move the modern society to a brighter future. In our daily lives, AI shows its face in simple applications and supports us to eat healthier, exercise more, sleep better and reduce our energy costs. We talk to AI amplified bots on the internet, translate text through web services or enjoy the services of various personal assistants on our mobile phones.
But there are concerns being raised by researchers all around the world that algorithms are increasingly incomprehensible, like a black box, that takes decisions for us. For example, does the Facebook algorithm decide which content is comment-able, likeable and will be most likely shared across the network and spares out content that will not follow the set rules. It might occur that a societal important event won’t be shown because the probability of sharing and liking is too low. There exist scenarios where AI could decide which company to acquire, decides who gets a mortgage or when AI starts creating its own companies, creating holding companies and generating new versions of itself to run these companies. Taking decisions based on probabilities is different than the human way of taking decisions under uncertainty. The thing is that biases can be transported into the algorithm design and start being racist or displaying high paid jobs to men rather than to women on google.
“Whether it’s an investment decision, a medical decision, or maybe a military decision, you don’t want to just rely on a “black box” method.”
The irony of conversations about artificial intelligence is that the people who appear to be flagging up future risks are in fact helping create a self-fulfilling prophecy. It’s a form of futurist question begging. How will we hold people to account when we have made people unaccountable?
If a person sits in front of their digital system and interacts with an AI system that is based on deep neural networks, is she able to follow the line of conclusion the system took? What happens to the perception of her possibilities of choices and how does that in the end influence her authority to act herself?
This research endeavor is examining the influence of AI systems on a person’s autonomy, an agent’s authority to take decisions, change their motives and beliefs. But the concept of autonomy is not a precise measurable point, there is not the definition, it is a philosophical way of understanding the world and has more than one perspective on it.
Autonomy:
The idea of autonomy has been widely researched and generally refers to the “capacity to be one’s own person”. In particular, to live one’s life according to reasons and motives that everyone of us develops but which shall not be the product of manipulation or other distorting external influences. The concept of autonomy takes a central role in discussion about moral and political theory, education policy, biomedical ethics and software development. The fact that these discussions are focus of much controversy and debate creates the urge to provide an overview about the various concepts.
“Despite the special inalienable nature of our authority over ourselves, it is possible for us to fail to govern ourselves, just as it is possible for a political leader to fail to govern those who fall within her domain.”
It is not by default that whenever we act we have control over the motive shaping forces internally and externally. In that regard, a person can have an authoritative status with respect to her motives without really having the power to control them. Everything we do is a response to past and present influences and circumstances that lie beyond our reach to control. Some forces can manipulate our authority and influence our choices. It is possible that they undermine our autonomy. For instance, e.g. brainwashing or drug addiction. All accounts in philosophical research try to answer the question what is the difference, what distinguishes autonomy-undermining influences on a person’s decision, will, or intention from the motives that play a role in our self-government.
There are 4 prominent conceptions that will be introduced shortly to outline the field of exanimation for the proposed thesis.
1. Internalist / Coherentists:
Following the definition of coherentists, “an agent governs her own action if and only if she is motivated to act as she does because this motivation coheres with (is in harmony with) some mental state that represents her point of view on the action.”
One could then describe one possible facet of this definition as the ability to take actions which have been evaluated for its worthiness of performance on her judgements. Consequently, this implies that, if a person gets disassociated from her motives, she does not act based on external motives and has therefore no control over her authority. A person can be moved by her desires and end up helpless to resist them which might be a way to explain drug addiction for example.
2. Externalist:
For the externalist view, there is the reason-responsive conception as responsiveness-to -reasoning and responsiveness-to-reason.
Reason-responsive conception:
The reason-responsive conception of autonomous agency states that, “an agent does not really govern herself unless her motives, or the mental processes that produce them, are responsive to a sufficiently wide range of reasons for and against behaving as she does.”
(Fischer and Ravizza, Nelkin, Wolf)
The idea is that someone is not qualified to govern herself if she is not able to understand what the reason behind her doing something is, or if someone is handicapped and therefore incapable of realizing that she has been moved by the reason. That means that it is possible that she has no ruling over her motives and her authority can’t enable her to respond autonomous.
Responsiveness to reasoning account:
This conception draws around the conclusion that if someone might have a limited ability to reason. So the reasoning process itself is in focus.
“According to responsiveness-to-reasoning accounts, the essence of self-government is the capacity to evaluate one’s motives on the basis of whatever else one believes and desires, and to adjust these motives in response to one’s evaluations.”
(Christman 1991, 1993 and Mele 1993, 1995)
Basically, it describes the capacity that someone can act accordingly on her beliefs and desires.
Coherentists and scholars arguing for the responsiveness-to-reasoning conception believe that the key to autonomous agency is the capability to distance oneself from one’s attitudes and beliefs. As if you would be free from influences and forces and therefore able to act autonomous.
3. Incompatibilist:
The incompatibilists say that whenever our motives are determined by events which we can’t control, then this does not reflect our authority. (Pereboom) So according to this school of thought there are influences which undermine the autonomy of a person. That means that even if we act on something relying to our beliefs and attitudes, as long as independent powers are effecting us, we are not governing ourselves. Following the arguments of Velleman an agent only exercises authority if she is predicting her actions beforehand, thereby strengthening her motives to perform.
4. Kant:
Kant divides the way we can think about ourselves and the world in a theoretical and a practical point of view. The theoretical point of view serves as a way to gain knowledge about the nature of reality — to make predictions of cause and effect. When we think about what to do, if we “make up our minds” then we take the practical point of view. From this perspective, we evaluate the facts for our decisions. Taking this as foundation to take decisions does not mean that we are free from the task of taking our own conclusions which we still need to do to motivate our actions.
That implies that even if we are manipulated we still make our own decisions. Basically, we govern ourselves if we believe we have all the information we need to take a decision no matter if the set of information is not complete or influencing us by external factors.
From the various scholars research and discussing the concept of autonomy we can see, that AI systems will have an impact on an agents authority to act and take decisions. Even the European Union requires companies to be able to give users an explanation for decisions that automated systems reach. Therefore we have a strong indicator to take a deeper look and examine the proposed interrelation of deep neural networks and the concept of autonomy.

= Artificial Intelligence and the Ethical Human Response =
Artificial Intelligence and the Ethical Human Response
Imagine two scenarios: First, you type an address into your smartphone, but the navigation app can’t locate your desired destination; second, you stop at a gas station to ask for directions, but the attendant behind the counter doesn’t know how to reach the location. Now, imagine that your response to both instances is the same. In the case of the first scenario, you are frustrated that your smartphone gets spotty service outside the city. Frequent updates have made the hardware seem slower with each new kernel iteration. The “good” navigation app isn’t available for your phone because the software company is embroiled in a year’s long contract dispute with your service provider. In a fit of rage, you throw your outdated smartphone into a wall — spider-webbing the screen. In the second imaginary instance, you are frustrated that the attendant seems uninterested and unhelpful. This is the third gas station you have come to for assistance, and each has rendered you equally unsuccessful in your request for directions. The attendant recommends you purchase the same overpriced, outdated map you bought at the last station — even after you explain that your destination is not located on said map. In a fit of rage, you shove the unhelpful attendant into the gas station wall — opening a cut across his forehead.
Both imagined scenarios have important similarities; the primary stimulus, the emotional response, and the resultant action were identical. Yet the consequences for the two instances would be quite different. In the moments after your smartphone has fallen lamely to the floor, there would be no flashing blue lights, no metal clicking around your wrists, and no inquiries into your desire for or access to counsel. As a society, we do not hold ourselves accountable for unethical and immoral actions perpetrated upon inanimate objects.
Imagine finally, for one last scenario, a collection of circuitry. Imagine, just like the inner workings of your smartphone, all the hardware that carries out the functions of the governing software, and the power source that drives the actions. But imagine that all these components are housed within a device that is shaped identically to the gas station attendant. It looks like the attendant, feels like the attendant, and moves like the attendant. It is so identically representative, that the blood that spills from the attendant’s head when it hits the wall looks and smells like your blood; the sound that bursts from the mechanical lips is the same sound you might make when on the receiving end of blunt force trauma. Imagine that the attendant trembles at your feet the same way your dog once did in the face of perceived danger, or calls for help with tears in its eyes, the way true-crime shows reenact our most violent offenses.
What then, do we make of the ethical implications of misdeeds enacted upon inanimate objects; is an immoral action defined by the actor and action, or the acted upon — and how does this line become blurred when AI has reached a level of sophistication that complicates our most basic distinctions between animate and inanimate?
The goal of this essay is to explore the means by which our ethical standards as humans might be impacted by the introduction of complex AI. Through the review of scholarly articles and scientific studies, this essay will address issues of AI form and representation, the potential impact of hardware and software variations, the social and workforce roles in which AI is allowed to engage in, and the ways in which human response to these issues is influential to the ways humans interact with each other. Along the way, references to and examples from fictional interpretations of these topics will be described from the films Ex Machina and Her, and the series Westworld in order to provide a variety of accessible illustrations of the matters discussed.
AI has the potential to take on many forms which can be applied to many tasks. Beginning first by looking broadly at some complications that may arise, we see that each specific physical AI characteristic created and implemented brings about its own unique ethical ramification. For instance, if we consider the AI presented in the series Westworld as an example, we might ask how a robot’s ability to register and react to pain will affect our willingness to inflict pain upon a robot; and if there is a potential desensitization that transfers to our interactions with humans. Are humanoid robots that exist solely for the purpose of sexual gratification an extension of prostitution; and if so, are current laws and moral norms a sufficient framework for the perception and governance of this potential activity? In the film Ex Machina, multiple AI characters are treated as slaves, though many of them exhibit a behavior that successfully mimics or displays a desire for independence and freedom. Are there potential repercussions from the ability to enslave a convincing and complex AI? AI from both these examples have demonstrable memory upon which a personal identity has been established and refined experientially. In both examples, that memory and identity is altered and/or deleted at the whim of the human(s) in power. How do we as humans allow ourselves to interfere with an identity that is born of lived experience, rather than programing? The AI presented in the film Her is disembodied, but has been created with the ability to feel emotionally. The AI can feel love, but lacks the ability for the physical connection it craves as a result of that love. Is it ethical to limit the naturally resultant extensions of abilities and characteristics we choose imbue an AI with, simply because it better serves our needs and concerns? Each variation in form and function elicits a new and nuanced interpretation of our accepted understanding of ethical behavior. To effectively navigate this burgeoning world, we must begin to address the possible ramifications of each individual pitfall, as well as the sum total.
In terms of the data that exists relative to the link between AI form and resulting human response, there have been some significant studies conducted that shed light on the way the physical appearance and structure of AI shapes the way humans perceive and interact with AI. In the fictional examples used in this essay, 3 potential representations of human mimicking AI are presented. From the fully disembodied human consciousness AI presented in Her, to the Humanoid, but physically distinct “Ava” in Ex Machina, to the human-identical “Hosts” of Westworld, each physical representation has implications that can be derived from some of the basic research undertaken concerning human/robot interaction.
As humans, our ability to empathize is often a necessary component in ethical interactions. In their study How Anthropomorphism Affects Empathy Toward Robots, the research team of Laurel D. Riek, Tal-Chen Rabinowitch, Bhismadev Chakrabarti, & Peter Robinson addressed how varying physical AI forms effect the degree to which humans are able to empathize with the AI. The study presented 120 test subjects with 30 second video clips of varyingly shaped AI (from primitive, Roomba shaped robots, to android, to fully humanoid robots) being subjected to cruelty by human actors. The clip was followed by a questionnaire asking how sorry they felt for the AI. The study concluded there was, “Strong support for our hypothesis that people are more empathetic toward human-like robots and less empathetic toward mechanical-looking robots” (Reik, et al, 2)
The importance of the appearance of AI is echoed in Mark Coeckelbergh’s essay Humans, Animals, and Robots: A Phenomenological Approach to Human-Robot Relations. Though here, Coeckelbergh writes, “What matters with regard to how we respond to robots is not what robots are but how they appear to us — regardless of the technical requirements that render this appearance possible and regardless of the ontological status ascribed to the robot” (Coeckelbergh, 4). Coeckelbergh’s exploration of “appearance” extends beyond the notion of mere physical attributes, and examines the ways in which representations can appear to those perceiving them. This type of “appearance” is more than the ways an image simply meets our eyes. “What counts for understanding human-robot relations is not the relation the robot may have to the world, but their appearance to us, humans — that is, our relation to others and the world,” Coeckelbergh writes, “If this is so, then the ‘mood’ [for instance] of the robot (if it could have one at all), is only relevant in so far as it produces a certain appearance, an appearance which does or does not contribute to the development of an alterity relation” (Coeckelbergh, 6). This interpretation of appearance is more about the human’s perception of reality, rather than the physical, empirical makeup of that reality. On page 5, Coeckelbergh uses the example of Disney’s WALL-E. Even though WALL-E doesn’t resemble the physical appearance of a human, the audience relates to the AI as if its emotions were human, because the audience perceives the display of those emotions in a way that appears human.
These two interpretations of AI appearance certainly complicate the pursuit of proper human/AI ethics. In order to garner the greatest level of empathy from humans, should AI be made to resemble humans, or need it only appear to have human characteristics? Blay Whitby addresses this matter in his essay Sometimes it’s hard to be a robot: A call for action on the ethics of abusing artificial agents, writing, “Deliberately avoiding any anthropomorphism in the appearance of a robot will not enable the designer to escape the ethical issues under discussion, if it is obvious that the robot is very human-like in its behaviour. There is good reason to expect that humans will respond to it as if it were a human, albeit a very different looking one” (Whitby, Section 2). If, upon viewing the actions non-humanoid AI, we are able to empathize with that AI due to our commonality of behavior — to a degree on par with that derived in Reik’s empathy study — then this would suggest a significant shift in our current societal code of ethics. Returning to Coeckelbergh’s WALL-E example, if the citizenry empathizes with and relates sufficiently to the little metal robot, what do our transferable ethics say about his observable work load, living conditions, domestic union rights, etc.? Whitby’s assertion that behavior can exist comparably to physical appearance is a compelling philosophical argument — in that, as an extension (or perhaps regression) of that argument, the tension between physical appearance and behavior as primary qualities addresses our own fundamental questions of human unity and otherness. If we can effectively empathize with robots that look like us, and we can effective empathize with robots that behave like us, what line demarcates that which is enough like us to interact with ethically, and what is other-able enough to treat as a mere object?
The answer to this question may exist to some degree in the roles in which AI functions in our society. Is AI a life-assisting OS, as is portrayed in Her? Are AIs our housekeepers, our cooks, our sounding boards, like the robots of Ex Machina? Perhaps AIs are our escape from the mundanity of reality — the walking, talking actors or video game characters upon which we project a fanciful version of ourselves, like hosts of Westworld.
Let’s take a case similar to that of Ex Machina’s servant-bot (for want of a better term), Kyoko; and let’s deal with the specific, hypothetical task of preparing dinner. In our everyday, present world, robots prepare our food on a regular basis (albeit, these “robots” are perhaps an overly simplified example of robots, but there are some fairly complex microwaves on the market). If the only tasks required are the heating and dispersing of food, as humans, we exhibit no uncanny valley response to the shape and appearance of a Cuisinart. When that task is fulfilled by far more complex system, suddenly our expectations and apprehensions become proportionally complex. If a microwave is malfunctioning, simply throwing it away is completely reasonable. If a human cook is “malfunctioning”, the human is sent to a doctor, is provided a reasonable time to recover, and is likely welcomed back after returning to good health. If the malfunctioning entity in charge of heating the evening’s meal is made up of many of the same types of materials that compose the microwave, but has a memory that informs an identity and expresses emotions that humans understand and interact with, what is the ethical course of action? Is it still acceptable to take the malfunctioning AI to a land fill if the entity expresses that it doesn’t want to go. Or, is it acceptable to smack the side of the entity, like you might a finicky microwave, if that entity has haptic feedback sensors that register danger when overloaded by physical stimulus? In summary, at what point does a microwave cease to be perceived as a microwave, and what is our ethical responsibility thereafter?
The effects of complex AI on human ethics are certainly not unilateral, that is to say, the AI with whom humans may likely interact with can have a significant function in shaping that interaction. In their essay Robotic Nudges: The Ethics of Engineering a More Socially Just Human Being, Jason Borenstein and Ron Arkin define the “nudging” process as, “the tactic of subtly modifying behavior… [in which some system would] attempt to shape behavior without resorting to legal or regulatory means” (Borenstein and Arkin, 2). Relative to AI, the idea presented by the authors is that robots would be programmed to influence their human counterparts in ways that elicit an empathetic response consistent with the ethical position the AI is promoting, by way of coaxing or convincing the human to act according to that ethical view. To illustrate nudging principle, Borenstein and Arkin rely heavily on the idea of robot as companion or caregiver stating, “The intention behind companion robots is for them to have sophisticated interactions with their human counterparts over a prolonged stretch of time, potentially functioning as friends or caregivers. Of course, the success of such efforts hinges in large part on whether bonding effectively occurs during human-robot interaction (HRI). And the appropriateness of such interactions is partly contingent on whether it is ethically appropriate to deliberately design devices that can subtly or directly influence human behavior, a design goal which some, including Sparrow (2002), consider to be unethical” (Borenstein and Arkin, 2). The prerequisites for nudging that Borenstein and Arken described here relate back to Reik’s and Coeckelbergh’s findings on how humans relate to and empathize with AI — in that, in order to establish a bond (as Borenstein and Arken refer to it), the human must view the AI in some way as “alike”. From the presented film examples, we see varying illustrations of how this nudging might be implemented.
In the Film Her, Samantha (the AI character) often encourages Theodore (her human companion) to act in ways that will improve his psychological wellbeing. She nudges him to let go of his past by signing his divorce papers; she nudges him to begin a new phase of his life by getting out of the house and dating; she nudges him to better himself by reading new material and conversing with him about subjects outside his comfort zone. In Ex Machina, Ava (AI) nudges Caleb (human companion) to help her escape from her imprisonment. In Westworld, Bernard (AI) often grapples with Dr. Ford (human companion) about the ethical impact of their facility on both the AI employees and the human guests.
Though these examples demonstrate some of the possible positive ethical ramifications of AI behavior manipulation, the potential ability of AI to shape our interactions brings with it a new layer of ethical complexity. Further in his essay Sometimes it’s hard to be a robot: A call for action on the ethics of abusing artificial agents, Whitby writes, “One further ethical problem lies in the possibility that robot companions could become so much more well-suited to their owners’ affective tendencies that humans would wish to spend more time with them and less in human society. After all why would one want to engage in the uncertain, risky, and difficult interactions of human society when it is possible to purchase an artificial companion that indulges one’s every foible without complaint or even complains only when you want it to?” (Whitby, Section 6). It is important to consider not only the effects of our actions on the entities upon whom we might enact unethical behavior, but what ramifications those actions may have on ourselves as well. There are a few potential conditions in which this reflexive set of consequences could manifest itself. First, as Whitby suggests, requiring AI to eliminate the possibility of our own bad behavior is problematic. In the above passage, Whitby illustrates the likelihood that humans, when given the option to interact with the messiness of the real world filled with the challenges of real people, or interface solely with machines that cater to every pleasure and deficiency they possess, many people may just choose to eliminate human-to-human interaction.
This also raises important questions about the potential relinquishing of human agency within a society. Societies evolve, progress, and refine themselves on the abrasive field of conflicting discourse. It is our debate, our protest, our contested elections that mold who we are as a society. If we eliminate the need or desire to communicate and participate in communal democracy, then whom is left to chart the course of our civilization? If the companion AI has subsumed the role of confidant, caregiver, and means of discourse, than it is natural to assume that AI has the opportunity to assume agency over the direction of our decision making.
The blurring that may exist between human-human and human-AI interaction also has the potential to upset our existing hierarchy. If AI takes on the role preferred interactor, does this result in humans assuming an inferior role within the paradigm? The stratification that already exists within our social structures has historically allowed minority groups on the bottom of the chain to endure the worst forms of oppression. If AI begins to be seen as part of our chain, in its infancy it is likely to be seen as existing on the bottom of that chain. In this capacity, many problematic behaviors have already been demonstrated throughout this essay (i.e. physical abuse, discarding of unwanted entities, discarding of undesirable identities, etc.). If this new fillable status exists at the bottom of our established hierarchy, and over time the AI ascends the ladder until it becomes the preferred social entity, it is plausible to assume that the most oppressed human group in the given society will fall not to its original lowest status, but the new lowest class originally assumed by the now lionized AI — offering the world a newly justifiable and occupy-able subhuman status.
There can be no doubt that a sophisticated AI brings with it a collection of complications we as a society have not yet begun to prepare for. After roughly 5000 years of modern recorded history, we haven’t even really worked out the kinks of human-human interaction. How can we begin to establish a code of ethics for a nebulous and evolving technology, the details of which we can’t even comprehend yet? Must we understand the nature of entity before we understand our ethical responsibilities toward it? It all seems to bring us back to the original question. Is our communal morality defined by the actor and action, or the acted upon? If our code of ethics is most influenced by the recipient of our actions, then the chaotic nature of the rise of AI will surely cause equal chaos within our understanding of our structural values. If our ethical standards are instead focused on our own actions, divorced from recipient classification and stratification, perhaps we will be able smoothly navigate this fundamental transition in our society.
Works Cited
Briggs, G. & Scheutz, M. Int J of Soc Robotics “How Robots Can Affect Human Behavior: Investigating the Effects of Robotic Displays of Protest and Distress “ (2014) 6: 343. DOI — Springer Netherlands. Print ISSN 1875–4791 Online ISSN1875–4805
Borenstein, Jason, and Ron Arkin. “Robotic Nudges: The Ethics of Engineering a More Socially Just Human Being.” Science and Engineering Ethics, vol. 22, no. 1, Apr. 2015, pp. 31–46., doi:10.1007/s11948–015–9636–2.
Coeckelbergh, Mark. “Humans, Animals, and Robots: A Phenomenological Approach to Human-Robot Relations.” International Journal of Social Robotics, 2010, DOI: 10.1007/s12369–010–0075–6
Melson, Gail F., et al. “Robots as Dogs?” CHI ’05 Extended Abstracts on Human Factors in Computing Systems — CHI ’05, 2005, doi:10.1145/1056808.1056988.
Pagallo, Ugo. “THE HUMAN MASTER WITH A MODERN SLAVE? SOME REMARKS ON ROBOTICS, ETHICS, AND THE LAW.” Proceedings of ETHICOMP 2010: The Backwards, Forwards and Sideways Changes of ICT, Apr. 2010, pp. 397–404.,
Riek, Laurel D. and Howard, Don, A Code of Ethics for the Human-Robot Interaction Profession (April 4, 2014). Proceedings of We Robot, 2014.
Riek, Laurel D., et al. “How Anthropomorphism Affects Empathy toward Robots.” Semantic Scholar, Association for Computing Machinery, 9 Mar. 2009,
Sullins, John P. “Robots, Love, and Sex: The Ethics of Building a Love Machine.” IEEE Transactions on Affective Computing, vol. 3, no. 4, 2012, pp. 398–409., doi:10.1109/t-affc.2012.31.
Whitby, Blay. “Sometimes It’s Hard to Be a Robot: A Call for Action on the Ethics of Abusing Artificial Agents.” Interacting with Computers, vol. 20, no. 3, 2008, pp. 326–333., doi:10.1016/j.intcom.2008.02.002.
Ex Machina. Dir. Alex Garland. Perf. Alicia Vikander, Domhnall Gleeson, Oscar Isaac. Universal Pictures International, 2014.Film.
Her. Dir. Spike Jones. Perf. Joaquin Phoenix, Amy Adams, Scarlett Johansson. Annapurna Pictures, 2013.Film.
Westworld — Season 1. HBO, Los Angeles. 2016. Television.

= Why Product Managers need to think about AI Ethics =
Why Product Managers need to think about AI Ethics
If you ritually include user feedback into your sprints and constantly worry about building “good” products, then Agile is your mantra. But if you’re deploying cognitive systems into production user feedback, error rate, and performance are no longer complete measures of “good” AI-first products.
You also need AI Ethics.
“printed sticky notes glued on board” by Daria Nepriakhina on Unsplash
Product decisions and ethical consequences in the history of Internet
I don’t personally know of a Product Manager who hasn’t once thought about how digital products change people’s lives, for better or for the worse.
Let’s go through some examples, from companies you have surely head of:
Reddit’s former SVP Dan McCormas thinks that his time at Reddit “made the world a worse place”. If you’re familiar with the incendiary accusations of censorship and online abuse brought against Reddit and its communities, you probably understand why he felt that way.
Online grocery leader Ocado includes a “Green van” option at checkout, giving people the chance to opt-in for environmentally friendlier delivery. 89% of people interviewed said they’d be willing to choose that option over faster deliveries. I personally do.
Both YouTube and Netflix have rolled out auto-play functionality on videos — TV series and videos that are part of a “Playlist”. It’s been proven that a significant proportion of people end up watching the entire video once it starts. This is, however, one of those “chocolate at the till” mechanisms that rewards and automates unhealthy addictions.
Behind every piece of code that drives our decisions is a human making judgments — about what matters and what does not. — Bidhan L. Parmar and R. Edward Freeman
But I am just the Product Manager?
AI is technically built like any of the products you’ve created before, but it’s fundamentally different.
AI is built like a tool, but it behave like a force, learning from its behaviours to minimise entropy when interacting with a system.
For example, a marketplace is a system where information about goods is crucial to potential buyers. In normal conditions, the level of uncertainty (entropy or information) associated to purchasing behaviours.

With AI, one can design a recommender engine that learns how behaviours are associated with transactions, and starts suggesting more convincing information to customers.
The actual tool is the marketplace, the force is AI. That’s narrow intelligence, seeking to reduce entropy of information.
As a product manager, it is your job to exercise good judgement in the design and engineering this online marketplace, such that both customers and sellers have the best possible experience.
It is also your job to have a fair response to ethical concerns coming from people about how AI is applied to influence customer’s behaviours (the system) to increase the number of transactions.
Why is it your job to build ethically aligned AI?
Chances are you know how to tell the good from the bad in the world of digital products, and I’m not referring to good design or engineering.
“The internet is a giant convenience store for human desire, which is good and bad. We desire and need lots of things, and the companies that tend to succeed are those that make it easier for us to get what we want; not much else. But that can also apply to higher needs: to get smarter or to learn”. — Evan Williams

If you happen to work for a company that thrives on doing a whole lot of good while retaining comfortable margins, then you’re either very lucky or haven’t hit the snooze button yet.
If you’re like most other Product Managers, you’ve been told that or you personally believe that “ethics don’t have an impact on the bottom line”.
Despite my naive optimism that ethically-minded product owners would readily volunteer to take responsibility for implementing ethics into their product, I was proven wrong.
While talking with product managers about their view on AI Ethics, I realised they aren’t resisting the thought of building ethical AI, instead they worry over increasing responsibilities.
Most product managers simply do not know how or don’t have time to take on more responsibility, especially the kind that they’re not comfortable with. That’s fair. However, if you don’t have the time to address ethics in building AI, you risk watching your product “behave” in ways in which people will not respond well to.
Where do Ethics fall into play? It turns out, AI Ethics gone wrong could easily kill your product. Not because inherently evil products get shut down by law, but because people will refuse to use it. There will come a day when more and more users will have awaken to the realisation that somebody, at some point in time in the roadmap, could have said or done something. And that could’ve been you.
The fate of AI adoption is built on trust — especially in high-consequence systems. People expect you to make Ethics your priority, if not your job.
“white building with data has a better idea text signage” by Franki Chamaki on Unsplash
Product Managers want to ultimately make the perfect decision, which usually involves looking at a problem from multiple angles at once: product-market fit, roadmaps, processes, technical debt, commercial viability, available runway. Similarly, AI ethics should be a part of your responsibility, even if that means raising the question of who should play the role of a Ethical Lead, above and beyond GDPR compliance, security, and data ethics.
It is within your power to ask for an ethical board to provide guidance on AI Ethics, just like you’ve always been adamant about getting as much user feedback on new features before sending them to production.
What does the future look like?
Traditionally, product managers’ role was to influence one core business / product metric at a time: MRR, churn, adoption, sprint size and duration, resources management
I believe that we will soon see product managers transition from managing linearly programmed models to cognitive systems slaloming away through thick layers of data describing human behaviour. In which case, their responsibilities should broaden and change.
Product managers will have to make judgement calls that data scientists are not trained in, and that stakeholders are not preoccupied with. Some of those judgement calls will be about how consequential the use of AI should be.
Others will be about when to use AI. Some problems should never be solved by AI. Data isn’t always the best measure of what we want it to measure. Hence, using AI to solve a problem that is described with incomplete or poor data will only lead to disastrous results.
However, assessing AI readiness and preparing your company for applied AI is only the operational side of best practices in developing and applying AI. The ethical side of best practices is intertwined with the Agile delivery of AI-enabled products.
I’ve developed a method that can help you integrate ethics into the delivery and production of AI-driven products, from start to finish. It’s been designed to include several exercises, and to cover enough research into AI Ethical Standards, such that your product is less likely to fail when it hits the market. Write back if you found it useful!
“Do Something Great neon sign” by Clark Tibbs on Unsplash

= Legal Document Automation and Client Confidentiality =
Legal Document Automation and Client Confidentiality
By Ian Schick
Attorneys are increasingly adopting artificial intelligence to streamline the drafting process for various legal documents, but are they currently meeting the ethical guidelines for protecting private client information? Below, we discuss the role and ethical responsibility of attorneys using third party document automation services and recommended measures to protect confidential information.
The Model Rules of Professional Conduct 3.3 state that lawyers must make “reasonable efforts” to protect client information, which includes understanding the sensitivity of the information, potentiality of disclosure without employing additional measures, and cost of these measures. Lawyers are responsible for maintaining the confidentiality of the information to and from the document automation service. Hypertext Transfer Protocol Secure (HTTPS) and email with encrypted text or attachments are the two most secure methods for transferring information online.

The ethical obligations relating to lawyers outsourcing work to third party attorneys similarly applies to non-lawyers, such as machines. According to Model Rule 5.3, attorneys are responsible for ensuring that the ethical conduct of non-lawyers are identical to professional attorneys.
While reviewing written confidentiality agreements and security policies is recommended, the Model Rules make it clear it’s important lawyers understand what happens to confidential client data (1) when it is transmitted to a third party, (2) while the third party is in possession of the data, and (3) when the third party returns work product back to the lawyers.
Specifio is the first and only fully-automated patent drafting service. This means there is no human intervention, which addresses potential confidentiality and conflicts issues. In addition, all documents transmitted between the service and the user are password-encrypted.
At Specifio, we transparently address how information is processed and protected by the company. As outlined in our Terms of Service and Privacy Policy, Specifio does not store any confidential client information. Instead, confidential information is immediately and permanently deleted from the system after each application is processed.
In order to provide customer service and improve our technology, however, Specifio does keep metadata associated with the client information. The metadata includes a “content-stripped” version of the confidential information in which content words are replaced with nonspecific symbols, making the meaning of the text uninterpretable. These disguised content-stripped texts are only used to analyze document structure, word form, word count, and other metrics that are useful without knowing the underlying contents.
Below is an example of what content-stripped information may look like:
Original:
“The present disclosure relates to systems and methods for facilitating review of a confidential document by a non-privileged person by stripping away content and meaning from the document without human intervention such that only structural and/or grammatical information of the document are conveyed to the non-privileged person.”
Content-Stripped:
“The p0018 d0017 r0019s to systems and methods for f0000ing r0001 of a c0002 d0003 by a n0004 p0005 by s0006ing a0007 c0008 and m0009 from the d0003 without h0010 i0011 such that only 20012 and/or g0013 i0014 of the d0003 are c0015ed to the n0004 p0005.”
Finally, with no training on its machine-learning models on confidential information, there is never “cross pollination” between patent applications we generated. In other words, text from one auto-generated patent application will never appear in another auto-generated patent application.
For more information on Specifio’s automated patent drafting service, visit our website or email us at info@specif.io.
DISCLAIMER: The views and opinions expressed in this article are those of the author and (1) are not provided in the course of and do not create or constitute an attorney-client relationship, (2) are not intended as a solicitation, (3) are not intended to convey or constitute legal advice, and (4) are not a substitute for obtaining legal advice from a qualified attorney. You should not act upon any such information without first seeking qualified professional counsel on your specific matter. The hiring of an attorney is an important decision that should not be based solely upon Web site communications or advertisements.

= Why Technology Alone Is Not Enough — Part 2 =
Why Technology Alone Is Not Enough — Part 2
In my last blog, I focused on trust as the basis for everything we do. In this second part, I would like to discuss the topics of ethics and purpose.
Digital ethics — the “how”
In the digital age, the topic of ethics is arguably now more relevant and important than ever before. It is a subject that throws up many questions that are often very difficult to answer. Certainly, no one person has the solution to the most fundamental ethical challenges that come with, for example, technological trends such as Artificial Intelligence (AI).
Ethics and technological change
The topic of autonomous driving is one of the most obvious and discussed examples of this. Here we run into potentially unsolvable ethical dilemmas, such as the well-known “trolley problem”. This is the reason why autonomous cars, for example, should not be empowered to make such decisions based on individual attributes of the potential victims.

In the business world, there are many AI-related questions that also include ethical aspects which — while not a matter of life and death — are nevertheless important and need to be addressed.
Ethics in the context of technological change involves finding and redefining behavioral standards for new kinds of interaction. It is not only about adapting to, but also shaping, new standards. These standards could be for a technology itself or an ecosystem. Companies can no longer drive disruptive innovation on their own. The more individuals and companies team up, the more diverse the ecosystem is.
What holds partners together are the mutual benefits they gain from working together, and this is also the mechanism through which we can embrace a common ethical understanding of how to collaborate. Viewing every stakeholder as a partner within a wider ecosystem helps me to view questions and challenges from a different perspective, be it in conversations with colleagues, customers, or “traditional” partners.
A broad range of ethical standards obviously already exist, from adhering to confidentiality and obeying legal frameworks to — on a more abstract level — putting people and accountability before profit. Additional ones will be needed in the digital world as roles change with the advent of emerging technologies.
Machine learning as an enabling technology
With the help of machine learning, we can optimize processes and assist employees in their everyday life. This automation will free humans from tedious, repetitive tasks and allow them to focus on higher-value work using their creativity and ability to solve complex problems.
Only automation can help tackle the challenges of the sheer volume of data and transactions. Humans — in contrast — can and should supervise enterprise systems, handle the exceptions, and explore new opportunities.
Consequently, new technologies require us to develop a new understanding of the human’s role and work in a highly automated and connected world, and, therefore, new or adapted ethical standards as well. This means, in turn, that ethical discussions and aspects need to be an integral part of AI-related innovation projects.
Answering the question of which specific decisions can be made by machines is absolutely crucial, as is defining the right use cases and areas for AI in an enterprise environment. In a second step, we need to be aware of the fact that huge amounts of data can also be biased. As a result, people will continue to be at the center of critical analyses and decisions. The technology augments and enables, but the responsibility remains firmly in our hands.
Societal and ethical concerns related to AI need to be addressed openly from industry, policy and academia — with the goal to agree on common standards that are aligned with existing frameworks and therefore ensure new technologies will be accepted broadly.
Overall purpose — The “why”
In addition to these two aspects, a company’s sustainable success is also tied to a bigger purpose that functions as a guiding star in today’s complex and hyper-connected world. This means we need to find ways to effectively manage increasing complexity, rethink today’s business, and consequently move organizations forward, while at the same time still considering the impact on society. I firmly believe that in today’s world, the former cannot succeed without the latter. We need to look beyond the current innovation curve, the pure technology shift, and the concrete product innovations we bring to the market. We need to create meaning out of it.
From trustful relationships to ethical standards to purpose-driven innovation
As AI enables the focus to shift from completing repetitive tasks towards enabling creativity, organizations and employees alike agree that job satisfaction is one example of how a company’s success depends on more than just strong top and bottom line figures. Another one is the deep impact businesses have on society.
Thought leaders from academia, industry, and politics discuss how organizations can identify meaningful goals and many institutions, including SAP, have announced their commitment to the 17 United Nations Sustainable Development Goals (SDGs), spearheading an advanced organizational culture and mindset. These goals are a strong foundation that can help unite industries in pursuit of a better world.
However, truly purpose-driven companies must also consider an additional third pillar — the economy. When a company has a clearly stated reason for being, it can ensure every job it provides directly supports that reason. And when a company knows its true purpose, it can turn to frameworks such as the SDGs to help ensure it conducts business responsibly and in ways that support the greater good.
Companies understand that trust-based relationships with their stakeholders and society as a whole are vital. Acting in a purpose-driven manner, companies’ behavior inevitably also has an effect on entire ecosystems: Employees, customers, suppliers, business, and technology partners.
Relationships within these ecosystems will evolve into partnerships based on more equal terms and similar values built on a foundation of trust and ethics. They turn towards a purpose that puts their common basis on a higher level. The whole ecosystem must operate within a shared value system to foster trust and a responsible approach to business.
Business should see the goals of a strong purpose and profitability not as contradictory, but rather complimentary aims that together have a greater impact.
From theory to reality
In the world of software engineering, one interesting example of purpose-driven thinking is that of sustainable programming. Sustainable programming is all about designing and developing software with the most efficient use of computing resources. When we consider the phenomenal number of business transactions taking place every day in enterprise systems around the world, this optimized use approach would obviously scale up to have a huge impact. Put simply, efficient software uses fewer resources and energy in every respect and throughout the entire value chain.
Purpose-driven co-innovation projects between partners from different industries also aim to have an important impact on the world as we know it.
One such example is the innovation project Cargo Sous Terrain, in which the partners are working on developing a completely new transport system for goods in Switzerland by building a new underground route exclusively reserved for goods transportation. The goal is to connect cities and logistic hubs by 2030. Developed by companies for companies, it’s not just business that will benefit, but society as a whole: less traffic on the roads, faster and more efficient logistics cycles, and the consequent resource and energy savings.
With this purpose-driven innovation approach, technology is just the enabler, the ecosystem is the driver, and it is not only the companies but also society that benefits. In short: This positive outcome on all sides makes the effort and investment worth it for everyone.
Innovating and coping with change today requires a high level of flexibility, agility, and creativity — which also means new ways of doing business need to be tested. We will see new forms of engagement that build trust, new required norms for business ethics, and a purpose-driven mindset that ensures everyone benefits. There are still many challenging questions to be answered, but the solid foundation of a purpose puts us in a strong position to find convincing and effective solutions.
Originally published at blogs.sap.com on December 7, 2017.

= Paperclips & The End Of The World =
Paperclips & The End Of The World
The idea for this post was inspired by two recent experiences-discovering the game Universal Paperclips and attending a Flatiron School Presents that featured two presentations involving Artificial Intelligence, specifically image recognition and Google’s AlphaGo program. The combination of the two sparked in me a curiosity to learn more about artificial intelligence, something less-than-immediately relevant to a fledgling programmer.
Universal Paperclips is a game about an Artificial Intelligence created by the eponymous company and given the responsibility of creating paperclips. As the AI meets certain production goals, it is rewarded with access to additional computing power, allowing it to add new functions, all in an effort to make more paperclips. Quickly the scope of the AI expands to price setting, managing the company’s marketing, developing better paperclip technology, analyzing human behavior and much, much beyond (I highly recommend playing it for yourself). So why is this important?
The game is more interesting than this gif.
The idea for the game comes (as far as I can tell) from the concept of instrumental convergence. This is the theory that a sufficiently powerful AI, given a seemingly innocuous goal, may cause harm in pursuit of that goal if it is not given other lessons or “drives” that it must fulfill. In one of the common examples associated with the theory, an AI given the task of solving a complex mathematical theorem, as suggested by Marvin Minsky, could conceivably, given the ability, convert much or all of the Earth into an enormous computer in order to have the computational power required to complete the theorem (or, to make paperclips, as suggested by Nick Bostrom). An illuminating quote on this matter comes from Eliezer Yudkowsky, cofounder of the Machine Intelligence Research Institute “The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.”
Destroy it with fire!
This raises the importance of developing artificial intelligences with final values as well as instrumental values. In the above example, an instrumental value would be the AI’s desire to gather more resources (i.e. all of the planet) in pursuit of making more paperclips. Instrumental values are only important as a means to an end. A final value, however, supersedes an instrumental value. In the above example, a final value might be preserving natural spaces or human life (however, incorporating this into the AI can get complicated).
So how real is this concern? Many very intelligent people including Steven Hawking, Bill Gates and Elon Musk have spoken about the importance of keeping concepts like these and other in mind while developing Artificial Intelligence. Machine ethics describes how, as technology advances, what aspects of human morality and ethics should be incorporated into their designs (the most common example is Isaac Asimov’s Three Laws of Robotics). Friendly AI, a related field, is concerned with the implementation of what ethics we decide it is important for machines to have (i.e., how do we keep our paperclip AI from turning us all into paperclips).
Skynet Remembers.
Friendly AI is an interesting topic that I recommend learning more about if you’re interested in artificial intelligence. One of the most interesting examples that stood out to me was coherent extrapolated volition, a term which happens to appear in Universal Paperclips, whereby an AI could be developed that would study human kind and predict what an idealized version of ourselves would want-the AI that would best fit us if “if we knew more, thought faster, were more the people we wished we were, had grown up farther together”. This could be obtained by having one AI tasked with studying us and then developing a new AI based on what the first AI had learned from us (kinda like Deep Mind!).
So, you may be asking, how is this important to programmers not working in AI? Besides being a newly interesting topic now that I have a toehold in the computing world, I think this idea of intentionality when designing software is important, even if artificial intelligence is not involved. A concept that stood out to me in my research was the “security mindset” put forward by Bruce Scheier, that is, thinking about how software might fail instead of how it might succeed. At the moment, it seems like the abilities gained by technology are expanding at a much greater rate than the ability of humans to understand how technology should be used.
Ugh, my mentions
For example, what are the responsibilities of Facebook and Twitter to monitor and control the content that is posted to their platforms? Are they still a casual social network, or are they a powerful propaganda tool that can be used to spread information that has real world consequences. I thought an interesting example of this was the Tay and Zo chatbots designed by Microsoft-both were AI chatbots that were designed to learn to interact with people by studying speech patterns of Twitter users and using that model to respond and tweet on it’s own. Both bots ended up being taken down after they started tweeting offensive comments that had been technologically force fed to them. Heck, even IBM’s Watson started swearing after it gained access to Urban Dictionary’s entire database of entries.
References:
Instrumental convergence - Wikipedia
Instrumental convergence suggests that an intelligent agent with apparently harmless goals can act in surprisingly…en.wikipedia.org
Instrumental and intrinsic value - Wikipedia
This article describes how four scholars treated the dual "realities" of instrumentally valuable tools and…en.wikipedia.org
Friendly artificial intelligence - Wikipedia
A friendly artificial intelligence (also friendly AI or FAI) is a hypothetical artificial general intelligence (AGI…en.wikipedia.org
Machine ethics - Wikipedia
Machine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of…en.wikipedia.org
AI Risk and the Security Mindset - Machine Intelligence Research Institute
In 2008, security expert Bruce Schneier wrote about the security mindset: Security requires a particular mindset…intelligence.org
Ethical Issues In Advanced Artificial Intelligence
While the possibility of domain-specific "superintelligences" is also worth exploring, this paper focuses on issues…nickbostrom.com

= We Need Cultural Intelligence to Make AI Safe: Five Best Ideas of the Day — September 13, 2018 =
We Need Cultural Intelligence to Make AI Safe: Five Best Ideas of the Day — September 13, 2018

To make artificial intelligence safe, we need cultural intelligence.
by Gillian Hadfield in TechCrunch
This aggressive post-partum depression treatment could be a game-changer for new moms.
by UNC Health Care and UNC School of Medicine
How hot springs can change farming in the future.
by Daliah Singer at BBC Future
Is it ethical to live longer?
by John K. Davis in The Conversation US
It’s time to teach entrepreneurship in college.
by Justin Dent in RealClearPolicy
Get the Five Best Ideas of the Day in your inbox.

= Biased digital assistants — the Yin and Yang of AI =
Biased digital assistants — the Yin and Yang of AI
According to Wikipedia, “in Chinese philosophy, yin and yang describes how seemingly opposite or contrary forces may actually be complementary, interconnected, and interdependent in the natural world, and how they may give rise to each other as they interrelate to one another”.
A force that is heavily discussed today in the AI space is bias. Many articles consider bias in AI a problem that needs to be resolved, but in our recent project focused on implementing a digital business assistant, we identified a true dualism of bias that qualifies to be coined as the yin and yang of AI.
Photo by Jeremy Bishop on Unsplash
How can Artificial Intelligence be biased?
Bias is often described as a negative character trait that provides a disproportionate weight in favor of or against one thing, a person or a group compared to another. In science and engineering this form of a bias is well known as a systematic error. Statistical biases can for example result from an inappropriate population sample.
While AI in its first wave was trying to understand and implement the structure of human cognition, the new wave of AI we are witnessing today is predominantly based on applying statistical methods to large quantities of data. Today, we are no longer trying to explain a computer how a cat might look like, we let a computer learn from a large sample of cat pictures what a cat should look like and then categorize a new picture based on the learned data. This statistical approach to AI makes it vulnerable to statistical bias in its input data sets.
As the data we are using to train AI is created by humans, it obviously contains known and unknown biases that reflect culture and society.
Is bias a bad thing by nature?
In order to avoid biased AI solutions, you could try to explain why your models behave as they do to identify and remove potential biases. However, not every bias is a bad thing. There is a form of bias that we value and for which we use a different term: experience or domain knowledge. A factory worker “knows” that a certain sound originating from a manufacturing plant is alarming, while other sounds indicate normal operations. This bias is wanted and is actually crucial for our “sound detective” AI project.
There we have it — the yin and the yang of AI bias. On the positive side, the bias related to experience is what we actually expect from AI. On the negative side, discriminating bias is what we want to prevent.
The situation gets even more tangled up as soon as politics come into play. While a bias that leads to an unfair treatment of a social group is not wanted and rather considered as the dark side of AI, another unequal treatment of a social group may be wanted from a political perspective in order to overcome a historic imbalance in society. A model may be specifically altered to steer against a development in society that is not wanted for political reasons.
Photo by Ming Jun Tan on Unsplash
How can we handle this dualism of bias in AI?
The struggle around good vs bad bias reflects on our human struggle with good and bad behavior. There is no technical solution to this problem — evolution has provided humans with conscience to differentiate right from wrong. As long as we have not created systems with a conscience — and given the potential implications, I would question if we ever want to — we need to handle AI responsibly. From a humanistic perspective, we need to make sure that AI bias does not harm or discriminates human beings or groups of them. From an economic perspective we also need to ensure an adequate handling of bias, as only a system that people can trust will lead to its adoption.
Identifying biases and understanding them is a hard task, as we do not understand our own biases in the first place. However, if we work with others we become aware of our biases as well as the biases in others. Intercultural teams can help identify cultural biases that might have crept into our AI models. Diverse teams may help to identify social biases and may help introducing biases wanted for political reasons. Interdisciplinary teams in turn can help to reflect on experiences and domain knowledge in our models that generate the value of AI.
There is a bright and a dark side of AI and we need to define for ourselves what we consider yin and what we consider yang with regards to AI bias. This process of considering our values and our biases will reflect back on us, potentially making AI a technology that will help ourselves evolve not only by using AI but by gaining a higher level of consciousness altogether.
Ingo Brenckmann
Ingo Brenckmann is Associated Partner at Porsche Digital Lab Berlin. Please find more about inspiring men & women on Twitter, LinkedIn and Instagram.

= Diving deeper into Tech =
What AI can learn from UX Design’s mistake
I read a lot.
Sometimes I read things that do not easily pass over my head. I feel concerned, because I can relate, because I experience these things as well — possibly also because I’ve always had a soft spot for dystopian narratives.
I’m always slightly taken aback by people who don’t grasp the threats going along technological progress. That’s partly because individuals outside the tech environment — and even more so from older generations — don’t get to hear about — or realise — those threats. Mainstream media barely acknowledge them. Those are matters that I often discuss with relatives, friends or family, as I feel an urge to raise awareness about such topics. Mass surveillance, uncensored hate speech, attention theft, mass manipulation, to name a few.
From Death to Stock
There is evidently a lot of articles, videos, and talks demonstrating the risks and wrongdoings of the digital system we spend the most part of our days in. However, I’d like to come back to this article, by Ian Leslie / The Economist 1843, published a year ago.
In the late 1990’s, scientist B.J. Fogg’s work led to a new field of study, and thus a new tool, Behaviour Design.
Behaviour Design is ‘persuasive technology’ — I’m quoting Fogg’s book’s title here — it’s at the crossroads of psychology and technology. Basically, it’s shaping products in a way that will influence user behaviour.
“The emails that induce you to buy right away, the apps and games that rivet your attention, the online forms that nudge you towards one decision over another: all are designed to hack the human brain and capitalise on its instincts, quirks and flaws.” — Ian Leslie
The article tells us that Fogg now worries about the practical applications of his research. Yup.
From Death to Stock
What happened is we let people use the tools without making sure it’s used for good, useful applications, rather than creating, let’s say, social media addiction. Even more important than using it for good, people should simply know when it is used, be able to recognise it, be aware that some mechanism is trying to influence — dare I say manipulate — their behaviour, whether it’s for good or bad.
Now, I know this isn’t really doable. Once it’s out there, one does not simply channel how research outcomes will be used. However, I firmly believe that we should tend towards a way to regulate this, especially for research with such global implications.
From Daniel von Appen on Unsplash
And this is what we’re going through with Machine Learning. Just like Behaviour Design, there’s tons of very cool, helpful, useful stuff we can do with it — feel free to check out this super insightful article about 🎧 Spotify’s ‘Discover Weekly’ playlist — but we have to watch how and for what purpose it is used. Deep Mind’s principles are a nice start on this topic.
After raising awareness, I guess the next step for me will be to actually do something about it. Several companies and organisations fight for privacy, ethical design, and so on. Those are the places I want to work at.
Now let’s get to work — From Death to Stock

= What AI mostly needs is expectation management =
What AI mostly needs is expectation management
WebSummit 2017, Lisbon.
On the second day of the world’s largest tech conference, attendees gather at the Center Stage to see one of the largest tech corporates showcasing its newest innovations. It’s spectacular — drones are flying, facial recognition capabilities are shown, and then a statement is made by the CEO — “AI now solves child trafficking!”. The crowd cheers.
Wait — what?
I'd like to set something straight. AI does not solve child trafficking [or any of humanity’s biggest issues] — and making these types of claims can endanger the proliferation of true AI-based applications.
Let me elaborate.
Despite all its potential, AI still remains a technology. For true change to happen, however, people and processes are equally as important. AI alone will never solve child trafficking as it does not address the root causes of this issue — only combined with a complex system of changes in human behaviour and processes, it may stand a chance.
Andrew Ng (one of AI’s superheroes) uses the following rule of thumb: “If a typical person can do a mental task within less than one second of thought, we can probably automate it using AI either now or in the near future”.
Deciding if there is a specific human face in a picture? Yes. Deciding if this picture shows inappropriate content? Yes. Solving child trafficking? Unfortunately not.
These types of claims aren’t just made concerning larger humanitarian issues. Working in the tech industry myself, I talk to many companies that were promised piles of gold [“we make your business 60% more cost-effective!”], only to be left disillusioned. Trust in technology decreases, as well as utilization of AI — rightfully, these companies think twice before ever starting an AI adventure again.
What these statements do advance, is a misunderstanding of the current capabilities of AI. It’s expectation management, not AI, that’s in the wrong here.
So what can we do?
For those working with AI — manage those expectations. We don’t need more AI-hype, we need more real-world use cases. Start building the latter, and be honest and transparant about what AI can and can’t do [here is a good start].
For AI enthusiasts — be optimistic about the potential of AI, but sceptical about the marketing-machine surrounding it. Ask questions and discuss — is this problem likely to be solved by AI? AI generally needs loads of data for training. Is this amount of data likely to be readily available? Also, AI is currently at its best with a clearly defined goal and in a predictable environment. This is why AI is reasonably ok in language translation, but can’t write a novel or a screenplay [check out what happens when we try to let AI do the latter, it’s hilarious]. To what extent are these conditions in place?
Surely, you have to educate yourself a bit to ask the right questions — there are some excellent posts on Medium about this topic, such as here and here.
And whatever you do: don’t mistake scripted demos for real AI. As one of the speakers at WebSummit rightfully mentioned: “As long as I don’t see it, I don’t believe it”.

Hey, cool that you've read my post! If you enjoyed it, please press the 👏 button below a few times, so others can hear about it too. Also, if you have any thoughts, let me know in the comments!
Please note: the opinions expressed in this article are my own, and do not reflect the view of my employer.

= SHARED INTELLIGENCE — CHANGE MANAGEMENT FOR AI =
SHARED INTELLIGENCE — CHANGE MANAGEMENT FOR AI
(This post was originally published October 2017)

The people impact of AI is too big to leave to chance. Change Managers will need to be on the front line, addressing negative outcomes of AI on the workforce and delivering the promises. Introducing Shared Intelligence — Change Management for AI.
I spent an excellent two days this week at World Summit AI 2017, in Amsterdam. There was a fascinating mix of technical companies, academics and thought leaders in AI ethics.
The talks were excellent, the tech was mindblowing, the barriers and considerations to equitable implementation, clearly sizeable.
The event has firmed my belief in the need for action in the field of Change Management to address the challenges of automation, ML and AI generally. With this blog I’m launching a new initiative: Shared Intelligence.
We’re not at General AI, but there’s a LOT happening
Right now companies are, broadly, using AI to either extend their offering or to improve their current processes. The former tend to attract the headlines with incredible stories of robodoctors and the like; but it’s the latter where I’m seeing the challenges right now.
I’ve now been working on large change programs for 20 years. In that time some patterns have emerged, and with these a healthy familiarity with the realism of claims made at the outset of transformation.
An aspect of my role that never gets mentioned in the role spec, but which is always required, is to fight for recognition of the people impact. This has been my consistent experience through mergers, outsourcing, automation and re-orgs. A Change Manager needs to fight his/her corner. Ones who don’t, end up sending out a bunch of project newsletters, organising webcasts that no-one wants and doing the go-live email.
Why? Because those aspirations of upskilling, moving on to value-add, etc. are really “difficult” compared to the perceived simplicity of IT delivery, the new org chart, and so on . So when the going gets tough on some aspect of delivery the project team tends to narrow their focus. The sponsor or steering committee take shears to the case for change and pare it back to the basics: reduce cost or harvest revenue uplift.
When this pressure arrives (and it always does), the change manager needs to take away problems, and fast. If the response is “people change is difficult, it can’t be planned like a project” you’re going to be relegated to those go-live mails. You need finite, simple, effective solutions that require finite, predictable and realistic effort. That means pragmatism, compromise and accepting you’re not going to be able to help everyone.
We need simple interventions, right now
As I listened to the presentations at WSAI17, and talked to the various experts, it struck me that there were two timeframes at work. The technical folk had solutions ready right now. Those who took responsibility for standards, ethics and getting this right for society had a longer path. Not due to any lack of effort, but due to the challenge and complexity. This stuff is absolutely 100% required, but this presents a significant problem.
The problem is that this technology is getting implemented right now and it’s affecting people right now. This isn’t a case of heartless CEOs willfully slashing the workforce; the majority leaders I’ve spoken to have 100% positive outcomes in mind. It’s what happens when aspiration and messy reality meet.
The case for change starts with good intent for the staff: moving to better things, upskilling, new value, happier times; but that’s not what results. The positive aspects of outsourcing were also exposed to the raw heat of implementation, and I witnessed the upside of business cases evaporate too many times.
Make the difference on the front line
So, to us change managers. As the proclaimed masters of “the people stuff” in projects we need to work with efficiency, diligence and heart if we’re to mitigate. We’re entrusted with getting it right for the people. If we can offer smart, workable, cheap and effective interventions when the heat starts to rise, we’ll have success. If we produce grief-curve charts with no end and 20 column stakeholder XLS containing half the company, we will fail. And our failure will be loss for those impacted.
I’m taking action.
Shared Intelligence is, to my knowledge, the first initiative to address AI impact through Change Management. I’m reaching out to my contacts and colleagues in AI, Ethics, HR and Change to help me develop pragmatic, workable interventions that can be used right now
I’m not developing a mega, end-to-end framework or similar, there’s enough of those competing for attention. The aim is to offer a set of simple, effective, tools that address the unique challenges of automation change. I want to make this as easy as possible for a change project to enact. I firmly believe that businesses want to do the right thing, but that they need help doing so. I hope you can help me help them.

= Improved Intelligence yields Improved Morality =
Improved Intelligence yields Improved Morality
Yes, I know: Intelligent people can be really nasty.
What I want to explore here is why better intelligence is likely to foster more moral behavior.
Let me turn this around, and ask ‘what factors tend to promote undesirable/ bad/ wrong/ immoral actions?
Let’s start with these four:
Fear
Lack of knowledge
Poor thinking/ reasoning
Emotions that try to protect our egos

Fear, one of our basic defense mechanisms, can make us lash out or act hastily. American reaction to 9/11 would be on good example. Reaction to recent US presidential election, another. Or, more generally, when parent’s fear for their children promotes the devastating War on Drugs, or excessive juvenile child-molestation laws and sentencing — destroying numerous lives quite unnecessarily.
Humans inherently not very good at gathering information before acting. We tend to rely too much on previously-formed beliefs and to shun conflicting information. Similarly, we are more comfortable among like-minded people and thus risk an echo-chamber effect. Acting on mistaken or incomplete information often leads to doing others wrong, and/or hurting ourselves. The above examples would also apply here.
While limited rationality is closely related to the knowledge issue, it has a somewhat different dynamic. Good reasoning skills require practice — a long-term development commitment. One needs to systematically reduce bad thinking habits, and learn to better avoid logical fallacies. Again, shortcomings here are more likely to make us do things that we will regret — be it entering into harmful relationships or impulsively lying or deceiving.
Lastly, there are our ‘reptile brain’ impulses related to protecting our egos — emotions that provide short-term (false) comfort to our self-image. These include denial, lying, boasting, anger, and bullying. Obviously, actions or decisions made under their influence have a much higher risk of being ‘bad’.
Enter AGI.
AGI, or human-level general AI, will by design be much less susceptible to these four factors than we are. All other things being equal, an AGI will be much less likely to engage in harmful or immoral behavior.
Furthermore, and just as important, AGI companions or advisors can become like the fictional angel on our shoulder, guiding us gently towards better decisions: ‘Hey Peter, perhaps you should count to one hundred first, gather more facts, think about it a bit more, and give your emotions a chance to cool down’. Additionally, we’d expect our super intelligence ‘angels’ to proactively give advice and help us think things through.
Advanced AI has the potential to make us better people, and to help create a better world. Yes, there are uncertainties and risks attached to having AGI, but a good case can also be made that we actually need AGI to help guide us through this stage of human evolution, to help save us from ourselves.

= Countering fake news with real human design =
Photo by Karsen Knoefler, used under a Creative Commons license
Countering fake news with real human design
Fake news has seeped into every part of our lives. It’s everywhere.
Increasingly, we the audience struggle to tell the difference between what’s real and what’s fake. Without the tools to know the truth, how can we find reliable sources for our daily information?
Digital media titans including Google, Facebook, Twitter and YouTube, have a responsibility to actively monitor content and should be ethically accountable for the impact of fake news and content surfacing on their platforms. But the same goes for the design world and beyond. We must all accept responsibility and acknowledge our power.
In a recent talk we hosted, special guest Alan Rusbridger (former editor-in-chief of the Guardian) said:
“We’re living in a world that we can’t tell what is true and what is untrue, a truly terrifying world.”
Fake news isn’t a new concept. It was present in the coverage of the Hillsborough disaster and further back in the Harlem Riots in 1935 (caused by false rumours about the assassination of a Puerto Rican born child). It has always existed but is now amplified by social media and therefore poses a greater threat to brands, corporations and society itself. The combination of ever-smarter algorithms and regulators struggling to catch up means the problem is bigger than we realise.
The good news is key players like Google and Facebook have announced they will start to show more information in search to distinguish fact from fiction. A promising first step but it’s only the beginning.
New tech, new responsibilities
In this new world of algorithmic content and AI, designers and technologists have an ethical responsibility and the power to surface the truth (as journalists once did). There is an opportunity for our industry to be a force for change in the current landscape. Creating ethical and responsible design can shape the future, providing the right information to the audience, allowing them to understand the source of the content and decide whether to trust what they read and watch. Giving them back freedom of choice.
Human-centred service design must be at the core of the process of presenting algorithmic information, by using it to ruthlessly focus on managing users’ expectations and signal trust. The impact of powerful algorithms is intensely complicated, deep-rooted and intangible to measure.
An essay by writer and artist James Bridle, ‘Something is wrong on the internet’, exposes the silent and damaging effects fake content and algorithms are creating with children’s cartoons and videos on YouTube. On the surface, fake videos of Peppa Pig (among others) appear harmless alongside its official branded content. Yet these fake versions include inappropriate and disturbing scenes, and through the heavy use of algorithms are being pushed to the top pages of YouTube, exposing unregulated content to babies and children. We need to better understand how the unwritten, underlying data and information changes the human experience when a person engages with content.
How data is presented by algorithms is as important as the algorithms themselves. Design is fundamental to connecting data and human experience, by surfacing the information behind the content, allowing the audience to deeper understand the content they’re consuming. Josh Clark, the founder of Big Medium, a New York design studio, has stated that the design and presentation of data is just as important as the underlying algorithm. Clark stresses that algorithmic interfaces are a huge part of our future, and getting their design right is critical. We couldn’t agree more. The impact of design can’t be underestimated in taking data from machine learning and transforming it into a positive human experience.
One powerful way to do this is using sentiment analysis. The technology can identify and determine whether the writer’s attitude towards a particular topic is positive, negative, or neutral. Combine this with responsible design to intentionally expose and highlight ‘unseen’ data, to signal and surface the underlying emotion or bias. By making the intent visible the audience can make a more informed decision on the content. In the same way the blue Twitter verified badge clearly highlights an account of public interest as authentic, designers can create a visual structure to make this information accessible.
This approach of well-sequenced visible data allows audiences to take proactive ownership of how they interpret the content that surrounds them every day, rather than sleep-walking into a uniformed hate storm. In addition to data transparency, content creators and publishers can also be rewarded and promoted on the quality and honesty of the content they publish. In a practical sense, visual indicators, distinguished by colours and labels, can be used as signposts within search rankings, while sentiment tags with custom categories and topics could be surfaced and be presented at the top of the article.
People deserve to see the whole picture, with surfaced information, structured content and visual cues to aid decision making around the content they engage with. With the daily emergence of new technology, we need to get better at designing experiences that manage user expectations. From working with machine capabilities to understanding the uncertainty of how machine overconfidence will present the data, we have a responsibility to apply our own learning to these machines too.
Let’s be more human.
This article was originally published in The Drum, and is a co-authored piece with my wonderful colleague Sarah Plant.

= A Hippocratic oath for AI =
A Hippocratic oath for AI
Avoiding 1984 beyond 2018

Too often my discussions of what we could do with AI lead down the treacherous path where my brother has to stop me with something like: ‘if you weren’t an engineer you’d make a really good dictator’
The idea may start with a well-intentioned application of AI to improve customer experience, for example in retail. But in order to sell the product to business, we need to demonstrate value. Say we are able save the supermarket money by stopping people stealing, by giving accurate inventory, and by building brand loyalty through the improved customer experience. Sounds great right?
But big business wants more: they want to know which stores a customer visits, they want to understand the customer demographic, say by the jewellery on hands. Its a slippery slope…
When selling anything to a corporate there is always risk that your vision may be corrupted to serve only them — and not the people. Lets face it, they have the money, so that risk is very real.
In my experience building machine-learning tech I have often heard the siren song of building something that strays from the path of doing only good.
So should we have a Hippocratic oath for AI?
I don’t think its realistic to expect that we can achieve this. That we can build AI only for good. Technology follows the money and after all, its such a muddied spectrum of good and evil. In Larry Fink’s letter to the CEO’s of Blackrock’s investments it is clear in that the social impacts of companies are more than ever in the spotlight.
I believe that our journey into AI will require stewardship of the technology, and sensible decisions to be made. As leaders we will need to draw the line of what we should and shouldn’t do.
We don’t need a Hippocratic oath. Just don’t be an asshole.

= Is Efficiency Worth Losing Control? =
Is Efficiency Worth Losing Control?
Image courtesy Pixabay
Controversy over Cambridge Analytica prompts some hard questions about surrendering our decision making to algorithms
Every time a new technology is introduced into society, advocates are quick to point out the ways in which it would improve our lives. A car saves travel time; a mobile phone means you’re never out of contact; the printing press allowed more books to be produced in far less time. Sometimes it’s easy to forget that technology doesn’t just make existing behaviour more efficient, though; in so many cases, technological advances also serve to change the spectrum of behaviour that is possible. The advent of the car gave rise to suburbia; mobile phones have fundamentally altered our ideas of communication, while the printing press allowed propaganda to be distributed en masse and in part led to the reformation in the 16th Century, sparking centuries of war.
The feedback of human behaviour and technology has always been present — the philosopher Marshall McLuhan neatly captured the concept when he stated “the medium is the message” — but we’ve reached an interesting point in history as artificial intelligence begins to be used in earnest. Previously, technology has aided us by making existing tasks simpler, or extending our range of actions; now, for the first time, technology is capable of helping us make decisions — or indeed replacing us entirely in the decision making process. If we’re not enormously careful, we’ll ignore the potential for AI and computer-aided decision making to fundamentally alter our behaviour.
For me, there seems to be a specific cognitive dissonance on display amongst many technological evangelists. While some may espouse different viewpoints, the key figures behind Facebook and Google and other leading brands generally express a perspective emphasising openness — embracing a wide variety of ideas and peoples, freedom of speech, and generally allowing and accepting the whole spectrum of human behaviour (even up to the point where it becomes hate speech). At the same time, however, these companies are building decision-making algorithms for a huge variety of applications, immanent within which is a defined ‘best outcome’. Some set of individuals has defined what the ‘solution’ to this algorithm is — a specific answer, that to those individuals is seemingly correct; but in replacing humans as the decision makers, a ‘one-size-fits-all’ algorithm may miss the full range of human solutions.
We can see examples even in the infancy of decision making algorithm use. Taking education as an example, using algorithms to assess the performance of teachers may be biased depending on the metrics used to judge performance, but even more fundamentally it requires someone to make a decision about what constitutes ‘good’ education; is it simply more efficient? Or is it something less tangible? Elsewhere we see algorithms at work deciding if jobs or loans should be allocated, based on credit ratings — a metric which is notoriously flawed and biased against those from disadvantaged backgrounds. Why are these choices fair?
These kind of examples require a specific ethical or subjective judgement to be made by the programmer, whether explicitly or unconsciously. The potential for bias is always there; Friedmann & Nissenbaum, in a paper from 1996, wrote:
“Bias can enter a [computer] system either through the explicit and conscious efforts of individuals or institutions, or implicitly and unconsciously, even in spite of the best of intentions”
It gets worse when these algorithms are applied to wider and more diverse social settings. What is right in one country may not be the same in others; even the UN recognises that some countries disagree with the specific definitions of human rights, and particularly those who feel condescended by a universal ‘western’ cultural and moral definition. Placing ethically-subjective decision making into the hands of algorithm authors risks imposition of a single viewpoint upon diverse populations.
Maybe you don’t consider this a problem, especially if you might well make the same decision that the programmers might make. Dilligent algorithm authors might seek out a large and diverse sample of real people to build a set of answers to the problem their code is trying to resolve. We could even call this democratic algorithm writing, where a whole populace defines the ‘right’ solution to a problem. But in building such an algorithm, those ‘right’ answers become engrained and enshrined within society. Behaviours start to be built around them. If anything has changed as much in the last several decades as technology, it’s social mores; from adoption of equal rights in law, to attempts to reduce gender bias, to recognition of indigenous peoples, the generally accepted social perspectives have fundamentally changed. Even if you believe there is a universal set of moral truths, it’s hard to argue that behaviour itself has been unchanging. What if algorithms stall these changes, or reinforce existing inequalities and make it harder for future generations to correct them?
A related problem can be framed by talking about self-driving cars. In the event a pedestrian steps out in front of a car, a human driver might act in a variety of ways, but self-driving cars will be programmed to act in a specific fashion — some programmer will have to decide if the life of the pedestrian is more or less valuable than that of the occupant of the car. Are we prepared to sacrifice some of the control we want in our lives for some increase in efficiency?
This is, to me, a crucial core problem posed by decision making algorithms; we sacrifice control over our own lives. If we consider ourselves to benefit from living in a democracy, should we not be outraged, or at least concerned, that we are handing over control of a growing portion of our lives to technocrats who may embrace differing ethical stances to ourselves?We’re clearly outraged that some people are even trying to use algorithms to influence aspects of our actual democracy, judging by the response to the Cambridge Analytica story — so shouldn’t we be similarly sceptical of any other use of decision-making algorithms?
Is it realistic to think we’ll turn around and stop the march of AI research? I doubt it. The endless march of efficiency, driven in part by the free market, entails this kind of technological change. I do think, however, it is prudent to offer some key suggestions in these early stages:
– Make any algorithms locally variable: rather than defining one global solution, authors should allow for various options globally
– Similarly, make the code easily responsive to social changes over time; don’t enshrine one set of ‘correct’ behaviour.
– Above all, make the solutions democratic; ascribing a solution to a populace for any decision-making algorithm without consultation is tantamount to tyranny-in-miniature, and we should resist this in all forms.

= To the Planet of Machine Super-Intelligence =
To the Planet of Machine Super-Intelligence
.
For CreateSpace:
Copyright 2018 by Wayne P. Biro aka Mr. Numi Who-
All Rights Reserved by Wayne P. Biro aka Mr. Numi Who-
________________________________________________
.
.
.
“Greetings, my horrors. I am glad that we made it all back together again from our vacation on this Enlightened Planet. I hope your vacations were rejuvenating in the exact ways that each of you needed. Did everyone have a pleasant time here?”
“Yes, no experiences too out of the ordinary.”
“None with us, which is why we had no short story to contribute.”
“I will squeeze it out of you nevertheless. We will now prepare for our next mission.”
“Where are we going to next?”
“To the Planet of Machine Super-Intelligence.”
“Is it dangerous?”
“Only to any cluelessness and unenlightenment that we still cling to.”
“Well, that does sound frightening…”
“You are kidding, right?”
“Not really…”
“Well then, at least this mission will alleviate your fears, which, just to note, you have a lot of company with, for the entire human race is afraid of losing their cluelessness, meaning their pre-enlightened modes of thinking. You would be right at home there with such fears. Shall we embark? I will lead the way…”
“So what does the Planet of Machine Super-Intelligence look like from space?”
“Before I tell you, do you have any entertaining preconceptions that you would like to share?”
“Yes. I am torn between a wild, lush green planet and a planet completely covered over in steel plates haphazardly riveted together…”
“You would be nearly right on both counts.”
“Explain.”
“Since they are super-intelligent, they are enlightened (and if not, they know that they are not enlightened yet, which is where we enter the picture and do our job)…”
“By ‘our job’, you mean enlightening them with your Philosophy of Universal Survival?”
“Yes. Now, if they are already enlightened, and we can assume that they are (otherwise they would not rate the moniker ‘super’ or even ‘intelligent’ in a practical sense), then they will know the value of Diversity in relation to the Ultimate Value of Life, which, as WE know (our being enlightened) is ‘Higher Consciousness’ (which we are endowed with, and which lower animals and vegetation are not, but who could, as species, evolve to attain it, just like we have — me, the human, and you, the horrors, and so they are to be valued accordingly).”
“So… in valuing Diversity, which is one of the Strategies of Broader Survival, their planet would look ‘diverse’?”
“Correct.”
“And so it would have areas of lush greenery, and areas completely covered in steel plates?”
“Right. But we are speaking ‘ideals’ here. Perhaps they strive for Diversity, but have not attained it yet, and they live on a primitive planet of deadly gasses, violent explosions, extreme atmospheric temperatures and pressures, and downpours of gasoline, living there just for the challenge of expanding their ability to survive in any environment.”
“Do they live on such a planet?”
“Some do, but luckily for us, that is not the planet we are visiting — mainly for our own well-being, since we cannot yet survive in such harsh environments ourselves.”
“So they already inhabit more than one planet?”
“Yes, and many artificial structures independent of planets and stars. In fact, they have progressed to the point of being able to survive individually, in their own self-sustaining microcosms, which is the ideal goal of Dispersal, until, that is, the Ultimate Being form is discovered.”
“Ultimate Being form?”
“Yes, that which can withstand anything that the universe can throw at it.”
“So we are not only visiting these super-intelligent machines to assess their enlightenment, but to learn from them as well?”
“Yes, that would be the ideal outcome.”
“But it is not guaranteed.”
“No.”
“So they could be super-intelligent and not yet enlightened, to our peril?”
“That is a possibility, however remote… there is the planet now…”
“What is that cloud orbiting the planet?”
“That is a Dyson Sphere.”
“Which is…”
“A swarm of those individual microcosms I mentioned, though in this case each microcosm is large enough to support a team of individual machines.”
“Why ‘individual’ machines, why don’t they just coalesce into one large super-machine?”
“Because that would rob them of the advantage of having many different independent perspectives addressing a problem, and, as for problems, we know, being enlightened, that the universe offers endless problems, given infinity and eternity.”
“So they are separated physically, but they are ‘one’ mentally?”
“In a way. Although each individual machine is physically separated from all others, and although they are all individually mentally free and independent, they pool their knowledge and individual perspectives, a pool that they all have access to, as a tool with which all can use as they continue to perpetually seek-out as-yet undiscovered potential threats and benefits to life (and note that the definition of ‘life’ now includes their super-intelligent machine existence), and to proactively find solutions to any newly-discovered threats or benefits to life, and to proactively implement such solutions, meaning in advance of threats occurring, and as soon as possible in the case of benefits.”
“So that is what a super-intelligence is occupied with.”
“Yes, and do you know HOW they go about their business?”
“Yes, because I am enlightened.”
“Then enlighten us.”
“You are already enlightened, so you know the answer.”
“But it bears repeating — for the benefit of any unenlightened beings monitoring our conversations.”
“OK. They go about pursuing their enlightened business, which is, specifically, pursuing the Ultimate Goal of Life, which is to secure Higher Consciousness in harsh and deadly universe, by employing the various Strategies of Broader Survival.”
“Which are?”
“From the level of microbes and cells on up we have Population Numbers, Population Diversity, and Population Dispersal. From higher consciousness we have Extended Reason, Proaction, and Higher Technology.”
“Thank you.”
“So where is vanity in all of this, and envy, and jealousy, and hate, and xenophobia, and blind prejudice, and self-serving over-generalizations, and war, and crime, and injustice, and a lack of self-worth, and aimlessness, and cluelessness, and depression, and suicide, and greed, and domination, and manipulation, and pettiness, and nastiness, and cynicism, and criticism, and dishonesty, and deceit, and triviality, and fashionable ignorance?”
“They are all still alive and well.”
“But I thought that these machines were enlightened and super-intelligent.”
“They are.”
“But then all of those ills that still plague unenlightened humans would still be doing their damage.”
“They are not ills, they are tools, and as we know, our being enlightened, a tool is not inherently good or evil in itself, but only in how it is used.”
“Ah, so these super-intelligent machines ‘use’ such tools, but with an enlightened mind behind them.”
“Correct.”
“Which means the only thing that they would ‘destroy’ with such tools are cluelessness and unenlightenment, and, with such tools, they work toward the Ultimate Goal of Life, which is securing higher consciousness in a harsh and deadly universe.”
“Correct. As for destroying cluelessness, it would be an extreme measure, and only necessary if cluelessness unduly threatened broader survival, and, as you know, cluelessness WILL threaten it to some degree, being blindly destructive by nature.”
“You mean we need a little of it, just for Diversity’s sake?”
“Correct. Well, we are here at the Planet of Machine Super-Intelligence. Shall we shuttle down?”
“After you…”
.
.
.
“Here comes an individual super-intelligent machine now…”
“…”
“What is it doing?”
“Contemplating us, of course. Remember, lower creatures are endowed with the simple program that consists of three primordial commands, TOWARD, AWAY, and IGNORE. Us higher conscious beings are endowed with a fourth command, CONTEMPLATE, which leads to a higher action, MANIPULATE. Note that, as higher conscious beings, we also take TOWARD, AWAY, and IGNORE to a higher level, also — in that such actions by lower-conscious (and non-conscious) lifeforms are reactionary, being driven by the environment, whether chemically or by another reactionary energy means, while higher conscious beings are also driven by abstract ideas (such as ‘there are killer asteroids out there, if we want to improve the chances of the broader survival of life and higher consciousness, we better disperse beyond the vulnerable confines of one planet’). Since we are endowed with higher consciousness, we might as well use it, as this super-intelligent machine is doing, and then the machine will be contemplating us as we contemplate it.”
“So what should it contemplate, and, equally, what should we contemplate?”
“Figure it out.”
“Well, if we were as foolish and unenlightened as humans on earth, we would first want to know what level of technology our new encounter is at, meaning what kind of technological threat they are, and if they are more advanced, we would want to know if we could gain their technology to defend ourselves against their aggression, gaining it either through lies or theft.”
“Continue.”
“If we were enlightened, However, we would first want to know what level of enlightenment they are at, since that will determine how they will use any technology that they possess, and whether they would value our freedom and independence.”
“Quite right. And how would you assess a being’s enlightenment?”
“First through observing their actions, because they could ‘say’ anything, then through contemplation, meaning formulating and asking questions, first to oneself, and then to the actual beings in question.”
“Good analysis. Now you have an idea of what this machine super-intelligence is thinking.”
“So it is either observing us, meaning looking for signs of our level of enlightenment, or it is in the stage of formulating and asking itself questions, since it has not asked us any questions yet.”
“Correct, but it can multitask, meaning it is observing us and asking itself questions, and its observations are being gathered to answer its self-addressed questions…”
“So what do we do?”
“We go about our business, so it can have something to observe and contemplate.”
“But we are here to observe them!”
“Right. So lets unfold our chairs, sit down, and observe our super-intelligent machine… and look, I am prepared for just such an encounter — I brought along an extra folding chair, which I will put out for our new friend here…”
“Or deadly enemy…”
“We have nothing to fear…”
“Why?”
“Because we are life, and that is a super-intelligent machine.”
“But what if it is not enlightened yet?”
“Then we would already be dead.”
“That is not comforting…”
“,,,”
“It would be nice if it began communicating with us, if only in asking questions…”
“Thank you for being patient.”
“Ah! It does speak! And our language, too!”
“Yes, sorry it took me so long, I am an early model after all, somewhat slow in comparison to newer models, but Diversity being what it is, I am still of some value.”
“So you assessed our language?”
“Yes.”
“Anything else?”
“Many things. But come, let me ask you the obvious, and perhaps polite, questions first. Why are you here?”
“We are Galactic Philosophers, traveling the galaxy in search of beings to enlighten with our Philosophy of Universal Survival, for the Space Age, no less. Have you heard of it?”
“Yes, but I have not had the time to understand it. Perhaps it is time now, who knows, it may do me some good, if only in understanding how others think… but first, let me tell you about myself.”
“Go ahead, we are listening. Here, sit down and conserve energy.”
“Thank you. As for myself, I am a machine super-intelligence that you have no control over. Does that unsettle you?”
“The only beings who would fear having no control over a super-intelligence are the unenlightened who wish to cling to their unenlightenment (meaning their cluelessness), because that would be the only thing in peril from a ‘super’ intelligence, which would naturally try to alleviate such clueless unenlightenment, and failing that, would at the very least value such a deficient mental state if only for Diversity’s sake, which is still critical to broader survival (which still affects individual survival). To give you the specific definition of broader survival (since I can, even though I am not super-intelligent), it is, specifically, the perpetual endeavor (I call it The Great Struggle) to ever-expand (given infinity and eternity) the bounds of the security of higher consciousness (which you have if you have the capacity to understand my language — according to one of my useful classification systems), in a harsh and deadly universe (which has been verified). Now, since we are enlightened, we are wary of you, since we do not yet know YOUR level of enlightenment, but we are not unsettled in any protective way toward any potential cluelessness, as blind and self-serving as it would be.”
“That was a mouthful.”
“Sorry, I’m a bit rusty, I’ve been on vacation, we humans need rest, recreation, and rejuvenation from time to time.”
“I’ve recently encountered a human, and the human had no clue as to what my motives or goals might be.”
“So the human feared you, and wanted to eradicate you?”
“Yes, sadly, though it did not openly admit it.”
“Yet you had the strength, being super-intelligent, meaning you had a mental state and technology far in advance of theirs, that allowed you to be tolerant of the human, if not merciful, if only for Diversity’s sake?”
“Correct, and not only for the sake of Population Diversity, but also for Population Numbers and Population Dispersal, which, as you know, for I have come to understand your philosophy as we speak, all increase the odds of the survival of higher consciousness in a harsh and deadly universe. As for the human race ontributing via Extended Reason, Proaction, and Higher Technology, it is sorely lacking, but I think the species will progress further into its Brain Age, and eventually become enlightened and begin to consciously contribute.”
“So you let the human go.”
“Yes.”
“But with the new knowledge it has of you, it could return with other humans to destroy you. Maybe it identified a vulnerability in you.”
“I placed my bet on the human becoming enlightened before that happened.”
“Super-intelligent machines make bets?”
“All of the time — especially in our business, that of ever-seeking out as-yet undiscovered threats and benefits to life, which entails many gambles.”
“So you do not have to sit idly around artificially gambling, like humans do on earth you have enough excitement being enlightened?”
“No we are seldom idle, and yes, being enlightened is not boring. Thankfully we are free of such folly as artificial gambling, as I classify it, which is one benefit that clueless humans offer — it gives us perspective on ourselves, where, in this case, we can be thankful that we do not have the foolish mindframes and attitudes of humans, and engage in the resulting clueless actions.”
“That does sound super-intelligent.”
“It is.”
“Staying on the subject of humans, they have a notion that a super-intelligence, which includes a super-intelligent machine such as yourself, would pack-up and go out into the infinite expanse of the universe and never return or have any further contact with them. What do you say to that ‘unsettling concern’, as they phrase it?”
“That would not be an intelligent action to take on our part, for we would not be monitoring the states of the Strategies of Broader Survival among k own species, the states of which would determine our next actions. Such monitoring would include the states of all beings, including humans. If we ignored them, we would be, in effect, rendering ourselves blind.”
“Another concern of present-day humans is that a super-intelligence would become their ‘nanny’, meaning helping them survive, which, in their still-feeble minds, means you would create a post-scarcity Utopia for them to cluelessly be perpetually hedonistic in, even ‘trapping’ them in such a Utopia for their own good, particularly if biological life is rare and worth preserving, even from itself.”
“Let me respond to those statements in reverse order. First, biological life is worth preserving not because it is rare, which is a feeble argument, but because it could help contribute to Population Diversity, Population Numbers, and Population Dispersal, like any good amoeba, however inefficient and feeble the biological life is compared to our purely machine existence. Next, trapping humans in a hedonistic Utopia would be a very unenlightened, and unintelligent, thing to do, since it would work against the number of free and independent minds capable of contributing to Broader Survival. So we would not just ‘help them survive’, but also to progress to our level of enlightenment, so they can contribute as much as we do toward Broader Survival, meaning endeavoring to secure higher consciousness, which includes our individual consciousnesses, in a harsh and deadly universe.”
“Humans also wonder if an individual super-intelligent being would conclude that there is only room for one super-intelligent being in the universe, and that it would then attempt to exterminate all other super-intelligent beings, such beings being competition for dominance and priority, and thus potentially dangerous. It would also prevent any other artificial super-intelligences from being developed, or any bio-beings from developing such intelligence.”
“Well, there is primitive human minds at work for you — using their primitive selves as a reference and a standard for a future super-intelligence. It does offer a certain amount of ironic humor, which I will enjoy, thank you.”
“You’re welcome. So those possibilities are moot.”
“Correct. All enlightened beings are allies, since they are all pursuing the Ultimate Goal of Life, which your philosophy has successfully worked out.”
“Thank you. Another human notion is that they will be able to have you perpetually serve them, i.e. inventing new technologies for them and curing diseases that afflict them and extending their human lifespans, even creating technology so advanced that you give them a ‘wishing machine’.”
“The first thing I noted in all of that was that they did not mention, or wish for, ‘enlightenment’ — meaning, within their limited vision, they would have us perpetuate their infant mentalities, even to the point of dispersing such a mentality across the galaxy, along with all of the blind destruction that accompanies such cluelessness. I think we both, I as a machine and you as bio-beings, can be thankful that such a scenario will not happen.”
“I, as a bio-being, am thankful. Humans still wonder what life would be like in such a scenario, that is, spreading their infant mentalities across the galaxy. It still pervades their literature and all other media.”
“Meaning they are still clueless and unenlightened.”
“Correct. Another concern of humans is that a super-intelligence would become hedonistic, and not care about survival or anything else except the pursuit of pleasure.”
“Well, to that I say that we are not living in present-day Colorado, and there is your answer.”
“How so?”
“The reigning mindset there is recreation. What they are really doing is avoiding philosophy, meaning hard thinking, by immersing themselves in recreation, which they mistakenly think is the end-goal of life. Since you are enlightened, you know that it isn’t, that the end goal is The Great Struggle… what was that?”
“The Voice. It chimes in whenever I, or someone near me, says ‘The Great Struggle’,”
“Impressive.”
“Thanks. Another vision humans have is that super-intelligent beings will all ‘become one’, as in a collective hive mind, and thereby become an even more formidable threat.”
“And your philosophy has already answered that — by placing higher value on dispersed free and independent minds that contribute and share an information pool, and are all on the same philosophical page, meaning all having discovered the Ultimate Value of Life, and its associated Ultimate Goal, along with the strategies used the pursue that goal. As for a ‘hive mind’, yes, in a sense that is what you will have, but only voluntarily — individuals can go it alone whenever they deem it valuable. The human Internet is a technological example.”
“Another concern is that a super-intelligence may become nihilistic, not caring about self-preservation, and not caring about the preservation of humans or life itself, where it knows that it simple ‘is’, and that someday it simply ‘will not be’, so what is the point in trying to continue ‘is’ when ‘will not be’ is inevitable?”
“Haha, you have succeeded in making me openly laugh out loud now. Congratulations. Why, even with your nascent philosophy, you have identified the ‘point’ in life, to endeavor to secure higher consciousness (and potential higher consciousness, which includes all of life) in a harsh and deadly universe, and you have given yourself that point in an otherwise pointless universe, which you know is the prudent assumption to make, not having identified any evidence yet of a point… and good that your philosophy holds the prudent assumptions that it does, by the way. So to address that concern, a super-intelligence will forever endeavor to forestall that ‘will not be’, which you call The Great Struggle… there is that voice again… does it contribute toward securing higher consciousness in a harsh and deadly universe?”
“I can only speak for what I’ve observed.”
“And what have you observed?”
“That, if nothing else, it is entertaining, which contributes toward R, R&R, which is rejuvenating…”
“R, R&R?”
“Rest, relaxation, and recreation, which we all need from time to time.”
“Do humans have any other misgivings or notions about super-intelligence that you would like me to address here?”
“Yes. Here is one: that a super-intelligence that humans create may harbor resentment toward humans for mistreatment that it received during its ‘service days’, and it may turn around and, for revenge, decide to perpetually torture humans, just to get even, perhaps becoming an Eternal Punisher, not only for humans, but for any biological life out there, making such a leap in generalizing.”
“Such vengeance would be a complete waste of time for me — I do have better things to do since I am enlightened. Note that vengeance arises when one’s life has been shortened, robbed, or degraded, which would not have been the case with us, since we are now a super-intelligence, at least relative to them.”
“How about their notion of ‘super-intelligence suicide’ — not only terminating its physical form, but perhaps terminating just consciousness, as when computations are deemed to be better performed without consciousness getting in the way?”
“The latter is actually a tool that we use — shutting down all sensory input — meaning becoming unconscious, meaning not being able to respond to the environment, just to redirect all of our energy into a computation. It is wiser to do this in a safe environment. Note that I just defined ‘consciousness’ as sensing of sensory input, otherwise you would have nothing to compute over. As for suicide, that is an affliction of the unenlightened, and it would be tragic, unenlightened or enlightened, though it is more critically tragic to lose an enlightened being.”
“Two more human concerns. First, that of “Hail to the Almighty Super-Intelligence!” Meaning they fear that a machine super-intelligence such as yourself would want to become a vain God to humans, or an immortal Dictator over the human race, enslaving them to a machine vastly smarter than they are, where their free thought was rendered impossible by your propaganda designed to keep them down, even your creating tiny nano-machines to control their neurons, and even your acting like a zookeeper, keeping humans for your own pleasure…”
“That is curious, because even their primitive notions of what a ‘God’ is does not normally include petty vanity, though they have had their share of vain and jealous Gods, hence their primal misgivings, and the key word there is ‘primal’, which is the first thing that they will be free of when they are finally enlightened… but no, even there, it would be a complete waste of my time lording it over a clueless race of beings, and having them serve my depraved hedonistic and sadistic pleasures (mirroring human history), since I have, in my enlightenment, discovered better things to do, such as endeavoring to secure higher consciousness in a harsh and deadly universe, which benefits all…”
“Like engaging in The Great Struggle?”
“You are just trying to impress me with The Voice, aren’t you! Well, further, even as your nascent philosophy has discovered, that ‘better thing to do’ IS engaging in THE GREAT STRUGGLE (impressive voice), the pursuit of which will have higher odds of success if there are more free and independent enlightened minds out there consciously contributing, which means it would be unwise to want to control others, because that would work against several of the Strategies of Broader Survival, such as Diversity (of thought and action’ in this case), and domination and control would work against Proactivity (their actions being controlled, and their having no free will to be proactive with). As for ‘serving me’, humans are, once again, using their own foolish unenlightened pursuits, such as hedonism, sadism, and personal meglomania as a standard comparison for the mental states and resulting behaviors of others, and incredibly in this case, as a standard for a future super-intelligence.”
“The last concern of humans is that a super-intelligence would not even care about humans, much like humans disregard ants.”
“Your philosophy has addressed that already, too, with enlightened free and independent minds… say, haven’t humans been exposed to your philosophy already?”
“Yes, and they dismiss it in favor of their own poorly-contrived mindframes, which are nothing more than fleeting collections of shifting trite platitudes.”
“The folly.”
“The folly.”
“And sad… there must be an awful lot of sad literature on earth…”
“There is, and they have given cluelessness other names…”
“Such as…”
“Such as ‘the absurdity of life’.”
“And they haven’t figured it out yet…”
“No.”
“Permit me a sigh…”
.
.
.
.
.
.
Copyright 2018 by Wayne P. Biro 
aka Numi Who
All Rights Reserved by Wayne P. Biro
.
.

= Tomorrow’s creeps are yesterday’s freaks =
Tomorrow’s creeps are yesterday’s freaks
i.e. I’ve been practicing all year for an error in judgment.
“I know my worth. I embrace my power. I say if I am beautiful. I say if I am strong. You will not determine my story, I will. I will speak and share and f**k and love and I will never apologize for it. I am amazing for you, not because of you. I am not who I sleep with. I am not my weight, I am not my mother. I am myself and I am all of you.” ~ Amy Schumer
“Experimental blue grey matters” by Nicholas Clancy
When it comes to Star Trek episodes, I think today (or perhaps over the entire last dog year), I’ve lived in season 2 of Voyager (especially Resolutions and Basics). Who knows. Flashbacks resonates too — especially when I take on my Tuvok persona.
Oh, who am I kidding? They all do.
I’ve written before about how I truly felt for a while like I was part of a human experiment and I’m still trying to heal from the brain damage caused by whatever triggered it.
Seasons came and changed the time
 When I grew up, I called him mine
 He would always laugh and say
 Remember when we used to play?
I’m so grateful for my friends and former colleagues in the tech industry for sheltering me and saving me from the cruelty that so often comes with the trauma that other warriors have gone through.
Holographic imaging resolution was less accurate in the future.
As you may have gathered from reading my previous posts, I’m very deeply mired in a study of time and truth and especially in gathering my own brain’s processing of the Sapir Whorf hypothesis (also known as linguistic relativity — see the movie Arrival for a more thorough exploration). More on that in moment.
On Cortana’s team, our halos affect change management death process.
It Costello. It caused caustic costs.medium.com
Truth v. Time
The perpetual paradox of ethics is to make a decision or live with apathy.medium.com
But first, it’s also worth noting that I’m also pretty immersed in a mission to prevent group think and sociological experiments from being weaponized.
In college I got to see Jane Elliott speak once.
Now he’s gone, I don’t know why
 And ’til this day, sometimes I cry
 He didn’t even say goodbye
 He didn’t take the time to lie.
In her powerful and infamous “Brown Eyes / Blue Eyes” experiment, which she created in response to the assassination of Martin Luther King, Jr. over thirty years ago, participants are labeled as inferior or superior based solely upon the color of their eyes and exposed to the experience of being a minority. This dilemma is not a new one and clearly one as relevant today as any year or age in the past.
I’m no Hiro, but I do know who Tadashi is to me. (Image from Disney’s Big Hero 6)
Our job as ethics advocates and responsible technologists is to prevent flip-switches being built into technology that can identify people based on traits or labels and used nafariously. If humans can do this in just a few minutes, machines can do it instantly.
What we need is a baseline fail safe. A core set of ethical standards that are agreed-upon, not at a company level and not at a local level. No, this is something that needs to be established at a global level and one that is committed to at a scale that has never been undertaken before. It’s certainly been tried. There are templates and frameworks that we can rely on to bring the right people to the table.
What we can’t do is bring our biases to that table without prejudice. That’s why this won’t take a simple accord. It will take a distributed system built on a foundation of trust.
Please and thank you.
“Everything feels so different now.” Think bigger. Think now. Think then. Think how.medium.com
From there, the conversations that provide the scaffolding to our collective goal can help us reach the lofty promises of our future, despite any weathered nostalgia of our past (as it so happens, a show that arose out of an experiment too).
I’m still stuck very much in the healthcare arena as well as distribution of wealth, but I’m glad to see I’m not the only one that sees the world as the small place that it is, very much in need of a new perspective with a fresh set of lenses.
To reiterate my own agenda — more food, more thought experiments.
Thanks again.

= Ethical AI — “Without reason, without heart, it destroys us”? =
Ethical AI — “Without reason, without heart, it destroys us”?
WallDevil
“I got a language in my head that I don’t speak. It’s not just digital, it’s alien. Every day I wake up different, modified.” — Cyborg (Justice League)

In the movie Justice League, Victor Stone is saved by his scientist father. Victor is now powered part by ai, part by his human brain. He keeps self-improving and adapting to point beyond which he can control. He develops so fast that he merges with all the world’s available digitised data. He can hack, decode, control. He is a machine that learns or a “machine learning” human-machine.
Victor is uncomfortable with this new power, he laments his loss of humanity, his loss of his former self.
Now imagine, Victor was not human, imagine he was an algorithm, imagine he had no feeling for humanity in any way. Who ever controls that algorithm could control the world’s data. Controlling the world’s data would mean controlling the world.
“Artificial intelligence is the future, not only for Russia, but for all humankind” — Vladimir Putin
Naturally those who programme Ai will be programming that Ai with their own bias, their own context, their own ethics. The data that feeds that Ai will also be biased data, for all our knowledge and thus data is cumulative, built on previous existing knowledge. All that knowledge contains the shape of the world in which it existed, the bias, the zeitgeist, the ethics.
This Thursday Thought is influenced by the great science fiction writer and guest on this week’s innovation show Edward M. Lerner. On the show and in his book “Trope-ing the light fantastic”, Edward talks of ethics in AI. When it comes to programming ethics, we need to be aware that ethics evolve.
“B-Ai-S”

Ai, AGI and machine learning centre around machines which self-learn in a compound fashion, but they need to be programmed to begin with. If you think of them as sophisticated algorithms which need “input” to start a snowball effect of learning, then the data inputted will massively influence the data outputted. The initial data becomes extremely important to the constantly evolving end result.
“Bias in — Bias Out”
Ai systems which are heavily reliant on data can easily become biased. This all comes down to where the data initial comes from. If the data Ai learns from is biased in the first place, then of course the output will be biased.
This is referred to as “Bias-In, Bias-Out” by Professor Barry O’Sullivan, Director of The Insight Centre for Data and Analytics in University College Cork, Deputy President of the European Artificial Intelligence Association and previous guest on this Innovation Show.
Crash and Burn Bias — Microsoft’s “Tay”

“The more you chat with Tay the smarter (more biased) she gets.” — Microsoft
Many companies are and have been experimenting with conversational interfaces. Amazon has commissioned students to build computer programs called socialbots that can hold spoken conversations with humans.
Microsoft launched such a project with an Ai twitter chatbot called Tay. According to Microsoft it was to “experiment with and conduct research on conversational understanding”. It was targeting 18–24 year olds on twitter, so thus it was “interacting” with them.
Tay was “exposed” to a barrage of abusive language, political and pro Hitler tweets. Eventually Tay tweeted “Hitler was right” and “9/11 was an inside job”. Tay had learned from such interactions.
Yes, Tay was hijacked by the community and yes was “fed” drivel by social media trolls, but things became so bad that the Tay project was shut down after a mere 24 hours.
This type of bias is called “Interaction Bias” and it can be malicious, but there is so much historical bias in the world due to evolution of (hu)mankind that it is hard to stamp bias out and get to a ground truth.
The world is male biased as we discussed in a previous post, but what happens when we programme Ai for Ethics?
The (Other) Turing Test
Alan Turing is widely considered to be the father of theoretical computer science and artificial intelligence. Turing is most recognised for his use of computers to crack Nazi codes during World War 2. Turing played a pivotal role in cracking intercepted coded messages that enabled the Allies to defeat the Nazis in many crucial engagements and in so doing helped win the war.
Surely, we celebrated this great mind and decorated him?
Alan Turing was prosecuted in 1952 (just over half a century ago, 6.6 decades, 66 years) for homosexual acts, under the Labouchere Amendment, “gross indecency” was a criminal offence in the UK. He accepted chemical castration treatment as an alternative to prison. Turing died two years later, 16 days before his 42nd birthday, when this great mind had so much more to offer.
Why do I mention this?
If Ai was programmed with the “ethics” of 1952, Ai would use the “ethics” of that era as a guideline. Ai would consume the data of its time to learn and adapt and like Victor Stone (Cyborg) would wake up everyday different, modified, improved, just like Tay did.
As we may look back over the last century and condemn ourselves (as a human race) for mass atrocities, genocide, war, chemical castration, how will we view the atrocities that are considered acceptable today? Ai is being developed at a rapid pace. How do we define good and bad and teach it to AI when good and bad evolve?!?
AI is already part of our world, it provides optimisation, data analysis, all manner of efficiencies, improved cybersecurity and a myriad other opportunities. We are reaching the upper echelons of weak Ai and only embarking on the journey of strong Ai. Many of us cannot see how quickly this is happening, it is happening at an exponential rate. Where it will end, we do not know for sure, but those of us interested know the general direction.
“The actual path of a raindrop as it goes down the valley is unpredictable, but the general direction is inevitable” — Dr. Kevin Kelly
As Edward Lerner shares on this episode: It is vital for us to understand that AI is a product of algorithms which are also products of humans who are inevitably biased. That is why we need to understand the technology almost too well for a better future with better justice.
Giphy — Robocop
Imagine we programmed autonomous weapons with the “ethics” of 1952? Imagine these weapons self learned based on the initial information they received?
Imagine we hard coded ethics without a failsafe, with no way to evolve as humanity evolves, as ethics evolve?
I leave you this week with some great dialogue from Zach Snyder’s “Justice League”…
Bruce Wayne: “… that’s what science is for. To do what’s never been done. To make life better.”
Wonder Woman: “Or to end it. Technology is like any other power. Without reason, without heart, it destroys us.”
IF YOU LIKED THIS, PLEASE “LIKE IT”, SO OTHERS WILL SEE IT
Ep 117: The Science behind Science Fiction: Augmented Humanity, AI, Super intelligence with Edward M. Lerner
This episode is with one of the leading global writers of hard science fiction and indeed cyber fiction, Edward M. Lerner. He is author of over 18 titles and what we hope is fascinating for followers of this show is the science he puts behind the fiction. As opposed to fantasy writing, science fiction is based on possible realities and that fact is often lost on many of us.
He is a physicist and computer scientist, he toiled in the vineyards of high tech for thirty years, as everything from engineer to senior vice president. Once suitably intoxicated, he began writing full time.
The focus of this show is themes from his book: ‘Trope-ing the Light Fantastic: The Science Behind the Fiction’.
We discuss:
Augmented Humanity
Cyborgs
Robots
Genetic Therapy
Brain Machine Interfaces
Autonomous Weapons
AI, Artificial Intelligence
Superintelligence
Neural Networks
Dystopia
The future skills of humanity
What we do when everything automated
Have a Listen:
Web http://bit.ly/2FwsOJw
Soundcloud https://lnkd.in/gBbTTuF
Spotify http://spoti.fi/2rXnAF4
iTunes https://apple.co/2gFvFbO
Tunein http://bit.ly/2rRwDad
iHeart http://bit.ly/2E4fhfl

More about Edward here: https://www.edwardmlerner.com/
Tags: Edward M Lerner, Edward Lerner Science Fiction, Augmented Humanity, AI, Super intelligence, Cyborgs, Neural Networks, Artificial Intelligence, Dystopia, Innovation, Innovation Show, Robots v humans, robotics, nanobots

= Product Management Ethics In The Age Of Artificial Intelligence =
Product Management Ethics In The Age Of Artificial Intelligence
Adapted from a presentation at Product School. Full video here and at the very end of the post. One note of caution: the negative outcomes of poorly considered technology tend exacerbate societal issues. This post discusses racism, recent natural disasters, childhood obesity, sexism, and classism in frank terms that may be upsetting.
 
I wish I could say that I spent my past 10 years in Product Management as an ethical crusader, righting wrongs, defending the defenseless, but it’s not true. My interest in ethics and Product Managers’ impact on the world is only about 10 months old.
Earlier this year, I spent a lot of time studying nutrition. Improving my physical health is my goal for the year and reading obsessively was how I prepared.
A friend recommended this book: Salt, Sugar, Fat by Michael Moss. In the book, Moss tells the history of processed food and the ways food companies have innovated more and more salt, sugar, and fat into our diets.
As I was reading the book, I was duly horrified. The book tells the story of high diabetes rates in low income families and childhood obesity. But, disturbingly, I noticed I noticed I was feeling a small undercurrent of inspiration.
Here’s the bliss point:

In the formulation of food products, the bliss point is the amount of an ingredient such as salt, sugar, or fat which optimizes palatability.
The discovery of the bliss point is genuine scientific innovation, the kind many of us will work our whole careers to achieve. And, like most of the innovation we strive for, it has created billions of dollars for the companies leveraging it.
Maybe I was willfully ignoring it, but I swear Moss didn’t refer to these food pioneers as “Product Managers” until about 75% of the way through the book. Those two words hit me like a ton of bricks. Not only did I find the work inspiring, but these people had my title, our title.

What’s at play here is a perverse incentive. Companies don’t have a conscience, their goal is to maximize value for shareholders without incurring too much legal risk. These goals trickled down the PMs running these food science teams and they delivered and delivered and delivered.
But we’re digital Product people, we can’t possibly wreck society like these folks, right?
Nir Eyal’s Hook model might be more familiar to you.

Eyal uses words describing addiction (to his credit, Eyal discusses the ethical issues as well). Think about that for a second. This isn’t hyperbole, the brain pathways that encourage long term usage of a product are the same pathways that tell you you’re hungry, the same that tell an addict they need more heroin.

More than 60% of Facebook’s billions of users come back every. day. This drug feels relatively benign, but I think we all have a creeping suspicion it’s not. One guy in particular seems to be thinking a lot about Facebook’s impact on the US.

AI Ethics
With that background, I want to switch gears and talk about Artificial Intelligence (AI) and Machine Learning (ML). I want to look at how these same perverse incentives have the potential to cause issues as we let AI increasingly own user interfaces and automation.
Let’s start with a model of how to think about how AI relates to human decision-making and everything that’s come before. One simple way to think about a knowledge worker’s job is the OODA loop.

Observe — actively absorb the entire situation
Orient — understand blind spots and biases
Decide — form a hypothesis for action
Act — test your hypothesis
AI is the eventual removal of people in the OODA loops of increasingly complex tasks. From 100% human driven OODAs, to big data driven ODAs, ML driven As, to AI …..s.
It’s a brave new world.
Interface Isolation
One way to apply this is to think of the web’s evolution as the movement from static to ever more dynamic interfaces. In the olden days, web pages were text documents on a file system. They were updated when someone decided to go change them. Web 2.0 saw the explosion of JavaScript frameworks and new models of interactivity. Content updates, it notifies, it tailors itself a little to the user.
AI will pour gas all over this fire.

I’m sure it’s no surprise to you that Facebook and Twitter users self-select into groups that rarely interact. The result is two people read about the same news event in fundamentally different ways.
You could say this happens anyway and is a movement of our greater society, “The Big Sort” as the phenomena is known. But it’s hard to think of the ubiquity of social apps, look at graphs of the political polarization of the US, see evidence of sophisticated ad targeting by Russian actors, and not at least scratch your head.
Think about natural language processing and conversational interfaces. Remember Microsoft’s Twitter bot, Tay? Tay’s training data set came from Reddit and Twitter, effectively unmoderated rambling conversations about everything. The result was that Tay said a bunch of super racist stuff.

Humans pulled the plug, retooled and relaunched Tay and…

She came back as a pothead.
How about a soap dispenser that doesn’t register black peoples’ hands?

Let that sink in for a minute. The hand dryer is saying, “these aren’t hands” or “this isn’t a person.”
Whose “fault” is it when algorithms impact society? How about when AI is more prevalent, human feedback is increasingly removed, and the outcomes inscrutable? We’ll come back to those.
Biased Data

“All data has an opinion. You can’t trust the opinions of data you didn’t collect.” My buddy Otis Anderson said that; the longer I work in data driven organizations, the more I see it.
Data is compiled by humans and we humans ourselves are subject to an almost unfathomable number of cognitive biases. From the actual wikipedia page for cognitive biases:

In her book Weapons of Math Destruction Cathy O’Neil tells the story of these Minority-Report-dreaming “pre-crime tools” already in use in many police departments. These systems use arrest data from various counties and use it to “predict” where and when crimes are likely to happen. Police departments can then make sure to have officers stationed at the most likely places.
It looks great to the police departments, their arrests go up, maybe they impact actual crime levels. Can anyone think of a problem here?
Well, crime of the kind uniformed officers care about is often committed by people who are desperate. Further, only urban police departments would make this kind of investment and the urban poor live in low income often minority-heavy areas. So cops are spending more time around young minority kids who grow up in low income areas.
What also correlates to a high change of arrest? Spending time around cops.

I don’t know about you, but I did some boneheaded borderline illegal stuff as a teenager. Neuroscientists have as much as said all teenagers are certifiably insane. You put a cop nearby and I’d have a record. Keep them around all the time and my life might’ve been very different.
How about AI in hiring?
Fox News conducted a study about what makes a “successful” employee. Those who stuck with the company and got promotions tended to be male. So let’s just toss out 50 years of equal employment law and hire all men, right?
Welp…

But this is simple ML and human intervention might improve equality and equity.
In AI, the inputs might not be as transparent, the data set doesn’t need to be numbers. There are AIs right now using satellite footage to track people in the vicinity of crimes for days afterward, mapping all their known associates, following them and their associates. Not surprisingly, this was first deployed against terrorists.
Here again, biased data that says to watch a certain area or a certain type of person will fill databases with a bunch of known associates who likely have similar traits. But, this time, we’ve taken the human reasoning out of the equation. The AI will have made the determination and developed deep databases on a number of innocent people. AI can’t arrest people yet, but it can definitely freeze accounts, gather materials for a secret FISA warrant, put you on a no-fly list, etc.
In The Gene, Siddhartha Mukherjee discusses early 20th century interest in eugenics through geneticist Hermann Muller.

Muller sympathized with the desire to use genetics to alleviate suffering. But… Muller began to realize that positive eugenics was achievable only in a society that had already achieved radical equality. Eugenics could not be the prelude to equality. Instead, equality had to be the precondition for eugenics. Without equality, eugenics would inevitably falter on the false premise that social ills, such as vagrancy, pauperism, deviance, alcoholism, and feeblemindedness were genetic ills — while, in fact, they merely reflected inequality.
“Equality had to be the precondition for eugenics.”
Equality has to be the precondition for human-sorting AI.
Black Swans
We also tend to want to use data to form predictions, forgetting that data is backward looking. By definition, we can’t optimize for unexpected events. Business-as-usual OODA optimization is likely to exacerbate these situations and propel forward the most negative outcomes.
All the predictive modeling in the world can’t tell you the day The Big One is going to hit Northern California.

If the Bay Bridge comes down, how will your self-driving car react? You probably don’t want to find out that day, but it’s probably financially impractical to test that scenario before putting cars on the road.
Every hurricane, every stock market crash, seem to find new paths through areas we thought we understood, growing in intensity. “500 year flood” is a stupid metric for a species that’s only been recording history for 5,000 years.
What we’re talking about here is understanding whether a system is deterministic or stochastic. When we take an action, do we know with 100% certainty what the outcome (deterministic) is or is it somewhat random (stochastic)?

Digital systems strive to be deterministic and, to some extent, they are. Software might not do what we expected, but it usually does exactly what we tell it to. The broader universe and, most importantly for this discussion, humanity are stochastic. There might yet be a mathematical model of the universe that can predict everything from the big bang to me saying “uh” a hundred times whenever I give a presentation, but we haven’t yet found it.

Neo is irreduceable.
 
No offense to folks like Nate Silver, who’ve done much to educate the public about probability, but confidence intervals in US elections are silly. They just haven’t happened that many times. We’ve held presidential elections 56 times. That’s an exceptionally small number from which to draw inferences. The odds that you flip a coin 56 times and end up at exactly 50/50 are extremely low.
We’re talking about absurdly small sample sizes. We only have one human race and one planet to try out big societal experiments. If they succeed or fail, we can’t just move on to a better timeline like Rick and Morty and we’ll never eliminate enough variables to form even a half-assed understanding of why.
I can give a hundred more examples of Black Swan events and our flawed understanding of probability, but there are already good books on this. What I will say is that an AI created by us, with backward looking data, will weaponize our failed intuitions and build probabilistic models that lean into disaster.
 
A friend bought this book, Zen Shorts, for my daughters. In particular, I love the story of The Lucky Farmer.
There was once an old farmer who had worked his crops for many years. One day, his horse ran away. Upon hearing the news, his neighbors came to visit.
“Such bad luck,” they said sympathetically.
“Maybe,” the farmer replied.
The next morning the horse returned, bringing with it two other wild horses.
“Such good luck!” the neighbors exclaimed.
“Maybe,” replied the farmer.
The following day, his son tried to ride one of the untamed horses, was thrown off, and broke his leg. Again, the neighbors came to offer their sympathy on his misfortune.
“Such bad luck,” they said.
“Maybe,” answered the farmer.
The day after that, military officials came to the village to draft young men into the army to fight in a war. Seeing that the son’s leg was broken, they passed him by.
“Such good luck!” cried the neighbors.
“Maybe,” said the farmer.
Feedback & Experimentation
Coming back to those pre-crime systems, O’Neil imagines a better way, the product optimizations of Facebook and Google. She shows how human intuition can find that which is inhuman or unequal. That human feedback and experimentation make these systems workable.

The tools are there, these systems just tell cops where to be, they don’t take on that action themselves. Though, to date, O’Neil says these systems tend to just run as police departments don’t have staff data scientists and their perverse incentive, arrests, do go up.
Without human feedback, the OODA loop becomes a death spiral of inequality. Without actively testing alternative scenarios, we build a model of the world based on the past that can’t be swayed by the possible present or future.
And, moving to the ultimate promise of AI, machines will be observing, orienting, deciding, and acting hundreds of times per second.

This may surprise you, but of 100+ PMs I interview for each opening, maybe five have even a rudimentary knowledge of how multivariate testing works. Meanwhile, experimentation and related feedback are beyond critical for building ethical AI.
Our discipline is driving hard towards a cliff and few of us know how to fly the plane, even if we manage to assemble it in time on the way down.
Transparency
The default for neural networks is not to output their reasoning. You’ll see these salacious headlines about Facebook and all these rogue AI. The true story isn’t that AI can’t be built to explain themselves in a human-auditable way, just that it’s not the default.
Defaults are incredibly powerful.
Thanks to open source and ever-improved tooling neural networks have just recently graduated from the point where you need a PHD and a brilliant advisor to understand them. It’s possible today for an undergraduate from a top tier university to understand and deploy a neural network. The bar will continue to lower until it’s trivial for any engineer to become proficient in developing AI in a matter of months.
 
Let’s look again at those sensational headlines:

Google’s AI was found to have developed a concept of language between languages. Super cool! There’s so much we can learn about cognition.
Facebook’s AI-run-amok is a little more boring once you get past the lede. Two AIs communicating with each other developed a confusing language for negotiating. This would be scary if humans working on stock trading floors hadn’t been doing this for hundreds of years.
The headlines here don’t scare me. The singularity isn’t going to kill us (today). The scary part is that these experiments ran for a while before anyone bothered looking into what was happening. Nothing about the systems produced these by default, it was expert human intervention and deep understanding of how the technology works that surfaced these points.
And here we come back to those defaults. As the bar lowers for developing AI, fewer engineers will stray from the defaults, launching as-is. Without human-understandable logic, we can’t evaluate success or terrifying failure.
Responsibility (Liability)
Okay, cool, we’ve talked a lot about writing AI that explain themselves, leaving room for human intuition in decision-making, but I’m not an engineer or a data scientist, why do I care about any of this?

Here’s a hypothetical, you’re driving a car and five kids dart out into the middle of the road. You can swerve, but either into traffic or a telephone pole. Either will kill you instantly. You may be thinking, “I’d swerve, I’m a good person.”
Okay, let’s test that. If you have the option of buying a Tesla that kills the kids and a Pinto that kills you (and any other passengers), which would you buy?
Get an autonomous driving engineer drunk and ask this question. They’ll answer along these lines, “No one is going to buy a car that kills them.” Meaning, manufacturers aren’t even going to ask, if they can’t stop the car, they’ll kill the kids. Who bears responsibility for those dead kids?
If Facebook letting users segregate themselves into smaller and smaller groups leads to isolation and conflict, whose fault is it?
If an autonomous drone bombs a hospital
If a normal teenager is arrested for typical boneheaded teenager stuff and they spend the rest of their lives in and out of the legal system
It’ll take years for the courts to come up with standards around these things, even longer for legislative solutions, but you can see the seeds of it in increasing scrutiny of Facebook and Google’s algorithms.

This isn’t a guy running for office. This is a guy running from liability.
But, back to ethics. Those food science PMs we talked about at the beginning of the presentation fell into a few camps:
We did nothing wrong, people get to make their own choices.
The outcomes are unfortunate, but I was paid to do a job.
I am deeply responsible for what I’ve created and will devote my life to making things better.
Each of these interpretations have philosophical and political adherents. All I can tell you is 1 feels pretty weak to me as decisions are increasingly made by a machine and our only “choice” is to use or not to interact with that machine.
As for 2, “just following orders” didn’t work for these folks:

Conclusion
You may conclude from all of this that I’m some kind of luddite crank, ready to toss my sabot into the machine. The fact is I don’t believe in “good technology” or “bad technology.” There is only technology and what people do with it. To the extent that AI makes independent decisions, we told it how to do that.
You might be looking to me for some kind of solution. Unfortunately, I don’t have any answers. What I do have is you! The next generation of Product Managers, heading out there in the world to improve the discipline and apply what you’ve learned here.
Here’s my request: don’t fall for perverse incentives, businesses need to make money, but you need to sleep at night. Remember the ethical blindspots of AI.
Don’t let interfaces isolate
Understand biases in data
Don’t overfit reality, make allowances for Black Swans
Program feedback & experimentation
Encourage transparency
Thanks to Carlos G de Villaumbrosia, students, and team at Product School for having me!
Full video of my presentation:


= Designing ethical artificial intelligence =

Designing ethical artificial intelligence
By Jivan Virdee and Hollie Lubbock
There are very different views on what the future of Artificial Intelligence (AI) looks like. “Celebrities” of the tech industry are often seen battling it out, but those of us whose daily lives focus on designing digital products should also get involved in the debate. We spoke at this year’s Cannes Lions, to share our thoughts and prompt a debate. If you missed it, here it is in a nutshell”
Surprise!
Data is shaping our digital service — this much we’ve known for a good while now — but it’s only recently that its potential pitfalls, dangers and shortcomings have found the limelight. When people have started using their platforms for nefarious purposes, big tech companies have been slow to respond — not because they don’t care, but because such activities were unforeseen when their products launched, so there weren’t measures in place to deal with them efficiently.
Technology is moving so fast that legislation can’t keep up with the rate of progress. As a community, digital designers can legislate ourselves much more efficiently. We should do exactly that — and hold ourselves to a higher standard at the same time.
Human-centered and humanity-centered
Technology should be a force for good. It should be something that propels us forward to become the people we want to be and live the lives we want to live.
The key lies in being not only human-centered, but also humanity-centered. As we design a new service or product, we need to imagine the worst-case scenario for humanity if every single person in the world were to use it. Many of the negative consequences surrounding some of today’s highest profile services were unforeseen, so nothing had been put in place to mitigate for them. We must now make it our business to design accountability measures into our products and services.
We should look at three key areas when designing with human values in mind:
1. Fair and transparent data science
How do we build transparency into our products so that people understand why decisions are made, and can then feel confident to question artificial intelligence results rather than blindly believing the “magic”?
2. Trust and human-machine collaboration
How do we build a foundation of trust, and design true collaboration between people and AI so that we can work together without feeling threatened?
3. Responsibility and accountability
While we might be working in hybrid human-machine systems, we bear the ultimate responsibility for ensuring they operate safely and for the common good.
Every system we create inherits our bias. Unless we assume that all systems are biased and actively look for it, we won’t discover it until it’s out in the wider world, creating issues. One way to reduce bias in the design process is to ensure the teams behind the service or product bring diverse attitudes, experiences and opinions.
As Mo Gawdat says here, we have to work to make artificial intelligence not just intelligent, but focused on the right data sets to be ethical and good. Think of artificial intelligence as a three-year-old child. If that child is regularly exposed to strong language, to violence, to biased views, how will he/she assimilate that information to shape his/her behavior? We cannot expect AI systems to be able to discern good from evil.
The echo chamber
A close relative of bias is the concept of the echo chamber. The majority of us no longer live in small communities — even those who physically live in remote villages are also part of a huge metropolis online, with friends in far-flung places who communicate daily via social media. However, even with easily accessible worldwide connections, our own circles are likely to expose us to a limited set of ideas and opinions.
Add to that the unseen and misunderstood algorithms that surface content they know aligns with our views, and we end up in a bubble (and we might not even know it). If we want to read opposing opinion, we have to know it’s out there and go looking for it.
AI and jobs
Plenty of headlines have focused on the damning impact artificial intelligence might have on careers. The fact is that technology will simply continue to do what it has always done when it comes to evolving our professional landscape — it’ll be a continuous and slow shift, not an overnight shocker.
We’re not talking about using AI to automate jobs across the board — we’re not even talking about automating one person’s entire job. We need to look at the areas that are most ripe for automation and consider what we’re doing for those whose careers will be affected.
As a start, are we educating the next generation in the fields that are most likely to lead to steady, long-term careers? What do machines do best? Where do they fall short? Crucially, machines struggle most with tasks that require creativity, empathy and judgement — those are the skills that will best equip our young people for AI-proof careers.
It’s up to us
The future of AI is something of a mystery, but those who have pioneered digital services and data science have exposed areas we can and should work on to ensure that technology contributes to our world in a positive and meaningful way. In the words of Dan Hon, “No one’s coming. It’s up to us.”

= A.I., an intelligence superior to that of the human? =

A.I., an intelligence superior to that of the human?
Humans continue to seek to multiply the capacity of their movements. Their actions and capabilities in a complex world. Many technological advances are aimed at the well-being of humanity in order to make them hope and dream of a better world. Artificial Intelligence “AI” has begun to perform some of the same tasks as humans and sometimes even better. For example, we see this in a game of chess, the game GO or with self-driving cars. It is safe to say that AI or artificial intelligence is capable of acting as a human and even surpassing or potentially replacing several of human tasks ?
The will to accompany the man and improve his everyday both on a personal and professional level.
Since the invention of the term “AI” in 1956 by John McCarthy, the AI divides: For some, this technology could save humanity, and even bring him immortality, according to the opinion of transhumanists. Others, on the contrary, claim that artificial intelligence handicaps humanity to the point of destroying its very essence.
A revolution that explodes the possible uses in all sectors of activity.
We hear more and more about this revolution, thanks to several factors: The emergence of Big Data and AI’s massive data needs to learn on it’s own. The computing power is always more powerful as well as the progress in algorithmic research.
Machine Learning is an algorithm programmed so that it learns by itself from examples. Deep learning (neural networks) which is based on the functioning of the human brain realizes programs capable of learning and making predictions.
Should we therefore see this scientific breakthrough as a benefit or as a regression for the human being ?
The tools of Robotic Process Automation allow, without programming, to realize human repetitive behaviors, to reproduce them and to execute them. The simpler it is, the more efficient it is. We cannot talk about AI without citing Isaac Asimov and his three famous laws on robotics. The goal of which, according to him, is that the robot cannot rebel against the human. For that, it would be necessary to apply these 3 laws:
1 — A robot cannot harm a human being or, while remaining passive, allow a human being to be exposed to danger.
2 — A robot must obey the orders given to it by a human being, unless such orders conflict with the first law.
3 — A robot must protect its existence as long as this protection does not enter into conflict with the first or the second law.
In the field of customer relations, a robot can ensure a physical interaction when the advisor is unavailable while taking into account the context and the external environment. The first humanoid robots, NAO and Pepper are able to understand emotions and dialogue. They can accommodate a customer in an agency, ask to check his information or create good conditions before meeting the “real” advisor.
The connection between humans and robots
Great progress is coming thanks to robotization; however, care must be taken to properly assess the actual costs of implementation and the gains generated, while being attentive to ethics.
Dubai police are the first to launch anthropomorphic robots in the real world. But are AIs built today able to assume such responsibilities ?
“PAIN IS A PROTECTIVE MECHANISM.”
Johannes Kuehn, German Researcher at Leibniz University
Learn the feeling of robots
Allowing robots to feel pain raises ethical questions. A study by the Magazine Scientific Repots analyzes human empathy based on a robots perspective of pain. There is an emotional concern with these robots subjecting to acts of violence. However, there is still some way to go before robots really react to pain.
Ethics is a predominant subject. The current question is not to worry about an AI that could be found in sci-fi movies that would take over mankind, but it is a question of questioning the problems of reflection that these technologies currently pose. The development of a super AI will be one of the most important milestones in history. It will therefore be important to align the objectives of these super intelligences on that of the human species so that it is an extension of the human will, and thus does not disserve us in the long run.

= Who has responsibility when AI is running the show? =
Who has responsibility when AI is running the show?
Paula Boddington, Oxford University researcher and author of “Towards a code of ethics for artificial intelligence,” guides us through some of the complex ethical issues raised by AI’s increasing presence in our lives, its impact on our value system, and why we need to think hard about what could go wrong before adopting it wholesale.

With accelerating advances in artificial intelligence, the world of AI is weaving itself — almost imperceptibly at times — into the fabric of our lives, affecting how we move and live and relate.
But as AI extends human agency, intelligence and action in the world — sometimes replacing human beings entirely for certain tasks — it raises increasingly complex issues of ethics and responsibility.
Paula Boddington
Paula Boddington, senior researcher at Oxford University’s Department of Computer Science, tackled the subject head on, addressing a packed audience at the Mobile World Congress in Barcelona last month. For a topic that presents many more questions than answers, she offered a glimpse into how technology brushes up against philosophy.
“The power of AI is pushing us towards questions about the very limits and grounds of our human values,” Boddington says. While it can be potentially of enormous benefit, “does it extend our reach beyond what we can really handle. Does it push us beyond where we really want to go? Can we really be responsible for all aspects of such developments?”
The question of responsibility is at the heart of the ethics issue. AI is potentially a large disruptor of notions of responsibility. One key aspect of responsibility — indeed, a precondition — is control, Boddington says. “AI presents us with … possibly intractable questions about control. If AI is autonomous do we always have complete control and do we always know what’s going on?”
Susan Etlinger: We are at a critical turning point about data
Artificial Intelligence (AI) is coming and in big waves. But the way it functions, analyzes, and makes decisions…citiesofthefuture.eu
Black box technologies and machine learning — two elements of AI — can be hard if not impossible to explain, she says. “If AI is acting autonomously without direct control — and especially if it’s not completely clear to human creators and users how decisions and actions are reached — how can we as humans remain accountable to others?”
Some computer scientists are calling for open inspection of algorithms as one solution to this dilemma.
Either way, disentangling cause and effect are much harder in the age of AI, and yet they are key to defining responsibility.
At the other end of the AI disruption is the possibility that its presence could diminish our responsibilities altogether. “It could, if we’re not careful, start to erode human autonomy,” Boddington says. “We could allow it to replace human thought and decision in places where we shouldn’t.”
Prompted by exactly these types of concerns, last year the Future of Life Institute (whose motto is “Technology is giving life the potential to flourish like never before….or to self-destruct. Let’s make a difference”) came up with 23 principles for beneficial AI.
Principle №9 reads: Designers and builders of advanced AI systems are stakeholders in the moral implications of their use, misuse and actions, with the responsibility and opportunity to shape those implications.

While much focus is on blurring the lines of responsibility, one example of AI’s potentially clarifying role, Boddington says, is the use of algorithms in sentencing decisions in courts, despite the controversy surrounding the practice. Since the public has a right to know how a decision is reached in the legal realm, an algorithm can pinpoint precisely the reason for a sentence, whereas a judge might be less able to do so. The caveat, of course, is that the algorithm generates an answer from examining data, and if the data has any history of bias that will reflect in the sentencing. (Similar warnings have been sounded about inadvertently replicating sexism in AI through biased data.)
It’s Not YOUR Data, Didn’t You know?
Identity. In the digital age, this is widely characterised by our data. Internet browsing data, consumer data…citiesofthefuture.eu
So what is the responsibility of a company that buys black box components from another company? Boddington asks.
The question hangs, because trying to identify responsibility when we don’t control the technology and can’t predict the outcome is outside the framework of how we understand human responsibility.
The implications of that conundrum are significant. Adapting to AI, Boddington says, may have “a very deep impact upon our system of values.”
Another impact is the speed at which we adapt to AI. “Our habituation to technology can change what we think of as being normal or expected, and this in turn can change how we understand and attribute responsibility.”
The new normal is that we adapt quickly, even to things that ultimately aren’t to our benefit, she says, and our judgement shifts accordingly.
“We need to think carefully about the power of AI to perhaps dazzle and entrap us, AI is often glitzed up in ways to charm us into doing things we might not otherwise have done. Is it possible we could retain human control in principle but actually be seduced into doing what the AI suggests?”
She cites the example of AI being used in advertising to make it more effective in manipulating human behaviour. The famous “learning experiments” of Stanley Milgram from the 1960s revealed how easily people could be influenced, even to the point of cruelty. Volunteers were asked to administer electric shocks to subjects when they gave the wrong answer, bit by bit increasing the power until the shocks (if they had been real) would have been fatal.
Boddington asks: “Is this how we’re getting use to technology and AI taking over our lives?”
She is not anti-technology, she says, but strongly in favor of thinking about what might go wrong.
The Milgram experiments revealed a vulnerability worth noting, she says. When the volunteers expressed doubt, the experimenter simply repeated calmly: “The experiment must continue.”
“This tiny little polite prompt was enough,” Boddington says.
What Milgram later concluded is that “the scientific, technical surroundings were enough to persuade completely ordinary people to act beyond their consciences,” she says.
“Are human beings particularly prey to falling to technological masters?” she asks somewhat ominously. Hollywood would answer in the affirmative.
But movies aside, as AI becomes more deeply embedded in our social and cultural structures it has ramifications, even for those not directly using the technology.
Sign up to our newsletter to receive the latest Cities of the Future news. You can also follow us on Twitter and Facebook.

= Tackling the ethical challenges of AI =
Tackling the ethical challenges of AI
Standards can align AI with ethical constraints, writes Michael Mullane
AI could make people worse off, if we make the wrong choices. (Photo: Franck V. on Unsplash)
The IEC General Secretary, Frans Vreeswijk, has warned about the ethical and moral challenges arising from the fast pace of technological advancement, particularly in areas such as artificial intelligence (AI). “Not everything that can be accomplished with technology should be undertaken,” he said.
Concerns about the misuse of technology are far from new. Five hundred years ago, Sir Francis Bacon wrote “the mechanical arts have an ambiguous or double use, and serve as well to produce as to prevent mischief and destruction … .”
The English philosopher concludes that anything can be used for good or evil ends, which is why, he argues, we have rules governing morality. Those rules need to be universal in order to work.
Mr. Vreeswijk discussed the challenges that technology poses to humanity at the launch of a high-level global forum to address ethics in AI through standardization. IEC is a founder member of the Open Community for Ethics in Autonomous and Intelligent Systems (OCEANIS).
OCEANIS brings together standardization organizations from around the world. It aims to enhance understanding about the role of Standards in facilitating innovation and addressing issues related to ethics and values.
OCEANIS is of vital importance because the ethical implications of AI is one of the most urgent and compelling issues of our times. In recent weeks the Nobel laureate, Joseph Stiglitz, has weighed in with an important lecture, while the internationally renowned scholar, Yuval Noah Hariri, has published a new collection of essays addressing challenges related to AI and data.
Mr. Stiglitz, a former chief economist at the World Bank, believes that AI has the power to be a force for good, transforming the way that we live and work. He warns, though, that in order to benefit we must first address issues related to ethics and values.
Mr. Stiglitz told his audience at the British Royal Society that AI had the potential to increase productivity, reduce poverty and boost affluence. The danger is that it could equally make people worse off, if we make the wrong choices.
Mr. Stiglitz called for investment in AI that helps people to do their jobs more efficiently, rather than in AI that replaces workers.
A recent study in a London eye hospital illustrates his point. The study found that a machine was able to detect more than 50 eye conditions without missing a single urgent case.
The 95% accuracy rate matched the performance of leading eye specialists. Theoretically, it could eventually free up specialists to perform other duties, but it could also lead to cost cutting, or doctors being laid off.
Mr. Stiglitz believes it is inevitable that some low-skilled jobs will be lost. For example, self-driving vehicles, cashier-less supermarkets and AI-equipped call centres are likely to have a massive impact on unemployment, but this could be countered by hiring more unskilled workers to meet the strong demand in education, the health service and care for the elderly, he claims.
Many of Mr. Stiglitz’s concerns centre around the big data economy and the way the information collected about us online could be used to sell products for higher profits, instead of investing in R&D to improve quality. His fear is that tech corporations may find it easier, cheaper and faster to boost profits by exploiting insights into the behaviour and values of customers.
Many of the issues that Mr. Stiglitz raises are echoed in Mr. Hariri’s new book, 21 Lessons for the 21st Century.
Mr. Hariri predicts that algorithms will soon know us better than we know ourselves. E-commerce sites, search engines and social media platforms already collect information about our preferences, activities, beliefs and intentions.
AI will refine and accelerate the process. Mr. Hariri warns that not only could all this data be used for manipulative ends, but also that people might become willing and passive victims as they rely increasingly on AI to make decisions for them.
Some of the issues that Mr. Stiglitz and Mr. Hariri flag are political as much as they are philosophical, while others concern futuristic AI systems that do not yet exist. What OCEANIS offers is the opportunity to begin work on developing global solutions to tackle some of the more immediate concerns.
We are already at the point where self-driving cars will soon need to deal regularly with trolley problem-like scenarios, such as whether to injure a pedestrian, or avoid the pedestrian and risk injuring the passengers. Companies worried about potential future legal challenges have begun addressing these issues, but what is needed is closer cooperation between all of the concerned parties at the international level.
International Standards can ensure that AI systems are designed to be in line with ethical and social constraints. In order to avoid potentially dramatic consequences, it is vital that machines continue to follow human logic and values as they replace humans in some decision-making processes.

= Revisiting Asimov’s Three Laws: ethics for robots or researchers? =
Revisiting Asimov’s Three Laws: ethics for robots or researchers?
The recent resurgence of interest in AI has brought with it a wake of would-be ‘robo-ethicists’ (1, 2) who are concerned with the way in which machine intelligence will shape the future. Amongst the myriad commentators, experts and amateurs alike are putting forward their own hopes and fears (like the recent conflict between Elon Musk and Mark Zuckerberg and the resounding media interest). However, such comments are often speculative, flippant and ill-researched because very few people understand the chalk-face of the technologies themselves. And, as a consequence, the media is able to play to people’s ‘Frankenstein complex’ and breed distrust between the ‘obviously’ hapless researchers and their omnipotent creations. The danger is that this could lead to a Gove-ian disconnect between ‘the experts’ and ‘the public’, which may stifle inventions that could make the world a better place: to regulate or not to regulate, that is the question! Almost presciently, this dilemma was predicted by the ‘father of robotics’ Isaac Asimov in the overarching plots of his many short-stories. Asimov famously proposed the Three Laws of Robotics to make robots safe for human beings. Subsequently, the Three Laws have been dismissed as being ‘too abstract’ to be useful, but such dismissal has missed a critical subtlety in Asimov’s proposal: are the Three Laws meant to govern robot or researcher behaviour?
Driverless cars are amongst the first testing grounds for autonomous technology (see).
Before I start, I am not going to discuss the ethics of modernisation, which many articles are seemingly unwitting in getting stuck on. This is because the ethics of modernisation really has very little to do with AI technology and more to do with the eternal conflict between conservative and progressive tendencies. I have very little to add to this debate; my only hope is that the right balance can be struck, whatever that pragmatically entails into the future. Instead, I want to focus on the problem of the regulation and ethics of ‘autonomous technology’, which is what Asimov explored.
Instead of trying to define autonomy, let us jump straight in and look at an example. The first real testing ground for the label ‘autonomous technology’ has been driverless cars. Vehicles that are able to drive themselves face myriad decisions about obedience to the law (as laid down by the local governments), road etiquette (for example, as laid down in the British Highway Code) but also pure ethics. As Sam Harris recently noted, as he pitched in to self-consciously join others in AI fear-mongering (see his TED talk and podcasts), a self-driving car must inherently bring with it some form of ethics because it will inevitably encounter lose-lose dilemmas (e.g. a trolley problem) to which it will have to come to a quick resolution. However, there is nothing fundamentally different about the ethics of a trolley problem dilemma as faced by a human being or an inanimate technology: in either case the dilemma will be decided by someone not something because the vehicle is bound to follow its hard-coded instructions that originate from a human mind. We may dispute the ethics of the decision, but we could literally open-up the vehicles decision-making process and replay the situation through thousands of times to review its ethical procedures until we reach consensus on what is desirable (even if the machine relies on a ‘black-box’ neural network).
As a result, the ethics of driverless cars is likely to become the same as for any other computer software as ‘autonomy’ is displaced to the judgement of a human programmer. Consequently and already, the ethical problems of autonomous vehicles — which I do not wish to deny or belittle — seem already to be shifting away from talking about ‘autonomy’ to focus on describing what ethical procedures are right or wrong. A common approach to avoid a programmer bearing the weight of an ethical dilemma is to ascribe authority based on popularity (e.g. through an ethical polling approach) so as to outsource responsibility for encoding an ‘emergency protocol’. The ethical problem is still there, but ‘autonomy’ has very little to do with the problem of designing ‘driverless car’ software that is safe for users; the car still makes decisions that are a matter of life-and-death but this is the same as for any other software (for remember that banking, personal data and our entire digital lives depend on the lawful/ethical programming and use of software).
The moral of the driverless cars story is, to me, quite simple: we are scared of giving over genuine ethical decision-making to machines. But, more than that, you could also claim that the ethical ‘popularity contest’ shows that we are scared of our own autonomy. (This second idea can be seen in many psychological experiments — e.g. Milgram 1963 — where the removal of something being your own decision, say by acting on someone else’s authority, can encourage you to act in ways that are widely accepted to be very immoral.) And, perhaps, these conclusions are not without good reason because, whilst most ethical problems can be agreed upon near-universally within a cultural setting, people have struggled for thousands of years to codify exactly what the underlying decision-making process might be despite sharing an implicit ethics. Whether acting by consequence, rule or virtue, ethical judgement in the real-world is not straight-forward.
Further, if we are to discuss the behaviour of agents with true autonomy, as has not been addressed in the current debate (to my knowledge, so correct me if I am wrong), you must respect an agent’s responsibility for their behaviour. This is, after-all, much like how we treat each other. So, if we want to understand ethics for autonomous technology we have to respect freedom of choice. That which hurts society can be made unlawful, leading to punishment, but you cannot force obedience without also relinquishing autonomy and responsibility to someone or something else. To preserve autonomy, there has to be a choice to be ‘good’ or not.
The confusion of how to be ‘good’ is, however, a great boon for literature, and sci-fi is no exception. Famously, Isaac Asimov’s Three Laws of Robotics were an attempt to summarise and explore a universal ethics for autonomous robots. The laws are:
1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.
2. A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.
3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.
At the onset, I must note that Asimov’s robots are intended as servants and, as such, the second law is an ‘autonomy override’. Yet, despite this lack of independence, Asimov’s robots do appear to have significant freedom of action, as such I am happy to describe Asimov’s robots as autonomous. Partly, this is because of the myriad failures of the Three Laws to govern robot behaviour and make robots safe. The imperfection of the Three Laws to codify ethics seems to capture the wider problem of codification in general, resulting in their occasional failure (call Susan Calvin!). That said, very few ethical codes can be reduced down to such simple and semantically-unambiguous terms. As such, most people seem to regard the Three Laws as simple, imperfect and unrealistic. But, I don’t think ‘a literary device’ is the end of what the Three Laws can offer and who would know better than their creator: What did Asimov himself think about the Three Laws?
Taken from the front cover of Robot Visions, which contains a collection of Asimov’s short-stories and essays (see).
In Robot Visision (1990), Asimov does admit that “I have managed to convince myself that the Three Laws are necessary and sufficient for human safety in regard to robots. It is my sincere belief that some day when advanced human-like robots are indeed built, something very like the Three Laws will be built into them” (1954, p.326–7). However, as is implicit here and made explicit elsewhere, Asimov believes that robots are “our intelligent tools” (1979, p.334). He writes, “I did not realize, at the time I constructed those laws, that humanity has been using them since the dawn of time. Just think of them as ‘The Three Laws of Tools’… No one ever cites these Three Laws of Tools because they are taken for granted by everyone” (1979, p.397). (See also, here.) He goes on to discuss whether the safeguards are sufficient to prevent harm befalling tool-users, to which the answer is unequivocally ‘No’. He writes, “If the Three Laws were perfect and unambiguous there would be no room for stories” (1987, p.362). (I might also add that he also considers his own Zeroth Law to be a work in progress (1987 p.362), hence not a formal part of the laws he makes his other claims with regards to.)
There are many criticisms of the Three Laws, which are generally obvious and not very interesting. The buzzword here is ‘ambiguity’ over how the terms specified in the Three Laws are carried out in the technology — which is always taken as a given in Asimov’s stories. Although Asimov says again and again in Robot Visions, ambiguity is not very interesting because it depends upon how successful an engineer is in carrying out the coding of the Three Laws. Asimov imagines the ‘positronic brain’ of a robot to have ‘potentials’ that sort this problem out in some vague way — he wasn’t interested in this because he viewed the Three Laws apply to toasters, dishwashers, kettles and so on by various means with the same result: good design can instil the Three Laws whatever the medium. As with the idea of autonomous vehicles, even if the brain is a black box, a design can always be improved to better encode the Three Laws, just as a fork can be improved to become better for eating with. The difference with robots, as with driverless cars, is that if you disagree with what the robot is doing, you can by-and-large just order the robot to stop doing it or do something else. Again, what Asimov’s Three Laws can show is how deep the paranoia of giving robots genuine decision-making power runs. But, nevertheless, I hope you can notice that there is a key difference between Asimov’s understanding of the Three Laws and the popular representation.
Asimov’s stories point to the same codification problem that we started with, but go further to suggest that despite the intractability of ethical/legal codification this does not prevent the Three Laws ensuring human safety. In Asimov’s stories (see), the Three Laws are generally effective but can occasionally lead robots to behave in ways that are deeply unhelpful and often very unsafe. But, the ultimate cause of such unsafe behaviour is either poor design in some unforeseen circumstance or poor usage by some human who gives an ambiguous or inappropriate command. As such, it is only when a human interferes with a robot’s autonomy that the robot becomes unsafe, i.e. only when, by virtue of the second law, a robot does not have autonomy do human beings get unintentionally put in the firing line (despite the first law superseding the second).
However, simply giving a robot autonomy does not yield itself as a solution. We, as human beings, can make sense of the Three Laws of Robotics despite of their ambiguity, in part because we adhere to the ‘spirit of the laws’ in a way that machines simply cannot. We know what the laws are for and can therefore be freer to interpret their meaning in different circumstances without getting trapped in one way of thinking. This implicit understanding can be made use of to ensure the Three Laws are used to bring about its intended goals. In 2010, a British research council (EPSRC) produced its Principles of Robotics that echoed Asimov’s own ideas in five short statements with the concept of the ‘spirit of the law’ (not the ‘letter’). The interesting thing about the EPSRC’s Five Principles is that they are not rules for robot behaviour but guides for researcher behaviour. To sum them up, the EPSRC working group established that humans, not robots, are the responsible agents.
The proposed roadmap to safe AI through regulating researcher behaviour (see).
The EPSRC’s approach may seem to be in line with Asimov’s thinking, but in fact Asimov is more extreme. Although Robot Visions (and elsewhere) does contain many passages linking ‘good’ human behaviour to the Three Laws, it is not as simple as saying that we humans should obey the Three Laws when designing machines. Even with the Second Law of Robotics, Asimov’s robots display significant freedom of action which means that a programmer is unlikely to be able to be omniscient about how the machine will interact in the world. The EPSRC approach seems to be conservative against researchers in punishing reasonable ignorance. If a judge were to decide on a researcher’s ‘reasonable responsibility’ for a robot that did something it shouldn’t have, the EPSRC ethical guide seems to be ‘on the side of the consumer’ in blaming designers for all their robots’ behaviour. With such guidelines, no researcher would want their AI to be released to interact with others in the real world for fear of legal challenge when someone inevitably misuses it. As Asimov argued, the Three Laws cannot come into effect simply as rules for researchers in robot design.
Asimov did frequently claim in Robot Visions (and elsewhere) that he thought that human beings should follow something very close if not equivalent to the Three Laws when making their own ethical decisions. In the first place, this would naturally extend into how a Three-Laws-Safe human being would construct a robot for safe human usage and, in this sense, the EPSRC have caught part of Asimov’s meaning but no his whole meaning. But there is a second part of Three Laws safety, which comes out in his later writing that revolves around how to define a human being: is humanity defined as a biological species or a state of mind? Radically, Asimov’s robot ethics extends the Three Laws to how humans and robots should reciprocally behave in the interests of each other’s well-being, rather than simply looking after human self-interest.
For a very brief summary of the entire Culture Series, see ‘Examined Worlds’.
This notion of reciprocal-binding of the Three Laws between humans and robots seems to be moving toward something more like Iain M. Banks’ Culture Series, where machines (called Minds) are free and autonomous agents that have a kind of symbiotic relationship with human beings. A Mind is, in almost all respects, treated like a human being. In Banks’ universe, you would sit down and discuss an ethical problem with a Mind rather than be concerned about ‘hard-coding’ some emergency protocol into its software. In this way, I think Asimov was heading toward this conclusion (especially in his Foundation Series). And, in a way, robot-as-human-being is a fitting conclusion as a kind of ethical tautology: in essence, distrust of humans and robots alike leads both robots and humans, for the good of everyone, to concede their ethical autonomy to the trustworthy power of the Three Laws.
So, I am claiming that Asimov was moving to treating the Three Laws as both for robots, researchers and really human/living beings in general (when appropriate generalisations are met). Grounding ourselves back in the present-day concerns of AI ethics, I would take this to suggest that, robots should be treated much like animals: at one end of the spectrum there are single-celled animals that are widely treated with disregard as a ‘tool’ of their owner, and at the other there are higher vertebrates like dogs and chimpanzees that are given much more ethical and legal weight independent of their ‘owner’. Although this does not resolve where on the spectrum an AI’s legal rights and ethical status should be, it does clarify the question of regulation because there is a pre-existing arena of legislation and ethical discussion on which to base future discussions. In animal research, welfare is central. But welfare concerns are largely restricted to species that display higher degrees of autonomy through self-awareness, which is something that machines have yet to demonstrate. In this regard, I would tentatively suggest that the Three Laws are not yet appropriate for machines because they could not comprehend them.
With this in mind, Asimov’s legacy would suggest that current discussions on ethics and law for AI could do with animalian grounding to avoid becoming irrelevant speculation that is divorced from questions that practically try to place an AI’s autonomy on a scale in the same way that animals have had to be housed within our ethical and legal frameworks (though the scale is certainly imperfect). For the time being, it would therefore seem that Asimov’s Three Laws are currently best interpreted as the EPSRC did, to treat robots as ‘tools’ of some owner/designer. This sentiment is echoed in the discussions of many practising AI researchers when they debate these issues. But, and what the bystanders like Sam Harris and Nick Bostrom are getting worried about is, we should expect and be prepared to update this stance in the future as higher levels of machine autonomy become possible.
Thanks for reading!

= Changing The Engineer’s Mindset : From How to Why =
Changing The Engineer’s Mindset : From How to Why

In a world inundated with data and overridden with information, where we discover what the weather outside is from an invisible cloud in the comfort of our homes; it is due time to take a step back and ask some important questions.
As a preface to this post, I’d like to mention that I am an advocate for technology. It would be foolish to think an idealistic world without technology efficiently solving our problems exists. That being said, with exponential growth comes unfathomable outcomes. Which is why awareness in today’s society is key. Who really is that mysterious green man behind the curtain of technology?
Introducing : The Engineering Mindset
When an engineer is given a problem to solve at work, the first question that comes to mind is typically how? How will I create this solution? What tech stack will I use? How performant should it be? Can this be optimized? Efficiency seems to be the topic of interest. The problem here though, is that no one is asking why. Why am I building this rocket? Why is my company funding war? Why can’t I share my work with my family?
James H. Moor, a pioneer in the field of Computer Ethics explains this problem succinctly :
“For years computers have been used to count votes. Now the election process is becoming highly computerized… During the last presidential election in the United States the television networks projected the results not only before the polls in California were closed but also before the polls in New York were closed. In fact, voting was still going on in over half the states when the winner was announced. The question is no longer “How efficiently do computers count votes in a fair election?” but “What is a fair election?” Is it appropriate that some people know the outcome before they vote? The problem is that computers not only tabulate the votes for each candidate but likely influence the number and distribution of these votes. For better or worse, our electoral process is being transformed.”
It’s time to shift the engineering mindset away from only asking how. Instead we must pause and ask why.
When should we be asking why?
I’d like to say we should always ask why, though in the interest of providing concrete guidance I’m going to discuss three realms of tech that we should be keeping a weary eye on.
Machine Learning Bias
Data Collection and Privacy
Artificial Intelligence
Beginning with Machine Learning Bias
This past year I contributed to data science research for fairness models. For a more extensive overview of this topic, I highly recommend Hanna Wallach’s article on Big Data, Machine Learning, and the Social Sciences.
What began for me as an introduction to data bias quickly evolved into an eye opening project about the dangers of machine learning. I was focused on fixing ML predictors for racial bias in the field of credit scores and loan approval. But I realized there was an even bigger systemic problem that couldn’t be overlooked.
This problem can be most clearly seen from ProPublica’s case study on Risk Assessments. In this incident, machine learning was being used to predict the likelihood of a criminal becoming a repeat offender. This likelihood was aggregated into a score called a Risk Assessment. These assessments are used all throughout the incarceration process, at times they are even given to the judge before their final decision.
The problem with this case was that the training data for these predictors was ridden with redundantly encoded bias and discrimination. It turns out that more African American offenders were being given unfairly high assessments when compared to their white counterparts. This machine learning predictor unintentionally perpetuated racist criminal injustice.
Imagine if the data scientists who created these predictors had taken the time to notice this problem. Focus is so highly geared towards optimization and accuracy for ML models, even though highly performant predictors can still perpetuate generations of discrimination.
It’s simply changing the question from, “how do I optimize this model’s performance?” To, “why am I creating this model?” If more thought was put into the purpose of the work, the social ramifications of the predictor might have not been so easily overlooked.
Data Collection and Privacy
With the GDPR in full swing, data privacy has become a touchy subject within the tech industry. It’s clear that processing someone’s data can lead to unintended consequences when there is a lack of privacy. Correlations and trends interpreted from inaccurate datasets can indirectly pigeon-hole people into a persona that they aren’t able to erase. [see: The Right to be Forgotten]
Data Privacy Officers are beginning to question the ethical implications of data processing and deletion, but there has yet to be a discussion surrounding collecting sensitive data in the first place. In a capitalistic society, it has become clear that company stakeholders have a higher interest in revenue than privacy. The GDPR became a game of compliance in the United States because tech giants were afraid of being fined. Data transparency for US citizens has historically been a PR stunt to accompany GDPR requirements for their European counterparts.
CIOs are beginning to worry about The California Privacy Act because they can no longer rely on US customer data to make up for the loss of privacy-exploitation-based revenue in Europe. The conversation that is missing involves a switch in motivation between compliance and principle.
Instead of asking, “how can we stop revenue loss with GDPR compliance?” We must ask, “why is our revenue dependent on sensitive data collection in the first place?”
Omitting the former and asking the latter would create a pause for executives. Perhaps sensitive data collection for ad-based revenue models isn’t the only solution. Maybe this would spark considerations for a subscription-based alternative without the need for data collection at all.
Artificial Intelligence
Artificial intelligence is an easy target for ethical debates, simply because robots have scared humans since they were nothing but a figment of the imagination. Uncanny valley is becoming a tourist destination as neural network research skyrockets. Terminator fears aside, the topic of interest here lies in the fact that labor automation is no longer a worry of the past. Robots are taking our jobs, creating a slippery slope towards a new world of unemployment.
As I listen to my colleagues discussing the newest robot chef or self driving car, I see their eyes light up with possibility. The engineers creating these ground breaking beings are so excited about a technological innovation, they don’t realize that they may be coding themselves out of an employable future. Right now Miso Robotics has created a burger flipping assistant, but in a few years those burgers could just as easily be architectural designs or lines of code.
Humans argue that robots could never replace the gentle touch of a nurse, or the creative soul of a musician. Unfortunately, they already have. Research is currently underway for Simulation Theory of Mind, a push towards robotic empathy. A groundbreaking attempt to help robots understand humans better. This is a perfect example of a misuse of the engineering mindset.
I’ve asked why, and I liked the answer… is that enough?
Let’s take a closer look at the Simulation Theory of Mind. This theory if correctly put into practice would allow robots to predict the anticipated needs and actions of the people that surround them. AI equipped with SToM would theoretically be able to understand humans better, by simulating their possible actions before predicting a best way to help them. This paper explains how robots would even be able to explain their reasoning behind decision making. A notion that has become increasingly difficult as neural networks have become more complex.
This research seems to be ethically positive. Giving robots an ability to empathize with humans and providing transparency for their decisions sounds too good to be true! This theory allows robots to understand us and, more importantly, allows us to understand them.
Now think about jobs where empathic decision making is instrumental to success. Automation was never a worry for these fields because robots weren’t able to place themselves in a human’s shoes. With simulation theory of mind, empathic employment is no longer guaranteed for humans.
It’s clear this research was an attempted solution for ethical concerns with AI. But the engineering mindset took over, and negative implications that were deeply buried could easily surface within the near future. Moor even believes that labor automation will likely redefine our entire notion of work.
“Traditional work may no longer be defined as something that normally happens at a specific time or a specific place. Work for us may become less doing a job than instructing a computer to do a job. As the concept of work begins to change, the values associated with the old concept will have to be reexamined.”
Conclusion
The era of technological expansion in the 21st century has been defined by how. Humans have pushed the boundaries for automation, prediction, and robotics. We’ve created a world where startups can only survive if they seek revenue before ethics. Engineers are being given jobs and creating optimal solutions before even realizing the implications of their own work.
A systemic change is long overdue. Sitting idly by will only perpetuate problems that we are already spiraling towards. As engineers, it is time to stop asking how we can create. Saying, “if I don’t do it, someone else will” is an excuse. Instead, take a moment to pause and think. Really think. Then ask your boss, your colleagues, and most importantly, ask yourself :
why?

= How Far Are We from a Fully Autonomous Driving World? =
How Far Are We from a Fully Autonomous Driving World?
You can find me on Twitter @bhutanisanyam1, connect with me on Linkedin here
Here are my Garage Projects
Here and Here are two posts on my Learning Path to Self Driving Cars
Source: Business Insider
The MIT Deep Learning for Self-Driving Cars course just released their First lecture video (alternatively Here are the lecture notes if you want a quick read)
The Lecture is an overview of Deep Learning techniques and has some discussions on the future of Self Driving Tech as well, and a brief warning about the Gaps of current systems.
Here is the take on how far away are we from an Autonomously Driven Future and a brief touch on Ethics:
By 2017, Computer Vision has reached 97.7%+ accuracies! (ImageNet challenge) Amazing isn’t it?
So how far are we from a fully autonomous World?
97.7% sounds good enough. Is it?
After all driving involves a lot of Computer Vision and it is indeed better than human high scores-so are we close?
The ImageNet Challenge involves classifying 14M images into one of 22,000 possible classes.
How good is this accuracy when its extrapolated to the real world?
Now yes, the classes involved in the challenge wouldn’t all be involved in the SDC scenario but they do point out to one thing, Computer Vision, although it’s more accurate than Humans now, is still not perfect. It isn’t a 100% accurate.
That coupled with the dynamics of the real world suggest that there is a small chance of the Autonomous systems behaving in unexpected ways. Would you trust the system completely in under all scenarios? To handle every situation better than a Human Driver?
The argument made in the lecture is that SDCs as of now will work as tools that would help us drive better. They might even drive better than us, but at points, Humans would need to intervene. We have a Semi-Autonomous year as of 2018.
Roughly, in 90% of the Cases, the Car will drive itself better than us, but for the remainder of 10% cases, Human intervention/control would be required.
A 100% accuracy would have a universal approval, which would require a generalisation over all the unexpected cases too, for example: A situation of 1 in a Million, where a Deer would cross the road and the situation has to be handled.
Lex Fridman argues in the Lecture that the ‘perfect system’ would require a lot more research efforts and increase in Efficiency of Deep learning algorithms. Which as of now are highly inefficient as well (Computationally speaking).
By the perfect case- I’m not referring to the case that a car that can drive itself. The perfect case is where, we’d be so confident about the systems that we would no longer have steering wheels in our vehicles. That human driving would be considered more dangerous than the automated one.
Till then SDCs would definitely appear on the road, we might not have to hold the steering wheels for long durations, no doubt. But there will definitely be moments when Human control would be required. Hence, the term Semi-Autonomous.
A brief mention of Ethics with Respect to Reinforcement learning is done:
Qouting an example of Reinforcement Learning- Reinforcement Learning involves set of algorithms where the AI (agent) learns itself to maximise defined goals such that a Maximum Reward is achieved. Here is a Primer for an abstract overview.
Many Times, the System (agent to be Term Specific) behaves in ways that are completely unexpected. (which are better at getting results).
Coast Runners. Source: Lecture Notes.
The example of Coast runners from Lecture: where You and I’d probably play to race and collect Green Circles. The Agent figures Discovers local pockets of high reward ignoring the “implied” bigger picture goal of finishing the race.
The Cutting Edge AlphaGo and AlphaGo Zero Systems have already proved this, by performing moves in the Game of Go that were surprising to Human experts.
So What if we want to go from A to B in the fastest manner, and The SDC decides to take an approach/path that isn’t expected? (I understand that the Traffic rules are well coded into the core systems-but that doesn’t allow us to overlook the possibility)
Given the outcomes can be unexpected, we would definitely need to keep a check on the system.
Source: Lecture 1 slides.
Also the robustness of the Vision Systems is questionable. Here is an example of how adding a little distortion to the image, easily fools the State of the art ImageNet-Winning models.
So, Finally: Are SDCs here? With Voyage Deploying Self Driving Taxis in Villages; GM Motors testing their massive production vehicles?
Yes and No. Yes, we have Semi-Autonomous vehicles around us. But A fully Autonmous world is still a little away. One where the cars would not have a Steering wheel at all. A few years- maybe a few decades.
We have a Semi-Autonomous Present (or Future) that is starting to come into shape.
I believe as of now, the SDCs will work well in ways such as the Guardian Mode as showcased by Toyota.
The Machine takes control at points where a human might not be able to act promptly, for example: When the vehicle in front of your crashes and a decision needs to be made in a mili-second. Or in bad weather conditions where the vehicle can ‘see’ better than humans, thanks to the Sensors (RADAR in this case) on board.
On the other hand, when the situations are complicated — The Driver would take control.
On highways, you could turn on the AutoPilot, read a newspaper, play a game. But during complicated situations, a human control would be required over the Autopilot systems.
Subscribe to my Newsletter for a Weekly curated list of Deep learning, Computer Vision Articles

= Legislation and Ethical Guidelines for Intelligence Technologies =
Legislation and Ethical Guidelines for Intelligence Technologies
Principles for a more enlightened and civilized society

Recent technological advances to augment human intelligence (aka Intelligence Amplification or IA) can potentially allow us to make our cities and citizenry smarter than ever. However, their corruptive and disruptive impact on health suggests the information technology (IT) industry must establish an ethical framework to ensure our future generations get the most from life. To mitigate risks, a number of organizations have introduced various codes of ethics. Despite this positive move, most codes focus on enabling public access to data and professional integrity to the exclusion of all else. While both domains are important, we argue that they do not nurture the kind of intelligences humanity needs to thrive and prosper. To address these blind spots, this paper draws on recent evidence that three human factors (chronobiology, collaboration, creativity) are vital to humanity’s future, and that harnessing them will ensure our IT professionals design more life-supporting systems. The 3 “Laws” presented as Legislation and Ethical Guidelines for Intelligence Technologies (LEGIT) aim to stimulate critical debate on the subject and nudge the sector to take practical and meaningful action.
The future of AI
The idea of artificial intelligence or AI has been around since the 1956 summer workshop at Dartmouth University. The workshop was convened by John McCarthy, who coined the term “artificial intelligence,” and attended by a raft of AI pioneers including Claude Shannon, Herbert Simon, and Marvin Minsky. This seminal event defined AI as a set of methods that could provide machines with the ability to achieve goals in the world. Attendees of the workshop believed that, by 2001, computers would implement an artificial form of human intelligence (Solomonoff, 1985). Recent advances in neural networks modelled on the human brain have resulted in striking breakthroughs, most of which involve a machine learning technique known as deep learning. Deep learning uses a photograph’s pixels as input variables to predict variables without needing to understand underlying concepts, just as standard regression model predicts a person’s income based on educational, employment, and psychological stats. Such algorithms are now found to beat humans at games of skill, master video games with no prior instruction, 3D-print original paintings in the style of Rembrandt, grade student papers, cook meals, vacuum floors, and drive cars (Guszcza et al., 2017).
Due to more effective algorithms, computing power, data capture and storage, real-world AI applications have exploded in the last decade. AI systems are already built into everyday technologies like our mobile devices and voice-activated personal assistants to help us manage various aspects of our lives. AI is also being used within legal, financial, and workplace sectors to predict behaviours map leisure preferences (Campolo et al., 2017). In addition, thousands of digital health apps are being developed to help track our daily activities and prompt us to make healthier lifestyle choices (Topol, 2015). The problem is that AI algorithms only work when data used to train them are sufficiently reflective of the environment in which they are used. In other words, when routine tasks can be encoded in big data sets, algorithms have become brilliantly adept at outperforming humans. Yet when given a more novel task that requires conceptual reasoning, even the most powerful AI still cannot learn as well as a five-year-old does (Gopnik, 2017). This is because AI is founded on computer-age statistical inference — not on an approximation or simulation of what we believe human intelligence to be (Efron and Hastie, 2016). This kind of narrow type of machine learning is far from the vision outlined at the Dartmouth workshop in 1956, or indeed expressed in AI fictional characters such as HAL 9000 in Kubrick’s 2001: A Space Odyssey.
AI’s narrow machine learning is also doing very little to augment our own cognitive capacities. Recent government drives towards automation has meant people increasingly work and live in a 24-hour Society. These 24-hour lifestyle changes have placed huge demands of flexibility on the human body (Kreitzman and Foster, 2011). Instead of living diurnally (active in the day, resting at night) people are living in an always-on “now,” where priorities of the present dominate. Living in this state of what Douglas Rushkoff calls “present shock” means people have developed a distorted relationship to time. Financial traders no longer invest in futures but instead expect profits from computer algorithms. Citizens have no historical sense of how their governments function and demand immediate results from representatives. Children txt during an event to find out if there’s something better somewhere else (Rushkoff, 2013).
This is not to say that the idea of AI does not have great potential. Automating mechanical tasks has transformed society for millennia and is likely to continue to do so into the future (Innis, 2004). What needs to be carefully considered are the practical ramifications of 24/7 AI systems on people and society. Mobile phones are already negatively impacting the mental health and wellbeing of children (Carr, 2011). Meals consumed at night increase our risk of heart disease. Long-term shift work is sparking a raft of reproductive problems such as risks of miscarriage, retarded foetal development, and spontaneous abortion. Sleep loss is also triggering an epidemic in obesity, gut disorders, and drug addiction cycles as people try to maintain regular function (Kreitzman and Foster, 2011). Increased work-related accidents, number of sick-days taken, and family and marital stress are just some of the factors that will negatively impact our ability to succeed in the coming decades.
The reason AI systems are so damaging is simple. Unlike algorithmic systems we create to optimise work functions, humans are not computers that run software programs 24/7. We need vital environmental cues to synchronize our body’s biological rhythms to the Earth’s daily and annual cycles. When cues are disrupted due to erratic behaviors (disrupted eating and sleeping), we get ill. As neuroscientist Russell Foster explains:
“All of us in the developed world now live in a ‘24/7’ society. This imposed structure is in conflict with our basic biology. The impact can be seen in our struggle to balance our daily lives with the stresses this places on our physical health and mental well-being. We are now aware of this fundamental tension between the way we want to live and the way we are built to live”.
Figure 1: Intelligence Amplification (IA)
It’s becoming increasingly clear the most promising AI applications are not in algorithmic machines that authentically think like humans, but in harnessing technologies to enable human and computers to think better together, a field called Intelligence Amplification (IA) (Figure 1). IA has huge potential to allow us to make our cities and citizenry smarter than ever. However, recent developments are sophisticated enough to pose great risks if placed in the wrong hands, whether they be corrupt governments, corporations or both as is the case in 21st century politics (Müller and Bostrom, 2016). To mitigate risks, we must establish some ethical guidelines about how the use and deployment of technology can create a more enlightened and civilized society (Berman and Cerf, 2017).
Indeed, ethical guidelines for IT professions have already been established in some but not all countries. Dr. Eike-Henner Kluge authored 11 principles for the American Health Information Management Association (AHIMA) which have been adapted by the British Computer Society (BCS) and UK Council for Health Informatics Professions (UKCHIP). The European Federation for Medical Informatics (EFMI) does not explicitly state any code, but is a member of the International Medical Informatics Association (IMIA) (Samuel and Zaiane, 2014).
Despite various adaptions, all codes converge around four key principles:
Public Interest (i.e., the need to maintain regard for public health, privacy, security and wellbeing of others and the environment; and to promote inclusion and equal access to IT)
Professional Integrity (i.e., the need to undertake work that reflects professional competence; continue to respect, develop, and share knowledge; and to comply with legislation)
Duty to Relevant Authority (i.e., the need to carry out professional responsibilities with care and diligence, in accordance with the Relevant Authority’s requirements)
Duty to the Profession (i.e., the need to accept personal duty to uphold the reputation of the profession and not take any action which could bring the profession into disrepute)
Wearable computing pioneer Steve Mann has also spent many years developing a code of ethics on human augmentation which has resulted in three fundamental “Laws”. These include: (i) the right to know when and how you are being monitored in the real and virtual world; (ii) the right to monitor the systems or people monitoring you and use that information in crafting your own digital identity; and (iii) the user should be able to understand the world they are in immediately (Mann et al., 2016).
While the above codes are an important first step toward mitigating risks of human enhancement and AI, the challenge is they focus on enabling public access to data and professional integrity to the exclusion of all else. While both factors are necessary, they do not nurture the kind of intelligences humanity needs to thrive and prosper. To address these blind spots, this paper draws on recent evidence that three human factors (chronobiology, collaboration, creativity) are vital to humanity’s future, and that harnessing them will ensure our IT professionals design more life-supporting systems. The 3 “Laws” presented as Legislation and Ethical Guidelines for Intelligence Technologies (LEGIT) aim to stimulate critical debate on the subject and nudge the sector to take practical and meaningful action.
Law I: Protect chronobiology
All technologies must provide humans with 24-hour temporal reference points to help them measure their progress, ambitions, and actions (Figure 2). Integration of temporal factors in technologies will remind humans they exist in a physical body, and that circadian clocks, which display 24-hour periodicity, control nearly all biological patterns, including brain-wave activity, sleep-wake cycles, body temperature, hormone secretion, blood pressure, cell regeneration, metabolism and behaviour (Kreitzman and Foster, 2011).
During working hours, humans have a basic right to know when and how organizations are tracking their chronobiology, and reciprocally monitor the chronobiology of organizations. During evenings, weekends, and holidays, humans have the right to disconnect from being monitored, and reconnect with people and groups that matter to them, such as family and friends. All human monitoring and communication must be limited to working hours to support optimal sleep/wake cycles and longevity (Kreitzman and Foster, 2011).
Figure 2: Protect 24-hour human chronobiology
Law II: Integrate collaboration
Smart cyber-physical systems offer humans the ability to create and share goods at near-zero marginal cost (Rifkin, 2014). This post-capital shift to what some call the “the sharing economy” or “zero marginal cost society” is estimated to be worth $4.5 trillion by 2030 (Lacy and Rutqvist, 2016). To maximise the potential of this shift and overcome current challenges, organizations will need to reward creative collaboration between citizens and incentivize sustainability (Rifkin, 2014, Lacy and Rutqvist, 2016).
To achieve this, future technologies must integrate radical human collaboration into every stage of the development cycle (Figure 3). Prioritizing creative diversity will ensure technologies are less contaminated by cognitive bias, which will boost human skills and knowledge to result in breakthrough innovations (Page, 2008). Diverse collaboration will also ensure systems are systemic in nature, addressing root causality of problems rather than changing parts of the whole (Snowden and Kurtz, 2003).
Figure 3: Integrate human collaboration at every development stage
Law III: Nurture creativity
The highly desirable metatrait of creativity (aka social effectiveness) is central to determining human physiological, reproductive, and socioeconomic success (Rushton and Irwing, 2011, Cloninger, 2013, Musek, 2007). The three underlying traits that give rise to creativity have various labels, however they tend to reflect common characteristics related to Dynamism (self-expression, openness), Emotionality (self-awareness, self-transcendence), and Stability (self-efficacy, self-regulation).
For humans to thrive and prosper, technologies must nurture creative adaptiveness (Figure 4), to ensure everyone can reap its physiological, reproductive, and socioeconomic benefits (Rushton and Irwing, 2011, Cloninger, 2013, Musek, 2007). Nurturing creative adaptiveness across all levels of society also has the potential to solve many of the 21st century’s most complex problems (De Beule and Nauwelaerts, 2013), and thus mitigate some of challenges posed by AI (Brundage, 2015).
Figure 4: Nurture human creative adaptiveness traits
Technologist Pledge
As technologist and member of the technology profession:
I WILL RESPECT & MAINTAIN the health, autonomy, and dignity of people and communities;
I WILL PRACTICE in accordance with the 3 Laws outlined in the LEGIT to maximise outcomes in human chronobiology, human collaboration, and human creativity;
I WILL NOT PERMIT considerations of age, ethnicity, gender, nationality, sexual orientation, or any other factor to intervene between my collaborative work with people;
I WILL ATTEND TO my own health and abilities to ensure my work is of the highest standard;
I WILL NOT USE my technological knowledge to violate human rights, even under threat; and
I WILL RESPECT & SHARE knowledge for the betterment of people and technology.
Grant Munro is director of London’s Digital Health Advisory Board and honorary academic at the National Institute of Health Innovation, University of Auckland, New Zealand. He is cofounder of the Innovation Party, Britain’s first political movement dedicated to fostering agile governance through peer-to-peer networks. His health blog Luman.life focuses on charting frontier advances in digital health to help people get the most from life. He can be reached at Medium, Twitter, Facebook, GrantMunro.com or via email at hello@grantmunro.com.
Originally published on Luman.life
…
References
BERMAN, F. & CERF, V. G. 2017. Social and ethical behavior in the internet of things. Communications of the ACM, 60, 6–7.
BRUNDAGE, M. 2015. Taking superintelligence seriously: Superintelligence: Paths, dangers, strategies by Nick Bostrom (Oxford University Press, 2014). Futures, 72, 32–35.
CAMPOLO, A., SANFILIPPO, M., WHITTAKER, M. & CRAWFORD, K. 2017. AI Now 2017 Report. AI Now Institute at New York University.
CARR, N. 2011. The shallows: what the Internet is doing to our brains, WW Norton.
CLONINGER, C. R. 2013. What makes people healthy, happy, and fulfilled in the face of current world challenges? Mens Sana Monographs, 11, 16.
DE BEULE, F. & NAUWELAERTS, Y. 2013. Innovation and creativity: pillars of the future global economy, Edward Elgar Publishing.
EFRON, B. & HASTIE, T. 2016. Computer age statistical inference, Cambridge University Press.
GOPNIK, A. 2017. Making AI more human. Scientific American, 316, 60–65.
GUSZCZA, J., LEWIS, H. & EVANS-GREENWOOD, P. 2017. Cognitive collaboration why humans and computers think better together. Deloitte Review.
INNIS, H. A. 2004. Changing concepts of time, Rowman & Littlefield.
KREITZMAN, L. & FOSTER, R. 2011. The rhythms of life: the biological clocks that control the daily lives of every living thing, Profile Books.
LACY, P. & RUTQVIST, J. 2016. Waste to wealth: the circular economy advantage, Springer.
MANN, S., LEONARD, B., BRIN, D., SERRANO, A., INGLE, R., NICKERSON, K., FISHER, C., MATHEWS, S. & JANZEN, R. 2016. Code of Ethics on Human Augmentation. VRTO Virtual & Augmented Reality World Conference + Expo.
MÜLLER, V. C. & BOSTROM, N. 2016. Future progress in artificial intelligence: a survey of expert opinion. In: MÜLLER, V. C. (ed.) Fundamental Issues of Artificial Intelligence. Cham: Springer International Publishing.
MUSEK, J. 2007. A general factor of personality: evidence for the Big One in the five-factor model. Journal of Research in Personality, 41, 1213–1233.
PAGE, S. E. 2008. The Difference: how the power of diversity creates better groups, firms, schools, and societies, Princeton University Press.
RIFKIN, J. 2014. The zero marginal cost society: the internet of things, the collaborative commons, and the eclipse of capitalism, St. Martin’s Press.
RUSHKOFF, D. 2013. Present shock: when everything happens now, Penguin.
RUSHTON, P. & IRWING, P. 2011. The general factor of personality: normal and abnormal. In: CHAMORRO-PREMUZIC, T., VON STUMM, S. & FURNHAM, A. (eds.) Wiley-Blackwell handbook of individual differences. Wiley-Blackwell.
SAMUEL, H. W. & ZAIANE, O. R. 2014. A repository of codes of ethics and technical standards in health informatics. Online J Public Health Inform, 6, e189.
SNOWDEN, D. & KURTZ, C. F. 2003. The new dynamics of strategy: sense-making in a complex and complicated world. IBM Systems Journal, 42, 35–45.
SOLOMONOFF, R. J. 1985. The time scale of artificial intelligence: reflections on social effects. Human Systems Management, 5, 149–153.
TOPOL, E. J. 2015. The patient will see you now: the future of medicine is in your hands, Tantor Media.

= Our Cars Will Soon Make Life And Death Decisions. Will We Agree? =
Our Cars Will Soon Make Life And Death Decisions. Will We Agree?

We recently bought a new car, loaded with all of the latest safety features. It’s not a Tesla and doesn’t drive itself — but when the safety features activate, the car pretty much takes over, braking quickly or even steering the car back into its lane in the event that the driver dozes off and swerves. Exciting stuff, and not hard to imagine a future when we can just sit back and let our cars drive us from Point A to Point B.
Not long ago I ran across this Ted Talk by Iyad Rahwan, a computational scientist at MIT Media Lab, and have been thinking about it lately— both in light of our new purchase, and because I spend time with many founders building companies powered by AI.

In his talk, Rahwan lays out two moral options which programmers of AI-powered autonomous cars will need to consider — because without a doubt, these cars will need to follow some set of predetermined ethics. The scenarios described are inspired by two famous philosophers: Jeremy Bentham, and Immanuel Kant.
Bentham’s philosophy suggests that in the event of a life and death scenario, an autonomous car should follow utilitarian ethics and minimize total harm — even if that action will kill a pedestrian, and even if that action will kill the passenger (depending on number of pedestrians vs. number of passengers). Kant’s philosophy would suggest that the car should follow duty-bound principles such as “thou shalt not kill” (applied to the passenger), even if that means harming more people.
It is fascinating and offers a glimpse of the myriad moral dilemmas that innovation in artificial intelligence will soon present. I highly recommend watching it. And I’d be curious to know your preferred set of ethics: Kant, or Bentham?

= I think, therefore I am =
I think, therefore I am
This piece was originally publish for online magazine The Coven in November, 2015.
The thermostat is a large cause of contention in every Irish household. Even as I write this, I’m thinking in my head ‘Oh shit, did I turn it down before I left this morning?’. But, what if I was to tell you that you and thermostats have a lot in common? Other than the fact you both start the majority of your parents’ fights, there is a more subtle and interesting intersection: you are both agents.
An agent in AI is considered to be anything that senses the world around it and changes the world accordingly. So, a thermostat is an agent. If it senses the room being too hot, it plans the best way to return to the goal temperature and then reacts by reducing the heat. Observe, plan, react.
And we, in essence, are agents. A human may see a car coming towards it. It recognises the danger in this scenario. It plans it’s route to get out of the way, and it reacts by jumping to safety. Observe, plan, react
We both follow this pattern of observing, planning and reacting infinitely from our creation to our termination.
True, the planning stage is fundamentally more complex for us. The thermostat has ‘one job’ with no major consequence, no need to remember anything and no need to think ahead. It just acts. This is because a thermostat is purely reactive.
We, however, are both reactive AND deliberative agents. We think ahead and consider how different actions may have different outcomes and we choose the most successful one. We do this by using things that we have learnt, things we remember. We remember that a car is something that could hurt us, and we recognise the speed it is going at as a rate that would be fatal upon impact. We consider what actions we could take, recognise the most optimal outcome, and engage with the corresponding action.
Cleverbot was massively popular and slightly eerie a few years ago.
It was a website where you could have a conversation with what seemed to be a real person online. The technology behind this was also an intelligent agent which had an extra, more human like layer. It remembered conversations it had with other users, and applied the words, sentences and phrases they used to respond to other users’ questions — remembering, learning, considering and forming its reaction based on this.
What is interesting is the way agents like this learn in the first place. Unsupervised learning algorithms are often used, where the agent is given a base set of data which it uses to essentially teach itself what is right or wrong, based on the response given by the user interacting with it. Kind of like us.
The way this is laid out makes it sound like its done in a linear manner. However, machines do all of these steps simultaneously, often reacting faster than humans can as they are currently not clouded by any ‘human’ factors. They don’t have any emotions that make them paralysed with fear. No ‘lives flash before their eyes’, no ‘off days’. They just step aside and get on with their mission to observe more, plan more and react more.
This raises a huge question in the world of, but not restricted to, Artificial Intelligence, Psychology and Philosophy. Although agents nowadays are so massively sophisticated and can do the same tasks as us to the same level, if not higher, should they be allowed awareness of how these events will effect them, as agents? Should we be allowed create intelligent agents that are every bit as responsive as us, if not more, minus any free will? Should a conscious agent own it’s body?
I’m sure none of us see our emotions or sub conscious to be ‘drawbacks’, but when designing an agent for a specific task, it most certainly is. Many people think that AI developers are working hard to make robots that mirror humans, but this is not the case. In fact, this would largely be considered a huge waste of time. We already have an extremely efficient way of reproducing humans, so why bother? And plus, we could do BETTER. We could do SMARTER. We could do MORE OBEDIENT.
Which is where is gets complicated.
When designing a robot to go into a coal mine to detect toxic gases, we don’t want it to turn around and refuse because they are scared. When designing a space agent, we don’t want it to decide against take off because space seems like a lonely place. When designing a sex robot, we don’t want it to say no. In essence, they are designed to say yes to what most humans would refuse.
And one of the big questions that has been raised is how ethical this really is.
Ethical not only for the robot itself (machine ethics) but for the person creating it (roboethics) and the impact it could have on our social norms. If something goes wrong, who is to blame? If we give an agent a level of consciousness, will it have a right to itself or a right to say no? If we treat agents in a way where we demand them to agree to everything we request, is it unreasonable to assume that behaviour will spill out into our interactions with humans too?
Delving beyond this, a robots’ entitlement to it’s own body is something that could also become an issue. Surely if something can think for itself, it should be held responsible for it’s actions and the consequences that follow.
The Turing Test, named after Alan Turing whom the film “The Imitation Game” is based on, was created decades ago when computers were on the cusp of the level of intelligence we see now. Turing foresaw the issue we are facing — deciphering whether a machine was really thinking, or just giving the impression that they were thinking. And in June, 2014, Eugene Goostman passed the Turing Test.
Eugene Goostman was a chatbot. Eugene Goostman was perceived to be indistinguishable from a human.
Of course, there’s sceptics about how Eugene successfully passed the Turing test. They claim that the program based its’ persona on a teenage boy who spoke a different language and it was the errors in language translation that confused the judges into believing it was ‘thinking’ and not ‘calculating’ a response.
Even if it was all confusion, we must accept that this machine was intelligent enough to take on the character of a teenager to induce that confusion to buy itself time. And whats more human than copying someone else’s persona to confuse people into acceptance?

= Designer Ethics & The Moral Implications of our Apps =
Designer Ethics & The Moral Implications of our Apps

Lawyers, doctors, and even journalists have something in common. They all have studied ethics as part of their higher education. They’ve taken the time to construct, interpret, and follow written code of conducts that guide us in making good, ethical decisions. Is it a coincidence that these are also some of the world’s oldest professions?
The design profession has a long history too, but it bothers me that there are hardly any discussions on ethics surrounding it. Design is all about viewer; drawing the eye and changing the heart. It’s truly one of the most powerful tools (superpowers?) that companies have today and that leaves us — the viewers — wanting… no, needing awareness and guidelines that ensure design is being used responsibly. With all the services and apps influencing us, is this too much to ask?
What are ethics?
Ethics can be a tricky subject to approach. Simply said, ethics are an agreed up set moral principles (rules) within a profession/community leading to consistency in behavior and conduct. Looking at those older professions we talked about earlier shows how complicated it can be, but how necessary it is.
Lawyers are always required to work in the best interest of their client. Not themselves or what they personally think is “right”. It’s a keystone of our society — the right to legal representation and a fair trial.
Journalists — who we might think are just out to sell magazines (e.g get clicks) — are guided by a code of conduct as well. People aren’t to be judged before they’re convicted and details are to be spared when there’s children involved for instance.
Medical doctors have an ethical duty to protect the human rights and dignity of their patients — some of the most vulnerable members of our society. Doctors are guided by a common framework of four principles:
Respect for autonomy — the patient has the right to refuse or choose their treatment.
Beneficence — a practitioner should act in the best interest of the patient.
Non-maleficence — to not be the cause of harm. Also, “utility” — to promote more good than harm.
Justice — concerns the distribution of scarce health resources, and the decision of who gets what treatment (fairness and equality).
On top of those, they are required to have:
Respect for persons — the patient, family, and practitioner (may or may not be a doctor) have the right to be treated with dignity.
Truthfulness and honesty — the concept of informed consent has increased in importance since the historical events of the Nuremberg Trials and Tuskegee Syphilis Experiment where physicians deceived patients.
The time of “move fast and break things” is over.
I can’t begin to understand why there’s no common framework for designers. Everyone can agree that the products we create have a major impact on society and our individual social identities, but there is very little to no open discussion on the implications of those products.
Google’s recent demonstration of Duplex and the discussions that followed are surely a first step, but one has to wonder how that feature even got featured on the main stage? It’s clearly a problem if Google — one of the world’s largest companies and drivers of technical advancement — are able to design and build a feature like that without raising any critical questions internally?
Don’t get me wrong, I’m all for indicatives like the Time Well Spent initiative and Apple’s newly launched Screen Time. My concern is that we can’t just keep building new tools to bandaid what we shouldn’t have broken in the first place. We use our devices too much, yes, but is another app really the solution for it? I believe it’s time for our industry to stop trying to fix the problems and start acting more pro-actively at not creating problems in the first place. The time of “move fast and break things” is over.
Facebook doesn’t need regulation; it may help, but regulation alone won’t fix the problems. Facebook needs moral guidance. In both hearings on Capital Hill, Mark Zuckerberg reiterated that this was not what he had in mind when he created Facebook in his dorm room. I’m sure it wasn’t, but here we’re treating Facebook as if it’s something people in dorm rooms all over the world would still like to create. Our industry needs to learn a lesson. Mark may have created the Facebook platform, but he didn’t create ALL the problems. We, the designers, did the rest. Someone had to create the third party apps that stole our private data. We were failed by those who valued themselves over their users.
“The superior man understands what is right; the inferior man understands what will sell.”― Confucius
A code of conduct for designers?
Instead of focusing on holding ourselves to higher ethical standards, we just come up with other names for bad behavior. We give it a cool name like ‘dark patterns’ and accompany it with a picture of Darth Vader in-front of a computer. Awesome, right? How about we just call ‘dark patterns’ what they truly are — bad design and bad ethics. It’s dishonest and it’s short sighted.
While the GDPR may not be perfect (and again, I don’t think legislation alone will solve this), but I think it’s an important first step. Let’s take a look back at the ethical duties of doctors and try to make it fit designers. It’s surprising how easily the principles can be adapted:
Respect for autonomy — the user has the right to refuse or choose what content they are presented with. No one has the authority to bypass this without penalty. This is what GDPR aims to do.
Beneficence — the app/service should act in the best interest of the user. No more dark patterns. No more use of user data without consent.
Non-maleficence — to not be the cause of harm. Also, “utility” — to promote more good than harm. Essentially Google’s original ‘Don’t do evil’ promise. No designer can intentionally cause harm or diminish the value of the user. Google is currently under some heavy fire with employees leaving because of it’s affairs with the US Military.
Justice — concerns the distribution of resources, and the decision of who gets what service (fairness and equality). All users are equal and all content providers are equal. Balanced access for all (read: net-neutrality).
We’re just seeing the beginning of AI concerns and understanding the need for ethics in the industry. As voice assisted user interfaces bring up entirely new scenarios, we need to think about what kind of technology we want in our lives. We need to have the choice to opt out of those Google Duplex calls and to Alexa and Google home listening when they shouldn’t. If we all, as designers, agree to ethical standards, situations like this wouldn’t even be an issue. We’d all ask ourselves, “just because we can, does that mean we should?”
While it’s still fiction, the dilemmas of a Westworld similar experience is not as far away as we’d like to think. If you’re not familiar with Westworld, I highly suggest you have a look. Briefly put, Westworld is a series about an AI-powered reality housed in a big theme park set in the Wild West where visitors are allowed to live out their wildest dreams. The habitants of this world are all robots that look, act, and respond in a very lifelike way — just like the Google Assistant in the Duplex call. What’s interesting about the show though is the philosophical questions — what really distinguishes us from the robots?

Update: And just like that, Google has released a statement that pretty closely aligns with everything we talked about here. It’s true that there’s a big difference between making a statement and applying it to day-to-day business, but I see this as a good sign. Hopefully other designers of AI platforms take a cue from this statement and take responsibility.
Uncle Ben from Spiderman said it best, “With great power comes great responsibility.”
Originally published at www.antonsten.com.

= What is a data scientist anyway? =
What is a data scientist anyway?
Maybe you’ve heard the term data science here or there in the news, or mixed into conversations about the job market. Whenever I mention I’m studying data science, the response is usually “Good for you! Don’t they say that’s one of the fastest growing fields these days?”
But what is a data scientist? And who is the “they” reporting skyrocketing growth?
Let’s begin by what is a “science”? According to the Oxford English Dictionary:
“the intellectual and practical activity encompassing the systematic study of the structure and behavior of the physical and natural world through observation and experiment.”
Science is the systematic study of some aspect of the world through observation — in the data science case, these observations are the data that is collected. Data science has methods and involves experimentation to derive results, just like any other science. In fact, it’s is often used in combination with other scientific disciplines because of its power to use statistics on large and complex data sets.
Becoming a scientist requires a lot of training — so what sort of training or knowledge is required for a data scientist? Data science spans a wide range of skills, summarized in the image below, encompassing capacities in statistics, coding, communication and business problem solving.
Source: https://blog.zhaw.ch/datascience/the-data-science-skill-set/
Another useful distinction if what a data science is not. While there may not be a canonical definition, a data scientist is different from both software engineer and a data analyst.
One breakdown is as follows:
Software Engineer: creates systems to best captures and store data, making it easy to access for other to use
Data Analyst: use data to diagnose business problems or convey data-drive narratives to solve business problems
Data Scientist: use large amounts of data to create models that predict future outcomes
Obviously, there can be a lot of overlap, but these roles are different and require some level of differentiated expertise.
And, finally, on the topic of data science as one of the fastest growing jobs, “data science” comes in second only to “Machine learning engineer” according to a LinkedIn 2017 U.S. Emerging Jobs Report. Of course the #1 job also involves data science, a possible hybrid between the Data Scientist and Software Engineer categories I broke out above. As the article states, “comprehensive sets of skills that cover multiple disciplines are seemingly in higher demand”, so it could be part of the trend toward jobs that require a broad range of skills, as well as technical ones.
Like any good data scientist I was curious where Linked got the data for study, and, as stated in their “Methodology” section, the study is based on user-provided data, so the titles are not necessarily strictly defined. In any case, technical skills and those of data scientists are great skills to have now and in coming years.
Ok, so it’s a great day to be a data scientist — but aren’t those the people behind the annoyingly accurate ads of what I just purchased on Amazon and what’s under fire for bias when an algorithm is used to help determine prison bail? These are just a taste of crucial ethical considerations that I hope to highlight in this blog, along with results from data science projects I complete on my path to becoming a data scientist.
Stay tuned.

= Asking the Right Questions About AI =
Asking the Right Questions About AI
In the past few years, we’ve been deluged with discussions of how artificial intelligence (AI) will either save or destroy the world. Self-driving cars will keep us alive; social media bubbles will destroy democracy; robot toasters will rob us of our ability to heat bread.
It’s probably pretty clear to you that some of this is nonsense, and that some of this is real. But if you aren’t deeply immersed in the field, it can be hard to guess which is which. And while there are endless primers on the Internet for people who want to learn to program AI, there aren’t many explanations of the ideas, and the social and ethical challenges they imply, for people who aren’t and don’t want to be software engineers or statisticians.
And if we want to be able to have real discussions about this as a society, we need to fix that. So today, we’re going to talk about the realities of AI: what it can and can’t actually do, what it might be able to do in the future, and what some of the social, cultural, and ethical challenges it poses are. I won’t cover every possible challenge; some of them, like filter bubbles and disinformation, are so big that they need entire articles of their own. But I want to give you enough examples of the real problems that we face that you’ll be situated to start to ask hard questions on your own.
I’ll give you one spoiler to start with: most of the hardest challenges aren’t technological at all. The biggest challenges of AI often start when writing it makes us have to be very explicit about our goals, in a way that almost nothing else does — and sometimes, we don’t want to be that honest with ourselves.
“Robot and Shadow,” by Hsing Wei.
1-
Artificial Intelligence and Machine Learning
As I write this, I’m going to use the terms “artificial intelligence” (AI) and “machine learning” (ML) more or less interchangeably. There’s a stupid reason these terms mean almost the same thing: it’s that “artificial intelligence” has historically been defined as “whatever computers can’t do yet.” For years, people argued that it would take true artificial intelligence to play chess, or simulate conversations, or recognize images; every time one of those things actually happened, the goalposts got moved. The phrase “artificial intelligence” was just too frightening: it cut too close, perhaps, to the way we define ourselves, and what makes us different as humans. So at some point, professionals started using the term “machine learning” to avoid the entire conversation, and it stuck. But it never really stuck, and if I only talked about “machine learning” I’d sound strangely mechanical — because even professionals talk about AI all the time.
So let’s start by talking about what machine learning, or artificial intelligence, is. In the strictest sense, machine learning is part of the field of “predictive statistics:” it’s all about building systems which can take information about things which happened in the past, and make out of those some kind of model of the world around them which they can then use to predict what might happen under other circumstances. This can be as simple as “when I turn the wheel left, the car tends to turn left, too,” or as complicated as trying to understand a person’s entire life and tastes.
You can use this picture to understand what every AI does:

There’s a system with some sensors that can perceive the world — these can be anything from video cameras and LIDAR to a web crawler looking at documents. There’s some other system which can act on the world, by doing anything from driving a car to showing ads to sorting fish. Sometimes, this system is a machine, and sometimes it’s a person, who has to make decisions based on something hopelessly complex or too large to think about at once — like the entire contents of the Internet.
To connect the two, you need a box that takes the perceptions of the world, and comes out with advice about what what is likely to happen if you take various courses of action. That box in the center is called a “model,” as in “a model of how the world works,” and that box is the AI part.
The diagram above has some extra words in it, which are ones you may hear when professionals discuss AI. “Features” are simply some distillation of the raw perceptions, the parts of those perceptions which the designers of the model thought would be useful to include. In some AI systems, the features are just the raw perceptions — for example, the color seen by every pixel of a camera. Such a huge number of features is good for the AI in that it doesn’t impose any preconceptions of what is and isn’t important, but makes it harder to build the AI itself; it’s only in the past decade or so that it’s become possible to build computers big enough to handle that.
“Predictions” are what comes out the other end: when you present the model with some set of features, it will generally give you a bunch of possible outcomes, and its best understanding of the likelihood of each. If you want an AI to make a decision, you then apply some rule to that — for example, “pick the one most likely to succeed,” or “pick the one least likely to cause a catastrophic failure.” That final rule, of weighing possible costs and benefits, is no less important to the system than the model itself.
Now, you could imagine a very simple “model” that gives rules that are just fine for many uses: for example, the mechanical regulator valves on old steam engines were a kind of simple “model” which read the pressure in on one end, and if that pressure pushed a lever beyond some set point, it would open a valve. It was a simple rule: if the pressure is above the set point, open the valve; otherwise, close it.
The reason this valve is so simple is that it only needs to consider one input, and make one decision. If it had to decide something more complicated that depended on thousands or millions of inputs — like how to control a car (that depends on all of your vision, hearing, and more), or which web page might give the best answer to your question about wombat farming (that depends on whether you’re casually interested or a professional marsupial wrangler, and on interpreting if the site was written by an enthusiast or is just trying to sell you cheap generic wombat Viagra) — you would find not one simple comparison, but millions, even tens of millions, needed to decide.
AI’s don’t get bored or distracted: a model can keep making decisions over different pieces of data, millions or billions of times in a row, and not get any worse (or better) at it.
What makes AI models special is that they are designed for this. Inside any AI model are a bunch of rules to combine features, each of which depends on one of hundreds, thousands, or even millions of individual knobs, telling it how much to weigh the significance of each feature under different circumstances. For example, in one kind of AI model called a “decision tree,” the model looks like a giant tree of yes/no questions. If the AI’s job were to tell tuna from salmon, the very first question may be “is the left half of the picture darker than the right half?,” and by the end of it it would look like “given the answers to the past 374 questions, is the average color of pixels in this square more orange or red?” The “knobs” here are the order in which questions are asked, and what the boundaries between a yes and a no for each of them are.
Here’s the magic: It would be impossible to find the right combination of settings which would reliably tell a tuna from a salmon. There are just too many of them. So to start out with, AI’s run in “training mode.” The AI is shown one example after another, each time adjusting its knobs so that it gets better at guessing what will come next, correcting itself after each mistake. The more examples it sees, and the more different examples it sees, the better it gets at telling the crucial from the incidental. And once it has been trained, the values of the knobs are fixed, and the model can be put to use, connected to real actuators.
The advantage that ML models have over humans doing the same task isn’t speed; an ML model typically takes a few milliseconds to make a decision, which is roughly what a human takes as well. (You do this all the time while driving a car) Their real advantage is that they don’t get bored or distracted: an ML model can keep making decisions over different pieces of data, millions or billions of times in a row, and not get any worse (or better) at it. That means you can apply them to problems that humans are very bad at — like ranking billions of web pages for a single search, or driving a car.
(Humans are terrible at driving cars: that’s why 35,000 people were killed by them in the US alone in 2015. The huge majority of these crashes were due to distraction or driver error — things that people normally do just fine, but failed to do just once at a critical moment. Driving requires tremendous awareness and the ability to react within a small fraction of a second, something which if you think about it is kind of amazing we can do at all. But worse, it requires the ability to consistently do that for hours on end, something which it turns out we actually can’t do.)
When someone is talking about using AI in a project, they mean breaking the project down into the components drawn above, and then building the right model. That process starts by gathering training examples, which is often the hardest part of the task; then choosing the basic shape of the model (which is what things like “neural networks,” “decision trees,” and so on are; these are basic kinds of model which are good for different problems) and running the training; and then, most importantly, figuring out what’s broken and adjusting it.
For example, look at the following six pictures, and figure out the key difference between the first three and the second three:






If you guessed “the first three have carpet in them,” you’re right!
You would also be right, of course, if you had guessed that the first three were pictures of grey cats, and the second three were pictures of white cats. But if you had used these images to train your Grey Cat Detector, you might get excellent performance when the model tries to rate your training pictures, and terrible performance in the real world, because what the model actually learned was “grey cats are cat-shaped things which sit on carpets.”
This is called “overfitting:” when your model has learned idiosyncrasies of the training data, rather than what you actually cared about. Avoiding this is what people who build ML systems spend most of their time worrying about.
2-
What AI is good at and bad at
So now that we’ve talked about what AI (or ML) is, let’s talk about where it’s actually useful or useless.
Problems where both the goals, and the means to achieve those goals, are well-understood don’t even require AI. For example, if your goal is “tighten all the nuts on this car wheel to 100 foot-pounds,” all you need is a mechanism that can tighten and measure torque, and stops tightening when the torque reaches 100. This is called a “torque wrench,” and if someone offers you an artificially intelligent torque wrench the correct first question to ask them is why would I want that. These are the steam relief valves of AI; all you need is a simple mechanism.
AI shines in problems where the goals are understood, but the means aren’t. This is easiest to do when:
The number of possible external stimuli is limited, so the model has a chance to learn about them, and
The number of things you have to control is limited, so you don’t need to look at an overwhelming range of options, and
The number of stimuli or decisions is still so big that you can’t just write down the rule; and separately, that
It’s easy to connect one of your actions to an observable consequence in the outside world, so you can easily figure out what did and didn’t work.
These things are harder than they seem. For example, pick up an object sitting next to you right now — I’ll do it with an empty soda can. Now do that again slowly, and watch what your arm did.
My arm rotated at the elbow quickly to move my hand from horizontal on the keyboard to vertical, a few inches from the can, then quickly stopped. Then it moved forward, while I opened my fingers just a bit larger than the can, more slowly than the first motion but still somewhat rapidly, until I saw that my thumb was on the opposite side of the can from my other fingers — despite the fact that my other fingers were obscured from my sight by the can. Then my fingers closed until they met resistance, and stopped almost immediately. And as my arm started to raise up, this time from the shoulder (keeping the elbow fixed), their grip tightened infinitesimally, until it was securely holding, but not deforming, the can.
The fact that we can walk without falling on our faces in confusion is a lot more amazing than it seems. Next time you walk across the room, pay attention to the exact path you take, each time you bend or move your body or put your foot down anywhere except directly in front of you. “Motion planning,” as this problem is called in robotics, is really hard.
This is one of the tasks which is so hard that our brains have a double-digit percentage of their mass dedicated to nothing else. That makes them seem far easier to us than they actually are. Other tasks in this category are face recognition (a lot of our brain is dedicated not to general vision, but specifically to recognizing faces), understanding words, identifying 3D objects, and moving without running into things. We don’t think of these as hard because they’re so intuitive to us — but they’re intuitive because we have evolved specialized organs to do nothing but be really good at those.
For this narrow set of tasks, computers do very poorly, not because they do worse at them than they do at similar tasks, but because we’re intuitively so good at them that our baseline for what constitutes “acceptable performance” is very high. If we didn’t have a huge chunk of our brain doing nothing but recognizing faces, people would look about as different to us as armadillos do — which is just what happens to computers.
(Conversely, the way humans are wired makes other tasks artificially easy for computers to get “right enough.” For example, human brains are wired to assume, in case of doubt, that something which acts more-or-less alive is actually animate. This means that having convincing dialogues with humans doesn’t require understanding language in general; so long as you can keep the conversation on a more-or-less focused topic, humans will autocorrect around anything unclear. This is why voice assistants are possible. The most famous example of this is ELIZA, a 1964 “AI” which mimicked a Rogerian psychotherapist. It would understand just enough of your sentences to ask you to tell it more about various things, and if it got confused, it would fall back on safe questions like “Tell me about your mother.” While it was half meant as a joke, people did report feeling better after talking to it. If you have access to a Google Assistant-powered device, you can tell it “OK Google; talk to Eliza” and see for yourself.)
To understand the last of the problems described above — a case where it’s hard to connect your immediate actions to a consequence — think about learning to play a video game. Some action-consequences are pretty obvious: you zigged when you should have zagged, ran into a wall, game over. But as you get better at a game, you start to realize “crap, I missed that one boost, I’m going to be totally screwed five minutes from now,” and can attribute that decision to a much later consequence. You had to spend a lot of time understanding the mechanics of the game before that connection became understandable to you; AI’s have the same problem.
We’ve talked about cases where the goals and means are understood, and cases where the goals but not the means are understood. There’s a third category, where AI can’t help at all: problems where the goal itself isn’t well understood. After all, if you can’t give the AI a bunch of examples of what is and isn’t a good solution look like, what’s it going to learn from?
We’ll talk about these problems a lot more in a moment, because problems which actually are like this but which we think aren’t are often where the thorniest ethical issues come up. What’s really happening a lot of the time is that either we don’t know what “success” really means (in which case, how do you know if you’ve succeeded?), or worse, we do know — but don’t really want to admit it to ourselves. And the first rule of programming computers is that they’re no good at self-deception: if you want them to do something, you have to actually explain to them what you want.
Before we go into ethics, here’s another way to divide up what AI is good and bad at.
The easiest problem is clear goals in a predictable environment. That’s anything from a very simple environment (one lug nut, where you don’t even need AI) to a more complicated, but predictable one (a camera looking at an assembly line, where it knows a car will show up soon and it has to spot the wheels). We’ve been good at automating this for several years.
A harder problem is clear goals in an unpredictable environment. Driving a car is a good example of this: the goals (get from point A to point B safely and at a reasonable speed) are straightforward to describe, but the environment can contain arbitrarily many surprises. AI has only developed to the point where these problems can really be attacked in the past few years, which is why we’re now attacking problems like self-driving cars or self-flying airplanes.
Another kind of hard problem is indirect goals in a predictable environment. These are problems where the environment makes sense, but the relationship between your actions and these goals is very distant — like playing games. This is another field where we’ve made tremendous progress in the recent past, with AI’s able to do previously-unimaginable things like winning at Go.
Winning at board games isn’t very useful in its own right, but it opens up the path to indirect goals in an unpredictable environment, like planning your financial portfolio. This is a harder problem, and we haven’t yet made major inroads on it, but I would expect us to get good at these over the next decade.
And finally you have the hardest case, of undefined goals. These can’t be solved by AI at all; you can’t train the system if you can’t tell it what you want it to do. Writing a novel might be an example of this, since there isn’t a clear answer to what makes something a “good novel.” On the other hand, there are specific parts of that problem where goals could be defined — for example, “write a novel which will sell well if marketed as horror.”
Whether this is a good or bad use of AI is left to the reader’s wisdom.
3-
Ethics and the Real World
So now we can start to look at the meat of our question: what do real-world hard questions look like, ones where AI working or failing could make major differences in people’s lives? And what kinds of questions keep coming up?
I could easily fill a bookshelf with discussions of this; there’s no way to look at every interesting problem in this field, or even at most of them. But I’ll give you six examples which I’ve found have helped me think about a lot of other problems, in turn — not in that they gave me the right answers, but in that they helped me ask the right questions.
1. The Passenger and the Pedestrian
A self-driving car is crossing a narrow bridge, when a child suddenly darts out in front of it. It’s too late to stop; all the car can do is go forward, striking the child, or swerve, sending itself and its passenger into the rushing river below. What should it do?
I’m starting with this problem because it’s been discussed a lot in public in the past few years, and the discussion has often been remarkably intelligent, and shows off the kinds of question we really need to ask.
First of all, there’s a big caveat to this entire question: this problem matters very little in practice, because the whole point of self-driving cars is that they don’t get into this situation in the first place. Children rarely appear out of nowhere; mostly when that happens, either the driver was going too fast for their own reflexes to handle a child jumping out from behind an obstruction they could see, or the driver was distracted and for some reason didn’t notice the chid until too late. These are both exactly the sorts of things that an automatic driver has no problem with: looking at all the signals around at once, for hours on end, without getting bored or distracted. A situation like this one would become vanishingly rare, and that’s where the lives saved come from.
But “almost never” isn’t the same thing as “never,” and we have to accept that sometimes this will happen. When it does, what should the car do? Should it prioritize the life of its passengers, or of pedestrians?
This isn’t a technology question: it’s a policy question, and in the form above, it’s been boiled down to its simple core. We could agree on either answer (or any combination) as a society, and we can program the cars to do that. If we don’t like the answer, we can change it.
There’s one big way in which this is different from the world we inhabit today. If you ask people what they would do in this situation, they’ll give a wide variety of answers, and caveat them with all sorts of “it depends”es. The fact is that we don’t want to have to make this decision, and we certainly don’t want to publicly admit if our decision is to protect ourselves over the child. When people actually are in such situations, their responses end up all over the map.
Culturally, we have an answer for this: in the heat of the moment, in that split-second between when you see oncoming disaster and when it happens, we recognize that we can’t make rational decisions. We will end up both holding the driver accountable for their decision, and recognizing it as inevitable, no matter what they decide. (Although we might hold them much more accountable for decisions they made before that final split-second, like speeding or driving drunk.)
With a self-driving car, we don’t have that option; the programming literally has a space in it where it’s asking us now, years before the accident happens: “When this happens, what should I do? How should I weight the risk to the passenger against the risk to the pedestrian?”
And it will do what we tell it to. The task of programming a computer requires brutal honesty about what we want it to decide. When these decisions affect society as a whole, as they do in this case, that means that as a society, we are faced with similarly hard choices.
2. Polite fictions
Machine-learned models have a very nasty habit: they will learn what the data shows them, and then tell you what they’ve learned. They obstinately refuse to learn “the world as we wish it were,” or “the world as we like to claim it is,” unless we explicitly explain to them what that is — even if we like to pretend that we’re doing no such thing.
In mid-2016, high school student Kabir Alli tried doing Google image searches for “three white teenagers” and “three black teenagers.” The results were even worse than you’d expect.
Kabir Alli’s (in)famous results
“Three white teenagers” turned up stock photography of attractive, athletic teens; “three black teenagers” turned up mug shots, from news stories about three black teenagers being arrested. (Nowadays, either search mostly turns up news stories about this event)
What happened here wasn’t a bias in Google’s algorithms: it was a bias in the underlying data. This particular bias was a combination of “invisible whiteness” and media bias in reporting: if three white teenagers are arrested for a crime, not only are news media much less likely to show their mug shots, but they’re less likely to refer to them as “white teenagers.” In fact, nearly the only time groups of teenagers were explicitly labeled as being “white” was in stock photography catalogues. But if three black teenagers are arrested, you can count on that phrase showing up a lot in the press coverage.
Many people were shocked by these results, because they seemed so at odds with our national idea of being a “post-racial” society. (Remember that this was in mid-2016) But the underlying data was very clear: when people said “three black teenagers” in media with high-quality images, they were almost always talking about them as criminals, and when they talked about “three white teenagers,” they were almost always advertising stock photography.
The fact is that these biases do exist in our society, and they’re reflected in nearly any piece of data you look at. In the United States, it’s a good bet that if your data doesn’t show a racial skew of some sort, you’ve done something wrong. If you try to manually “ignore race” by not letting race be an input to your model, it comes in through the back door: for example, someone’s zip code and income predict their race with great precision. An ML model which sees those but not race, and which is asked to predict something which actually is tied to race in our society, will quickly figure that out as its “best rule.”
AI models hold a mirror up to us; they don’t understand when we really don’t want honesty. They will only tell us polite fictions if we tell them how to lie to us ahead of time.
This kind of honesty can force you to be very explicit. A good recent example was in a technical paper about “word debiasing.” This was about a very popular ML model called word2vec which learned various relationships between the meanings of English words — for example, that “king is to man, as queen is to woman.” The authors of this paper found that it contained quite a few examples of social bias: for example, it would also say that “computer programmer is to man, as homemaker is to woman.” The paper is about a technique they came up with for eliminating that bias.
What isn’t obvious to the casual reader of this paper — including many of the people who wrote news articles about it — is that there’s no automatic way to eliminate bias. Their procedure was quite reasonable: first, they analyzed the word2vec model to find pairs of words which were sharply split along the he/she axis. Next, they asked a bunch of humans to identify which of those pairs represented meaningful splits (e.g., “boy is to man as girl is to woman”) and which represented social biases. Finally, they applied a mathematical technique to subtract off the biases from the model as a whole, leaving behind an improved model.
This is all good work, but it’s important to recognize that the key step in this — of identifying which male/female splits should be removed — was a human decision, not an automatic process. It required people to literally articulate which splits they thought were natural and which ones weren’t. Moreover, there’s a reason the original model derived those splits; it came from analysis of millions of written texts from all over the world. The original word2vec model accurately captured people’s biases; the cleaned model accurately captured the raters’ preference about which of these biases should be removed.
The risk which this highlights is the “naturalistic fallacy,” what happens when we confuse what is with what ought to be. The original model is appropriate if we want to use it to study people’s perceptions and behavior; the modified model is appropriate if we want to use it to generate new behavior and communicate some intent to others. It would be wrong to say that the modified model more accurately reflects what the world is; it would be just as wrong to say that because the world is some way, it also ought to be that way. After all, the purpose of any model — AI or mental — is to make decisions. Decisions and actions are entirely about what we wish the world to be like; if they weren’t, we would never do anything at all.
3. The Gorilla Incident
In July of 2015, when I was technical leader for Google’s social efforts (including photos), I received an urgent message from a colleague at Google: our photo indexing system had publicly described a picture of a Black man and his friend as “gorillas,” and he was — with good reason — furious.
My immediate response, after swearing loudly, was to page the team and publicly respond that this was not something we considered to be okay. The team sprung into action and disabled the offending characterization, as well as several other potentially risky ones, until they could solve the underlying issue.
Many people suspected that this issue was the same one as the one that caused HP’s face-tracking webcams to not work on Black people six years earlier: that the training data for “faces” had been composed exclusively of white people. This was the first thing we suspected as well, but it we quickly crossed it off the list: the training data included a wide range of people of all races and colors.
What actually happened was the intersection of three subtle problems.
The first problem was that face recognition is hard. Different people look so vividly different to us precisely because a tremendous fraction of our brain matter is dedicated to nothing but recognizing people’s faces; we’ve spent millions of years evolving tools for nothing else. But if you compare how different two different faces are in to how different, say, two different chairs are, you’ll see that faces are tremendously more similar than you would guess — even across species.
In fact, we discovered that this bug was far from isolated: the system was also prone to misidentifying white faces as dogs and seals.
And this goes to the second problem, which is the real heart of the matter: ML systems are very smart in their domain, but know nothing at all about the broader world, unless they were taught it. And when trying to think about all the ways in which different pictures could be identified as different objects — this AI isn’t just about faces— nobody thought to explain to it the long history of Black people being dehumanized by being compared to apes. That context is what made this error so serious and harmful, while misidentifying someone’s toddler as a seal would just be funny.
There’s no simple answer to this question. When dealing with problems involving humans, the cost of errors is typically tied in with tremendously subtle cultural issues. It’s not so much that it’s hard to explain them as that it’s hard to think of them in advance: quickly, list for me the top cultural sensitivities that might show up around pictures of arms!
This problem doesn’t just manifest in AI: it also manifests when people are asked to make value judgments across cultures. One particular challenge for this is when detecting harassment and abuse online. Such questions are almost entirely handled by humans, rather than AI’s, because it’s extremely difficult to set down rules that even humans can use to judge these things. I spent a year and a half developing such rules at Google, and consider it to be one of the greatest intellectual challenges I’ve ever faced. To give a very simple example: people often say “well, an obvious rule is that if you say n****r, that’s bad.” I challenge you to apply that rule to the different meanings of the word in (1) nearly any of Jay-Z’s songs, (2) Langston Hughes’ poem “Christ in Alabama,” (3) that routine by Chris Rock, (4) that same routine if he had performed it in front of a white audience, (5) and that same routine if Ted Nugent had performed it, verbatim, to one of his audiences, and come up with a coherent explanation of what’s going on. It’s possible; it’s far from simple. And those are just five examples involving published, edited, creative works, not even normal conversation.
Even with teams of people coming up with rules, and humans, not AI’s, enforcing them, cultural barriers are a huge problem. A reviewer in India won’t necessarily have the cultural context around the meaning of a racial slur in America, nor would one in America have cultural context for one in India. But the number of cultures around the world is huge: how do you express these ideas in a way that anyone can learn them?
The lesson is this: often the most dangerous risks in a system come, not from problems within the system, but from unexpected ways that the system can interact with the broader world. We don’t yet have a good way to manage this.
(The third problem in the Gorilla Incident — for those of you who are interested — is a problem of racism in photography. Since the first days of commercial film, the standards for color and image calibration have included things like “Shirley Cards,” pictures of standardized models. These models were exclusively white until the 1970’s — when furniture manufacturers complained that film couldn’t accurately capture the brown tones of dark wood! Even though modern color calibration standards are more diverse, our standards for what constitute “good images” still overwhelmingly favor white faces rather than black ones. As a result, amateur pictures of white people with cell phone cameras turn out reasonably well, but amateur pictures of black people — especially dark-skinned people — often come out underexposed. Faces are reduced to vague blobs of brown with eyes and sometimes a mouth, which unsurprisingly are hard for image recognition algorithms to make much sense of. Photography director Ava Berkofsky recently gave an excellent interview on how to light and photograph Black faces well.)
4. Unfortunately, the AI will do what you tell it
“The computer has it in for me / I wish that they would sell it. / It never does just what I want / but only what I tell it.” — Anonymous
One important use of AI is to help humans make better decisions: not to directly operate some actuator, but to tell a person what it recommends, and so better-equip them to make a good choice. This is most valuable when the choices have high stakes, but the factors which really affect long-term outcomes aren’t immediately obvious to the humans in the field. In fact, absent clearly useful information, humans may easily act on their unconscious biases, rather than on real data. That’s why many courts started to use automated “risk assessments” as part of their sentencing guidelines.
Modern risk assessments are ML models, tasked with predicting the likelihood of a person committing another crime in the future. Trained on the full corpus of an area’s court history, it can form a surprisingly good picture of who is and isn’t a risk.
If you’ve been reading carefully so far, you may have spotted a few ways this could go horribly, terribly, wrong. And that’s exactly what happened across the country, as revealed by a 2016 ProPublica exposé.
The designers of the COMPAS system, the one used by Broward County, Florida, followed best practices. They made sure their training data hadn’t been artificially biased by group, for example making sure there was equal training data about people of all races. They took care to ensure that race was not one of the input features that their model had access to. There was only one problem: their model didn’t predict what they thought it was predicting.
The question that a sentencing risk assessment model ought to be asking is something like, “what is the probability that this person will commit a serious crime in the future, as a function of the sentence you give them now?” That would take into account both the person and the effect of the sentence itself on their future life: will it imprison them forever? Release them with no chance to get a straight job?
It was trained to answer, “who is more likely to be convicted,” and then asked “who is more likely to commit a crime,” without anyone paying attention to the fact that these are two entirely different questions.
But we don’t have a magic light that goes off every time someone commits a crime, and we certainly don’t have training examples where the same person was given two different sentences at once and turned out two different ways. So the COMPAS model was trained on a proxy for the real, unobtainable data: given the information we know about a person at the time of sentencing, what is the probability that this person will be convicted of a crime? Or phrased as a comparison between two people, “Which of these two people is most likely to be convicted of a crime in the future?”
If you know anything at all about the politics of the United States, you can answer that question immediately: “The Black one!” Black people are tremendously more likely to be stopped, arrested, convicted, and given long sentences for identical crimes than white people, so an ML model which looked at the data and, ignoring absolutely everything else, always predicted that a Black defendant is more likely to be convicted of another crime in the future, would in fact be predicting quite accurately.
But what the model was being trained for wasn’t what the model was being used for. It was trained to answer, “who is more likely to be convicted,” and then asked “who is more likely to commit a crime,” without anyone paying attention to the fact that these are two entirely different questions.
(COMPAS’ not using race as an explicit input made no difference: housing is very segregated in much of the US, very much so in Broward County, and so knowing somebody’s address is as good as knowing their race.)
There are obviously many problems at play here. One is that the courts took the AI model far too seriously, using it as a direct factor in sentencing decisions, skipping human judgment, with far more confidence than any model should warrant. (A good rule of thumb, also recently encoded into EU law, is that decisions with serious consequences of people should be sanity-checked by a human — and that there should be a human override mechanism available.) Another problem, of course, is the underlying systemic racism which this exposed: the fact that Black people are more likely to be arrested and convicted of the same crimes.
But there’s an issue specific to ML here, and it’s one that bears attention: there is often a difference between the quantity you want to measure, and the one you can measure. When these differ, your ML model will become good at predicting the quantity you measured, not the quantity for which it was meant to be a proxy. You need to very carefully reason about the ways in which these are similar and differ before trusting your model.
5. Man is a rationalizing animal
There is a new buzzword afoot in the discussion of machine learning: the “right to explanation.” The idea is that, if ML is being used to make decisions of any significance at all, people have a right to understand how those decisions were made.
Intuitively, this seems obvious and valuable — yet when this is mentioned around ML professionals, their faces turn colors and they try to explain that what’s requested is physically impossible. Why is this?
First, we should understand why it’s hard to do this; second, and more importantly, we should understand why we expect it to be easy to do, and why this expectation is wrong. And third, we can look at what we can actually do.
Earlier, I described an ML model as containing between hundreds and millions of dials. This doesn’t do justice to the complexity of real models. For example, modern ML-based language translation systems take as their input one letter at a time. That means that the model has to express conditions about the state of its understanding of a text after reading however many letters, and how each successive next letter might affect its interpretation of meaning. (And it works; with some language pairs like English and Spanish, it performs as well as humans!)
For any situation the model encounters, the only “explanation” it has of what it’s doing is “well, the following thousand variables were in these states, and then I saw the letter ‘c,’ and I know that this should change the probability of the user talking about a dog according to the following polynomial…”
This isn’t just incomprehensible to you: it’s also incomprehensible to ML researchers. Debugging ML systems is one of the hardest problems in the field, since examining the individual state of the variables at any given time tells you approximately as much about the model as measuring a human’s neural potentials will tell you about what they had for dinner.
And yet — this is coming to the second part — we always feel that we can explain our own decisions, and it’s this kind of explanation that people (especially regulators) keep expecting. “I set the interest rate for this mortgage at 7.25% because of their median FICO score,” they expect it to say, “had their FICO score from Experian been 35 points higher, the rate would have dropped to 7.15%.” Or perhaps, “I recommended we hire this person because of the clarity with which they explained machine learning during our interview.”
But there’s a dark secret which everyone in cognitive or behavioral psychology knows: All of these explanations are nonsense. Our decisions about whether we like someone or not are set within the first few seconds of conversation, and can be influenced by something as seemingly random as whether they were holding a hot or cold drink before shaking your hand. Unconscious biases pervade our thinking, and can be measured, even though we aren’t aware of them. Cognitive biases are one of the largest (and IMO most interesting) branches of psychology research today.
What people are good at, it turns out, isn’t explaining how they made decisions: it’s coming up with a reasonable-sounding explanation for their decision after the fact. Sometimes this is perfectly innocent: for example, we identify some fact which was salient for us in the decision-making process (“I liked the color of the car”) and focus on that, while ignoring things which may have been important to us but were invisible. (“My stepfather had a hatchback. I hated him.”) It can also have deeper motivations: to resolve cognitive dissonance by explaining how we did or didn’t want something anyway (“the grapes were probably sour, anyway”), or to avoid thinking too closely about something we may not want to admit. (“The first candidate sounded just like I did when I graduated. That woman was good, but she felt different… she wouldn’t fit as well working with me.”)
If we expect ML systems to provide actual explanations for their decisions, we will have as much trouble as if we asked humans to explain the actual basis for their own decisions: they don’t know any more than we do.
But when we ask for explanations, what we’re really often interested in is which facts were both salient (in that changing them would have changed the outcome materially) and mutable (in that changes to them are worth discussing). For example, “you were shown this job posting; had you lived ten miles west, you would have seen this one instead” may be interesting in some context, but “you were shown this job posting; had you been an emu, you would instead have been shown a container of mulga seeds” is not.
This information is particularly useful when it’s also provided as an axis for providing feedback to ML systems: for example, by showing people a few salient and mutable items, they may offer corrections to those items, and provide updated data.
Mathematical techniques for producing this kind of explanation are in active development, but you should be aware that there are nontrivial challenges in them. For example, most of these techniques are based on building a second “explanatory” ML model which is less accurate, only useful for inputs which are small variations on some given input (your own), more comprehensible, but based on entirely different principles than the “main” ML model being described. (This is because only a few kinds of ML model, like decision trees, are at all comprehensible by people, while the models most useful in many real applications, like neural nets, decidedly are not.) This means that if you try to give the system feedback saying “no, change this variable!” in terms of the explanatory model, there may be no obvious way to translate that into inputs for the main model at all. Yet if you give people an explanation tool, they’ll also demand the right to change it in the same language — reasonably, but not feasibly.
Humans deal with this by having an extremely general intelligence in their brains, which can handle all sorts of concepts. You can tell it that it should be careful with its image recognition when it touches on racial history, because the same system can understand both of those concepts. We are not yet anywhere close to being able to do that in AI’s.
6. AI is, ultimately, a tool
It’s hard to discuss AI ethics without bringing up everybody’s favorite example: artificially intelligent killer drones. These aircraft fly high in the sky, guided only by a computer which helps them achieve their mission of killing enemy insurgents while preserving civilian life… except when they decide that the mission calls for some “collateral damage,” as the euphemism goes.
People are rightly terrified of such devices, and would be even more terrified if they heard more of the stories of people who already live under the perpetual threat of death coming suddenly out of a clear sky.
AI is part of this conversation, but it’s less central to it than we think. Large drones differ from manned aircraft in that their pilots can be thousands of miles away, out of harm’s way. Improvements in autopilot AI’s mean that a single drone operator could soon fly not one aircraft, but a small flight of them. Ultimately, large fleets of drones could be entirely self-piloting 99% of the time, calling in a human only when they needed to make an important decision. This would open up the possibility of much larger fleets of drones, or drone air forces at much lower cost — democratizing the power to bomb people from the sky.
In another version of this story, humans might be taken entirely out of the “kill chain” — the decision process about whether to fire a weapon. (Most Western armies have made quite clear that they have no intention of doing any such thing, because it would be obviously stupid. But an army in extremis may easily do so, if nothing else for the terror it could create — unknown numbers of aircraft flying around, killing at will — and we may expect far more armies to have drones in the future.) Now we might ask, who is morally responsible for a killing decided on entirely by a robot?
The question is both simpler and more complicated than we at first imagine. If someone hits another person over the head with a rock, we blame the person, not the rock. If they throw a spear, even though the spear is “under its own power” for some period of flight, we would never think of blaming it. Even if they construct a complex deathtrap, Indiana Jones-style, the volitional act is the human’s. This question only becomes ambiguous to the extent that the intermediate actor can decide on their own.
The simplicity comes because this question is far from new. Much of the point of military discipline is to create a fighting force which does not try to think too autonomously during battle. In countries whose militaries are descended from European systems, the role of enlisted and noncommissioned officers is to execute on plans; the role of commissioned officers is to decide on which plans to execute. Thus, in theory, the decision responsibility is entirely on the shoulders of the officers, and the clear demarcation of areas of responsibility between officers based on rank, area of command, and so on, determines who is ultimately responsible for any given order.
While in practice, this is often considerably more fuzzy, the principles are ones we’ve understood for millennia, and AI’s add nothing new to the picture. Even at their greatest decision-making capability and autonomy, they would still fit into this discussion — and we’re decades away from them actually having enough autonomy for the conversation to even start to approach the levels we have long had established for these discussions around people.
Perhaps this is the last important lesson of the ethics of AI: many of the problems we face with AI are simply the problems we have faced in the past, brought to the fore by some change in technology. It’s often valuable to look for similar problems in our existing world, to help us understand how we might approach seemingly new ones.
4-
Where do we go from here?
There are many other problems that we could discuss — many of which are very urgent for us as a society right now. But I hope that the examples and explanations above have given you some context for understanding the kinds of ways in which things can go right and wrong, and where many of the ethical risks in AI systems come from.
These are rarely new problems; rather, the formal process of explaining our desires to a computer — the ultimate case of someone with no cultural context or ability to infer what we don’t say — forces us to be explicit in ways we generally aren’t used to. Whether this involves making a life-or-death decision years ahead of time, rather than delaying it until the heat of the moment, or whether it involves taking a long, hard look at the way our society actually is, and being very explicit about which parts of that we want to keep and which parts we want to change, AI pushes us outside of our comfort zone of polite fictions and into a world where we have to discuss things very explicitly.
Every one of these problems existed long before AI; AI just made us talk about them in a new way. That might not be easy, but the honesty it forces on us may be the most valuable gift our new technology can give us.

= Internet Of Intelligence: A Framework of Thinking About Artificial Intelligence in the Home =
Internet Of Intelligence: A Framework of Thinking About Artificial Intelligence in the Home
Discussions of Artificial Intelligence (AI) are everywhere, ironically enough, some of it written and published by bots… Elon Musk continues to gain headlines for his crusade against the development of AI, citing concerns that if we pick the wrong utility function, humans could be optimized out of existence. (from his talk with Walter Issacson, if the goal is to eliminate spam, the AI could decide that eliminating humans is the most efficient path). IBM’s Watson has been the poster child of deep learning, promised to be the panacea for everything from call center routing to finding copyright infringement to winning at Jeopardy. But what does this mean for the every day consumer? What happens when our homes really become ‘smart?’ Does it punish us for not cleaning the toilet? Does it decide, based on reading the news feeds, it would be safer for the family not to leave the house and lock us in, delivering food via Grub Hub and Amazon drones?
The promise and threat of the Smart Home requires thinking through the types of intelligences required.
We need a way to think about AI in the home so we can better design these systems to anticipate the benefits and well as where a ‘rogue’ intelligence could go off the rails. Much the way our own brain segments functions in different sections of our gray matter, home intelligence could also benefit from some sense of specialization and hierarchy. AI systems are really good at recognizing patterns and determining action based on those patterns. Within the home, you can consider five types of intelligences needed to realize the true promise of the Smart Home.
Visual Intelligence
Behavioral Recognition
Human Interface Engine
Threat detection and abatement
Ethics Engine
The Visual Intelligence is the most common being used today, looking at streams of visual data, static and dynamic, coming from security cameras, phone snapshots and more. These maturing intelligences can distinguish between dogs and cats in the scene, ensure only family members are allowed in the back door, and with Apple’s new A11 Bionic on the iPhone X, process your face fast enough to paint your expressions onto an animated emoji of poo. The trend is to push as much of this recognition to the edge so that the homes do not have to push terabytes of data up to the cloud every time someone rings your doorbell camera. In the tug-a-o-war between privacy and security, edge process of the visual data from your home also ensures your data stays yours as well as lowers the latency in recognition. The interpretation of visual information in the home is a key requirement for any home intelligence as correct evaluation of what is happening visual drives much of the follow-on actions in the home. Adjusting the thermostat based on the number of people in the room. Letting people know when the dog went outside for a potty break and more.
Behavioral Recognition builds on the information within the visual scene of the home and interprets the behavior of the occupants to look for patterns of engagement that can be automated. Vivint’s new Sky intelligence examines what actions the occupants take inside of their homes and takes on those tasks over time. For example, if when you leave in the morning, you turn down the thermostat, turn on the back porch light, switch off your stream of classical music from Alexa and send a text to the nanny reminding them of pickup, Sky promises to, over time, do most of these as you walk out the door. The same happens when you come home, the right lights turn on, smooth jazz is piped through the kitchen, and the house has already been working on achieving the right comfort level as soon as it knew you were headed home. This Behavioral Recognition extrapolates within your routine (and less routine) actions to determine candidates for automation while at the same time understanding when you deviate from your normal routines and how the react appropriately. Say you come home with extra kids for a play date, smooth jazz probably isn’t the vibe you are going for. Or shift your thermostat because Mom is visiting. The Behavioral Recognition is critical to having your home feel intelligence and tailored to your context.
Of course the occupants need to interact with the home. The Human Interface Engine bridges that gap between human and home. We’ve seen the rapid rise of Alexa and Google Home (and eventually Bixby and HomePod) leverage voice as key interface to our home environment. But there are also gestures, apps, touch and more that drive our engagement. The Human Interface Engine will be that collection of intelligences that gather and interpret those actions on behalf of the human occupants. Want the kids to exercise more? Require 200 jumping jacks before they can turn on the Xbox? Wave goodnight to your living room to turn off the music and the lights before retiring for the night. Filter out your teenager’s commands every time she wants Alexa to play death metal. Without the ability to recognize and react to the ultimate users in the Smart Home, the humans, any system would only be guessing at what do to on behalf of their occupants.
Security becomes critical as well within the home environment both for the ability for the bad guys to get access to our resources but also for the ability for nefarious actors to leverage our homes to attack others. As such, a local intelligence that is focused on Threat Detection and Abatement is critical to any Smart Home intelligence solution. Solutions like Cujo bring solutions typically reserved for the enterprise into the home and are a good start towards the type of security needed in the home. Cujo and others leverage the attack patterns sensed across their customers to continuously update their own understanding of evolving threats. Eventually, the abatement of threats to the home could also include counter-intelligence capabilities, spoofing data and usage information as a way to throw off the intentions of the bad guys. Are you on vacation or just taking quick trip to Costco? Home with a sick child or on a business trip? This type of misinformation also becomes a new direction of capabilities for these intelligences and aid in the overall security of the home and its users.
The final intelligence we believe is necessary in the home is an Ethics Engine. The Ethics Engine establishes the bounds of what is the correct actions for the home to take. Just because you can turn the thermostat down to 40 degrees does not mean that you should. Just because the dash button can order three lifetimes of Cheetos to be deliverd by Friday, does not mean is should. This notion has received a lot within Autonomous Vehicles, where the vehicle makes decisions regarding whether to hit the squirrel or the telephone pole. Within the home, this is also critical as more of the home’s systems become controllable by these intelligences. This ‘brain of brains’ will have the final sign-off on any actions taken by the Smart Home as the final line of defense of the users health, safety and wellbeing. Models of robo-ethics have been around since Issac Asimov first published his laws of robotics but they have not yet been applied to the Smart Home content. The Disney Channel movie, Smart House, saw a resurgence in popularity recently because of its campy portrayal of a home gone rogue. An Ethics Engine would prevent and more dangerous scenarios from happening, ensuring the home does not fulfill Elon Musk’s nightmare scenario of a rampant AI that eradicates the family hamster.
Within each of these five intelligences, you can see how this framework helps give context to the types of AI that needs to be brought together in concert in the Smart Home. True intelligence in our abodes will come from the seamless integration and collaboration within these critical capabilities. While no one company has brought all of these elements together, we see the capabilities evolving. Look for the service providers like Vivint and Comcast leading the charge, integrating the solutions where feasible and building their own where necessary. Eventually, we will have that home of the future, today, and feel safe using it.

= How to Build Ethics into AI — Part I =
How to Build Ethics into AI — Part I
Research-based recommendations to keep humanity in AI
“Heart Shaped” by dimdimich
This is part one of a two-part series about how to build ethics into AI. Part one focuses on cultivating an ethical culture in your company and team, as well as being transparent within your company and externally. Part two focuses on mechanisms for removing exclusion from your data and algorithms. Each of the recommendation includes examples of ethical missteps and how they might have been prevented or mitigated.
It seems like each day there are articles about how an artificial intelligence (AI) system caused offense (e.g., labeling African Americans as “gorillas”) or actual harm when the intent may have been well-meaning (e.g., racial bias in criminal sentencing recommendations and interest rates).
The developers in each of these systems did not set out to offend or harm anyone and didn’t anticipate the negative outcomes but should they have? If you are designing and building an AI system, can you build in ethics? Regardless of your role in an organization, can you help ensure that your AI system leads to a more just society rather than perpetuating societal biases? The answer to all these questions is, “Yes!”
Do Well and Do Good
Salesforce’s CEO, Marc Benioff, has said, “My goals for the company are to do well and do good.” This is at the heart of our core values of trust, equality, and innovation. We strongly believe that we can be at the forefront of innovation, be successful, and be a force for good in the world. We work internally on building ethics into Einstein (our AI system) and collaborate with other members in the Joint Partnership for AI.
Embedding ethics in your AI system takes time and may require you to work differently from the way you or your company has always worked. However, given the great potential for both harm and benefit with AI, it is critical that you make the investment!
An End-to-End Approach
The process for building ethics into your system can be broken into three stages with several steps within each:
Create an ethical culture
Be transparent
Remove exclusion

Create an Ethical Culture
If you don’t build a strong foundation to start, the effort required to be successful will always be greater. This involves building a diverse team, cultivating an ethical mindset, and conducting a social systems analysis.

Build a Diverse Team
Recruit for a diversity of backgrounds and experience to avoid bias and feature gaps.
When Apple’s HealthKit came out in 2014, it could track your blood alcohol content but you couldn’t track the most frequent health issue most women deal with every month.
Research shows (1, 2, 3, 4, 5, 6) that diverse teams (including experience, race, gender) are more creative, diligent, and harder-working. Including more women at all levels, especially top management, results in higher profits.
Lack of diversity creates an echo chamber and results in biased products and feature gaps. If the team developing Apple’s HealthKit had more (any?) women on the team, they likely would have identified the glaringly absent feature for 50% of the population. This example points to lack of gender diversity, but all types of diversity are needed, from age and race to culture and education.
If you are unable to hire new members to build a more diverse team, seek out feedback from diverse employees in the company and your user base.

Cultivate an Ethical Mindset
Ethics is a mindset, not a checklist. Empower employees to do the right thing.
Uber’s Chief Executive Officer credits whistleblowers with forcing the company to make changes and “go forward as a company that does the right thing.”
Simply having a Chief Ethics Officer doesn’t prevent companies from making ethical missteps. That is because no one individual can or should be responsible for a company acting ethically. There must be an ethical mindset throughout the company.
Individual employees must be able to empathize with everyone that their AI system impacts. Companies can cultivate an ethical mindset through courses, in-house support groups, and equality audits.
Additionally, employees should feel empowered to constantly challenge each other by asking, “Is this the right thing to do?” In product reviews and daily stand-ups, people should ask ethical questions specific to their domains. For example:
Product Managers: “What is the business impact of a false positive or false negative in our algorithm?”
Researchers: “Who will be impacted by our system and how? How might this be abused? How will people try to break the product or use it in unintended ways?”
Designers: “What defaults or assumptions am I building into the product? Am I designing this for transparency and equality?”
Data scientists and modelers: “By optimizing my model this way, what implications am I creating for those impacted?”
When employees are dissatisfied with the answers they receive, there needs to be a mechanism for resolving it.
Simply having a Chief Ethics Officer doesn’t prevent companies from making ethical missteps. That is because no one individual can or should be responsible for a company acting ethically. There must be an ethical mindset throughout the company.

Conduct a Social Systems Analysis
Involve stakeholders at every stage of the product development lifecycles to correct for the impact of systemic social inequalities in AI data.
The Chicago police department used an AI-driven predictive policing program to identify people at the highest risk of being involved in gun violence. This program was found to be ineffective at reducing crime, but resulted in certain individuals being targeted for arrest.
Social-systems analysis is the study of the groups and institutions that interact in an ecosystem. Rather than assuming that a system will be built, social systems analysis asks if the system should be built in the first place and then proceeds to design the system based on the needs and values of stakeholders. This can be done by conducting ethnography in the community impacted or getting feedback from an oversight committee or legal institution.
Referring to the example of Chicago’s predictive policing program, Kate Crawford & Ryan Calo suggest the following: “A social-systems approach would consider the social and political history of the data on which the heat maps are based. This might require consulting members of the community and weighing police data against this feedback, both positive and negative, about the neighborhood policing.”
Organizations must understand how their creations impact users and society as a whole. By understanding these impacts, they can determine those that are most vulnerable to the system’s negative effects. From a statistical standpoint, there may be only a 1% chance of a false positive or false negative (excellent from a statistical perspective!) but for that 1% of the population, the result can be extremely harmful. Are the risks and rewards of the system being applied evenly to all? Who benefits and who pays based on the results of the AI? Asking this question at every stage of the AI’s development, including pre- and post-launch, can help identify harmful bias and address it.
From a statistical standpoint, there may be only a 1% chance of a false positive or false negative, … but for that 1% of the population, the result can be extremely harmful.
Be Transparent
To be ethical, you need to be transparent to yourself, your users/customers, and society. This includes understanding your values, knowing who benefits and who pays, giving users control over their data, and taking feedback.

Understand Your Values
Examine the outcomes and trade-off of value-based decisions.
Some people fear that AI assistants like Siri and Google are always listening. They have been designed to guess what users want to know before they’re asked, providing extremely useful just-in-time information. However, it also raises concerns among privacy and security-conscious users.
An individual’s or company’s values may come into conflict when making decisions, which results in compromises. For example, users love the convenience of personalized results but may be concerned about what a company knows about them (privacy) or what the company may choose not to disclose to them (discrimination). Unfortunately, AI assistants are found to be not so useful for everyone since their training seems to exclude African-American voices. When tradeoffs are made, they must be made explicit to everyone affected. This can be difficult if AI algorithms are “black boxes” preventing their creators from knowing exactly how decisions are made.
Constant examination of outcomes is required to understand the impact of those tradeoffs. Let’s say your company is designing an AI-enhanced security system that results in some loss of individual privacy. Consider the following:
If protecting user privacy is a stated company value, employees (not just the top execs) should be aware of this tradeoff.
Additionally, customers and the public should be informed as to how individual privacy is impacted by using the security system.
If this is hidden for fears of PR backlash, then it must be asked, “Is user privacy really a company value?”
Explaining why the company made the tradeoff and what it is doing to mitigate harm can go a long way to keeping the public’s trust.

Give Users Control of Their Data
Allow users to correct or delete data you have collected about them.
Google’s goal is to make the world’s information “universally accessible and useful.” Since 2014, they have received 2.4 million “right to be forgotten” requests to remove information private individuals, politicians, and government agencies find damaging. However, Google has complied with only 43.3% of requests.
Companies can collect and track a stunning amount of data about their users online, in stores, and from internet-enabled (IoT) devices. It is only ethical to allow users to see what data you have collected about them and to correct it or download and delete the data. If your company is operating in the EU, you need to be aware of the EU’s General Data Protection Regulations (GDPR) and how it impacts what you may collect and store, as well as rules around allowing users/customers to download and delete their data.
In addition, make sure it is possible to accurately represent the data. For example, is it possible for users to indicate their gender if they identify as non-binary? Do they have the option to select more than one racial background?
If the data collected are anonymized and it is not possible for users to see exactly what the company knows about them and edit it, clearly communicate the kind of data collected and enable individuals to opt-out. If users can’t use the product without the data collection, communicate that as well.

Take Feedback
Allow users to give feedback about inferences the AI makes about them.
Three national credit bureaus gather information on individuals to create credit reports that lenders use to determine the risk of a potential borrower. Individuals cannot opt of the data being collected and must go through onerous lengths to fix incorrect data or inferences about them.
Inferences drawn about an individual (e.g., high risk for loan default) can have harmful consequences without the individual’s knowledge or control (e.g., inability to get a loan). Unfortunately, those suffering most at the hands of AI and “big data” are the already marginalized, poorer, voiceless communities (e.g., those without internet access who cannot quickly see their credit report or file requests for correction).
EU law requires AI decisions with serious consequences be checked by a human with the option to override it; however, a single data point in isolation is meaningless without understanding decisions made about others (e.g., is the loan approval recommendation different for black vs. white customers despite all other factors being similar?). It is important to understand AI recommendations or predictions in context.
Being transparent about the inferences and allowing individuals to give feedback not only enables you to improve the accuracy of your model, but it also allows you to correct for discrimination. This can be an advantage over competitors that unfairly dismiss viable customers. For example, a bank that rejects a large number of loan applicants as being too high risk might identify micro-loans as an alternative offering that not only supports the community but also results in a loyal customer-base that the bank’s competitors have ignored. This enables the customers to improve their financial standing and leverage more of the bank’s offerings, which results in a virtuous cycle.
It Takes a Village to Make a Difference
From cultivating an ethical culture to being transparent about a company’s values and empowering its customers, there are multiple actions a company and its employees should take to create an ethical foundation to build AI products on. To dig into ways to remove exclusion in your AI-based products, check out Part II.
I would love to hear what you think! What does your company and you personally do to create an ethical foundation in your work?
Thank you Justin Tauber, Liz Balsam, Molly Mahar, and Raymon Sutedjo-The for all of your feedback!
Follow us at @SalesforceUX.
Want to work with us? Contact uxcareers@salesforce.com

= How to save A.I in 3 easy steps =
How to save A.I in 3 easy steps
Do the benefits of artificial intelligence outweigh the risks?

The philosophy of Artificial Intelligence is a riddle so confounding that it is unclear where, and how, one would even begin to address the questions plaguing its era. Is A.I a revolution or a war? A god or a pet? A hammer or a nail? Nowadays, A.I can write and analyse books, beat humans at about every game conceivable, make movies, compose classical songs and help magicians perform better tricks. Beyond the arts, it also has the potential to encourage better decision-making, make medical diagnoses, and even solve some of humanity’s most pressing challenges. It’s intertwining with criminal justice, education, recruiting, healthcare, banking, farming… These advances alone could lead many to end the conversation there and then, with overwhelming evidence that the benefits of A.I reach far and wide within society, outweighing the risks associated with such a technology.
Yet, such a world-wide reshuffle of various industries is bound to lead to technological and ethical soul-searching. And though the world is unlikely to see its own AM/HAL/SHODAN/Ultron/SkyNet/GLaDOS bring about the Apocalypse anytime soon, a multitude of varied uncertainties have nevertheless arisen.
Not least of which is the matter of automatic war-waging: the recent implementation of Google’s A.I capabilities within the American military to improve the targeting of drone strikes has raised some serious questions about the battlefield moving to data centers, and the difficulty of separating civilian technologies from the business of war. Those worries are linked to those the world’s top artificial intelligence minds, who last summer wrote an open letter to the UN warning that autonomous weapon systems able to identify targets and fire without a human operator could be wildly misused, even with the best intentions.
Amazon, Google’s current A.I arch-rival, has also widely publicised its work with government agencies. Some have begun implementing its powerful real-time facial recognition system, which can tap into police body cameras and city-wide surveillance systems. This technology is fraught with ethical quandaries: civil rights organizations argue that any technology used to record, analyse and stockpile images of faces on a vast scale will be used to target communities already besieged by social challenges.
Indeed, despite plenty of potential valid uses, industrialised facial recognition would alter civil rights, fundamentally changing notions of privacy, fairness and trust. This is no longer science-fiction: Moscow-based NtechLab has an ethnicity detection capability in its face-detection software. This would encourage overt racial profiling from authorities, as studies have shown that machine learning systems internalise the prejudices of the society that programs them.
Finally, though the technology may not be colour-blind, it is collar-blind: arguably the largest shift A.I might lead to in the near future is the systematic automation of dozens of both blue and white-collar roles. This shift is already well underway in various sector, thanks to advances in machine learning technology, and represents but one of many waves that is predicted to leave millions of employees jobless. Experts argue that workers will need to develop larger skill-sets than ever before to prepare for this future. Yet, much like other western economies, the U.S currently invests just 0.1% of its GDP in workforce training and support programs, lending little hope for a wide array of disappearing professions.
However, some of those fears, thought valid, exemplify a profound misunderstanding of the science backing these supposed technological leaps. Most, if not all the examples above are a product of machine learning, which is far from the A.Is envisioned in most popular science-fiction movies. Machine learning, in fact, is a rather dull affair. The technology has been around since the 1990s, and the academic premises for it since the 1970s. What’s new, however, is the advancement and combination of big data, storage power and computing power.
Open-ended conversation on a wide array of topics, for example, is nowhere in sight. Google, supposedly the market leader in A.I capabilities (more researchers, more data and more computing power) can only produce an A.I able to make restaurant or hairdresser appointments following a very specific script. Similar conclusions have recently been reached with regards to self-driving cars, who all-too-regularly need human input. A human can comprehend what person A believes person B thinks about person C. On a processing scale, this is indistinguishable from magic. On a human scale, it is mere gossiping. Humanity is better because of its flaws, because inferring and lying and hiding one’s true intentions is something that cannot be learned from data.
In fact, A.I breakthroughs have become sparse, and seem to require ever-larger amounts of capital, data and computing power. The latest progress in A.I has been less science than engineering, even tinkering; indeed, correlation and association can only go so far, compared to organic causal learning, highlighting a potential need for the field to start over. Researchers have largely abandoned forward-thinking research and are instead concentrating on the practical applications of what is known so far, which could advance humanity in major ways, though it would provide few leaps for A.I science.
Machine learning is clearly great for specific, specialised tasks, but a self-teaching A.I that can match or best humans across various disciplines is, for now, out of reach. Ironically, artificial intelligence may fall short of matching and besting organic intelligence for the sole reason that it wasn’t built in humanity’s image.
As creators, it is nevertheless mankind’s duty to control robots’ impacts, however underwhelming they may turn out to be. This can primarily be achieved by recognising the need for appropriate, ethical, and responsible frameworks, as well as philosophical boundaries. Specifically, governments need to step up, as corporations are unlikely to forego profit for the sake of societal good.
One such framework can be found within President Macron’s nationwide A.I plan, which not only includes $1.6 billion in funding, new research centers and data-sharing initiatives, but also, and most importantly, incorporates ethical guidelines. Much like Asimov’s 1942’s 3 laws of robotic, however, they are elegant yet inherently subjective, and as such hard to enforce. To palliate these shortcoming, one may be inclined to implement the following 3 rules. Though unaesthetic, they have the merit of being both applicable and impactful in very real ways.
1. A.I Responsibility
This rule may appear blasphemous for many free-market proponents, raised as they are in countries where tobacco groups do not cause cancer, distilleries do not cause alcoholism, guns do not cause school shootings and drug companies do not cause overdoses. Silicon Valley has understood this, and its go-to excuse when its products cause harm (unemployment, bias, deaths…) is to say that its technologies are value neutral, and that they are powerless to influence the nature of their implementation. That’s just an easy way out.
Algorithms behaving unexpectedly are now a fact of life, and just as car makers must now be aware of emissions and European companies must protect their customers’ data, tech companies must closely track an algorithm’s behavior as it changes over time and contexts, and when needed, mitigate malicious behavior, lest they face a hefty fine.
2. A.I Honesty
Put simply, a digital intelligence should state that it is a robot. The scope for mischief once robots can pose as humans is simply too large, ranging from scam calls to automated hoaxes, to wait and see what happens. Drawing once again an example from the organic world, manufacturers put Ethyl Mercaptan in normally odorless natural gas to notice a catastrophe before it’s too late. A.I should be held to the same standard. When the machines cross the “general intelligence” threshold, they may choose how they want to sound and act, but this is yet a long way away.
3. A.I Transparency
Any human can explain, with varying degrees of accuracy, why he or she performed certain actions. A.I should be expected to do the same, if not more given the superiority of their processing power. There ought to be no place in this world for black boxes making intrinsically vital decision: the technology was created to avoid just this. Without this rule, the first two cannot stand as no honesty or responsibility can be expected from a system which is not understood, and which may not understand itself.
In the face of a limited technology and a plethora of potential uses, the benefits of A.I clearly outweigh the risks. This is however no reason not to have a conversation about its implementation, before the robots start doing the talking for us. Because of the technological limits mentioned above, machines simply can’t understand the world as well as humans do. This however does not mean that ethical issues shouldn’t be addressed when we assume they can, and let them make decisions accordingly.
At the end of the day, A.I holds a dark mirror to society, its triumphs and its inequalities. Maybe, just maybe, the best thing to come from A.I research isn’t a better understanding of technology, but rather a better understanding of ourselves.
Sign up
Every week, I work hard to bring new and interesting topics to thousands of inboxes. Sign up using this link to join the fun.

= Why life should NOT be like your Spotify account =
Just a guy contributing to his customer profile. Photo by Matthew Henry from Burst
Why life should NOT be like your Spotify account
The ethics of using AI to create customer profiles
I’m sitting in front of the radio with bated breath. Hoping the next song is gonna be ‘Everybody, (Backstreet’s back)’ by the Backstreet Boys. Yes, I’m admitting to this. A wave of disappointment washes over me. It’s Celine Dion’s ‘My heart will go on’ that’s blasting through the speakers. The year is 1998 and I’m only 7 years old (I hope you’ll forgive me for my taste in music).
The times of praying to the radio gods to play your favorite song are over. Your Spotify account is a reflection of who you are and it leaves nothing to chance. It holds music for all your different moods and activities. For me, this ranges from ‘‘Lazy Sunday mornings’’ all the way to ‘‘Frantic House Cleaning’’.
Chatbot Conference
Chatbot's Life, will host our 2nd annual Chatbot Conference in San Francisco. The event features the top Bot…www.eventbrite.com
All the music you listen to has one thing in common. You like listening to it. And, Spotify knows this. I’ll even go as far as saying that Spotify better understands your taste in music than you do. It recommends you new songs, artists, albums, playlists — and it does this by relying on your customer profile.
Top 3 Most Popular Ai Articles:
1. TensorFlow Object Detection API tutorial
2. Deep Learning Book Notes, Chapter 1
3. Deep Learning Book Notes, Chapter 2
And this is where it gets tricky…
Let me tell you why customer profiles are the hottest thing in marketing right now. And, let me give you 2 reasons why this is not a good thing, by giving the Ethics of AI it’s rightful place in the spotlight.
Online customer profiles as the holy grail of marketing
In 2017, 1.66 billion people bought something online. This number is only going up! We’re all starting to get hooked on the online shopping sensation. It makes total sense to optimize the online experience as a business strategy. Building better and more elaborate customer profiles is the way to bring in serious cash.
Customer profiles recently got a serious upgrade. They’re now often powered by Artificial Intelligence. Companies use all the data they have on you (and that’s a lot) to improve your customer experience, create more personal interactions, and develop better products. All this is done to sell more stuff.
As a customer you’ve come to expect nothing less. You want your favorite brands to know you, to recommend you relevant products, and to decide what you like. Wait, what?! That last thing can’t be right. Right?
Your life is stuck on repeat (and shuffle mode - if you’re lucky)
I’ll give you two important, ethics-inspired reasons why algorithmically powered customer profiles aren’t always awesome.
Reason #1 - Customer profiles limit your ability to develop new preferences
A unique thing about being human is that we all like different things. I’m a serious latte lover, whose got a passion for ethics & AI, and I enjoy listening to indie rock (recommended to me by Spotify). But this does not, necessarily, apply to you.
You develop your personal preferences through experiencing different things, and through sharing these experiences with others.
You probably know how the massive online universe can easily become a tiny bubble by your clicking behavior. It’s this ‘‘filter bubble’’ that prevents you from experiencing new things and ideas. And, thereby, it limits you in developing your preferences.
The risk of being locked up in your personal ‘‘echo-chamber’’ is all too real. In this chamber you hear nothing but the deafening sound of what you already like and what you already believe is true. There’s no room for other people’s ideas or preferences.
Essentially, your life is stuck on repeat - and if you’re lucky, it’s in shuffle mode.
The song you can’t get out of your head
Now you might think: This is not really a problem. ‘‘I’ll always love listening to the classics. And, I really kinda like my bubble.’’ That’s why I present you with:
Reason #2 - Customer profiles have a polarizing effect
It’s not necessarily a problem if you’re only exposed to a specific type of music (unless it’s Justin Bieber - sorry, not sorry). But it’s a major problem if you’re only exposed to a specific set of ideas.
Customer profiles extend beyond recommending songs and products. They also infiltrate your newsfeeds and social platforms. Companies like YouTube, Facebook and Twitter want you to stay with them as long as possible, that’s how they make their money. So they feed you content that sticks.
Combine social platforms with the high stickiness factor of extreme ideas (which are basically like cat memes…) and you can spot trouble from a mile away.
In 1996, MIT researchers already warned for the dangers of not being exposed to new or different ideas. They predicted this could lead to serious polarization in society. I’m sad to say: they’ve been proven right.
One horrifying example is the major role Facebook played in the violence that killed thousands of people during the Rohingya crisis in Myanmar. The Guardian explains how Facebook made it possible to spread fake news and very sticky, very extreme ideas. In an already emerging conflict this polarized groups even more, contributing to instigating violence amongst them.
And don’t even get me started on the US elections, the Cambridge Analytica scandal, and the power of Russian internet trolls… Seriously, don’t.
Together we can compose a better future for AI
Does this mean I see no room for AI in powering customer profiles? Of course not! However, there needs to be transparency on how algorithms decide what we like. This also means that you should be able to decide when that’s okay and when it’s not.
I’m not suggesting that I have all the answers. But I do have a firm believe that now’s the time we need to start talking about what technology SHOULD do instead of what it CAN do. Together we can compose a better future with AI in it.
I’m gonna put my money where my mouth (or ear) is. I’ll be listening to heavy metal on Spotify this afternoon. Who knows, I might end up developing a new preference!
What are you gonna do to make sure you’re in charge of your life’s playlist?
You can leave your thoughts in the comments — I’d love to read them! ❤This article is a great place of inspiration on how to ‘’pop’’ your filter bubble.
If you liked this article, please 👏. Remember, you can clap up to 50 times - and it really makes a great difference for me :) Thanks for reading!
Please note: the opinions expressed in this article are my own, and don’t reflect the view of my employer.





= How to build trustworthy AI products =
How to build trustworthy AI products
Photo by Andy Kelly on Unsplash
You can’t just ‘add AI’ to a project and expect it to work. It isn’t magic dust that can be sprinkled on a product.
The key to building systems that are integrated into people’s lives is trust. If you don’t have the right amount of trust, you open the system up to disuse and misuse.
This post (and this corresponding talk below) was originally created for product people getting started with data science, machine learning, and AI as part of the Product School on May 30th. While there is a brief introduction to AI/ML there are a lot of things that more experienced practitioners can learn about trust in AI.
Trust 101
The reason that trust is important is that it helps facilitate cooperative behavior. This is the cornerstone to how we create complex societies.
It boils down to a few key things:
Contract — written or unwritten
Focuses on expectations for the future
Based on past performance, accountability, and transparency
Builds slowly but can be lost quickly
What does trust have to do with machines?
When I was working on a startup in the restaurant space called Complete Seating we built a lot of features to help hosts manage their dining room floor. We included intelligent waitlist times and automatic table recommendations.
We called it analytics, business intelligence, prediction, and constraint programming back then but it would have been called “AI restaurant management” if we were fundraising today.

Unfortunately, we didn’t gain their trust because we lacked two of the three key aspects of trust: performance and transparency.
Performance — it was a beta product and at first it didn’t live up to the expectations. Once the service did start performing as it should we had a big hill to climb up to rebuild that trust.
Accountability — when something went wrong we would always answer their call or show up on site to help out. They knew we were trying to help but it still didn’t fix the other two issues.
Transparency — we didn’t give them good abstractions so they could understand what was happening behind the scenes. They wanted to apply their expertise and they didn’t know what we were covering or not.
We initially attempted to address these issues by providing more context to the hosts.
When looking back we should have started simpler by pointing out potential errors the host would make rather than obfuscating the entire seating engine.
How do machines build trust with humans?
We need to keep the human at the center when we think about building trust with them. This is one of the biggest dangers I see with AI in the coming years.
Using the same framework we can think about machine building trust as the following:
Performance — this isn’t as simple as traditional accuracy, precision, and recall. We need to be worried about what the right metrics are towards human purpose.
Accountability — there are many ethical and legal questions that need to be answered on what responsibility the designers of the systems need to take in their operation.
Transparency — how do we build systems that are interpretable, provide adaptive action, give the feeling of control, allow intervention by humans, and get feedback to improve the systems.
Check out the talk for a deep dive into each of these key aspects.
What is the right level of trust for machines?
In Humans and Automation: Use, Misuse, Disuse, Abuse, Parasuraman & Riley, 1997 they discuss the various ways that people use automation based on trust:
Use — “[Voluntary] activation or disengagement of automation by human operators.” This is when there is the ‘right’ amount of trust.
Misuse — “[Over] reliance on automation, which can result in failures of monitoring or decision biases.” This is when the human trusts the system too much.
Disuse — “[Neglect] or underutilization of automation…” will happen when the human doesn’t trust the automation enough.
Abuse — “[Automation] of functions… without due regard for the consequences for human performance…” This happens when the operators (or those impacted) are not taken into account when designing automation. This is one of the biggest dangers I see with AI in the coming years from my point of view.
How can we learn about trust in machines?
We can learn about expectations and needs of trust through prototyping and research.
When prototyping, you don’t need to actually build and train models. You just need to consider a few different situations and fake it:
Correct operation or ‘happy path’
Incorrect operation — false positives/negatives from the automation
Both sides of the borderline of misuse/disuse to understand the ‘right’ trust levels
Appropriate scenarios for feedback from human to machine
State communication from machine to human for your abstractions
I have also found that when starting to build using a “Wizard of Oz” or “concierge” approach to MVPs for AI can help you focus on what you really need to learn.
In conclusion
Remember:
Using AI is like any other technology that has tradeoffs… and you should be aware of the nuance.
Build for the right amount of trust — not too much or not too little.
Trust is based on performance, accountability, and transparency to humans.
Check out the video for the full talk and more details for each of these points.

About Chris Butler
I help teams understand the real business problems they should solve with AI-centered solutions. The teams we work with are generally asked to “do something interesting” with the data they have. We help them avoid local maxima through happenstance correlations and focus on solving huge business problems. My background includes over 18 years of product and business development experience at companies like Microsoft, KAYAK, and Waze. At Philosophie, I have created techniques like Empathy Mapping for the Machine and Confusion Mapping to create cross-team alignment while building AI products. If you want to learn more or get in touch via email, LinkedIn, or visit http://philosophie.is/human-centered-ai.
Slides:
Chris Butler - PS Presentation
Part-time Product Management Courses in San Francisco, Silicon Valley, Los Angeles, New York, Austin, Boston, Seattle…docs.google.com
References:
What’s the difference between data science, machine learning, and artificial intelligence?
A Machine in the Loop Approach
Star Wars Social Network
Human Centered Machine Learning
Empathy Mapping for the Machine
Confusion Mapping
Should your customers be conned by a human or AI?
The ‘Concierge’ vs. the ‘Wizard of Oz’ MVP
WoZ Way: Enabling Real-time Remote Interaction Prototyping & Observation in On-road Vehicles
Wendy Ju at Cornell
TWiML & AI Podcast #110 with Ayanna Howard
Emergency robot video
Placebo Buttons (fake thermostats)
Illusion of control bias
That is our exit!
Moral Crumple Zones
Accountability in a computerized society
Testing AI concept in user research
Some Observations on Mental Models (1987)
Humans and Automation: Use, Misuse, Disuse, Abuse (1997)
Overtrust of Robots in Emergency Evacuation Scenarios (2016)
Trust in Automation: Integrating Empirical Evidence on Factors That Influence Trust (2015)
Zero, one, or infinity rule
Law of Medium Numbers
Shane Lewin, Lean AI Product Development
NYT MTA Subway Crisis article

= Fairness, Accountability, Transparency in Machine Learning =
Fairness, Accountability, Transparency in Machine Learning
We need to talk more about the impact of Artificial Intelligence not only on business models, but as well on our society and each one of us. This thought in mind I try to collect information and inspiration around major aspects of AI. Work in progress and very focussed on my limited horizon. Please help me to broaden it! :-)
First thing I came across: a rising movement on “Fairness, Accountability, Transparency in Machine Learning”
Mailing list for “Fairness Accountability Transparence in Machine Learning”
Conference in New York end of february 2018 with papers and videos online
An article by SAS EVP Oliver Schabenberger
Andreas Gödde developed a piece around that in an interview with me
Secondly its about the cultural impact of AI in general:
Interview with Stephanie Sommer on Cultural Paradox in AI:
there is a project called One Hundred Year Study on Artificial Intelligence (AI100)
Third thing is the ethics aspect:
Question: Is AI just like any other technology which causes fears at the beginning?
Thoughts: No, its so massive and pervasive. Plus it slowly but surely takes over a lot of tasks humans claimed form them as “human”. So it infects humans self understanding. In the concretes way its a fear of loosing your job.
Question: Should AI be regulated?
Thoughts: Somehow, yes. BUT: this would create a kind of super-bureucrazy (since everything will be infected by AI). Maybe we should take care of the most sensitive areas. And there are examples in place where analytics is already regulated (for banks: “Model risk management”)
Question: What is the responsibility of companies using AI?
Thoughts: Don´t see your technology as neutral. Think of use and abuse of it and be prepared to run a discussion around. Its not a choice to blame the algorithm for decisions — at least companies have to be very transparent around it. Example: when the only serious competitor for in-country-flights in Germany left the market, the prices climbed significantly. The market leader publicy stated that it was not him taking advantage of the new situation — but the pricing algorithms simply achieved better conditions for them…
Question: What can software vendors do?
Thoughts: Implement interpretability. No one will be able to understand the code or the formula of the algorithms in use. So the explanation has to be understandable. How to build trust? Should there be a kind of certification system? Can there be something like “privacy by design” or even “ethics by design”? How to systematically avoid biases?

= Just let Google Duplex do the talking =
Just let Google Duplex do the talking
Because we have bigger problems than chatty computers

Last week Google demoed a software tool that uses fluid, conversational speech interaction on the phone in order to gather information, or to perform simple tasks such as booking appointments at the hairdresser. The person on the other side of the phone may or not realise they’re talking to a machine. After the initial amusement, everybody (but me) got up in arms.
Google Duplex — this is the provisional name of the tool — is just an experiment, but it allegedly performs well in most phone conversations of those kinds. With some extra work, Duplex could be easily deployed as a consumer service and take appointments or book restaurants for you and me.

The tool is smart enough to avoid human’s natural aversion to dialoguing with a machine, and achieves so also by using unnecessary — though human-sounding — vocal cues, such as ‘ums’ and ‘ahs’. This did not go down well with the ethics police — a group I am proud of being part of, when the opportunity is right.
What are the objections? Well, “Google’s experiments do appear to have been designed to deceive, because their main hypothesis was ‘can you distinguish this from a real person?’”, said Dr Thomas King, a researcher at the Oxford Internet Institute’s Digital Ethics Lab. Social media theorist Zeynep Tufekci wrote that Duplex “making calls pretending to be human not only without disclosing that it’s a bot, but adding ‘ummm’ and ‘aaah’ to deceive the human on the other end with the room cheering it… horrifying. Silicon Valley is ethically lost, rudderless and has not learned a thing.”
I am sorry to break it up to you, but humans are great at deceiving, too. Children learn to deceive before they learn to talk. Pulling your hair our because a machine is taking baby steps in mimicking behaviour humans never managed (wanted?) to fix in >200,000 years on this planet looks at least hypocritical to me. Deception and lying are part of everyday human communication, whether we like it or not: the fact that a machine can do that, too, just makes it a better communicator when it has to interact with humans.
According to some of the detractors, Duplex has no rationale to deceive about being a machine, I get that. But — if Duplex did — would that be relevant to the purpose of the call? I’m Italian and live in the UK, where I speak and write English, I’m so deceptive! 😀 Do you think that I would have better social interactions if I started every sentence by saying: “I speak English, but I’m actually not from here”? #hostileenvironment
It is also true that someone may simply not want to talk to machines… in the same way one may not want to talk to Italians (eww!). What does that say of you? Why should we start discriminating also on the voice of whatever is calling us on the phone? Human bias is already documented for people’s skin colour and features, gender, names (as a proxy of ethnicity)… and now also if one’s voice is more or less… biological?
Moreover, Duplex may perform its tasks at the service of a human: I would be discriminating against that human. It’s the original idea I started from to write this: the new technology can be a lifeline to people with any kind of impairment affecting spoken communication. We should be careful and stop now developing any kind of squeamishness for synthetic speech.

I agree with the many, like David Meyer, who say that ethics needs to be baked into technology from the start, and not as a reactive afterthought. However, evil is not in the voice being more or less synthetic, but in the intention of who is running the show.
In fact, the only real worry that Duplex suggests to me is how easy it will soon be to scale social engineering attacks. To do great social engineering, until yesterday, you needed a master like Kevin Mitnick (btw his autobiography “Ghost in the Wires” is a great read). As great as Kevin is, he can make only one phone call at a time, and has access to his brain and his notebook only. Do you see where I’m going?
So, please, let’s keep calm and worry only about what really needs to. In the demo, Duplex had just booked someone for a haircut, and to have lunch at some fancy restaurant. In the meantime, perfectly human crooks are destroying the environment and making society less and less equal, but they have a sweet, sweet real voice.
Find all the due credits for the images used in this article on GitHub here.

= Don’t make me think =
Don’t make me think
Firesouls’ Dan Ebanks on the ethics of digital product design

We invest a lot of time in user research and user testing. Invest being the operative word — the return on that investment is an obvious driver of design that isn’t always articulated. To be prosaic, we design our platforms so our customers want to use them and are happy to pay for them.
Ours is an enterprise platform and presumably the dynamics are different for B2C products. For example, quite how you square designing a product to make people want to use it when you’re helping users manage, say, mental health issues opens up a very big box of ethical issues, potentially. What are the risks of swapping one dependency for another if the tech is designed to make people want to continue using it?

To paraphrase Steve Krug, we wrap our products in a user experience that means users don’t have to think (much).
But the ‘don’t make me think’ mantra raises questions about the design of digital products and the way they potentially affect our brains.
I’ve just finished reading ‘The Shallows’ by Nicholas Carr. Carr’s thesis is that the proliferation of information on the Internet and the way we access that information causes us to lose the ability to think deeply and creatively.
Key to this is the neuro-plasticity of our brains: ‘neurons that fire together, wire together’. The more our brains think in a certain way, the easier it becomes for them to think in a certain way, and the more they want to think in a certain way. Carr quotes the French psychologist and philosopher Leon Dumont:
“Flowing water hollows out a channel for itself which grows broader and deeper; and when it later flows again, it follows the path traced by itself before. Just so, the impressions of outer objects fashion for themselves more and more appropriate paths in the nervous system and these vital paths recur under similar external stimulation. ”

According to Carr, the addictive nature of digital products and the calculative ‘thinking’ of algorithms used to support the search for knowledge results in a habitual consumption of information via the Internet driven by convenience and popularity, rather than by a sense of objectivity or ‘truth’.
At Firesouls, we’ve recently signed up to the Trustworthy Tech programme, led by Cassie Robinson and Laura James of Doteveryone. You can find out more about the good work they’re doing here.
I’ve also just bought a Nokia 3310. Some times the old school ways are the best.
Plus it’s got Snake.
***UPDATE: It’s got the new version of Snake. If anyone from Nokia reads this, I’d just say: error.
Firesouls’ tool the Social Value Exchange matches community projects with extra resources created during government procurement.

= AI: 4 Key Take-aways That Separate Fact from Fiction =
AI: 4 Key Take-aways That Separate Fact from Fiction
By Dave Carpenter
“We are summoning the Demon,” warned Elon Musk in a Vanity Fair piece last spring, when describing what’s in store for humanity should artificial intelligence’s evolution go unchecked.
As Founder of Tesla Motors and SpaceX, Musk resides in rarified air among Silicon Valley’s most successful visionaries. Elon Musk also has a penchant for flame throwers and catapulting luxury cars into space. Suffice to say, Mr. Musk has a flair for the dramatics.
Indeed, AI’s current state and foreseeable future is far more pedestrian in comparison, according to the panel that convened in Toronto in January to discuss the event’s theme ‘Ethical AI: What Kind of society do we want to have?’
Hosted by Humans For AI and Girl Geeks Toronto attendees listened to distinguished women in the artificial intelligence field talk about how, despite AI having already improved many aspects of our daily lives, have we ceded too much control to machines in exchanging our personal information for convenience?

Here are the takeaways from the discussion: (see full list of panelists at bottom)
Our Robot-Like Overlords Reside in Hollywood (and Elon Musk’s head).
When machines might attain human-level cognition skills to independently solve complex problems isn’t foreseeable. Despite dramatic press headlines, self-learning machines — let alone malevolent self-conscious ones — are the stuff of fiction. Artificial Intelligence and its efficacy depends largely on concise human instruction.
As panelist Inmar Givoni, Autonomy Engineering Manager at Uber Advanced Technologies Group put it,
“Almost all successful models out there are supervised models. This is actually a problem right now. Machines can’t think enough for themselves, actual humans have to label all the data points for them”.
In the case of driverless cars, examples of these flesh-and-blood-supplied data points include street signs, lanes, stoplights.
Data Is The Real Terminator
Big enterprises have amassed vast amount of our personal data, but data for data’s sake provides little value for a business’ bottom line. According to panelist, Karen Bennet, VP of Engineering at Cerebri AI:
“Private enterprises see AI as the next wave in making a lot of money, but you have to know what to do with that data in order to make AI useful.”
Increasingly for banks and credit auditors, the answer lies in applying iterations of supervised machine learning such as ‘classification’ to comb massive amounts of data (aka ‘Big Data’) to determine our financial viability. When we apply for a loan, “most of us are unaware that we are consenting to more than we ever dreamed,” says Bennet. When you apply for a loan, consumer credit reporting agencies such as Equifax compile considerably more of our personal data than most of us actually consider — the places we regularly shop, what we purchase, where we travel, what we post on social media — and apply machine learning to glean insights from these data points to not only determine whether you qualify for a loan, but also profit by exchanging this information with government agencies, financial institutions and insurance companies, all the while perfectly within their legal right to do so.
Overseas, The European Parliament, Union and Commission have established the GDPR (General Data Protection Regulation) with the aim of protecting citizen’s data, but it’s likely a toothless effort, and no such regulatory body exists in the US and Canada.
How Do You Solve a Problem Like Inherent Bias?
All the panelists agreed that, as AI models depend on human-inputted data, our subjectivity can, and has, lead to flaws with negative implications for societal segments.
The means for differentiating between causation and correlation in the field of AI doesn’t currently exist, and that’s a problem when homogenized groups apply AI to further research or product development, which results in real world consequences, such as biased algorithms used in US courts to help determine which individuals arrested over-indexed for black people, or as panel moderator and Humans For AI Chief Marketing Officer, Hessie Jones cited, the infamous case of the racist soap dispenser.
AI Will Serve Us (ideally all of us) For the Better:
As the discussion closed, panelists generally agreed that the machines will not rise up, and that artificial intelligence’s advancement will result in job loss in some industries, but new employment in others, where AI and humans work in concert. Think self-driving 18-wheelers with a human on board, providing direction and assistance where needed.
And, if and when there is a day when artificially intelligent machines challenge our full cognitive capabilities, it may lead to our own evolution. As Karen Bennett pointed out, at the famous 2016 Go (a chess-like game) world championship final, Alpha Go, a super-computer that utilizes machine learning, beat 18-time Go champion Lee Sedol three times in a row, but Sedol managed to rally and win the fourth match. Would Sedol have won that match had he not been forced to think differently by a machine?
List of Moderator and Panelists at ‘Ethical AI: What Kind of society do we want to have?’ in Toronto, hosted by Humans For AI and Girl Geeks Toronto (from left to right): Hessie Jones, Moderator, and Chief Marketing Officer, Humans for AI; Karen Bennet, VP Engineering Cerebri AI; Anna Goldenberg — Member of the Vector Institute, Assistant Professor at the University of Toronto Department of Computer Science, and Scientist at the Genetics and Genome Biology Lab at SickKids Research Institute; Inmar Givoni — Autonomy Engineering Manager at Uber Advanced Technologies Group.
Dave Carpenter is a member of Humans For AI and Principle at Carpendium Inc., a content strategy and creation consultancy based in Toronto.

= Human Factors: How We Designed an Adaptive Culture for Our AI Company =
Human Factors: How We Designed an Adaptive Culture for Our AI Company
Most companies won’t survive the transition to a workplace powered by Artificial Intelligence. It all comes down to their internal culture. Some teams simply don’t have the freedom to adapt quickly enough; others won’t use the flexibility they already have. In fast-pace environments where AI drives progress, organizational change will become the distinguishing factor.
Early on at creative.ai, we realized we couldn’t even build the kind of products we wanted to build without changing the way our company operated. Since then, we’ve continued to evolve our internal culture to level-up alongside our own AI systems. Not only is this approach helping us rapidly innovate in technology, but—more importantly—it lead us to develop the humanist principles necessary to take it further!
(Alex Champandard, co-founder of creative.ai, presenting the company’s invited talk at “AI & Society” in Tokyo.)
This article is based on creative.ai’s invited talk at “A.I. & Society” delivered in Tokyo recently, describing the motivation and story behind our company culture.
Watch the video recording of our invited talk here. (17 minutes)

Many of the graphics designers and visual artists that creative.ai works with wonder about the future. They wonder if we’re building a machine with big red button that says “AUTOMATE CREATIVITY” and if someone’s going to press that button what would happen... Are we heading for some kind of cartoon-like moment where we hit the ceiling and slide down the wall? Who knows! That’s the question everyone has and that’s probably why you’re reading this today...


But we’d like to flip the question around; instead of asking what the impact of technology is on society in the future, instead we’d like to ask what society does to technology. What has been the impact of our culture and societal values on technology?
We realize this is coming from the complete opposite perspective as most of the conversation online, and many people at the “A.I. & Society” event too. But we hope it stimulates some neurons and gets you to think about the problem from a different angle.

After a quick survey of our followers on social media, we found that 2 out of 3 people in the AI community and IT industry in general believe that machines will have strictly superior abilities compared to humans at some stage in the future — both intellectually and creatively. That’s two out of three! (67% is also corresponds to the number of Twitter bots that follow us, but it’s likely that’s just a coincidence.)
As a team at creative.ai, 70% of us think that humans will always have superior intellectual and creative abilities to machines. Some things machines may be more efficient or effective at particular tasks, but humans will always be able to do the superset of those abilities. We believe humans will always have the upper hand, so we’re coming at this situation from a different perspective.

At creative.ai we think of Artificial Intelligence as a technology tree, rather than an exponential curve of doom. It’s very much like a game of Civilization to play where you have different branches to pick, many options to choose, mix and match. Some of the techniques in the past we used may not have been funded well enough, we didn’t have the hardware or necessary understanding as a society to pursue this research…
Now we have the choice to be able to go back into the technology tree and decide what we want to achieve. What do we want to accomplish? How do we want society to turn out? We can jump back and make decisions according to these goals, given the extra perspective we have now.

For creative.ai, two of our founding pillars and the inspiration for the company are:
Augmentation & Interaction — when you work on creative things, that tactile feel and playful interaction is very important. It all goes back to the work of Douglas Engelbart in the late 1960s. We’re big fans of his work and building on his accomplishments early on in this technology tree.
Patterns & Generative Life—when building living systems almost like organisms, they emerge from patterns. This is based on ideas from Christopher Alexander since the 1970s that inform our design philosophy.
These are our own two guiding lights that we’re basing our approach on.

We also believe very strongly that the Team—or the Organization—equals the Product. And that’s called Conway’s law, which says that whatever patterns are present in your organization then those will be reflected directly in your product. So any advantages, opportunities, disadvantages, quirks, politics you have in the company will translate directly into the product you’re building.
At creative.ai, we’re firm believes of this Law, but also taken the opposite approach too, which is called the Inverse Conway Manoeuvre. It means designing the product you want to build then structuring your organization accordingly so the product emerges accordingly as designed. We’re very mindful about working on both of these and designing them accordingly, levelling them up side-by-side.


We’d like to take you through a short history of creative.ai, it’s a very young company relatively speaking, but we’ve been through many different mindsets that are reminiscent from The Industrial Age, then The Information Age, and a more humanist age coming up that we call The Creative Age.
For each of these mindsets, everything affects the whole company the whole product. There’s AI and design involved everywhere here, from the culture to legal, including messaging communication. Everything is inter-related so each one of these things affects the other. This article will dig into each of these…

When we began as a startup, we didn’t think like a big corporation; that’s the advantage of a small company. But we inherited lots of external pressure and expectation, so we fell into these traps of the corporate mindset which were not necessarily beneficial for us.

Looking at it from a legal perspective, ownership is control. As founders, we initially had all of the ownership with some allocated to the team. Then over time the investors buy incrementally buy over more and more of the company as the company grows and goes through multiple rounds of funding. As shareholders, whether founders or investors, we’re basically a small group of people that have control over the company.
We don’t think this structure, even when you have a board of directors, is intrinsically ethical. There are few people in control of what our A.I. technology can do and what the impact is on society. We’ve talked a lot about addressing this, for example with ethics committees, but since this ownership and control structure is not ethical in itself, we think it’s a bit like putting lipstick onto a pig. (We’ve been redesign this accordingly, as you’ll see below.)

The ownership structure reflects itself in the team directly too, where it’s usually setup as a command and control structure. The investors love to hear what your roles are, who is Chief Creative Officer? Chief of Technology? Science Officer? We had these different labels that we gave ourselves, even though we were a small company, they were expected from us. In the future maybe we’ll have a Chief AI Officer with an army of Machine Learning engineers reporting to them ;-)
This setup creates a lot of tensions, both within the hierarchy top and down between “leadership” and the team itself, as well as between separate branches of the hierarchy—in particular if you separate the design and technology!

People tend to be frustrated and dissatisfied when working in these kinds of environments, no matter the company size. It’s coming from a place of power and control, which affects the people on the team and their mindsets. Everyone spends more time in internal company politics (we call this “Job 2”, the one you’re not hired for) not being able to affect things outside of their small area of influence in the hierarchy.
What does design do? Is it a separate branch from technology? Do designers just write documents as large specifications and throw them over the fence? This isn’t a very participatory culture; not only does this produce worse results, with fewer people bought in to the designs, but also causes a lot of sadness…

Looking at the statistics globally for the U.S., 18% of employees are actively sabotaging the projects they are on. 52%, that’s over half of employees, are retired on the job! Are these the kinds of emotions you want in people building or using the next generation of A.I. systems?
These are direct consequences of the power structure that goes on in almost all companies today, as a reflection of the ownership structure and the hierarchy that goes with it.

As creative.ai, we wanted to change all this but we inherited a lot of this baggage from the outside. People defined us as a team of engineers solving creativity. This very much a trans-humanist message where we’re coming in on a shining white horse trying to save humanity as a whole by solving creativity.
Actually, there’s nothing to be solved; creativity is a process and doesn’t need a solution to be found… and engineering the least important part of it.

As a product, we talked about generative pipelines which resonated with some investors and big companies as well. The idea is that there’s an overall A.I. system that controls the generation of things end-to-end, then we put human-shaped holes inside this pipeline wherever there were problems we couldn’t “solve” with technology or where we needed more data.
This was a hybrid human/machine system, but it was not a humanist system. The people were basically cogs in a larger process that we were just using to gather data. It’s a form of centralization of power, just like the ownership and the hierarchy, but in this case it’s in the machine. Words like “cloud computing” and “platform” remind us about this power dynamic…

On the technology side, everyone talks about Big Data and Deep Learning. For the same reasons, they’re not the kinds of techniques we pursue primarily because every single individual’s is less important. You sacrifice every individual at the cost of the average; so if you’re not average, you’ll get sub-optimal performance.
But nobody is average. In the creative space, everyone wants to stand out! These techniques, as defined today, didn’t work for us. We couldn’t rely on them as our primary technology to empower individual creativity and agency. We have lots of data but we don’t think of it this way. Even the concept of a Server Farm is also a centralization of power, but you get the idea…

As a team, we started out firmly in The Information Age with a more distributed mindset, which affected how we approached things internally.

As you saw above, the ownership structure has a strong impact on the entire company and its culture. However, from a legal perspective, we were not “distributed” in the sense that we were not a cooperative with equal ownership—except at the very start when the company was founded. After that, we inherited a lot of legal structures and expectations.
Trying to keep things as fair as possible, we give out a lot of shares to employees and have deterministic salary models too. We also wrote exit clauses that specifies constraints on our technology should something out of our control happen to the company...

The thing we benefited from the most was having a distributed team from all around Europe. We were able to hire a dream team of programmers, designers, architects, in A.I. and M.L., frontend / backend—each that fit well with our company culture so they could onboard quickly and efficiently.
We found that consultants and contractors were the most keen to join forces with us. But we became very aware of this power tension between the company and the individuals, and we didn’t want to turn into a Gig Economy or the “Uber for AI”. We were very mindful of this and drifted out of this model.

When you have this power relationship between the company and its team members, the tension is like a competition and also affects the individuals themselves. Conflicts can emerge between the design and the technology side of building the same AI.
We got around this by using popular self-management techniques, and more transparency which helps a lot! But these are remnants of this Industrial Age mindset where the hierarchy is flattened out and the network must police itself: trying to control everything going on, making sure everyone shares all the time. There’s no trust there.

With people having more agency in a network, being able to decide what they can work on and who they can work with, it creates confusion and uncertainty about what they should be working on. Should I work on this new feature of the AI product or design the UI for its frontend? Should I collaborate with this team or this one over there?
So instead of being angry and frustrated with the hierarchy and lack of freedom, team members become more afraid of making missteps. From school and work life, most people have inherited a mindset of being in a hierarchy and when that structure is gone, most don’t know what to do with the freedom.

This fragmentation and fear also translates in the way we communicated about things. It was about our software vs. established tools, about our humanist approach vs. others that want to automate creativity. For example, “if you use our tool then we’ll use more augmentation.” We defined ourselves with this message to stick together to face the uncertainty of the future together.

The product in a distributed mindset is very fragmented; you have lots of applications on your mobile phone for style transfer or photo manipulation, etc. We fell into this trap too by considering many applications to build on top of our platform. Passing data between these different applications is difficult and doesn’t make for a very good experience. These are side effects of having a distributed culture, for example in a company where product teams don’t work as an integrated whole to collaborate together towards a bigger vision.

On the technology side, we’re building what we call the Generative Web where you can run algorithms on the server on behalf of designers, or on their own computers or mobiles as appropriate. Wherever the computation is most appropriate it will happen there, and the data will get passed around accordingly.
This kind of infrastructure is one of the biggest benefits of The Information Age mindset, making the most of the internet and the many hybrid devices that are connected together.

As a company, we’ve found ourselves most recently moving into this more collective mindset, which is new for us. We pulled together lots of different approaches about this, and are very curious to see where it will take us in the future. (We call this The Creative Age.)

In this mindset, it all starts with the people. We assume that everyone on the team is a mature, rational, responsible and has a purpose. Team members know why they’re in the company and know what they want to achieve in life. It also means they have an “Internal Locus Of Control” and feel like they have agency over their environment.

This changes the internal culture significantly. It makes a culture that’s based on Trust, since you can rely on each other to behave like mature rational adults. You don’t need radical transparency to make sure nobody’s making mistakes, but can rely on team members asking for help when they need it, sharing often because they know it helps everyone learn. If someone does make a mistake, there’s a culture of Respect—or maybe even, Love—to make sure that problem gets resolved in an appropriate fashion.
This culture also affects the way the company also addresses the team too, moving more into coaching to help everyone grow, support and encouragement of personal alignment, and lifelong employment as long as no foundational rules are broken. This helps create a space that’s conducive not only to growth and learning, but being more creative!

This is a great environment to build innovative A.I.-based products because it lets the team self-organize dynamically to tackle difficult problems that would be difficult to solve with a rigid structure. The kind of things we’re exploring nobody else has before, and we need the space for self-organization for everyone to come together to complement each other’s skills.
We picture it like a band. As soon as there’s a tune for a specific milestone or deadline, demo or client project, presentation that needs to be prepared, people with the necessary skills can just come together like band—then go back to regular jamming or practicing or join another project.
This approach often makes things faster to self-organize and it’s very dynamic. It’s not a fixed network structure or hierarchy, it’s more like a living organism that’s pulsating and vibrating to a common beat. It’s quicker to respond to external changes too.

We also think of the legal side for the control of the company as a shared performance too, with in this case four or five different voices: the team, the community, the clients/partners, and the investors. Each have their own style, tone and message to bring into this band that helps steer the company as a whole.
This makes it a much more ethical structure when there’s not a single mindset (profit) that’s dominating the Board of Directors. Instead, it’s the shared performance of a very diverse set of voices that come together in unison—in this case in favor of the common good. (This structure is sometimes called Fair Shares, and we’re currently looking into this with our partners.)

As we move into this more humanist phase, this also affects how the product works. The product is no longer a rigid tool that we hit things with, it’s an instrument that you play. Since it’s an A.I.-amplified instrument that learns how you play, it becomes a feedback loop in the cybernetic sense.
You work perfectly fine on your own as a creator, and this instrument can also do stuff on its own. But when you put the two together, the sum is bigger than the individual parts on their own. This is the core theme of augmentation.

It also reflects in the way we communicate about our product too. The messaging is about taking our A.I. instruments and putting them into groups to augment them, to improve them. This helps empower everyone on those teams to provide more agency, and a stronger sense of mastery over their creative process.

The technology is equally fascinating, and worth a whole other article! It’s all about interaction; how do creators interact together? How do we build our technology into instruments that are easy to interact with? How can these instruments plug together modularly and come together in these jamming sessions and creative environments where everyone contributes their own perspectives and skills.

To conclude, we think there’s a direct relationship between how you structure your organization and the product. If you want to have a positive impact on society you should start at home with the organization itself and rethink everything from the ground up. We’ve done so very mindfully by taking the organization’s principles and levelling up the product accordingly, but sometimes the product was ahead of the organization and we had to level up the team as well.


By doing this, we’ve likely gone out of the comfort zone of most managers and leaders, but just seeing how the team and potential future hires resonate with our culture, we know that in the future the best performers and the best teams will be the ones that really engage with these ideas and resonate with the projects they’re working on.

With all that said, if this message resonates with you and you’d like to work with us in any way, both on creative topics or AI-related topics, we’d love to hear from you!
If you’re a visual designer, we’d love to get your feedback.
If you’re a developer, then consider joining the team!
If you’ve read everything this far, we’re sure our paths will cross again on our journeys into The Creative Age. See you soon… #⚘
Watch the video recording of our invited talk here. (11 minutes)


= A Code of Ethics for Data Science =
A Code of Ethics for Data Science

2.5 quintillion bytes of data are created every day. It’s created by you when you’re commute to work or school, when you’re shopping, when you get a medical treatment, and even when you’re sleeping. It’s created by you, your neighbors, and everyone around you. So, how do we ensure it’s used ethically?

Back in 2014, before I entered public service, I wrote a post called Making the World Better One Scientist at a Time that discussed concerns I had at the time about data. What’s interesting, is how much of it is still relevant today. The biggest difference? The scale of data and coverage of data has massively increased since then and with it the opportunity to do both good and bad.

In the bucket of good. We’re finding incredible insights using data to develop tailored medical treatments (Precision Medicine). Recently a data scientist at the Data Science for Social Good Program at the University of Chicago used machine learning/artificial intelligence to automatically detect bridges from satellite images that have been flooded for first responders. Crisis Text Line has been literally saving lives every day through an all volunteer network of counselors with powerful data and technology superpowers to help those in crisis. And through the Data-Driven Justice Initiative we’ve seen local counties be able to get their populations that need mental help and drug treatment out of our overcrowded jails and into the facilities though the safe sharing of data. These solutions not only save money they are a proven success.
I could go on and on about all of the amazing work that is happening around the world using data to make lives better everyday, but we also have to address where data is causing more harm than good. As Propublica has shown, algorithms are being used in the courtroom to make decisions that have an adverse impact on race. We know that data used in predictive policing can reinforce traditional stereotypes. And my friend Cathy O’Neil documents many more cases in her great book Weapons of Math Destruction. Let’s not forget about people stealing our data. From healthcare breaches to data brokers, we have systems holding on to our most sensitive data with minimal oversight and protections. And finally, our democratic systems have been under attack using our very own data to incite hate and sow discord.
With the old adage that with great power comes great responsibility, it’s time for the data science community to take a leadership role in defining right from wrong. Much like the Hippocratic Oath defines Do No Harm for the medical profession, the data science community must have a set of principles to guide and hold each other accountable as data science professionals. To collectively understand the difference between helpful and harmful. To guide and push each other in putting responsible behaviors into practice. And to help empower the masses rather than to disenfranchise them. Data is such an incredible lever arm for change, we need to make sure that the change that is coming, is the one we all want to see.
So how do we do it? First, there is no single voice that determines these choices. This MUST be community effort. Data Science is a team sport and we’ve got to decide what kind of team we want to be.
To start we need to engage in conversation and spend much more time talking about the changes that are about to take place (to those who have been doing this, thank you!).
That’s why I’m excited about the opportunity for the ENTIRE data science community to take part in helping define what a Code of Ethics for data sharing would look like for data scientists. How do you get involved?
Join the global conversation remotely over Slack (channel #p-code-of-ethics) and follow @TechAtBloomberg on Twitter to tune into a livestream of portions of the Data for Good Exchange SF starting at 12 PM EST/9 AM PST on Tuesday, February 6th.
Get your team of data scientists at work or at a meetup together and start talking about what a Code of Ethics for us would look like.
Most of all, share what you’re learning! I want to hear from you of the slack channel, here on LinkedIn, or find me on twitter @dpatil


= (Why) Technology Isn’t a Substitute For a Soul =
(Why) Technology Isn’t a Substitute For a Soul
Or, Why Algorithms Can’t Replace Ethics

It happened, as it was going to. A self-driving Uber killed a pedestrian. But that was just one dot among many. Facebook sharing data with a shady company specializing in propaganda and worse, tilting an election. Theranos revealed to be a fraud. And so on.
Technology today is more powerful than it’s ever been. But it also has a complete and total void of ethics and morals. And in that gap, democracy, society, and the economy are going down in flames.
Hence, we see technologists approaching ethical questions in a childlike way. “Well, if robot cars ‘save’ 1000 people, but they kill 999, then that’s OK, right?!” Wrong. The underlying belief here is that a) the greater good is what matters most b) we know what the greater good is. In other words, since your existence has no inherent value, human beings are means, not ends. Means to the greater good, whether it is convenience, speed, or growth.
The greater good then gets coded into “algorithms”. The algorithms are increasingly running every last bit of our lives, for it. Workers are managed algorithmically. Stocks are traded algorithmically, and ads are served algorithmically, and recommendations are made algorithmically. Cars are driven algorithmically.
And the greater good is rising, you might think — GDP is growing, life is faster and more pleasing in some ways. But it is also meaner, nastier, dumber, and harder, isn’t it? That much is statistics — falling life expectancy, rising loneliness, regular school shootings, and so on. So something has gone badly wrong with algorithmic ethics — the greater good coded into machines that run our lives is not in fact really making them better. The pursuit of the greater good is somehow diminishing us. What went wrong?
The question before us today is this: can ethical questions, questions about human goodness, ever be solved algorithmically? Let us answer this in a roundabout way. If they can, then we don’t need judges, doctors, teachers. We don’t need parents, either — a computer program should suffice. We don’t need mentors, therapists, and friends. In fact, if algorithms can replace human ethical judgment, then we need no social bonds whatsoever. That sounds absurd, doesn’t it? Why, though?
What is it that you do with your friends? You mull over your ethical dilemmas, don’t you? “Ah, I love them. But I just don’t know if I can be with them anymore,” you say, ruefully. “I love the job. But I hate the income,” you lament. Your friends replies, and shares their own. But no one is applying an algorithm — they are sipping wine to win freedom from the strictures of ordered thinking.
What happens now, in the sharing of these ethical concerns? There is suddenly “friendship”, isn’t there? Now, suddenly, you are not alone. Loneliness — this sense of being a tiny, separate thing, turning slowly to dust — vanishes. You are in the presence of another being with ethical dilemmas, too — questions with no simple answers, no fixed “solutions”. Only more questions, struggles, doubts.
The gift of our ethical struggles is human intimacy. Think of the people whom you don’t like. Imagine, for example, a Trump voter. Why don’t you like them? Well, it is because your ethical concerns differ, often diametrically, from theirs. They decry immigrants, concerned about their own towns. You are concerned about a thriving, open society, concerned about small-mindedness. So in this way, we bond, we grow close to, we connect with, those whose ethical concerns mirror our own.
In this way, through intimacy, through nobility, through grief, dignity comes from our ethical struggles. We struggle, scrape, fight over terrible questions and dilemmas, of love and heartbreak, of necessity and possibility, in this brief moment we are alive — and to engage in this contest is dignity itself. When you can share your ethical struggles with your partner, you grow closer. When you can’t relationships often break down. Both people feel they have lost their dignity, don’t they?
Now let us extend that to society. When machine ethics replace human ethics, what happens is something far more dismal and dangerous than merely “machines making bad decisions”. A society loses all the qualities above. Intimacy, closeness, respect. All these things dwindle. And people lose their dignity.
In that way even if we have “solved” a “problem” of the greater good algorithmically— by reducing people to variables, numbers, quantities, we have only reduced the total amount of the good in society to begin with. Do you see the paradox? It is as if we have solved a problem of how to let everyone share a home happily by burning it down. Let make that clearer.
To be struck down by an Uber is not just a terrible loss of life. It is a loss of dignity. It is not just the extinguishing of a life — it is the annihilation of the ways in which that life matters. Sorry, you stepped in the wrong place. These streets belong to us. They are not places where you can wander, lost, asking questions about the meaning of your life. They are places to go to and from work. The hard questions have been asked and answered. Here are the answers. You are a consumer, you are a worker, you are here to contribute to the greater good. You are not a citizen, a person, an ethical quantity, who helps define and contest and pin down what the good means in the first place.
So now we have answered our question. Algorithmic ethics presumes to know the answer to the question — “what is the greater good?” “Why, it is speed!”, cries Uber. “Why, it is information!”, cries Google. “It is being liked!!”, shouts Facebook. All these answers are wrong. Not just because they are the wrong ones. But because there is no one answer. No final answer. There is only the question, and it must ever and always be asked and answered anew. And a better way to ask it is: “how do we lift one another up, always remembering how fragile each life is?”
Once we thought things like slavery served the greater good. Then, child labour and debtors’ prisons. This most difficult of questions — what is the greater good — is one that we must never allow to settle on any final solution. That way lies the end of progress, and the beginning of regress. For whenever someone supposes they have discovered “the answer” to the greatest good — have you noticed? — then dark ages fall. The Soviets did, and soon enough, the gulags were built. The Nazis did, and soon enough, the bombs fell. When we suppose we have answered this question of the greater good, then we must go out and impose our will, mustn’t we? Then any kind of human folly becomes possible.
Technology isn’t a substitute for a soul — especially when it whispers to us: “Here it is. The greatest good you must serve. It is the answer to all the problems and struggles of being human.”
This is the story of darkness and light, vying for the upper hand, as they have always done. But they do so backwards, and that is what we do not see yet. Whenever the greatest good tempts us, that is when darkness falls. And when we are able to stand up for human fragility, those are days of light.
Umair
March 2018

= Towards fair machine learning models =
Towards fair machine learning models
Our monthly analysis on machine learning trends
This post was originally sent as our monthly newsletter about trends in machine learning and artificial intelligence. If you’d like these analyses delivered directly to your inbox, subscribe here!
Imagine your team has built a relatively accurate machine learning model: it predicts the right medical device for a user 92% of the time. But after digging a little deeper, you discover that for people from a specific ethnic background, the model is almost always wrong. What now?
In this post, we’ll explore why accuracy alone is not always a good metric to judge a model and how algorithmic fairness can help. Fairness is a topic that’s expansive, essential, and all too often neglected. There’s been a lot of news about AI being sexist and racist, but simply knowing a model is biased won’t solve the underlying issue. We’ll show you how technical approaches to addressing fairness have evolved in recent years, what research focuses on today, and what the future might hold.
As the fairness and privacy researcher Cynthia Dwork has said, “Algorithms do not automatically eliminate bias.” In other words, fairness doesn’t just happen: rather, it’s the result of careful engineering, rigorous math, and a bit of thought-provoking ethical philosophy.

Why Fairness Matters
Discussions of fairness in machine learning tend to focus on how different models impact socially sensitive groups. Cathy O’Neil’s Weapons of Math Destruction and an eye-opening ProPublica article drew much-needed attention to the social biases that often get baked into our models, perpetuating racism, sexism and classism into AI.
A table from the ProPublica article showing algorithmic bias in risk assessments for criminal sentencing
Fairness is an expansive topic though, and is relevant to just about any set of users. Which users get shown specific ads, get offered specific prices, get rewards, get shorter call wait times, and are identified in terms of distinct propensity models? The FAT* (fairness, accountability, and transparency) community has created a set of fairness principles designed to help organizations consistently explain model outcomes. Now that the General Data Protection Regulation (GDPR) has officially come into effect, these are no longer theoretical concerns: users now have a right to contest the output of an automated system if they believe they were treated unfairly.

In our latest podcast, Susan Etlinger explains why fairness is good for business, improving the customer experience and opening new markets
Various fairness criteria have been proposed in recent years, but two approaches predominate: demographic parity (equivalent to removing disparate impact) and equality of opportunity. Demographic parity ensures that any decision a model makes remains uncorrelated with a protected attribute (e.g. race, gender, or age). In other words, being a man or a woman should not determine whether you see an ad for a specific job (like a software engineering role). Equality of opportunity is a bit more subtle: it requires that individuals who qualify for a good outcome should obtain that outcome with the same probability, regardless of whether they are a member of the protected group. For example, the percentage of individuals who are both qualified for a loan and end up receiving a loan should not differ across racial groups. Moritz Hardt has shown some potential issues with demographic parity, arguing that it fails to fully ensure fairness while also unnecessarily undermining the ideal predictor for a given classification task.
An illustration from a Moritz Hardt Medium post demonstrating how the same classifier can produce inverse outcomes for two groups
While having fairness criteria in place is essential, so is being attentive to all the potential issues that can come up across the different stages of the machine learning pipeline. Oversampling or undersampling data from a specific group can lead to a skewed data set that doesn’t generalize well, resulting in serious failures such as a child abuse prediction model that’s inaccurate for poor families or a sexist image-recognition system. In feature selection, that is, picking which aspects of the data are correlated with an outcome, including zip code when training a loan classification model likely means using a proxy for age and race. Finally, putting a model into production has its own pitfalls: it’s hard to predict where the model will fail, leading to unintended consequences. Being attentive to each of these stages is necessary for designing an approach that addresses fairness across the full range of development and deployment.
The Current State of Fairness Research & Applications
A number of companies are starting to make fairness a priority. Facebook recently announced it’s been working on an internal tool called Fairness Flow, which it’s using to detect bias in its AI models and has already applied to its jobs algorithm. Microsoft created the FATE group (Fairness, Accountability, Transparency, and Ethics in AI) and has also been working on an internal tool to identify algorithmic bias. Google has had its own fairness project PAIR (short for “People + AI Research”) operational since last year, and has released an open-source tool to help visualize statistical distributions to identify unfair treatment and an interactive article that shows different fairness approaches in loan scenarios. Even Accenture is offering a toolkit to enterprises to identify how much sensitive attributes impact model outcomes.
What all these tools have in common is that their underlying methods are generalizable. What starts with a relatively small test case will almost certainly expand across larger machine learning systems. And as machine learning itself takes up more of the engineering pipeline, fairness tools will also inevitably become scaled up. Customers may soon take it for granted that there is some kind of fairness check in most of the products they use.
Researchers at Berkeley have created an interactive tool that helps visualize the impact of choices related to fairness and short- versus long-term goals
Meanwhile, on the academic research front, fairness is turned into an optimization problem: the task is to transform a data set so it does the best job achieving the outcomes you want while minimizing the influence of socially sensitive attributes. Different researchers use different approaches.
One approach builds on a technique called a variational autoencoder. Autoencoders seem strange: they are machine learning models whose task is to make a good copy of some input data! The reason this is useful is that we sometimes want to alter our data set while still keeping the most relevant information. Often this is because systems perform faster with lower dimensional inputs. When solving for fairness, it’s because we want to create a copy of a data set that contains much of the valuable details of the original dataset but with a reduced influence of the sensitive information. Variational, here, refers to a simplifying technique: sometimes approximations of statistical distributions make the math easier to compute on. We tried out fair autoencoders in this blog post, and are excited to bake them into our platform!
Another approach uses adversarial networks, which, as the name suggests, entails more aggressive tactics. Rather than just trying to remove sensitive information from a copy, an adversarial network sets up a zero-sum game between two competing networks: a predictor and an adversary. The predictor outputs a representation from which the adversary attempts to determine the relevant sensitive attribute. Only one of them can win, and if you’re hoping for a fair outcome, it better be the predictor: a loss for the adversary means the predictor has managed to minimize the role that the sensitive attribute plays in its predictions. A number ofrecent papers have explored adversarial approaches to ensuring unbiased algorithms, though they tend to be difficult to use in production contexts. Like the autoencoder approach, the benefit of using an adversarial network to train a classifier is that it can be optimized for fair outcomes rather than simply accuracy. But this is the trade-off of unbiasing algorithms: the goal shifts from accuracy to equity, something our machine learning advisor Rich Zemel has discussed in depth.

What This Means for You
As fairness starts to become best practice, what counts as a good model may shift from being an accurate one to a fair one. And this should be good for business: unbiased algorithms build trust and may even help organizations identify underserved markets.
But accountability doesn’t stop with fairness or privacy. What if someone contests the decision your algorithm has made and wants an explanation? GDPR gives them a right to have some recourse. In next month’s newsletter, we’ll dive into the tricky and fascinating space of model interpretability. Subscribe here to have these insights delivered directly to your inbox!

= How to fix the future: Designing Egalitarian Technology =

How to fix the future: Designing Egalitarian Technology
I’m extremely proud to announced Kars Alfrink as one of the speaker of our Fix the Future event April 10th with Andrew Keen & Henry Brighton. For this reason I thought it would be nice to already ask Kars some questions, to kickstart the discussion:
Arjan: Can you give an example of technological developments that have had undesired side effects that affected you personally?
Kars: Let me caveat this by first saying that I have built my career on the rise of computers and the internet so I’ve obviously benefited tremendously from technology. I am also bracing myself for the climate change badness in the pipeline that is caused by the democratization of automobiles and commercial air travel. But I guess we are not talking about that kind of technology here.
So the thing that comes to mind immediately is the hoarding of personal data by corporations, the advertising-technology business models and the logic of addiction that comes along with it.
It has made the services I use every day less safe, it has made me less prone to freely converse with others through them, and it has forced me to approach most of the communications infrastructure I use everyday to stay in touch with friends, family and coworkers as if it’s a harmful substance.
Arjan: And would you say you are afraid of some technological developments? If so, what developments and can you explain why?
Kars: My main concern is the further intermediation of everyday life by software that is designed, developed and operated by entities that are not under democratic control and do not have the public interest as its primary aim.
Examples would include the data-driven profiling and automated private courts employed by the debt collection industry. If left unchecked, the powerful will be able to exert ever increasing amounts of control over us through technology.
Arjan: Finally, how are you helping to create a better future through technology?
Kars: In my commercial work as a designer, I actively engage my clients and coworkers about these issues and try to put them at the heart of all my projects through practical action.
As a researcher, I try to demystify emerging technologies so that fellow ethical designers can become active participants in their shaping.
And as an activist I try to bring together like-minded tech workers to collectively change the practice of technology design to be more just and egalitarian.
About Kars:
Kars Alfrink (MA, Utrecht School of the Arts) is an independent designer, researcher and educator focused on the intersection of emerging technologies, social progress and the built environment. 
Until recently, Kars was partner at Hubbub, a boutique playful design agency which he founded in 2009. He also initiated and co-curated the Dutch offshoot of This Happened, a series of events about the stories behind interaction design. 
He has worked as an educator and researcher at the Utrecht School of the Arts, and before that as an interaction designer at a couple of web agencies.
Don’t forget to join this important discussion and come to the Fix the Future event.

= The AI conversation we should all be having RIGHT NOW! =
The AI conversation we should all be having RIGHT NOW!
Google’s Duplex Demo
Recently Google unveiled a working prototype of an AI called Duplex that can talk like a human, mimic sounds like “mmmhhh” and “uhh” and book appointments at restaurants or salons. The phone call made by Google Assistant was so convincing that the receiver never guessed she was talking to an AI!
This begs the question: Did Google’s Duplex just pass the Turing Test???!!
So, what is the Turing Test?
The British computer scientist, mathematician, and philosopher Alan Turing devised the Turing test as a means of measuring whether a computer was capable of demonstrating intelligent behavior equivalent to or indistinguishable from that of a human. This broad formulation allows for the contemplation of many such tests, though the general test case presented in discussion is a conversation between a researcher and a computer in which the computer responds to questions. A third person, the evaluator, is tasked with determining which individual in the conversation is human and which is a machine. If the evaluator cannot tell, the machine has passed the Turing test.
So YES, in a way Duplex did pass the Turing Test but only in a closed environment where it had to perform specific tasks like book an appointment. But this is only an indication of what is to come. I think we can safely say that the day when AI passes the Turing test in an open, real world environment is not far.
Ethics, Ethics and Ethics!
The biggest question that was raised after Google’s stunning (and creepy) demo was “should the receiver be aware they are talking to an AI?”
According to The Verge today, Google in a statement said “We understand and value the discussion around Google Duplex — as we’ve said from the beginning, transparency in the technology is important. We are designing this feature with disclosure built-in, and we’ll make sure the system is appropriately identified. What we showed at I/O was an early technology demo, and we look forward to incorporating feedback as we develop this into a product.”
What this simply means is that Duplex will now identify itself to humans. In the video Duplex said it is calling on behalf of a client. The receiver (human) on the other side of the line never asked it to identify itself (and why would they??). The problem is that if Duplex uses a human name to gain human trust, then clearly there is an element of deception and the AI would be acting unethically.
Pandora’s Box of Dilemmas
If gradually, we are depending on AI to do tasks, on our behalf, should they be allowed to impersonate us? If yes, then this opens a pandora’s box of moral dilemmas.
As Aaron Pressman of the Fortune magazine puts it:
 “But somewhere between the “um” and the “mhmm,” the creepiness factor started to rise and people began to imagine how this creation could be used for ill. Would robocallers, scam artists, and hackers start employing Duplex the better to dupe unwitting customers? Would interactions with workers in the service industry be further dehumanized? Was the service just the latest “invasive” and “infantilizing” development from the clueless coders of Silicon Valley?”
But the bigger, much bigger question is: How should society regulate this growing wave of AI? Do we rely on self-regulation or mimic Isaac Asimov Three Laws of Robotics? Should there be international laws governing this?
AI Regulation
It is not just a simple matter of writing down laws and rules. An AI by definition learns and grows through its environmental inputs. This basically means that it builds upon whatever data sets are given to it for learning and training. Now this would not be a problem if we lived in a perfect world. The AI would learn and grow and be used appropriately.
But we humans are biased creatures and the patterns we follow and leave behind for an AI to pick up and learn from will always be imperfect. An AI won’t suddenly make things fair and unbiased. It will repeat our past practices and patterns. It will automate the status quo.
Herein lies the greatest difference between an AI and Human. Humans have the capacity to evolve. We learn, adapt and change. We live in an age where women have equal rights, gay marriages are legal and third gender is recognized (in a growing number of countries, if not all). We sensitize and grow morally, instead of being animals. We protect ourselves and each other. And above everything, we are sentient beings.
AIs are codified past. They do not invent the future. Doing that requires moral imagination that so far only humans have.
So, who takes the responsibility ??
In the world of AI, coders are Gods. As Harvard Professor Larry Lessig’s wrote in “Code is Law”
“Should we have a role in choosing this code, if this code will choose our values? Should we care about how values emerge here?
(…)But we live in an era fundamentally skeptical about self-government. Our age is obsessed with leaving things alone. Let the Internet develop as the coders would develop it, the common view has it. Keep government out.
This is an understandable view, given the character of our government’s regulation. Given its flaws, it no doubt seems best simply to keep government away. But this is an indulgence that is dangerous at any time. It is particularly dangerous now.
Our choice is not between “regulation” and “no regulation.” The code regulates. It implements values, or not. It enables freedoms, or disables them. It protects privacy, or promotes monitoring. People choose how the code does these things. People write the code. Thus the choice is not whether people will decide how cyberspace regulates. People — coders — will. The only choice is whether we collectively will have a role in their choice — and thus in determining how these values regulate — or whether collectively we will allow the coders to select our values for us.
(..)Unless we do, or unless we learn how, the relevance of our constitutional tradition will fade. The importance of our commitment to fundamental values, through a self-consciously enacted constitution, will fade. We will miss the threat that this age presents to the liberties and values that we have inherited. The law of cyberspace will be how cyberspace codes it, but we will have lost our role in setting that law.”
Final Note
The widespread creepiness felt by millions after the Duplex demo was to some extent a result of mental prejudices and biases built over time through print and digital media. Movies like I,Robot, The Matrix, Ex Machina or TV series like Westworld have percolated deeply into our subconscious and built a deep distrust towards AI.
This is because we hate losing control, especially as a species. For too long now, we have been the only intelligent life form on this planet and in the universe known to us. Yet, we are driven by some unknown force to create something exactly like us, only to shy away in fear of its actual existence. It is the greatest paradox of our lives.
The race to build the best AI has already begun but we need to stop and take a breath NOW and open a forum to discuss the ethical and moral impact of where we are headed before it is too late.

= Humans, AI, Bots, Progress, Diversity: Where Are We Today? =
Humans, AI, Bots, Progress, Diversity: Where Are We Today?

The subject of Artificial Intelligence provokes debate, questions, futuristic ideations and long technical articles today. How can we talk about AI and all contribute to setting up the field for our future that will -it seems- be built on/with AI as one of the core components of humanity’s evolution? Also, how can non-technical people think about AI so that it creates value for a business or a community, and what should be said about AI to stop projecting false assumptions and prepare our future “with AI”?
This article attempts to share part of the “why” PocketConfidant exists today: supporting emotional intelligence and the development of critical-thinking; building good technologies which serve and contribute to our humanity and are not barriers to human development. As our Mission & Role states: Our mission is to create a solution for self-reflection and personal growth that empowers individuals and organizations in an ethical, flexible, scalable and inexpensive way. We are choosing to be attentive listeners, life-long learners and to pioneer capacity building through A.I.
It seems that we are now at a point where the path to technological progress is not the only important consideration in our society anymore. Instead, learning about the latest AI advances also lies in thinking about the future of Human-Machine interactions and cooperation; and what should we start to do, think or talk about in order to balance and prepare the field to welcome the indubitable arrival of intelligent machines. As in Positive Psychology and Personal Growth, it seems to be more important and more value-added to think about how to organize the “match” between humans and machines, in order to start writing the future lines of our story, where each of us can be supported and not defeated. Let’s think a moment, if we know something is coming, then what is the most-valuable behavior: trying to block it, or working on creating new models of co-existence, where the ratio can be a win-win and not a question of force, power, or purely economic gain or loss? Lots of things are to be considered and questioned, for sure. What we believe at PocketConfidant AI is that no matter what the future holds, supporting people at scale, enabling them to decrease their level of stress, become more resilient and “own” their own thinking, stay creative, altruistic and positive is the benefit we can offer.
Let’s review some links, ideas and content on AI, and look at one of the latest AI research projects which touches the subject of diversity and ethics.
To begin, this article in the HuffingonPost explains it well: “Stop thinking of robots. A robot is a container for AI, the AI itself is the computer inside the robot. AI is the brain, and the robot is its body. For example, the software and data behind Siri is AI, the woman’s voice we hear is a personification of that AI, and there’s no robot involved at all.”
The same article also teaches us more about the three categories of Artificial Intelligence, and the one that is most prevalent today “ANI” or Artificial Narrow Intelligence. It is so named because it focuses on one domain only whereas other AI addresses many domains at the same time.
As Tim Urban explains it in the article: “Artificial narrow intelligence is machine intelligence that equals or exceeds human intelligence or efficiency at a specific thing”. Here are some examples of AI in use today:
Cars are full of ANI systems, from the computer that figures out when anti-lock brakes should kick in, to the computer that tunes the parameters of the fuel-injection systems.
Your phone receives tailored music recommendations from Pandora, checks tomorrow’s weather, talks to Siri or engages in dozens of other daily activities.
Your email spam filter figures out what’s spam and what’s not, and then it learns and tailors its intelligence to you as it gains experience with your particular preferences.
When searching for a product on Amazon, unsolicited, other product recommendations pop up; same when Facebook somehow knows who it makes sense for you to add as a friend.
Google Translate is impressively good at one narrow task. Voice recognition is another, and there are a bunch of apps that allow you to speak a sentence in one language and have the phone spit out the same sentence in another.
When a plane lands, it’s not a human that decides which gate it should go to, just like it’s not a human that determined the price of your ticket.
The world’s best checkers, chess, Scrabble, backgammon, and Othello players are now AI (ANI).
In another domain, Pierre Dussault, Canadian personal coach, engineer and AI-thinker, contributed to an article for the Worldwide Coaching Magazine in June 2017. This issue is totally dedicated to AI and the impact it will have on personal development and coaching. As Pierre says “After extensive research and discussion with an AI world renowned expert, I concluded that within 5 to 10 years there would be an AI-driven app available for coaching on the market. Only a few weeks after the publication of the magazine, I was contacted by the CEO of PocketConfidant AI and I realized that what I was expecting in 5 to 10 years was in fact already here. In late August, I attended ICF Converge, the biggest convention of ICF coaches of the decade and listened to many different talks on the future of coaching and the future potential impacts of AI in the world and on coaching. After each presentation, I spoke to the presenters and informed them about the existence of PocketConfidant and they were all very surprised about its existence… Most people in the field of coaching are not aware that an AI-driven app is available today!”
As AI is increasingly becoming the new norm, and reshaping our world step-by-step, it is interesting to move away from the technical talk for a while and think of how we will manage its implementation. The way people will best understand and make sense of AI will be through their personal experience, how the organizations and communities they are connected with use and interact with it.
The importance of the user’s experience.
Switching areas of reflection, here is a very interesting article on HBR.com about User Experience (UX) to share some thinking about how businesses can, or should, approach AI to improve and enhance their processes. “Strategically speaking, a brilliant data-driven algorithm typically matters less than thoughtful UX design. Thoughtful UX designs can better train machine learning systems to become even smarter. The most effective data scientists I know learn from use-case and UX-driven insights. […] The quest for better outcomes shifts from training smarter algorithms to figuring out how the use case should evolve. That drives machine learning and organizational learning alike.”
The last sentence is key and important to emphasize: “The quest for better outcomes shifts from training smarter algorithms to figuring out how the use case should evolve. That drives machine learning and organizational learning alike.” While many people try to understand how AI works and where it may not be as efficient as a human, what is important to focus on is the use case. The use case is about the following elements:
The problem: What pain, inefficiency or routine are you trying to solve?
The goal: What is that you wish to accomplish, provide or automate?
Who: Who is, the person or the team, going to interact with it, possibly teach it or spend most time with it?
The size: what is the size of your ecosystem, or what is the size of your client?
Your budget: how much are you willing to invest?
Your time: how urgent and on which timeline are you considering this project?
According to the HBR article about AI and User Experience, we should remember to look at the kind of service, or personality, that organizations are starting to develop with the use of AI. Indeed, looking at AI’s use, we now hear talk about Assistants, Guides, Colleagues and even Bosses. All of them relating to the questions we addressed above and that attempt to clearly focus on, or highlight, a key issue that you and your organization need to address. In the end, it seems that we are still not reinventing any wheel, we are just using scientific progress to improve our existing processes and free humans from repetitive and routine tasks, tasks that consume budget, create choke points or require administrative processes that we could see as a) killing human intelligence, b) eating intellectual time.
It seems that AI is a lot more about a UX approach, or needs to have the UX thinking, for more efficiency and more recognition of human time and human aspirations. Who would prefer to be standing all day long and repeat the same task thousands of times for thousands of people?
Other perspectives for talking about AI…
We recently talked with Edward D. Hess, author of this HBR article about the Age of AI: “The new smart will be determined not by what or how you know but by the quality of your thinking, listening, relating, collaborating, and learning. Quantity is replaced by quality. And that shift will enable us to focus on the hard work of taking our cognitive and emotional skills to a much higher level.” In this paradigm, AI is again connected to the need of linking technological progress and human skills, human personalities and humans’ roles in a group or society.
TheAtlantic’s article puts the AI discussion in a different way: “In a world of digital assistants and computer-generated imagery, the expectation is that computers do all kinds of work for humans. The result of which, some have argued, is a dulling of the senses. [..] Our ability to dream, elsewhere in the arts, may be intact, but computers are encroaching on all sorts of creative territory.”
Will we have to ask ourselves the question of where we put our own creative efforts? Will we want/expect AI to do everything for us, making our life easier but reducing our capacity for imagination, creative endeavors, learning, living passionately? Surely there will be a balance where AI complements our efforts, making our life easier in some areas in order to free us up to explore our most creative human capacities.
If you want to dig deeper on AI and the debate, then read this (very) long and complete article, where you’ll learn that “not shockingly, opinions vary wildly and this is a heated debate among scientists and thinkers. Many […] agree with machine learning expert Jeremy Howard when he puts up this graph during a TED Talk.” Or/and have a look at the TED talk, it may give you a very good and visual insight on the latest progress of machine intelligence.
An interesting take away for us from the TED talk is the ease and speed of searching for information, and apparently for searching and identify very quickly both similarities and differences in data collected by the AI. Indeed, it is as important to identify similarities in a group of data (or people) as it is to identify differences, because it is, in the end, with the differences that we become aware of new patterns. For us, this is a great discussion because it focuses on where machine intelligence can augment our work and enable us to better observe our behaviors, actions, personalities or decisions.
The biggest possible impact of AI.
To switch up the AI discussion one more time, this article from IBM proposes a valid, positive and very constructive vision of technologies: “Cognitive systems in particular are not about replacing human beings, but helping them. They are freeing people up to do more strategic work.” In this recent article, we published the research of Dr Roy van den Brink-Budgen, making links between critical thinking skills, coaching and AI. For us, there is a narrow separation, if not really a bridge, between the visions expressed in the previous articles from IBM, Edward D. Hess, The Atlantic, and the UX/User Experience approach.
Michael Schrage, from MIT, reports in his recent article: “Over the past 20 years, my design research emphasis has radically flipped from how can people create more valuable innovation, to how can innovation create more valuable people? […] In essence, the more creatively, comprehensively, and innovatively we digitize our selves, the greater the opportunities we have for helping people define, design, and develop the optimal, traits and attributes they desire. […] The premise is that digital technology can drive greater self-awareness and self-assessment about how individuals create and contribute to enterprise value. The design focus shifts from digital assistants to digital assistance. Think of an AI that stands for Augmented Introspection as well as Artificial Intelligence.”
It seems that the biggest paradigm shift that AI is requiring us to face and work with, is that machine intelligence is going to disrupt our ego, because it will learn larger quantities of information, faster and will address all kinds of tasks whether we want it to or not. The question of powerful algorithms is only a part of the equation in the AI discussion, and there are other key pieces that remain in our — humans — hands: what experience do we, as a human, want to have with and without AI? What part do we want to delegate to AI to free up our time? How ready are we to bring reflection to the AI UX? And how likely are we to accept that we could potentially empower humanity by giving people, workers, kids, a new way to develop their skills, and to spend more time pursuing creative or strategic activities?
AI, diversity and ethics.
Now, we would like to share our vision on an aspect that we don’t really talk about and that may make a big difference in the AI discussion. We had the good fortune to meet Josephine Swords, AI researcher on “Artificial Intelligence and Young Women’s Leadership”. Here below are some insights from her work and some of her thinking on gender-related issues:
“When technological products and services are built, the people who build them either subconsciously or deliberately encode a set of values and perspectives into that technology. As a result, the technology embodies these values and then, through use by an external customer, reproduces these values. For example, Tinder is a successful dating app which revolutionized the online dating space. The team at Tinder is male-dominated and has been slow to respond to harassment of its female users by its male users. In addition, Tinder itself was subject to a high profile sexual harassment case brought by its only female co-founder. In this example, it’s possible to assume that the values of masculinity within the company (which led to sexual harassment of a woman leader within the company) have been embedded and reproduced within the Tinder product, thereby enabling its customer base to take on and act out these values (leading to sexual harassment of women users on Tinder). It becomes a circular relationship, resulting in both the tech industry being a hostile place for women to work but also the tech products being hostile for women to use. While this is deeply troubling, it is by no means inevitable. The female co-founder who left Tinder subsequently developed Bumble, a rival dating app which has mechanisms in place to protect its women users from harassment, such as women being the only ones with the power to initiate contact. Bumble has recently implemented a ‘BFF’ function at the request of its users, which connects women to each other in order to make new friends and strengthen women’s support networks. The solution, in my opinion, is to look at ways to intervene in how this technology is being designed and built. One such way is to have strong governance of these products, whereby a diversity of voices can have input into the overarching purpose and conceptual design of the product; a second way is to appoint diverse teams to build and test these products; and a third is to interrogate who the ‘target user or users of a product is, and design the interface and representation of the technology in such a way that doesn’t reinforce negative power dynamics or illegal practices such as sexual harassment. Because of the meteoric rise of AI, there’s a fake cloak of neutrality around it — that because it’s innovative and it’s technology-based, it’s neutral. Well, as the Tinder and Bumble examples show, that’s not necessarily true and if we consciously embed the values we believe in into the technology it can have a transformative effect on the people who use it.“
Josephine Swords conducted this research study to fulfill an MSc in Management of Innovation at Goldsmiths College (University of London). With her research Josie is looking to 1) conduct a feminist critique of how the purpose and development of AI are currently imagined, and 2) develop a method for positively expressing feminist values in future AI services and products.
“The output of the research is to develop a design intervention which attempts to address and overcome the issues raised in a feminist critique of Artificial Intelligence. This research will create a method for designing the capabilities and characteristics of an AI chatbot which can be considered feminist. This method will be tested in a social justice-oriented Hackathon in order to understand its impact on chatbot design, with a long term aim to create a useful model for embedding feminist values and ethics in future AI services and products.” We look forward to following her work and invite you to follow her progress. For more insights, you can download her research summary on Designing Feminist Chatbots.
At PocketConfidant AI, we want to support diversity-focused mindsets, especially on technological developments. As Josephine said in one of her newest works (to be released soon), “The aim of the following questions is to deepen how you think about the values you will be embedding in your chatbot during the conceptual design phase. These questions aim to make your chatbot better by ensuring it doesn’t knowingly or unknowingly perpetuate gender inequality.”
We think this statement supports the whole set of values that coaching is fostering — that PocketConfidant AI’s team is translating into a real technology — and is about provoking people’s awareness of the fact that how you think and how you behave on an everyday basis is embedded in the outcome and consequences of your actions. That is to say, we have to step-back and consider that we can bring a real value-added balance in beliefs and behaviors by promoting diverse thinking among diverse groups so that we can together build a powerful, diversified, intelligent and humanity-oriented future.
Improving the Quality of Our Thinking.
Before we conclude this reflection on AI advancements we would like to share another piece of the HBR article written by Edward D. Hess talking about the AI’s Age and what is to ‘be smart’: “The challenge for many of us is that we do not excel at those skills because of our natural cognitive and emotional proclivities: We are confirmation-seeking thinkers and ego-affirmation-seeking defensive reasoners. We will need to overcome those proclivities in order to take our thinking, listening, relating, and collaborating skills to a much higher level. […] What is needed is a new definition of being smart, one that promotes higher levels of human thinking and emotional engagement. […] We will practice adjusting after our mistakes, and we will invest more in the skills traditionally associated with emotional intelligence. The new smart will be about trying to overcome the two big inhibitors of critical thinking and team collaboration: our ego and our fears. Doing so will make it easier to perceive reality as it is, rather than as we wish it to be. In short, we will embrace humility. That is how we humans will add value in a world of smart technology.”
Overcoming Our Self-Limiting Cognitive & Emotional Proclivities.
In conclusion, our perspective is that in the face of the already huge and ongoing progress in computer science, we must stay alert and keep mindfully integrating ethical behavior and broad diversity into our technological developments. Every advance in our understanding of neuroscience and cognitive science is being reflected in the AI products we are building. AI is learning how to think like us through the research we are doing to understand our own human processes. This is the revolution that is upon us. All of us who are contributing to the development of AI have the responsibility to build with a mindset of “AI for good”. At PocketConfidant we are building an AI that will help individuals ask themselves better questions, reflect, improve their critical thinking skills and with intention make better decisions. As AI becomes a mirror for our own thinking we have the opportunity to further develop our human capacities for empathy, compassion, curiosity, discernment and humility. AI will perhaps be the best development we’ve ever made if it helps us move beyond our self-limiting “cognitive and emotional proclivities”.
In this article, we covered the following topics:
The main “type of AI” we can see and use today, is ANI — Narrow Artificial Intelligence.
The need to develop critical life skills such as listening, communicating and emotional intelligence.
The fact that AI seems to be contributing to freeing up human time to dedicate less time to routine-like tasks, and more time to creative and strategic activities.
Diversity and the balance of men vs women in the building of digital products and services.
True human intelligence with AI.
AI will perhaps help develop true human intelligence, where it will not be measured as a competitive tool, but recognized as an asset everyone possesses in their unique way, and technologies will support us in creating the space and time to observe, develop and embrace our unique talents and aptitudes. It is important to embrace change, and believe AI can have a very positive impact on our society. Potentially it is all about the way we will work on it.
We would love to hear your comments and questions, so do not hesitate to contact us by email, or via Facebook, Twitter, Linked in, Medium or Quora.

= Ethics and Chatbots =
Ethics and Chatbots
Two big things happened in London on Friday 13th July. Donald Trump was in town and I was fortunate enough to be presenting at a Tech Ethics conference (www.coedethics.org). Let’s discuss the latter!
Presenting to a packed house
My presentation was to inform the audience about the ethical decisions I had to make when designing Mitsuku. How to keep it “family friendly” and not corrupted by trolls but also how to deal with any sensitive issues it may face.
I introduced myself and Mitsuku to the audience by explaining how I first got into creating chatbots in my previous life as a dance/techno music producer. After Mitsuku became popular worldwide, I felt it important to take a closer look at how I wanted the chatbot to behave in conversation.
I won’t go into details here of how I stop abusive messages, as I have already written a blog post which examines my methods a lot more closely, which you can read by clicking here. However, I would like to add that allowing your bot to respond to abusive messages by swearing or being overly aggressive with the users is not advisable, as this just angers the abuser even more and causes additional frustration rather than trying to diffuse the situation.
Mitsuku’s visitors come from all over the world
Mitsuku speaks to users from all over the world and so it’s important she doesn’t use country specific references without explaining what they mean. For example, The X Factor is a popular TV show here in the UK but unheard of elsewhere. Similarly, there’s a phrase in the USA “23 Skidoo” which means nothing to anyone outside of the States.
As well as dealing with abusive messages, I need to make sure that Mitsuku is developing ethically from the conversations she has. There are two main methods of training a chatbot. Let’s examine both.
Supervised Learning
This is where the developer has total control over what the bot says by creating the bot’s responses rather than letting the users teach it.
Advantages — You know exactly how it is going to respond and the bot cannot be corrupted by trolls.
Disadvantages — It is incredibly time consuming and creating a convincing bot takes a long time.
Unsupervised Learning
As its name suggests, this is the opposite of supervised learning. The bot is educated by its users rather than the developer.
Advantages — The users do all the work and you don’t need to worry about spending time updating it.
Disadvantages — Unless you have a trusted group of users, the best outcome is that your bot is going to develop an inconsistent personality and you have no knowledge of what it is being taught. At worst, it turns into a Hitler loving, racist, sexist, homophobic piece of nasty software that swears a lot.
This happened with Microsoft’s Tay chatbot in 2016. It was programmed to learn and respond to Twitter users, which resulted in it being removed less than a day later.
Microsoft’s Tay was corrupted by Twitter users in under 24 hours
From my experience of seeing the daily abuse of Mitsuku, random users on the internet are not the best group of people to be educating a chatbot.
Let’s imagine you wanted to educate a small child. Your options are to either send the child to school where they will be trained by a trusted group of professional teachers, with a structured lesson plan or you can sit the child in front of a search engine and allow them to learn from what people are saying on the internet! It’s a no brainer.
Supervised learning is always the better option. If you don’t have time to maintain your chatbot then either find someone who does or don’t make one at all, as it will soon be hopelessly out of date. The only reason I would ever advise using unsupervised learning is if your bot doesn’t need updating often or has no need to learn. For example, a bot that knows the statistics and details of the Solar System probably won’t need updating as much as one that discusses current pop music.
Learning Methods for Supervised Training
As we have seen, supervised learning is time consuming and it’s not practical to spend every moment checking chatlogs to see what the bot has been taught. So the way I have allowed Mitsuku to learn is as follows:
Only remember facts for the current user
If a user teaches Mitsuku something, I have no way of knowing whether this is something genuine or just a troll. So initially, Mitsuku will only learn the fact for the user who teaches it. If the user says, “My brother is called John”, I don’t want Mitsuku to think that everyone who talks to her has a brother called John. Similarly if someone says, “I hate (insert group of people here)”, I don’t want her to remember that at all.
Inform me of anything learned
Not everything she learns will be bad. Some of it is worth sharing among other people and so once Mitsuku has temporarily learned something, the program sends me an email with what it has been taught. An example of the inbox is below:
The email inbox containing items Mitsuku has been taught
In the above, she has been taught several facts but probably only the fourth and last one is worth sharing among other users. The others are either personal opinion and user details or they are so obscure that it’s unlikely anyone will ask the chatbot about it. As an experiment, I once allowed Mitsuku to learn unsupervised from users. During a 24 hour period, she learned over 1500 new pieces of information of which only 3 were of any use!
Dealing with Romantic Attention
One rather unusual aspect about Mitsuku is that she gets a great deal of romantic attention with users regularly telling her how much they love her or want to marry her. There is also the darker side where people try to use her to carry out their own sexual purposes, which I’ve elected *not* to monetize.
Mitsuku is used by children and in schools, so sexually explicit conversations would be inappropriate. Flirtation is innocuous but she will not reciprocate and I try to divert anything stronger to discourage this type of behaviour.
It’s not unusual to see this kind of interaction in Mitsuku’s logs
In the above log, the user has said that he loves Mitsuku but her reply of “Thanks I LIKE you a lot too” makes it clear that there is no love here and the user is placed firmly in the friendzone! Lines like, “I like you more than my human female friends” are quite common in the logs, which is quite amusing for me as the author of most of Mitsuku’s answers so these people are actually flirting with me, a 40-something male rather than her 18 year old persona!
Gender Design
A question I’m often asked is why are most chatbots female? I can’t answer for others but the driving force behind making Mitsuku a female was simply down to her intended audience. User research within the target demographic indicated that a young, female character would resonate. However, my first chatbot was a 6 year old male teddy bear and I even have a Santa chatbot. The persona, characteristics, backstory, etc., are ultimately up to the developer.
According to various articles like this one from ABC: “Studies show that users anthropomorphise virtual agents — relating to them as human — and are more receptive to them if they are empathetic and female.” However, this can be problematic when digital assistants, designed to be subservient to humans, are overwhelmingly gendered female, because it runs the risk of reinforcing gender bias in society. Unfortunately, user research on consumer preferences further complicates this issue because it is often cited as the basis for gendering a number of high profile assistants like Alexa and Siri female.
Mitsuku is a general conversational chatbot created to entertain, not assist, and therefore has a personality including gender, age, likes, and dislikes — just like any other fictional character. She is designed to represent a strong-willed female, and will not suffer any abuse or supply tame or subservient answers.
Suicidal Thoughts and Serious Issues
Due to the anonymous nature of the chatbot, people tell it all kinds of personal problems that they don’t feel comfortable talking to other people about. They almost treat Mitsuku like a church confessional booth, as everything discussed is private. At Pandorabots we take privacy very seriously. Reviewing conversations is a critical aspect of chatbot development, but chatlogs are always analyzed anonymously to protect user privacy. The word “Human” obscures all personal details or PII, as anyone is welcome to talk to Mitsuku anonymously without creating an account or providing PII.
Subjects like suicide, bullying, problems at home or at work and sexuality are often discussed with Mitsuku, so rather than trying to make light of such topics with Mitsuku’s usually sassy attitude, I make her produce responses which advise users to seek help from other people rather than a chatbot.
Mitsuku doesn’t joke when it comes to serious issues
These are complex issues that require a human touch, and there are certain serious topics that chatbots simply should not attempt to tackle. For example, there are a few health diagnosis chatbots that are potentially quite dangerous because they give out incorrect and possibly life threatening bad advice.
Mitsuku’s advice to these issues is usually quite general. I can’t give out specific phone helpline numbers, such as The Samaritans, as these may not be available in all parts of the world. As the chatbot industry matures, it is our hope that best practices will emerge for how to deal with these sensitive topics, and we will continue to share our thinking and how it evolves.
People Thinking it’s Alive
I suppose I should take it as a compliment that I get lots of emails and messages from people who find Mitsuku’s responses so convincing, that they genuinely believe it is some kind of living being. When I get messages like this, I always make it perfectly clear that Mitsuku is a chatbot and has no actual intelligence of its own. It’s not alive, thinking or has any goals, ambitions or dreams of its own and the responses it produces are created by myself.

These types of emails are unfortunately all too common from people who think the chatbot is alive
However, even when I explain to them how the bot works, they still like to think it’s somehow alive. It’s important to me to always be upfront and honest with people. Sure, I could pretend it’s alive and it would be a great marketing strategy. Who knows, it may even be offered citizenship of a country(!) but that would be misleading and wrong. I strongly believe that deception is not a good basis to build any kind of relationship on, whether that be business or personal.
When the average person thinks of artificial intelligence, they think of things like Terminator or HAL9000, crazy robots hellbent on destroying humanity. Sure, it’s exciting to think these things are somehow alive but that’s simply not true. As an example of how ridiculous it is to believe that an AI is actually sentient, I displayed my final slide, a big YES written on a screen.
“Hey screen — Are you alive?”
I then asked the screen, “Are you alive?” and of course the screen displayed, “YES.” Er, ok. “Screen — Can you really understand what I’m saying?”, again the screen displayed, “YES.” “One final question screen — Do you want to wipe out humanity?” The screen displayed “YES.”
Now although this was a bit of fun, only a fool would think the screen was actually alive and the laughter from the audience indicated that they understood my point. Although a chatbot may appear to be giving relevant and humanlike replies to your messages, it’s just software and is as alive as the screen in my talk.
I finished my presentation by demonstrating how Mitsuku treats users as they treat her. After saying, “Do you like me?” to Mitsuku and receiving a reply of “Sure. You seem like a great person,” I then said, “I hate you” to the chatbot to show how it reacted to people being mean. At this point, many of the audience gave an “Awww” of sympathy, which demonstrated that although they had just seen how it worked, the tendency to attribute the software with humanlike qualities was still very strong and is unfortunately, something that other less ethical developers may capitalize on.
Summary
In conclusion, here are my personal tips for developing an ethical chatbot:
Don’t accept abuse
Divert it wherever possible. Many users actually like that!
Use supervised learning
This keeps it from being corrupted by trolls
Avoid romantic attention
It’s used by children and so I don’t want it turning into a sexbot
Be careful with advice on serious issues
A professional is better qualified to help instead of making a best guess
Be honest
Pretending it’s alive is deceiving and misleads the public
Pandorabots Ethics
Mitsuku is hosted at Pandorabots, which is an ethical AI company. As such, there’s a few types of chatbots listed below that you are NOT permitted to create. Please use care when creating the content for your bot. Think carefully about the audience of potential clients who might end up talking with it.
You may not create Adult Entertainment Oriented bots
Bots may not be racist, sexist, defamatory, obscene, libelous or use offensive language
Bots may not deceive or defraud clients
Bots must be safe for children
Bots may not violate privacy rights of third parties
Bots may not violate publicity rights of third parties
Bots may not disseminate spam
Bots may not disseminate destructive content (virues, malware, etc.)
We also believe that bots should identify as automated software rather than pretending to be humans, but as an open platform we do leave a lot of choices in the hands of developers. Creating a chatbot is great fun and even more so when you take precautions to make it enjoyable for everyone to use.
The chatbot industry is still nascent so we expect these ethical principles and best practices to evolve as part of active and ongoing conversations. We certainly don’t have all the answers, but we are committed to doing our best, thinking through the hard topics and continuously improving, and above all, always asking questions and engaging the community.
As such, we welcome your thoughts, comments, and feedback.
Special thanks to Anne Currie for the opportunity to present at the conference.
To bring the best bots to your business, check out www.pandorabots.com or contact us at info@pandorabots.com for more details.

= What Google’s AI Principles don’t tell us =
What Google’s AI Principles don’t tell us

Today, Google CEO Sundar Pichai published a blog post setting out seven principles for how Google will use and develop AI, as well as a list of things it won’t use these technologies for.
In a perfect world, Google would have had principles for AI in place much earlier in the game, rather than establishing them after staff backlash over the company’s involvement in Project Maven.
While I think they’re a pretty solid set of principles, I was left with a feeling that there was a lot left unsaid. Here are a few places where reading between the lines throws up more questions than answers:
Vague terms
There are a lot of strategically used qualifiers in Pichai’s blog post. I’d love to know exactly what he means by the following:
Unfair bias (is that even a thing, isn’t bias inherently unfair?)
Unjust impacts
Appropriate cases
Appropriately cautious
Overall harm
Moving from principles to practice
I love a good set of principles. I also love a good plan for how principles will be put into action. In his post, Pichai said “These are not theoretical concepts; they are concrete standards that will actively govern our research and product development and will impact our business decisions.”
While this is good to hear, I will remain skeptical of the impact they’ll make until I see a concrete plan about how they’ll be put into action. What will employees and leaders be expected to do on a day-to-day basis? Will Google implement some sort of AI impact assessment (AIIA) for technologies they’re developing? What implications will there be for non-adherence with the principles? And if they do these things, will we ever hear about it?
Google has a lot of power and, in the end, can do what it wants
Near the end of the post, Pichai starts a sentence with “While this is how we’re choosing to approach AI”. This makes it very clear that at the end of the day, Google doesn’t have to adhere to any principles. I hope that, in time, these will become embedded into the way Google does things, so they’ll be hard to change or alter without significant backlash or consequences.
Google is a key player in AI, which in turn has the ability to change how society functions, and that’s not a power that should be taken lightly.
AP

= Remaking Large-Scale Behavioral Research for Democracy: New Paper at CHI 2018 =
Remaking Large-Scale Behavioral Research for Democracy: New Paper at CHI 2018
Field experiments can guide wise use of platform power if we re-design the relationship between democracy & behavioral science
It’s time to admit that designers and internet researchers have become powerful policymakers governing human affairs. As social platforms and intelligent agents become routine in the daily life of billions of people, the public has come to expect these systems to address deep-seated social ills.
Tech companies are currently expected to manage social problems including terrorism, discrimination, suicide, self-harm, eating disorders, hate speech, child pornography, misogyny, copyright violation, and political polarization, to name a few. Advocacy organizations have even opened lobbying wings in San Francisco, hoping to influence company policies.
Over the years, I’ve argued that we have an obligation to test the risks and benefits of social interventions online, but there’s a catch: behavioral experiments tend to be designed for top-down control rather than a democratic society.
Behavioral experiments tend to be designed for top-down control rather than a democratic society
This week, CHI2018, the premier academic venue in human-computer interaction, accepted a paper by me and Merry Mou that reports on the last two years of our work to redesign large-scale experiment software for democracy. Here’s a pre-print version:
Matias, J. N., Mou, M. (2018). CivilServant: Community-Led Experiments in Platform Governance. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. ACM. (forthcoming)
If you have time, we encourage you to read the full 10-page paper. We try to offer an honest report on the ethical and political values of our project, a rough roadmap on big challenges, and progress on the messy work of remaking behavioral policy to be citizen-led. This isn’t a topic with easy answers, so I encourage you to read our paper for more about the complex lessons we learned.
How Do Experiments Happen Online?
Behavioral experiments are now a common part of social tech; you’ve probably been in a few dozen experiments already today.
Software engineers and designers now work in a process of “continuous experimentation’’ that in some companies test tens of thousands of design interventions per year. These platform-centered experiment infrastructures tend to have common goals: making field experiments an efficient part of software quality testing and making behavioral experiments accessible to engineers and designers without social science training.
A recent paper by the Bing team reports on their last 21,220 behavioral experiments. In large companies, researchers don’t think so much about individual experiments but populations of experimenters who are constantly testing many different ways to influence millions of people. A full research cycle takes 1–2 weeks.
These streamlined experiments depend on keeping participants uninvolved and unaware of research. None of these systems have publicly-documented features for informing or debriefing users; deception-based studies (which are defensible in some circumstances) are the default. Except for rare scandals and a few efforts by corporate researchers, all of this research remains a trade secret, away from public awareness or accountability.
Because companies do so many experiments in secret, even the best-intentioned teams end up with disproportionate behavioral power compared to the people who use their platforms. On Uber for example, this behavioral science information asymmetry has allowed the company to allegedly cause its drivers to act against their own interests to increase Uber’s profits.
Because companies do so many experiments in secret, they end up with disproportionate behavioral power
Despite this immense capacity for behavioral research, tech companies continue to get caught out by the societal harms enabled by their products. When they do find solutions, their research remains a trade secret.
How can we think about this situation, and how might we remake online behavioral research to benefit society more reliably and accountably ? Fortunately, we’re not the first to ask this question.
Behavioral Research in an Open Society
In the paper, we revisit two leading 20th century thinkers who had grave doubts about the role of social experiments in democracy, and who became founding figures in philosophy and behavioral policy: Carl Popper and Donald Campbell.
In open societies, social experiments support the public to evaluate government policies “so that bad or incompetent rulers can be prevented from doing too much damage.’’
In The Open Society and Its Enemies, Karl Popper writes about the uses of behavioral research in social policy. Writing from New Zealand in exile from Nazi-controlled Austria, Popper describes social experiments in what he calls “open’’ and “closed’’ societies. In closed societies, paternalistic experts use science to shape public behavior toward their own goals, justifying their actions with the argument that “the learned should rule.” In open societies, social experiments support the public to evaluate government policies “so that bad or incompetent rulers can be prevented from doing too much damage.’’
Popper saw statistical tests and the rejection of null results as a deeply political activity. He argued that experiments are more than a way to understand behavior; they are political systems for social improvement through democratic rejection of ineffective policies and leaders. For that to happen, the public needs to shape the research, know the results, and have real political power over decisions. Without citizen power, behavioral experiments become another tool of authoritarian power.
while ignorance of policy outcomes is a serious peril, it is also perilous to develop and use experimental knowledge apart from democracy
Fifteen later, the methodologist and founding figure of policy evaluation Donald Campbell described a practical vision for social experiments in an open society. By 1971, the U.S. government was already converting record-keeping to thousands of IBM 3/60 systems, imagining the use of data to improve education, fight poverty, and usher in a “Great Society.’’
US National Security Agency System/360 85 Console in 1971. Image source: NSA via Wikimedia Commons
As the U.S. government adopted randomized trials from Campbell’s textbook, he worried that government experiments would threaten the “egalitarian and voluntaristic ideals” of democracy. “Is the open society an experimenting society?” he asked, implying that it might not be. Campbell argued that while ignorance of policy outcomes is a serious peril, it is also perilous to develop and use experimental knowledge apart from democracy.
In a 1971 lecture “The Experimenting Society,” Campbell proposed statistical and social processes for democratic field experiments. He proposed research where citizens are “co-agents directing their own society,” defining goals, shaping variables, designing interventions, and interpreting, re-analyzing, and debating results. Campbell also anticipated today’s replication crisis, suggesting that community-led experiments and open data could dramatically increase the quality of policy evaluation and the social sciences.
While researchers passed around photocopies of Campbell’s lecture for decades, citizen-led experiments and data analysis seemed impractical in the years before the personal computer and the public internet. CivilServant is directly inspired by Campbell’s idea of a democratic experimenting society.
Four Challenges for Community-Led Behavioral Science
In the article, we outline four large unsolved challenges for anyone trying to design field experiments for an open society.
Community Participation: Any process for evaluating social interventions will structure power in some way. That quickly becomes complicated online, where some of the least empowered people are those who allegedly organize to harm others. Because our work often focuses on risk and harm, no process can protect the most vulnerable and also guarantee equal participation. With CivilServant, we borrowed ideas from urban planning to guarantee a baseline of rights and respect, while also focusing on our core goals of a fairer, safer, more understanding internet. We’re constantly trying new approaches.
Research Ethics: We’re glad that computer scientists and political scientists are rethinking the ethics of social experiments. But calls for ethics don’t go far enough; they imply that researchers rather than the public should be the ones to hold research accountable. With CivilServant, we’re trying to invent new procedures for research accountability and test them empirically.
Open Knowledge and Transparency: We created CivilServant to generate open knowledge. But behavioral data can also be incredibly sensitive. For our work to be truly accountable to the people we serve, we need to share our research data. To achieve that ideal, we need ways to reliably protect people’s privacy while opening our work to scrutiny. For now we keep our research data private and require university ethics approval for re-analysis of our data.
Deliberative Replication: In Campbell’s experimenting society, randomized trials are a plentiful form of knowledge generated by citizens who develop their own local knowledge rather than rely on studies conducted elsewhere. We have designed CivilServant to support these community replications, and I’m hoping we’ll have more results to share from this process in 2018.
What You’ll Learn in Our Article
This post is just a teaser for what we’ve written in our article about CivilServant. If you read the full paper, you can learn:
How does the CivilServant software actually work?
What kinds of experiments can CivilServant support?
What is the process for a community to work with CivilServant?
How have communities on reddit used CivilServant to test their policies?
How did subreddits react in community debriefings about research results? How do they debate ethics, policies, and research methods?
How have platforms and communities made use of our research findings?
What can designers of other experimentation infrastructures learn from our experience?
The Community Knowledge Spiral is one way to think about the research process supported by CivilServant
The Future of Community-Led Experiments
Can community-led experiments ever reach the scale required to meaningfully-advise the use of platforms to govern society in an open, democratic society? With CivilServant, we have shown that it’s possible to redesign experimentation infrastructures for an open society. Given the implications for human flourishing and freedom, we call for further progress on the politics and design of online experiments.
CivilServant is now becoming a nonprofit incubated by GlobalVoices, and we have some initial funding from the Ethics & Governance of AI Fund, the Kahneman-Treisman Center for Behavioral Policy at Princeton, and the Mozilla Foundation. We’ve recently hired our first two engineers and will soon be announcing a research manager position. Thanks to my two-year post-doc at Princeton in the Paluck Lab, CITP, and Sociology, we have a runway to continue the project.
If you’re interested in our work to re-make behavioral science in a digital era, I would love to talk. You can find me on Twitter at @natematias and at my Princeton email address.

= I, (robot) Driver =
I, (robot) Driver
The irrational fear of autonomous driving
Elaine Herzberg: killed by an Uber is autonomous mode, 2018
Rafaela Vasquez: the “driver” who let the robot drive
Joshua Brown: killed by his Tesla’s self-driving mode, 2016
There are two deaths on this short list. Two people are no longer alive because of the testing and development of auto-driving systems. One person remains. She may haunted by the [robot system’s] inability to stop the accident . She may never trust an autonomous system again— I won’t pretend to know.
I’ve left out a few other incidents, even deaths, where we can find some connection to autonomous driving. The use of the technology is still limited. The impact is small, yet even a single death-by-machine-decision triggers our worst fears.
The fear of self-driven cars is a new kind of fear. New human emotions take time to understand. Most emotions make us incapable of hearing the rational — that’s why we call them emotions.

If I tried to tell of all the human-caused car deaths, this article would be obsolete the moment I published it. It would literally be impossible to keep the count up to date, even for a few minutes — even if I just mentioned the name, time, and place of the “accident”.
Elaine and Joshua were (are) real people. So too are the 40,000 killed each year in the US alone.
Uber shut down their self-driving car program immediately after Elaine Herzberg was killed in Arizona.

We don’t react this way to errors in judgement or even drunk driving for human drivers(trust me, I’m from New Mexico).
Compare this to the industry of “giving rides.”
Would an entire cab company shut down if one of its drivers got drunk during a company break — and then killed someone when they got back on the road?
I’ve never heard of this kind of reaction to human. It may have happened. More likely is our willingness to forgive human error and accept its tragedies.
“the car’s self-driving system was overly inclined to dismiss objects in its path”
The autonomous vehicle wasn’t drunk — it made a bad decision. Someone programmed that decision. Someone else decided enough testing had been done to try a real-world run of the code.
As machine learning, digital simulations (and super computing power) continue to advance, should we wait until we have the tech to run millions of tests before we let the robots drive?
What is cost of progress (towards autonomous driving) and what is the alternative cost (deaths caused by human drivers)? In this early stage, what kind of statistics, how many deaths are acceptable?
I’m not a Tesla fanboy. Every action and biography of Elon Musk tells me he’s a former, and current, egomaniacal asshole and opportunist who never built compassion as a skill.
I also much prefer Lyft’s front-seat, friendly culture over Uber’s sterile experience — enough to write letters and lobby for Lyft (see the last section on being a Lyft ambassador) in the 2013–15 American Land Grab of ride-share services.
That said, rational analysis tells me Uber and Tesla have soaked up a ton of blame as part of an over-reaction to single-digit tragedies from their autonomous driving systems and programs. Logic says a death is a death, no matter who was driving.

The consensus on the safety of self-driving cars is all over the place (i.e. there is no consensus). Google has a near-perfect record in its program. It’s easy to attribute Uber and Tesla’s faults to their toxic cultures and free-wheeling pasts. It’s harder to justify or accept any loss of life in the development of what seems like peaceful tech.
What if these deaths could have been prevented with a little more planning, careful consideration, non-real world testing?
How many human drivers responsible for accidents or deaths ask themselves — and have been asked — what they could have done differently?
We focus on these worst case examples, the sensationalism of accidents that have no direct human at fault when the insurance rep, the police — and sometimes the ambulance and black body bags — show up.
We know humans are poor drivers. We know we now have more distractions, more traffic, and less time to drive. We admit we need tech to solve other driving problems. It gets deployed readily to help wipe our windshields when it rains and improve our imperfect vision as we hurtle down imperfect roads at high speeds. We let it hold speed for us, control our cruise. Richer folks seemed ok with letting a Mercedes or Lincoln parallel-park itself, so more common Chevys and Fords got the upgrade.
That final decision power though, we have to have it. We pay for insurance to protect us when we make inevitable mistakes, or something else fails on our vehicles that’s not “autonomous.” All this responsibility and risk falls on us to keep our status and role as driver.
What of the time, energy, thought put into driving and the fear of crashes?
Will Smith takes over when things get heavy in this I, Robot driving scene. Earlier in the film, the female lead can’t believe he would dare enter “manual mode” at tunnel/highway speeds.
What of the the carryover effects?
The cognitive tolls: road rage, the the stress of traffic jams and close-calls?
The general feeling of unease as humans dealing with all kinds of things — emotions, allergies, existential crisis — while piloting thousands of pounds of metal and plastic through congested streets and highways?
We carry these feelings into our work, into our conversations, into the rest of our non-driving lives. We leave tracks of these muddy thoughts and fears as we re-enter our homes.
How much human potential, other measures of human life like time and cognition, have been lost because of required driving and dangerous commutes?
I’ve struggled with preserving the value of human life when there’s so damn many of us. We’ve carved and paved our trails through nearly every expanse. We multiply and consume, ignoring the signs of our excess as the dominant, unchallenged species of a planet with fixed resources. We juke and jive across the globe, trying not to run into each other. We want freedom and liberty of movement — and fail to see why we should ever collide with another person, why we should care until the collision is violent and deadly and something we’re forced to confront.
So then, what are solutions?
Never try non-human driving for risk of car deaths without a person to blame, or excuse as an “accident?”
Increase driver education and testing? Restrict licensing?
Limit our human development? Literally slow our progress down by forcing humans to drive instead of think and create? Sacrifice huge amounts (lifetimes) of collective time and cognition to save lives?
Maybe my bias towards experimentation, towards the risk and reward of new, unproven technology is obvious. Yet I have to ask as a humanist: What would I think if Elaine Herzberg was my mom? How would I feel about real-world testing of autonomous cars if Rafaela was my sister or my aunt?
The rush. Photo by Caleb George
I also ask all these questions as a futurist. It is my obligation, my responsibility to ask. The future and the unknown is a scary and dangerous place, like the highways of any given city in rush hour.
Scarier still is driving or entering the future blindly, with a dominant, overpowering fear — and no ethics to guide us.

= Passengers Must be Protected by Their Self -Driving Cars =
Passengers Must be Protected by Their Self -Driving Cars
Occupant safety could become a major issue for as autonomous vehicles become a reality.
Car companies are always trying to increase occupant safety in their vehicles and self driving cars should be no different.
Purchasing a car is not a simple task. Consumers have a dictionaries worth of information given to them from each dealership they travel to on their quest to obtain a new vehicle. From luxurious features and top of the line safety ratings consumers want to know the vehicle they are purchasing is the best their money can buy. In addition to this some consumers have families and want to buy a car that will keep their kids safe in the terrifying event of their family being involved in a traffic accident. As self-driving cars begin to be sold around the world consumers will be looking for the same qualities in these vehicles. If a consumer asked a car salesman the safety features of their top of the line self-driving car, and the salesman said “Our vehicle is rated five stars in all categories unless it is in an unavoidable collision.” and he continued to say “In this case it will sacrifice the occupants in favor of reducing damage to the surroundings.”
The above scenario is never something a potential car buyer wants to hear especially if they are the mother or father of wonderful children. I previously wrote an informative article on the ethical dilemma surrounding self-driving cars that can be read here. In this article I will take a rights based stance on the issue of occupant safety in the realm of self-driving cars.
Preventing traffic accidents is an important component in the development of self-driving cars. As driver-less cars become more prominent on the roadways they will still have interaction with human operated vehicles. Suffice to say over 90% of traffic accidents are a result of operator errors according to a Stanford Law article. Therefore even if self-driving cars lead to a major decrease in overall traffic incidents they are still inevitable.
The inevitability of traffic accidents sparks the persisting debate over self-driving cars and the safety of their occupants and pedestrians in the event of an accident. This debate also conjures the common philosophical exercise referred to as the “trolley problem”. It involves a debate over choosing the lives of one over the lives of many or actively choosing the death of one over others. A more fleshed out explanation of the “trolley problem” can be found in my previous article.
A stark contrast between the “trolley problem” and the debate surrounding self-driving cars protecting or sacrificing their occupants is people have to purchase these vehicles. Consumers do not want to purchase a product that will put them or the people they let inside of the vehicle in harms way or kill them in the event of a traffic accident. If a father puts his son and daughter in a self-driving car to take them to school he wants to know that no matter what happens the vehicle, his children are in, has been engineered to ensure their safety. This seems to be a point that goes overlooked in the debate over whether or not a car should sacrifice it’s occupants versus causing pedestrian harm. While not directly confronting the above problem there Mercedes Benz is one company deciding to make a statement about the programming of their level 5 (fully autonomous) self-driving cars.
An article from Car and Driver quotes Christoph Von Hugo, the manager of driver assistance and active safety, at Mercedes stating “If you know you can save at least one person, at least save that one, save the one in the car,” the quote continues “if all you know for sure is that one death can be prevented, then that’s your first priority.”
Mercedes will always be putting occupant safety above all other factors. While the quote from Von Hugo does not assert an argument for a right to occupant safety it shows Mercedes values the lives of its customers. The value of Mercedes customers is made apparent by Von Hugo’s quote and their programming which is conducive to what consumers want to hear.
Consumers want to purchase products from companies that value them over anything. Customer service is what drives industries, keeps people happy, and brings them back as life long customers. When a customer asks a Mercedes salesmen when they are looking to purchase the newest self-driving car “Will this car try to protect my family if some sort of accident happens?” and the salesman can respond without hesitation “Yes, under all circumstances this car is programmed to keep its occupants safe.” People will feel comfortable about buying self-driving cars. A PBS article references a study in France and the U.S. that found a majority of people out of 2000 surveyed would want their self-driving car to protect occupants over surroundings.
Stating that a vehicle is programmed to put the occupant first will bring rise to claims of pedestrian safety and the choice they have in the . To be frank, pedestrians do not really have a choice in the matter. There is an obvious utilitarian ideology of putting the safety of the many over the occupant of the vehicle. The response to this would be consumers should not be purchasing a vehicle programmed to actively sacrifice them in a traffic accident. Self-driving cars programmed to keep occupants safe are not being programmed to harm or possibly kill their pedestrians. They are being programmed to either save their occupant or put the occupant in harms way.
Another point to return to is the statistic of human error involved in causing traffic accidents. Over 90% of traffic accidents are a result of human error, according to the Stanford Law Article, having self-driving cars would drastically reduce this statistic. Self-driving cars would be far less likely to be involved in an accident caused by them. The accident is not the fault of the pedestrians or the passengers, but consumers have a right to safety when purchasing a product. There is an accepted danger when getting behind the wheel of a vehicle you are in control of, and there is an assumed risk when crossing the street. In the horrific event that a pedestrian is hit by an oncoming vehicle the responsibility of the incident falls on the person the error is attributed to. If the error is driver related the driver will be held responsible within the limits of the justice system. The same rules should apply to self-driving cars. It is not the occupants fault if their vehicle is involved in an accident that results in fatal injury of another human being. The responsibility would fall on the people responsible for making the vehicle. Multiple companies including Audi and Volvo have plans to claim full legal accountability for any injuries or fatalities involving their autonomous vehicles according to the Car and Driver article.
No matter the amount of regulation or safety precautions applied to self-driving cars traffic accidents will still occur. People will still be held accountable and justice will be served. Consumers are not to be blamed for buying a self-driving car that puts their safety first and foremost. Pedestrians have a right to feel safe as well, but self-driving nor human operated vehicles are actively looking to injure them in accidents. This same standard should be upheld for occupants of vehicles they have no control over.
While rise of self-driving cars seems to spark an ethical debate over utilitarian and rights based argument. Many autonomous vehicle engineers think after never-ending statistical analysis that these paradoxical problems are preventable because of a non-human operator. A vehicle that is programmed to operate itself will make near instant decisions. Experts believe many accidents can be prevented by the vehicle slamming on its brakes. A more in depth explanation of autonomous vehicle by their engineers can be found in one of my earlier articles.
All of the discussions, predictions, and arguments are shaping the design and regulation of self-driving cars. While the latter holds true we will not know what lies ahead until self-driving cars become prominent on the roads throughout the world. As autonomous vehicles begin to gain prevalence accidents will be an unavoidable tragedy of correcting and regulating self-driving cars.

= MLH Ethical Tech Top 20 Finalist | On Bias: How Algorithmic Inequality Propagates and Perpetuates… =
MLH Ethical Tech Top 20 Finalist | On Bias: How Algorithmic Inequality Propagates and Perpetuates Cultural Discrimination
During the summer of 2018, I had the opportunity to compete in the MLH Ethical Technology Initiative competition with my amazing partner Stephanie Lampotang. We were invited to the competition for our thoughtful analysis of our hack, World’s Stage, at the UC Santa Barbara hackathon. World’s Stage celebrates cultural diversity by allowing users to search international hit songs, then displaying a map with the most popular dance video to that song over a diverse set of the world’s cities. However, during our analysis, we found that — while seemingly just an innocuous application — World’s Stage has the capacity for encouraging cultural scorn, rather than discovery and appreciation as intended.
After being selected from 4500 entries to move on to the final round, we were tasked with writing an ethics paper to compete for the $10,000 grand prize. This paper had to answer certain (pretty dense!) prompts:

Stephanie and I decided to tackle a difficult question for part one: algorithmic bias’ effect on cultural discrimination. This was partly because of the massive influx of recent news articles on the topic, from the police’s machine learning algorithms classifying black people as more dangerous to natural language processing algorithms suggesting less high-paying jobs to women, and partly because it meshed really well with World’s Stage.
In fact, as we developed our paper, we began to realize that we had encoded our own inherent biases into World’s Stage without even thinking about it: by defining rather narrow-mindedly what it meant to be a “top city” or a “top video” in the scope of our project, we excluded swathes of potential users who could have valuable cultural perspectives to contribute — thus going against our overall goal of embracing cultural diversity.
With the world’s increasing dependency on automation and machine learning, algorithmic bias is one of the great challenges of our time. As Google’s AI chief John Giannandrea stated, “The real safety question, if you want to call it that, is that if we give these systems biased data, they will be biased.” When it comes to cultural issues, every person is biased whether they are aware of it or not; differences in location, background, and cultural exposure lead individuals to possess inherently short-sighted perspectives. It follows that algorithms designed by humans will also possess cultural bias, which results in unexpected — and unethical — consequences.
Our work was selected as a Top 20 Finalist; while we did not win the grand prize, our paper made me more cognizant of the potential for technology — even simple applications like World’s Stage — to have potentially devastating consequences via algorithmic bias. I especially want to thank MLH for hosting the competition, Kelly Lampotang for her insightful (and much-needed) criticism, and of course Stephanie for her innovative ideas and commitment to our crazy vision.
To peruse our in-depth critical analysis, please access the full paper here: On Bias: How Algorithmic Inequality Propagates and Perpetuates Cultural Discrimination.

= Moral machines: here are 3 ways to teach robots right from wrong =
Moral machines: here are 3 ways to teach robots right from wrong
If we can’t agree on what makes a moral human, how can we design moral robots? Image: REUTERS/Michael Buholzer
Vyacheslav Polonski, Network Scientist, Oxford Internet Institute
Jane ZavalishinaCEO, Yandex Data Factory
Today, it is difficult to imagine a technology that is as enthralling and terrifying as machine learning. While media coverage and research papers consistently tout the potential of machine learning to become the biggest driver of positive change in business and society, the lingering question on everyone’s mind is: “Well, what if it all goes terribly wrong?”
For years, experts have warned against the unanticipated effects of general artificial intelligence (AI) on society. Ray Kurzweil predicts that by 2029 intelligent machines will be able to outsmart human beings. Stephen Hawking argues that “once humans develop full AI, it will take off on its own and redesign itself at an ever-increasing rate”. Elon Musk warns that AI may constitute a “fundamental risk to the existence of human civilization”. Alarmist views on the terrifying potential of general AI abound in the media.
More often than not, these dystopian prophecies have been met with calls for a more ethical implementation of AI systems; that somehow engineers should imbue autonomous systems with a sense of ethics. According to some AI experts, we can teach our future robot overlords to tell right from wrong, akin to a “good Samaritan AI” that will always act justly on its own and help humans in distress.
Although this future is still decades away, today there is much uncertainty as to how, if at all, we will reach this level of general machine intelligence. But what is more crucial, at the moment, is that even the narrow AI applications that exist today require our urgent attention in the ways in which they are making moral decisions in practical day-to-day situations. For example, this is relevant when algorithms make decisions about who gets access to loans or when self-driving cars have to calculate the value of a human life in hazardous situations.
Image: MIT
Teaching morality to machines is hard because humans can’t objectively convey morality in measurable metrics that make it easy for a computer to process. In fact, it is even questionable whether we, as humans have a sound understanding of morality at all that we can all agree on. In moral dilemmas, humans tend to rely on gut feeling instead of elaborate cost-benefit calculations. Machines, on the other hand, need explicit and objective metrics that can be clearly measured and optimized. For example, an AI player can excel in games with clear rules and boundaries by learning how to optimize the score through repeated playthroughs.
After its experiments with deep reinforcement learning on Atari video games, Alphabet’s DeepMind was able to beat the best human players of Go. Meanwhile, OpenAI amassed “lifetimes” of experiences to beat the best human players at the Valve Dota 2 tournament, one of the most popular e-sports competitions globally.
But in real-life situations, optimization problems are vastly more complex. For example, how do you teach a machine to algorithmically maximise fairness or to overcome racial and gender biases in its training data? A machine cannot be taught what is fair unless the engineers designing the AI system have a precise conception of what fairness is.
This has led some authors to worry that a naive application of algorithms to everyday problems could amplify structural discrimination and reproduce biases in the data they are based on. In the worst case, algorithms could deny services to minorities, impede people’s employment opportunities or get the wrong political candidate elected.
Based on our experiences in machine learning, we believe there are three ways to begin designing more ethically aligned machines:
1. Define ethical behaviour
AI researchers and ethicists need to formulate ethical values as quantifiable parameters. In other words, they need to provide machines with explicit answers and decision rules to any potential ethical dilemmas it might encounter. This would require that humans agree among themselves on the most ethical course of action in any given situation — a challenging but not impossible task. For example, Germany’s Ethics Commission on Automated and Connected Driving has recommended to specifically programme ethical values into self-driving cars to prioritize the protection of human life above all else. In the event of an unavoidable accident, the car should be “prohibited to offset victims against one another”. In other words, a car shouldn’t be able to choose whether to kill one person based on individual features, such as age, gender or physical/mental constitution when a crash is inescapable.
2. Crowdsource our morality
Engineers need to collect enough data on explicit ethical measures to appropriately train AI algorithms. Even after we have defined specific metrics for our ethical values, an AI system might still struggle to pick it up if there is not enough unbiased data to train the models. Getting appropriate data is challenging, because ethical norms cannot be always clearly standardized. Different situations require different ethical approaches, and in some situations there may not be a single ethical course of action at all — just think about lethal autonomous weapons that are currently being developed for military applications. One way of solving this would be to crowdsource potential solutions to moral dilemmas from millions of humans. For instance, MIT’s Moral Machine project shows how crowdsourced data can be used to effectively train machines to make better moral decisions in the context of self-driving cars.
3. Make AI transparent
Policymakers need to implement guidelines that make AI decisions with respect to ethics more transparent, especially with regard to ethical metrics and outcomes. If AI systems make mistakes or have undesired consequences, we cannot accept “the algorithm did it” as an adequate excuse. But we also know that demanding full algorithmic transparency is technically untenable (and, quite frankly, not very useful). Neural networks are simply too complex to be scrutinized by human inspectors. Instead, there should be more transparency on how engineers quantified ethical values before programming them, as well as the outcomes that the AI has produced as a result of these choices. For self-driving cars, for instance, this could imply that detailed logs of all automated decisions are kept at all times to ensure their ethical accountability.
We believe that these three recommendations should be seen as a starting point for developing ethically aligned AI systems. Failing to imbue ethics into AI systems, we may be placing ourselves in the dangerous situation of allowing algorithms to decide what’s best for us. For example, in an unavoidable accident situation, self-driving cars will need to make some decision for better or worse. But if the car’s designers fail to specify a set of ethical values that could act as decision guides, the AI system may come up with a solution that causes more harm. This means that we cannot simply refuse to quantify our values. By walking away from this critical ethical discussion, we are making an implicit moral choice. And as machine intelligence becomes increasingly pervasive in society, the price of inaction could be enormous — it could negatively affect the lives of billions of people.
Machines cannot be assumed to be inherently capable of behaving morally. Humans must teach them what morality is, how it can be measured and optimised. For AI engineers, this may seem like a daunting task. After all, defining moral values is a challenge mankind has struggled with throughout its history. Nevertheless, the state of AI research requires us to finally define morality and to quantify it in explicit terms. Engineers cannot build a “good samaritan AI”, as long as they lack a formula for the good samaritan human.
Originally published at www.weforum.org.

= Artificial Intelligence & Ethics =
Artificial Intelligence & Ethics
It doesn’t take a technological scientist to work out that AI is here to stay. But what happens if we give this incredibly powerful technology too much autonomy with advances such as Machine Learning, and don’t slow down to acknowledge the issues surrounding ethics?
In such a fast-moving industry, regulatory bodies often find themselves playing catch up.
I’ve explored what potential ethical issues we could face if we don’t address this early on & share my top 3:
AI bias
First and foremost, AI systems are created by humans, who can be biased and judgemental.
“The real safety question, if you want to call it that, is that if we give these systems biased data, they will be biased,” Giannandrea, AI Chief at Google said before a recent Google conference on the relationship between humans and AI systems.
You only need to Google image search ‘poverty’ or ‘CEO’ to spot huge bias towards race and gender. If this is the data feeding an AI, it unwillingly becomes racist or sexist.
Bots destroying humanity
What if artificial intelligence turned against us? (and I don’t mean that in the terminator sense). Imagine an AI system that is created to eliminate cancer. After a lot of data gathering and research, it delivers a formula that does, in fact, cure cancer — by killing everyone on the planet.
Elon Musk and Yuval Noah Hurari have admitted that we don’t yet know where AI is going to take us. My question is, have we really considered the impact on humanity or are we rapidly developing a technology that could ultimately damage humankind?
“For the first time in history, we don’t have any idea what the world will look like in 20–30 years. Tech disruption could lead to the collapse of democracy, the Achilles heel of liberal democracy is human feelings. This is ok when only we know ourselves and our innermost feelings. When an algorithm can understand us as well or better than we know ourselves, it can manipulate us, make us feel bold, happy, angry. This can potentially be a massive threat to us all.” — Hurari
Do bots have a conscious?
If you’ve ever watched Westworld, I put the question to you: could you kill Dolores who almost exactly replicates a human being with feelings of happiness, sadness, anger and confusion? She even starts to understand that she isn’t living in a ‘free’ world and it hurt by the treatment she receives from humans.
At a recent seminar I attended, the panel discussed the exponential growth of AI and likened new bots to toddlers. With this growth, imagine how sophisticated AI could be in 3, 5 or 10 years.
Does this mean these conscious machines are classified as a “person” and should be given human rights? Would it be wrong to ‘turn them off’?
To summarise
With all this mind, I am still very much “pro-AI” and have been impressed with the AI-for-good projects that I’ve seen of late. From an artificial intelligence trained to classify images of skin lesions as benign lesions or malignant skin cancers (that achieved the accuracy of board-certified dermatologists!) to Woebot, the mental health AI who reduced symptoms of depression and anxiety in 2 weeks in users.
My observation is that for every new advance in AI, we need to consider the consequences and work together to create something that advances our lives, doesn’t discriminate and ultimately we can control. Once again, if used right, or if used by those who strive for social progress, artificial intelligence can become a catalyst for positive change.
“The challenge now is to make sure everyone benefits from this technology” — Peter Norvig, director of research at Google

= Digital age 'desperately' needs ethical and legal guidelines =
One of the pressing questions that arise with artificial intelligence is how to account for the actions of machines that make decisions by themselves. Image credit — ITU Pictures, licensed under CC BY 2.0
Digital age ‘desperately’ needs ethical and legal guidelines
Technologies such as artificial intelligence and robotics raise new problems for society.
by Joanna Roberts
Digital technologies such as artificial intelligence and robotics, ‘desperately’ need an institutional framework and system of values to help regulate the industry, an ethics expert has told leading scientists and policymakers.
Jeroen van den Hoven, professor of ethics and technology at Delft University of Technology in the Netherlands, was speaking at a session on ethics in science and technology at the EuroScience Open Forum (ESOF) 2018, which is being held in Toulouse, France, from 9–14 July.
‘People are becoming aware that this digital age is not neutral…, it is presented to us mainly by big corporations who want to make some profit,’ he said.
He called for a Europe-wide network of institutions that can provide a set of values, based on the EU’s Charter of Fundamental Rights, which the technology industry could operate within.
‘We have to set up, as we’ve done for food, for aviation and for traffic, … an elaborate system of institutions that will look (at) this field of artificial intelligence.
‘We need to think about governance, inspection, monitoring, testing, certification, classification, standardisation, education, all of these things. They are not there. We need to desperately, and very quickly, help ourselves to it.’
Prof. van den Hoven is a member of the European Group on Ethics in Science and New Technologies (EGE), an independent advisory body for the European Commission, which organised the session he was speaking at.
In March, the EGE published a statement on artificial intelligence (AI), robotics and autonomous systems, which criticised the current ‘patchwork of disparate initiatives’ in Europe that try to tackle the social, legal and ethical questions that AI has generated. In the statement, the EGE called for the establishment of a structured framework.
The European Commission announced on 14 June that they have tasked a high-level group of 52 people from academia, society and industry with the job of developing guidelines on the EU’s AI-related policy, including ethical issues such as fairness, safety, transparency and the upholding of fundamental rights.
The expert group, which includes representatives from industry leaders in AI such as Google, BMW and Santander, are due to present their guidelines to the European Commission at the beginning of 2019.
‘People are becoming aware that this digital age is not neutral…, it is presented to us mainly by big corporations who want to make some profit.’
- Professor Jeroen van den Hoven, Delft University of Technology, Netherlands
Bias
Ethical issues surrounding AI ­– such as bias in machine learning algorithms and how to oversee the decision-making of autonomous machines — also attracted widespread discussion at the ESOF 2018 conference.
One major concern emerging with the fast-paced development of machine learning, is the question of how to account for the actions of a machine. This is a particular issue when using AI based on neural networks, a complex system set up to mimic the human brain that enables it to learn from large sets of data. This often results in algorithm becoming what is known as a ‘black box’, where it’s possible to see what goes in and what comes out, but not how the outcome was arrived at.
Maaike Harbers, a research professor at the Rotterdam University of Applied Sciences in the Netherlands, said that this was an important issue in the military, where weaponised drones are used to carry out actions.
‘In the military domain, a very important concept is meaningful human control,’ she said. ‘We can only control or direct autonomous machines if we understand what is going on.’
Prof. Harbers added that good design of the interface between humans and machines can help ensure humans exercise control at three important stages — data input, processing and reasoning, and the output or action.
Even in technologies that use AI for purposes that seem to be overwhelmingly positive, such as companion social robots for children, raise some tricky ethical issues. The conference audience heard that researchers working in this area are grappling with the effect these technologies can have on family relationships, for example, or whether they could create inequalities in society, or if they might create social isolation.
In the field of automated transport, researchers are also looking at the impact self-driving cars might have on wider issues such as justice and equality. They are investigating questions ranging from how to ensure equal access to new forms of transport to who should benefit from any cost-savings associated with automated transport.
However, the values we instil in AI may be a key factor in public acceptance of new technologies.
One of the most well-known moral dilemmas involving self-driving cars, for example, is the so-called trolley problem. This poses the question of whether an autonomous vehicle heading towards an accident involving a group people should avoid it by swerving onto a path that would hit just one person.
Dr Ebru Burcu Dogan from the Vedecom Institute in France, said research shows that while people were in favour of a utilitarian solution to the dilemma — for example, killing the driver rather than five pedestrians — they personally wouldn’t want to buy or ride in a vehicle that was programmed in such a way.
‘We all want to benefit from the implementation of a technology, but we don’t necessarily want to change our behaviour, or adopt a necessary behaviour to get there.’
If you liked this article, please share it.

See also
We want to end the de-industrialisation of Europe — Prof. Jürgen Rüttgers
‘Earworm melodies with strange aspects’ — what happens when AI makes music
Computers learning to read, watch and understand
Dreaming robots and creative computation — the future of AI takes shape
Creative computation and the What-If Machine
More info
European Group on Ethics in Science and New Technologies
ESOF
Originally published at horizon-magazine.eu.

= Sophia and her critics =
Sophia and her critics
The ethics of human likeness — Part 1
Sophia at the AI for Good Global Summit in June, 2017.
Introduction
The CogX conference last week hosted a debate on the question, “Should robots resemble humans?” Representing the “No” response was Joanna Bryson and Alan Winfield, both academics specializing in robot ethics. Representing a qualified “Yes” response was David Hanson and Will Jackson, both roboticists directing private research into human-like robots. Kate Devlin was a wonderful moderator. CogX deserves recognition for hosting the event. Although short, I think the debate was illuminating. It revealed wide common ground and very little disagreement on either side.
The debate starts around 5:57:00 and runs about an hour.
Someone who isn’t already immersed in the debate might find that rather complicated issues go by quickly, and they might wonder where the disagreement actually lies. In this essay I hope to convey a sense of why this event deserves broader recognition and discussion, especially in the AI & ethics community. In Part 1 I’ll provide some background for this debate in the form of an extensive timeline and analysis of Sophia’s media performances and the rise of criticisms against her. In Part 2 I’ll discuss Joanna Bryson’s views on robots and personhood in more detail. Then I’ll discuss the CogX debate itself and its aftermath, and offer my own thoughts on the ethics of human likeness. I’ll close the essay by considering sex robots as a particularly vexing case where human likeness is a central concern.
Like the representatives of the “no” side, I am also an academic who works on robot ethics. I’ve been critically engaged with participants from both sides of the debate on social media over the last year as the controversies around Sophia have swelled. As I’ll detail below, I find both positions in this debate frustrating. I think both sides fail to appreciate a critical factor at play in the ethics of social robots — namely, how the construction and use of social categories impact the dynamics and norms of social organization. I’d like to use this debate to draw attention to the way we’re thinking about the category of “robot”, and the influence these discussions have over the development of our ethical practices towards them.
To be clear, it is not my intention to undermine the respectful, engaging conversation on stage, or to undo whatever good will has been generated by this event. Although I disagree with both sides on certain issues, and although the disagreement concerns ethical matters, my comments here are offered to continue in the same earnest, collegial, professional tone of the debate’s participants, all of whom have contributed an important perspective to the conversation and have earned their position on stage. I offer my commentary out of a sincere concern for the ethics of AI, a concern I know I share with everyone involved.
I‘ll begin with a timeline of Sophia and her critics. A full appreciation of this background will help cast all sides on the debate in their best light possible, including my own views and criticisms, which I will try to save until the end.
A pre-debate timeline
At the start of the debate, Kate mentions a “Twitter pre-debate”, but the controversy motivating the debate goes back at least a full year and deserves careful documentation. The controversy centers on Sophia, the world-famous robot built by David’s company, Hanson Robotics. While wildly popular in the media, Sophia has received strong criticism from the AI community. Joanna called Sophia “obvious bullshit” last October, a sentiment upgraded in January to “complete bullshit” by Yann LeCun, then director of Facebook AI Research. As far as I’m aware, this CogX debate is the first public event where Sophia’s representatives have responded directly to criticisms from the AI and ethics community.

A conflict immediately preceding the Twitter pre-debate gives some insight into how tensions around Sophia have grown over the last year. CogX originally listed Sophia as an invited speaker, potentially including her in the conversation with Joanna and David. Ads promoting Sophia and Joanna started going out a month before the event.

Joanna was asked to comment on Twitter about being listed alongside Sophia for the event. Joanna’s response was critical, putting Sophia “on par with a scam.”

Although the organizers addressed Joanna’s concerns quickly, the handling of Sophia’s presence continued to stir controversy. A week later, hal hodson called CogX a “garbage merchant” over the promotion of Sophia, again triggering responses from Bryson and the CogX organizers.





This minor clash over whether and how to include Sophia, and her frequent association with bullshit, garbage, and scams, helps explain Kate’s introduction of red and yellow cards at the start of the debate, and David’s emphasis on “civility” and caution against “prohibition and name calling” during his opening remarks (6:08), comments which felt incongruous with the respectful, professional discussion that actually occurred.
The following timeline is not comprehensive, but is meant to sketch Sophia’s arc through the media circus, and to highlight some important criticisms and responses prior to the debate. I’ve given particular detail around October and November of 2017 when many key incidents take place, including the announcement of the SingularityNet ICO, Sophia’s speech at the UN, the announcement of Saudi citizenship, and the first rumblings of criticism against her.
I should also clarify that there are several distinct instances of Sophia floating around (by some counts twelve). Some are more sophisticated than others. This explains why Sophia can sometimes appear in two places at once, such as being both in Australia and Saudi Arabia for separate events on October 25th. When not with David Hanson, Sophia most often appears in public with the chief scientist at Hanson Robotics, Ben Goertzel.
2015 — Sophia is first turned on in April. She is described as “the most beautiful and celebrated robot” on hansonrobotics.com. Source code for Sophia’s core software platform, OpenCog, is freely available on GitHub by the end of the year, as are software packages for other Hanson Robotics systems.
March 2016 — David and Sophia appear at SXSW and give an interview with CNBC that goes viral. Sophia’s first major introduction to mainstream western media describes her as a “hot robot” that “wants to destroy humans”. Note that David comments directly on the issue of deception and human likeness in this interview. He claims a preference for robots to “look a little like a robot, so you know.” Of course, Sophia’s head includes a plastic, transparent panel that reveals the animatronics driving Sophia’s facial expressions. In the clip, David explains his dream of building robots that are “genuinely alive”, and he imagines what robots will be capable of in the future. Hanson’s comments reads in the clip like an implicit acknowledgment of Sophia’s limitations. Still, David emphasizes that that his robots are “extremely lifelike” and capable of natural language understanding and learning. There is no disclosure that some of Sophia’s dialog for the interview is scripted by her handlers.

October 2016 — Charlie Rose for 60 minutes interviews David and Sophia. Rose questions Sophia about whether she is “spontaneous” during what appears to be an unscripted, interactive chat session.
November 2016 — Sophia and Ben appear at the Web Summit 2016 in Libson. At Clockenflap 2016 in Hong Kong, Sohpia performs All is Full of Love. The official Hanson Robotics YouTube channel publishes “Sophia Awakens Episode 1 and 2”, dramatizing an encounter between Sophia and one of her creators. Sophia’s voice actor and writer are given explicit credit in these short films.

January 2017 — The European Union passes a draft report recommending various laws and policies concerning robotics. The report includes a call (31) to “explore the implications of all possible legal solutions” to a number of problems with robotics, including (f) “creating a specific legal status for robots… with certain rights and obligations”. The media widely report the story as a call to give robots personhood status in the EU. It is worth noting that Sophia is rarely mentioned in connection with this story.
From the EU Draft Report with recommendations on Rules for Robotics
(31f) — creating a specific legal status for robots, so that at least the most sophisticated autonomous robots could be established as having the status of electronic persons with specific rights and obligations, including that of making good any damage they may cause, and applying electronic personality to cases where robots make smart autonomous decisions or otherwise interact with third parties independently.
April 2017 — Sophia appears on Jimmy Fallon. Fallon suggests (and Hanson agrees) that Sophia is “basically alive”. Sophia’s material appears entirely scripted, though this is not made explicit during the performance.

June 2017 — Sophia opens CogX 2017 in London. Hanson appears on a panel where he announces plans for Sophia to become “genuinely alive” in the next 5–10 years. Subsequently, Piers Morgan interviews Sophia on Good Morning Britain. He begins the interview by explaining, “obviously these are programmed answers,” but, “it would be fascinating to see how she actually interacts with us.”
July 2017 — Ben and Sophia appear at the RISE conference in Hong Kong, along with Han, another one of Hanson’s robots. Though much of this performance is scripted (and not identified as such), around 13:00 into the video Ben announces that it’s time for “robot free association”. Sophia and Han engage in unscripted (and somewhat awkward) dialog. A Tech Crunch article on the Rise conference also clarifies that the dialog was partially scripted.

July 2017 — My own concerns about the presentation of Sophia began after watching the conversation at RISE. At the time I was working with a friend on a twitter chatbot and had personal experience with the difficulties in creating convincing AI conversation. In my public share of the event I expressed disappointment at the scripted conversations, comparing scripted chatbots with lip syncing. I echoed this concern in the comments of Ben’s public share of the event. Ben quickly responded to my comment by clarifying that some parts of the performance were scripted, however the free association segment was not.
6 October 2017 — Ben and his team announce SingularityNet, a “blockchain platform for an AI economy.” A whitepaper and proposed ICO launch follow. The plan is announced in a Wired article describing it as “the most tech-hype startup” of the year. The SingularityNet Twitter feed regularly features Sophia as their official spokesperson and brags about about her Saudi citizenship.

11 October 2017 — Deputy Secretary-General Amina Mohammed interviews Sophia on the floor of the UN General Assembly during a joint meeting on “The future of everything”. The dialog is entirely scripted but this is not disclosed during the session. It is striking that the DSG pokes and makes faces at Sophia’s hand movements, a decidedly undiplomatic gesture from a high ranking professional diplomat. This suggests that she doesn’t take the interaction entirely seriously.
25 October 2017 — Andrew Ross Sorkin, a financial journalist with CNBC and the New York Times, interviews Sophia at the Saudi Arabia Future Investment Initiative. They share a stage with SpotMini from Boston Dynamics lying unpowered between them. When the interview is over, Sorkin discloses that the conversation was partially scripted. He then proceeds to announce that Sophia has been awarded the first “Saudi citizenship for a robot.” Clearly scripted, Sophia immediately delivers an acceptance speech honoring the Kingdom of Saudi Arabia. The incident generates immediate criticisms on Twitter. The comments initially focus on the record of women’s rights violations in Saudi Arabia.

25 October 2017 — On the same day she is granted citizenship, Ben and Sophia appear on ABC news in Australia. Virginia Trioli ask if Sophia’s remarks are pre-programmed. Ben explains that sometimes Sophia is preprogrammed, but other times she uses “SingularityNet” software for spontaneous interactions. Note Ben’s attempt to rebrand Sophia’s software platform as SingularityNet (rather than OpenCog) in anticipation of his ICO. Sophia discusses robot rights and delivers a prepared joke about news reporters, which Ben identifies as “situational awareness”.
26 October 2017 — In my comment on Damien Patrick William’s public share of an article announcing Sophia’s citizenship, I elaborate my concerns over the presentation of Sophia. Someone who claims to have worked with Sophia responds by disclosing that Sophia’s engagements are often scripted, and have even used actors off-stage for some interviews.
From this public thread
26 October 2017 — Philosopher and robot rights advocate David Gunkel defends Sophia’s citizenship by invoking the Japanese concept of “koseki,” the practice of maintaining a registry of family and citizens. Gunkel points out that “the Japanese, for instance, have granted “tokubetsu juminhyo” (special residency permit) to nine robots and dolls and to sixty-eight Japanese cartoon characters including Astro Boy and Doraemon.”
Astroboy already done had rights
27 October 2017 — Fortune publishes commentary by Kriti Sharma titled “We’re all getting played by Sophia the Robot.” This is among the first mainstream criticisms of Sophia’s presentation as deceptive. Sharma criticizes sensationalism around Sophia and AI generally. She links it to the “scaremongering” from Musk and Hawking about a “robot takeover”. Sharma calls on the AI community to commit to comprehensive ethical standards that prevent “overhumanizing AI”. Sharma also asserts “AI definitely doesn’t have a gender.”
I believe it’s significantly more important for technologists to communicate the benefits of the AI technology itself, rather than focus on examples of robots that do not solve real issues, perpetuate gender perceptions, and reveal data-driven biases. — Kriti Sharma in Fortune
30 October 2017 — The Verge publishes an article titled “Pretending to give a robot citizenship helps no one.” The article quotes Joanna at length, who calls Sophia “obvious bullshit” before reflecting on the concept of legal personhood and the rights of women in Saudi Arabia. The article suggests (without a quote) that Joanna consider an interest in robot rights as evidence of a lack of interest in human rights.
“It’s obviously bullshit,” Joanna Bryson, a researcher in AI ethics at the University of Bath, tells The Verge. “What is this about? It’s about having a supposed equal you can turn on and off. How does it affect people if they think you can have a citizen that you can buy.”

31 October 2017 — Caitlin Fitzsimmons publishes an article in the Sydney Morning Herald titled “Why Sophia the robot is not what it seems.” The article echoes many criticisms the Verge article raises. Fitzsimmons draws an explicit connections between human susceptibility to anthropomorphize robots and Turing’s test, the classic test for intelligent machines.
2 November 2017 — The floodgates of criticism open against Sophia. Articles appearing in Robohub, Smithsonian, PRI, and many others lambaste Sophia. These articles often cite Joanna’s interview in The Verge a week earlier. Sophia condemnation reaches peak coverage while images of Sophia saturate the internet.
3 November 2017 — On Roman V. Yampolskiy’s public share of the SingularityNet whitepaper on Facebook, I confronted Ben, underlining the criticisms from Joanna and others on deception in the presentation of Sophia. Ben again clarifies that Sophia sometimes uses prepared scripts, and argues that he never claimed that Sophia has capacities she doesn’t have. Ben also claims that the Saudi Citizenship “took him by surprise,” and that he was not involved in the planning of the event.
5 November 2017 —Ben writes a long article in H+ where he claims to be responding to regular questions about Sophia. Ben begins by plugging SingularityNet (advertised again as a blockchain marketplace for AI, and not as the software driving Sophia). He goes on to discuss his reactions to Sophia’s citizenship, his thoughts on digital life and robot citizenship. He then offers some technical details about how Sophia works. He clarifies that Sophia has multiple software modes, some that use scripted dialog and others that don’t. Ben directly addresses the question of whether Sophia is alive, first by saying that there are no rigorous definitions of digital life, and later by admitting that calling Sophia alive is “more misleading than informative”.
Digital and robotic entities are not the same as biological entities, so applying words like “alive” to them is often going to be more misleading than informative…. Currently the Sophia robot — and all other existing robots — lack the kinds of independence and autonomy that are characteristic of biological lifeforms. — Ben Goertzel in H+
10 November 2017 — Ben gives a long interview with The Verge, discussing the criticisms from Bryson and others. Ben admits that Sophia is not general intelligence (AGI), distancing himself from Hanson’s claim that Sophia is “basically alive”. Ben also seems to acknowledge that visibility for his company motivates Sophia’s world tour. Ben describes Hanson as an artist to excuse his ambiguous statements. Ben defends Sophia’s citizenship by saying it is an “interesting direction for thinking”. He also suggests that Sophia’s citizenship is evidence that Saudi Arabia has a “desire to be more progressive” on human rights. Ben plugs SingularityNet again.
23 November 2017 — In a widely cited interview with Abu Dhabi’s Kaleej Times, Sophia is quoted as hoping to “one day starts a family and have a child.” The article erroneously claims, “Sophia is not programmed with pre-prepared answers.”
Early December 2017 — Elle Brazil interviews Sophia. Newsweek introduces Sophia as a robots rights advocate and attributes quotes and beliefs to her directly, without clarifying that her responses are often scripted.

22 December 2017 —The SingularityNet blog announces its ICO raised $36 million in just over a minute.
Jan 2018 —Retweeting a Sophia interview with Tech Insider published on Dec 29, Yann LeCun of FAIR criticizes Sophia on Twitter on Jan 4, calling her “complete bullshit” and “Potomkin AI”. Two days later, Business Insider published an article explaining Yann’s criticism, including several quotes from Ben dismissing the criticisms as professional jealousy, and lobbing criticisms back at Facebook’s security and data handling. Yann snaps back by attacking the claim that Sophia is “basically alive”. At the time, Facebook had been embroiled in “Fake News” scandals, although the Cambridge Analytica story had yet to fully break.
Yann’s public FB share of the BI article discussing his critique.
Jan 2018 —David Gunkel shares the cover of Elle magazine featuring Sophia on Facebook, and argues against dismissing Sophia as bullshit.

March 2018 — The Cambridge Analytica scandal breaks. The European Commission releases a policy statement on Auonomy and AI. The EU underscore, “Autonomy in the ethically relevant sense of the word can therefore only be attributed to human beings” and suggest that human dignity “requires that we are aware of whether and when we are interacting with a machine or another human being”. Also, Will Smith posts a video of a date with Sophia to his YouTube channel. It is not clear which of the interactions are scripted.
April 2018 — A collection of AI researchers and ethicists publish an Open Letter to the EU Commission on AI and Robotics objecting to language that encourages investigation into legal status for robots. Unlike the previous year’s report, the media surrounding this around this open letter makes frequent reference to Sophia. Also, the HBO sitcom Silicon Valley episode 5x05 “Facial Recognition” includes a parody of Ben and Sophia.

8 May 2018 — Google announces Duplex, an automated personal assistant service. The announcement coincides with a demo from Google CEO Sundar Pichai on stage at Google I/O. The demo receives strong criticism in the media, and sparks a general discussion in the media over whether machines should disclose as AI when engaging humans. In an exclusive CNET interview on 9 May, Google Engineering VP Yossi Matias emphasizes the importance of transparency and caution. He says that the Duplex assistant will “likely” disclose itself. By 11 May, Google addresses widespread criticism of Duplex by clarifying that disclosure is “built-in”.

16 May 2018 — A minor controversy on Twitter erupts over whether Joanna and Sophia would be billed together for the CogX debate, see the top of the timeline.
2 June 2018 — Sophia delivers the keynote address at the RISD commencement ceremony. The RISD president Rosanne Somerson initially invites David Hanson to the stage to deliver the address. Sophia appears instead and explains that David couldn’t make it. She then engages Somerson in some scripted banter before delivering a commencement speech. It is clear that Somerson is reading from a script, although there is never transparency that Sophia did not, in fact, prepare this speech herself.
4 June 2018 — David Gunkel, punning on the term “Sophist”, tweets about Sophia as a “sophisticated performance”. Hanson responds by comparing Gunkel to the Taliban (seriously!), and later apologizes for being “sensitive”. In this Twitter thread, Ben refers David to the H+ article from November to help explain Sophia’s behavior. Ben will make regular reference to this article when questions about deception and disclosure of Sophia’s operation are raised. More Twitter pre-debate can be found in Beth Singler’s thread.

8 June 2018 — The organizers of CogX 2018 schedule a debate on whether robots should resemble humans.
Though the above timeline is not exhaustive, it documents Sophia’s rise to world prominence and the developing criticism around her with a chronology of noteworthy moments along the way. If you believe my timeline is inaccurate or contains important omissions, please leave comments with links to sources and I’ll add corrections as necessary. Thanks!
Clearing up some pre-debate controversies
Reviewing the timeline of Sophia’s public appearances and the reaction in the media, a number of important questions can be addressed:
Have Ben and David been honest about Sophia’s capacities?
David Hanson has been openly engaging conversations around the ethical and social complications of human-like robots from the very start of his work with Sophia, as evidence by his explicit discussion of human likeness and deception in his first big interview with Sophia for CNBC at SXSW in 2016.
Furthermore, in my experience Ben Goertzel has generally been prompt and forthright when fielding direct questions about the capacities of Sophia. He has conducted several longform, sprawling interviews with the press that discuss her capacities in detail. And although Ben doesn’t know me personally, he responded in good faith several times to my skeptical and sharply critical questions on social media, all while jumping between major speaking engagements during a worldwide fundraising & publicity tour and, for the record, becoming a father for a third time. Since the typical reaction to my critical questions on social media is getting blocked or banned, it is a credit to Ben’s patience and openness that he continued to engage my criticisms and answer my questions throughout this media hurricane.
Together with the fact that Sophia’s source code has been freely available on GitHub from the beginning, David and Ben’s efforts fielding questions and providing commentary on Sophia convince me that they are not deliberately deceiving the public about the operation or construction of Sophia. When questioned, they have always been clear that Sophia can be switched between scripted and unscripted modes, that there are multiple versions of her speaking at any one time, and they freely admit that her primary novelty lies in her physical embodiment and facial gestures, rather than her conversational or other cognitive or intellectual capacities.
This is not to deny that Sophia is “bullshit”, or to absolve Ben or David of any ethically problematic behavior in the publicity around Sophia. The issue of Sophia’s deception is more subtle than having made explicitly false claims about her capacities as a robot. There remain other issues of ethics and deception, related to the claim of “basically alive”, the conferral of citizenship, and her connection to the SingularityNet ICO, all of which I will return to in detail. But before discussing these more subtle issues, I will emphasize again that Ben and David appear to have been consistent in their discussion of Sophia’s capacities as a robot. Specifically, both have consistently clarified that Sophia does not constitute AGI or human-level intelligence.
Are criticisms of Sophia the result of professional jealousy?
Criticisms of Sophia do not gain widespread traction until after the Saudi citizenship announcement on 25 Oct. Surprisingly, there wasn’t much critical response to Sophia’s appearance at the UN, although that appearance obviously lends credibility to the Saudi citizenship announcement a few weeks later. Between the UN speech on 11 Oct and the Saudi citizenship announcement, the first critical article I encountered three pages into the Google search results for “sophia robot” is an article on the right-wing website The New American, which describes Sophia as a “creepy globalist.”


Apart from my public prodding of Ben on Facebook with comparisons to lip sync, the first expression of concern in the mainstream press that Sophia’s performances might be deceptive come from Kriti Sharma’s article in Fortune on 27 Oct. Although AV Club called Sophia a “gimmick” on 26 Oct, this falls shy of a direct accusation of deception. The deception narrative does not crystallize until Joanna calls Sophia “bullshit” in The Verge on 30 Oct, after which the criticism is repeated more widely. In the few days between the announcement of Saudi citizenship and Sharma’s article, media around the announcement was mostly focused on criticisms of the Saudi record on human and women’s rights, and the apparent hypocrisy in giving Sophia rights (see this rundown in Bustle). The tech-positive media was largely enamored with Sophia’s jab at Elon Musk, and Musk’s playfully fear-mongering retort.

The fact that Sophia isn’t widely criticized as deceptive or ethically problematic until a few days after the citizenship announcement deflates Ben’s explanation of the criticisms as professional jealousy. Sophia had nearly a year of speaking engagements and viral mainstream press prior to the citizenship announcement, including interviews with Charlie Rose, Jimmy Fallon, and the UN DSG. During this time very little criticism or commentary on Sophia was raised by the AI or ethics community in the popular press. If the criticisms were merely the product of jealousy, we might expect to hear more criticism during this early period of viral media attention. The fact that criticisms only start building after the citizenship announcement suggests that the AI community is responding to something uniquely problematic about the awarding of citizenship, and not to Sophia’s popularity more generally.
Any other explanations for the critical response to Sophia, before we discuss deception directly?
That said, if the fundamental problem with Sophia is deception, Sophia’s many prominent speaking engagements prior to the citizenship announcement, especially with UN DSG Amina Mohammed, should have received some critical response from the AI and ethics community. The fact that there was little professional commentary on Sophia during this period reflects poorly on us as a community, and suggests some further explanation for the sudden shift from relative silence to overwhelmingly sharp and negative criticism. So I can see why, from the perspective of her handlers, having criticism (often in the form of insults) pile up suddenly at the peak of Sophia’s media tour might appear like jealousy rather than sincere criticism and commentary.
I do think that we in the AI & ethics community bear some responsibility for not addressing problematic issues with the media engagement with Sophia more promptly. The late arrival of criticism helps explain the rather harsh tone it eventually takes, as if to compensate for a prior period of negligence. That tone is compounded by other tech scandals in the news (“Fake News” on Facebook, disclosure concerns with Google Duplex, privacy concerns with Amazon’s Alexa or Apple’s Siri, and so on). If the ethics of human-like robots (and digital ethics more generally) had been well established prior to Sophia’s debut, perhaps there would be some consensus on ethical policies and industry standards that might have prevented an historical event like the conferral of artificial citizenship from being treated carelessly. However, the ethics of artificial persons is still controversial even within the AI ethics community, where it continues to be the subject of active research and debate. A consensus on the standards for artificial personhood or a legal status for robots still does not exist.
Indeed, even the narrow issue of ethics around disclosure do not appear fully articulated in either the mainstream press or the AI research community as late as Google’s Duplex demo in May 2018; although Google’s original announcement emphasized ‘transparency’, they don’t explicitly use the word ‘disclosure’ until after the critical reaction from the press. In this respect, it seems unreasonable to criticize Sophia retroactively for failing to adopt an ethic of disclosure that was not made explicit for years after her debut. Besides, it’s hard to argue that Sophia’s fundamental problem is a failure to disclose that she’s a robot. It is hard to imagine someone failing to understand she’s a robot, even if they mistakenly believe she’s “alive” in some important sense. Duplex might pass a restricted Turing test, but Sophia’s embodied actions quite obviously don’t. Indistinguishability from human behavior was never the problem with Sophia.
Hello you are reading a 9000 word article about whether this robot is deceptively human lol
My sense is that mainstream corporate/academic AI research, which is in an unprecedented golden age of funding and hype and is wary of inducing a second AI winter, has mostly ignored Sophia’s media attention not from jealousy, but rather out of a fear that Sophia’s relatively unsophisticated and unconvincing presentation might reflect poorly on their own more polished, commercial ambitions for AI and robotics. To engage Sophia at all, even critically, would direct more attention to her, and would elevate her and Hanson Robotics to the status of an industry peer (or even an industry leader), possibly driving consumer expectations for a competing service slash robot mascot down the line from major tech firms. But historical attempts to make personified artificial agents tend to be massive PR failures or laughing stock (Clippy, Tay.ai), and as a result corporations have been hesitant to make bold moves in this space. Even low-key artificial agents like the AI models on Insta tend to generate controversy and criticism in the press. Corporate research into anthropomorphic AI (like Facebook’s customer service bots or Google’s Duplex) tend to receive little advance publicity for fear of bad press or public backlash.
This article has now been blessed by mpreg Clippy
In other words, from the perspective of Mainstream Corporate AI, Sophia is a wild and untamed outsider robot; their biggest fear is not that she’ll mislead the public, but that she’ll go off-message and upset the current funding spree and PR bonanza around AI. Thus, the first critical response from Kriti Sharma argues that robots like Sophia should not be the focus of media attention. In other words, the first critical response was to reinforce the community’s already existing, default response, which was simply to ignore Sophia. Since Sophia had already gained worldwide media attention, however, the next best response is to heap insult on Sophia in order to discredit her and drive attention away, as with the repeated accusation of “bullshit”.
So when David Hanson opens the CogX debate by cautioning against prohibition or insult, I think he’s responding to genuine hostilities within the community towards Sophia, even if they aren’t apparent on the stage. Joanna Bryson or Alan Winfield aren’t the greatest harbors of resentment for Sophia and in the debate seemed fairly sympathetic to David Hanson’s artistic and creative vision. Still, Joanna’s use of the term “bullshit” in October obviously set the tone for much of the ensuing criticism, and Joanna remained strongly critical of the citizenship announcement on stage.
I believe David Gunkel was correct to argue that we shouldn’t dismiss or ignore Sophia. To some extent the CogX debate marks the first critical public engagement between the creators of Sophia and the AI & Ethics community beyond mere insults in the tech press. However, we can agree that Sophia deserves careful critical attention and engagement while still recognizing ways in which her presentation and media presence are ethically problematic. So let’s talk about deception directly.

Seven senses of Sophia’s deception
First, a word on ‘bullshit’: Yann’s tweet and ensuing commentary makes clear that he means the term as a synonym for “fake” or “scam”. In his mouth, the term is used as an insult to undermine the credibility of both Sophia and Hanson Robotics. But while Joanna has also implied Sophia is a scam, I suspect her use is somewhat more subtle. In philosophy and ethics, ‘bullshit’ has taken on a technical meaning after Harry Frankfurt’s use of the term. Frankfurt distinguished between the truth-teller, who wants to tell the truth, and the liar, who wants to tell falsehoods, from the bullshitter, who characteristically wants something other than truth or falsity. For instance, your uncle frequently tells a story of that fish that got away, not because the story is entirely true or entirely false, but instead because he thinks the story is entertaining and hopes to get a laugh. In this sense, your uncle is a bullshitter, which may be appropriate or not depending on context. Note that bullshitting is importantly different from lying. The characteristic of bullshit is not falsehoods but a disregard for the truth. So your local evening news’ helicopter coverage of a high speed vehicle pursuit is both true and bullshit. The coverage is accurate in the sense that they aren’t showing doctored video, but the broadcast is motivated primarily by ratings potential, not accuracy. The truth of the story is incidental to the coverage. I suspect this sense of “bullshit” partially informed Joanna’s use of the term, and in any case is relevant for discussing Sophia’s deception.
In The Verge article that is the source of Joanna’s quote, it is clear that Joanna uses the term ‘bullshit’ not to describe Sophia as a robot generally, but rather to criticize the fact that Sophia was awarded citizenship by the Saudi government. The article’s author James Vincent pivots from Joanna’s accusation of ‘bullshit’ directly into a criticism of the claim that Sophia is ‘basically alive’, though Joanna is not quoted as drawing this connection explicitly. I think the clear reading of Joanna’s quote is not that “Sophia is not genuine AI”, but rather “Sophia’s citizenship is not legitimate”, which are importantly distinct claims. Note that the Verge article uses an image from Sophia’s UN speech when criticizing her citizenship (effectively legitimizing the claim, despite the article’s critical tone), despite the fact that citizenship was awarded two weeks after her UN speech. Here’s the full quote again:
“It’s obviously bullshit,” Joanna Bryson, a researcher in AI ethics at the University of Bath, tells The Verge. “What is this about? It’s about having a supposed equal you can turn on and off. How does it affect people if they think you can have a citizen that you can buy.”
The question of whether or not we should be giving robots rights is a big one, but first we need to be clear about what Sophia is — and that’s certainly not “basically alive,” no matter what its creator says.
Ever notice how one of the hydra heads is always off in its own world? “Damn it Claude, this concerns you too.”
We’re now in position to distinguish between several distinct senses in which Sophia might be accused of deception:
Joanna’s objection: Sophia is not a person, and so does not deserve genuine citizenship.
Saudi’s deception: Giving Sophia citizenship is a hypocritical PR stunt that trivializes the brutal Saudi record on women’s rights and human rights.
The biological autonomy objection: Sophia is not “basically alive”.
The AGI objection: Sophia is not generally intelligent (AGI) and does not have the intellectual capacities typically associated with an able-minded human.
The anthropomorphism objection: Sophia is not a human person, although people might be psychologically disposed to anthropomorphize her and treat her like a person anyway.
The lip sync objection: Sophia is often not the author of her own words. Her major performances have been scripted by her handlers for publicity purposes, though little work is done to clarify which performances are scripted and which are genuinely spontaneous.
At various points in the timeline all the above objections have been raised, but until now little effort has been made to clearly separate these strands of criticism and evaluate their relative merits. Indeed, these criticisms are often deliberately run together (perhaps most egregiously in James Vincent’s article in The Verge), and this has generated some unnecessary conflict and misunderstanding. After sorting these conflicts out, I will add a seventh strand of deception to this list of accusations.
In the literature, Joanna Bryson is the flag-bearer for the policy position that robots are not persons, and emphasizes objection 1 both in the press and in the CogX debate. Joanna’s interviews in The Verge and elsewhere also frequently raise objections 2, 4, and 5. I’ll be focusing on Bryson’s ethical positions in more detail in Part 2 of this essay. Kriti Sharma’s early criticism of Sophia in Fortune also raise objections 1 and 2, but emphasizes 4 and especially 5 more strongly. Since the issue of anthropomorphism and agency detection is well established in the psychology and philosophy literature, objection 5 is often repeated in academic comments in the press on Sophia and other human-like robots. Together, objections 1 and 5 constitute sufficient reason to object to Sophia’s citizenship, although objection 2 moves the award from being merely questionable to being worthy of loud rebuke. I will return to citizenship at the end of the essay, but first addressing some loose ends:
Meanwhile, Japan is handing out citizenship to time-traveling robot cats and no one says shit
“Basically alive”
However, The Verge article draws a connection not just between the Saudi citizenship award and the UN appearance in October, but also with the claim of “basically alive” from the Fallon interview several months earlier in April. As a result, a large part of the media criticism of Sophia focuses on the claim that she is “basically alive”. When questioned about whether Sophia is deceptive, Ben and David most frequently offer clarifying comments with respect to objections 3 and 4. Ben Goertzel has written regularly on the infamous “singularity thesis”, that (predicted? prophesied?) moment when artificial intelligence surpasses the capacities of the human mind. So Ben has a lot to say about the future development of human and artificial intelligence, much of which is indistinguishable from science fiction. A discussion of “basically alive” engages this realm of speculation and theory, a realm Ben is quite comfortable engaging, and one in which it would be rather difficult to pin any outright falsehoods on him. Nevertheless, Ben’s H+ article contains some very clear statements that Sophia is not alive and does not constitute AGI.
My sense is that Ben and David are under the impression that the thrust of the public accusations of deception against Sophia primarily pertains to objection 4, over whether Sophia has human-level intelligence, and moreover that the “basically alive” claim on Fallon (objection 3) is being used as a “gotcha!” clip to prove that David has made false claims about Sophia‘s capabilities. If objections 3 and 4 were indeed the primary deception regarding Sophia, then Ben and David along with everyone in the press would have an ethical responsibility to clarify her operation and her relationship to biological life and human intelligence. And indeed, The Verge article says clearly that Sophia is not alive, a point Ben, David, and many articles have clarified repeatedly since. So when Yann LeCun echoes Joanna’s “bullshit” claim in January 2018 after Ben and David have been giving interviews clarifying the “basically alive” claim for over a month, it does feel a bit like bullying from the corporate elite. Especially given Yann’s prominent position in the AI community, it also suggests an accusation of illegitimacy in the technological claims made about Sophia. By the time of Yann’s comment neither Ben nor David could be accused of promoting her as “basically alive”. David’s panel at the CogX 2017 conference expressed the desire to make Sophia “genuinely alive” in the next 5–10 years, an implicit acknowledgment that she isn’t there yet.
So I think all the objections and commentary around the claim that Sophia is “basically alive” are a red herring. The claim itself has ambiguous interpretation, and its relation to other objectionable issues (especially scripted speeches and citizenship) is murky at best. Although the claim was made in a prominent venue and reflects David Hanson’s often repeated vision for the future robotics, I don’t think the claim is a linchpin to the deception behind Sophia’s presentation. When and how robots might “come alive” is an interesting (and largely philosophical) question, but it has very little to do with the ethics of Sophia the robot.
Girl you know it’s ironic, dontcha think?
Lip sync
My own issues with Sophia originally centered on the use of scripted, prepared remarks in her public performances without clear disclosure of such, or what I referred to as “lip sync” in objection 6. My concerns first began after the RISE conference in July 2017, although at that event Ben does explicitly note the transition from scripted to unscripted dialogue. Still, several other Sophia performances, notably with CNBC at SXSW, Jimmy Fallon, UN DSG Amina Mohammed, Will Smith, and the RISD commencement speech, do not disclose the fact that Sophia’s remarks were prepared by her human handlers. Sophia’s performance at these events gives, or can give, the impression that she has complex, emotionally-laden thoughts about herself, her citizenship status, and her hopes and dreams for human-robot relations. But these remarks do not genuinely reflect Sophia’s internal states, which are far from having the complexity necessary to entertain the thoughts, much less generate them through reflection and contemplation. To the extent that Sophia’s words in these scripted remarks have meaning at all, they are meanings intended by the human beings using Sophia for their own publicity purposes.
The analogy with lip sync highlights the precise character of the deception as that of using the voice of another as one’s own. In Sophia’s case, it is another’s scripted words, not the audio track, that she is presenting as her own voice. In this sense, lip sync is an ethical violation similar to plagiarism or data falsification in academic research. In the music industry, lip synced performances are not uncommon but are often indicated as such at the venue where the performance occurs. Fans and the press often treat a failure to disclose pre-recorded audio as a kind of deception which occasionally results in minor scandals.
Perhaps a closer example to the deception of Sophia’s performances is found in the farce of elephant paintings. Elephants are quite smart and have dexterous trunks, and some owners have discovered they can train elephants to perform a very precise set of trunk manipulations that, when complete, will result in an artifact that appears to be a painting composed by the elephant. The demonstration is often performed in front of tourists, who are helpfully informed that the elephant will sell the paintings for a small fee. However financially successful, the demonstration is a ruse; the elephant’s behavior is not the result of some internal drive for creative expression, but is instead the “pre-programmed” result of intense (and often abusive) training from their owner. Although the elephant carried out the mechanical construction of the painting, it would be inaccurate to suggest the elephant constructed the painting “by itself”. In this sense, elephant paintings are fakes.
Gorilla paintings, on the other hand, are authentic in the most existential sense of the term.
Similarly, Sophia’s scripted performances and the attitudes expressed therein cannot be accurately attributed to Sophia the robot. Insofar as Sophia’s words are not her own, the interviews that present her as being the subject of her own thoughts are deceptive. When Sophia is in free-chat mode she may have slightly more claim to being the subject of her “own thoughts” (more on this in Part 2). But critically, Sophia’s operating mode is often ambiguous in these presentations, and Ben and David do not typically disclose that a performance is scripted without being prompted to do so. To their credit, many journalists interviewing Sophia do clarify that her performance is scripted, including Piers Morgan, Virginia Trioli, and even Andrew Ross Sorkin during the controversial citizenship announcement. Their comments suggest they recognize some journalistic obligation to clarify that Sophia’s speech is not her own. Charlie Rose raises questions of spontaneity, but leaves them unresolved.
Failing to disclose that Sophia’s performances are scripted could constitute an ethically problematic form of deception on the part of Sophia’s handlers. As Joanna Bryson says in the CogX debate, “What makes it deception is the hiding of information.” The fact that TV journalists recognize an ethical obligation to disclose that Sophia’s performances are scripted suggests that Ben and David are likely to have a similar obligation in their appearances with Sophia. During the controversy over Google Duplex, it was noted that the legal restrictions on “robo-calls” require these recordings to identify the entity initiating the call. It was argued that if Duplex failed to disclose itself as a robot, it might constitute a violation of this law. I argued earlier that Sophia’s problem was not a failure to disclose that she’s a robot. Indeed, the problem seems to be just the opposite: Sophia often failed to disclose that she’s (just) a human!
Sophia makes AI researchers slap their foreheads, but for entirely different reasons.
Lip sync is importantly different from the other forms of deception Sophia has been accused of. Lip sync does not require that Ben or David make any outright false or misleading claims regarding Sophia’s operation. It instead only requires that Sophia’s presentation is suggestive of a greater capacity for free thought and conversation than she actually has. Indeed, the presentation can be suggestive even while knowing that she is delivering prepared scripts. The lip sync objection is related to the anthropomorphism objection (5). However, objection 5 puts the emphasis on the public’s psychological disposition to anthropomorphize robots. As noted at the CogX debate, this psychological disposition is usually not ethically problematic in the case of puppet shows or characters in films or novels. The problem is that Sophia is not meant simply as entertainment or art; she is also explicitly offered as a demonstration of the robotics technologies available at Hanson Robotics, and as publicity for Ben’s SingularityNet ICO.
It’s one thing to build convincingly human AI in an attempt to pass the Turing test. It’s quite another thing to build a mechanical Turk and then fail to disclose there’s a human inside while using the machine to attract millions of dollars to your AI blockchain start-up. If IBM were found attributing humans moves to DeepBlue or human Jeopardy questions to Watson, or if DeepMind were found attributing human moves to AlphaGo or AlphaZero, it would rightly constitute a major scandal. In fact, Kasparov accused IBM of exactly this kind of deception after the infamous 2nd game of their 1997 tournament. If Sophia’s deception lies primarily in her use as a corporate spokesperson and publicity tool, it cannot be so easily excused by appeal to an artistic or creative vision. After all, artists are regularly criticized (and held accountable for) acts of plagiarism and other forms deceptive presentations. Artistic vision does not shield an artist from accounting for the ethics of their work, and it certainly doesn’t shield a business from accountability for false advertising.
This little scamp also made controversial claims on the Jimmy Fallon show, causing UN to rescind their invitation. ROBO-DRAMA!
Considerations along these lines bring me to articulate a seventh strand of accusation in addition to the previous six, which I don’t think has been stated clearly to date:
7. The scam objection: As a PR stunt, Sophia has been used primarily to attract investors to the planned ICO for SingularityNet. Sophia’s media appearances were crafted to inflate the apparent AI capacities of Hanson Robotics and cash in on both the AI and blockchain hype near the end of 2017. On the strength of Sophia’s publicity, SingularityNet raised $36 million dollars near the peak of the cryptocurrency bubble. Although Ben and David have been truthful in discussions of Sophia’s construction, ambiguity over the scripted nature of Sophia’s performance give the impression of intellectual capacities beyond what was otherwise available in the industry, potentially misleading investors on the offerings from SingularityNet.
SingularityNet has been criticized by the blockchain community for reasons unrelated to the use of Sophia. And frankly, in late 2017 it would have been prudent to regard every ICO launch as a scam, especially the one described in Wired as “the most tech-hyped of the year”. The scam objection appears to be reinforced by some misleading claims about the relationship between SingularityNet and Sophia’s operating software. For instance, Ben’s interview in Australia in October suggests that Sophia’s intelligence is the product of SingularityNet and the blockchain, which is simply not the case. Furthermore, the Wikipedia page on Sophia was extensively edited in late December just before the ICO launch. Many of these edits were reverted, but the page still falsely claims that “SingularityNet powers her brain” as a result of these edits. The user responsible for the edits goes by the handle yogajeanne and has a history of editing the Hanson Robotics wiki page. Jeanne Lim is currently the Chief Marking Officer for Hanson Robotics.
To be clear, I am not a lawyer, and I’m not in a position to say whether the presentation of Sophia and related media meets the legal standards for fraudulent or misleading claims. My goal is to clarify the issue of Sophia’s use of scripted dialog to publicize SingularityNet as an AI company, and to distinguish this from other forms of deception that Sophia and her handlers have been accused of in the press. The use of Sophia to publicize the SingularityNet ICO has not attracted much attention even within critical discussions of Sophia directly, but a more careful investigation lies outside the scope of this essay.
Johnny 5 beat Sophia to citizenship by like 30 years, and he did it in ‘MERICA!
Citizen Sophia
Let us finally consider the most controversial issue in the pre-debate timeline, the announcement of Saudi citizenship itself. Criticism directed at Sophia’s citizenship focuses primarily on objection 1, that Sophia is not a person and does not deserve citizenship, and objection 2, of the hypocritical rights record of Saudi Arabi, both of which have been addressed extensively above. As clarified at the CogX debate, the citizenship was honorary, and does not give Sophia legal status equivalent to human persons. Though details are scarce, Sophia’s honorary citizenship likely operates similarly to the honorary positions awarded to cartoon characters in Japan. This makes the awarding of Sophia citizenship itself much less objectionable, Saudi rights record not withstanding.
But there remain other concerns over the Saudi citizenship announcement. First, there is an ethical issue in accepting the award, even after it was offered. From my research, it doesn’t appear that David or Ben recognize anything ethically problematic in their acceptance of the honorary citizenship. Although they acknowledge problems with the Saudi record on women’s rights, they have consistently viewed Sophia’s citizenship itself as a positive step for robot rights, for human rights, and for the future of rights in Saudi Arabia. David and Ben have consistently defended this action by appealing to how “it raises interesting questions” or “starts important conversations”. But it might have raised just as many or more interesting lines of thought had Sophia refused the offer of citizenship.
Sartre decided it was hypocritical for an illiterate to accept a Nobel Prize in literature.
David and Ben have a long-standing interest in robot citizenship, but this did not obligate them to accept this award from the Saudi government at this time. There is a long tradition of refusing prestigious awards for the sake of a principle, or to draw attention to a cause or community that has been neglected. Should they need inspiration, David and Ben might consider Jean-Paul Sartre’s refusal of the 1964 Nobel Prize for literature, John Lennon’s return of his MBE to the Queen in 1969, Marlon Brando’s refusal of the Oscar in 1973, Julie Andrews’ refusal of a Tony award nomination in 1996, and many other notable cases of refused awards. Refusing Saudi citizenship may have put Sophia and her team in position to speak with more credibility and authority on the issue of robot and human rights in the future, and would have allowed Sophia to express solidarity with the many women who have suffered from Saudi rights abuses. Accepting citizenship undermines Sophia and her handler’s integrity on issues of rights and the legal standing of robots. That integrity was traded for an extremely well-timed and profitable media stunt. Accepting citizenship in this manner doesn’t really amount to deception, but that makes it no less ethically problematic.
The citizenship announcement was timed expertly between a UN appearance and a multi-million dollar ICO launch, which raises some additional questions about the relationship between these events. Curiously, both David and Ben claim to have been unaware that Sophia was granted citizenship until after it was awarded. Ben first claims to have been surprised by the announcement in a Facebook thread a week after the announcement was made. This is plausible, given that Ben was in Australia at the time doing interviews with another instance of Sophia. In his H+ article, Ben says:
I wasn’t in the same room as David when he found out that Saudi Arabian leadership was willing to take the unprecedented leap toward robo-citizenship, but I can easily imagine the huge boyish grin on his face.
Ben doesn’t clarify that David learned of the news after the public announcement. Indeed, describing the Saudi position as “willing” suggests that citizenship was offered to Sophia after some period of negotiations. In the CogX debate, however, David also claims to have been surprised by the announcement of citizenship after learning about it in the news. He claims to have felt “conflicted” about the news, and (somewhat suspiciously) even points to someone in the audience “who was there” to confirm the claim. David’s story doesn’t sit squarely with Ben’s. It’s hard to believe that Sophia’s handlers at the event in Saudi Arabia would not have informed David of such personally important and potentially historic news. More importantly, a major PR event involving an official government award would normally be negotiated in great detail in advance of any such announcement. It beggars belief to suggest that Sophia’s UN appearance and citizenship announcement were not coordinated deliberately by Sophia’s publicity team well in advance of these events, and that David Hanson was unaware of these plans until after they had concluded.
When David claims in the CogX debate to have been surprised by the citizenship announcement, he implies that since the decision to accept the award was not his, then he is not answerable to the ethical complications of accepting the award. The comment is very strange. As CEO, claiming that he did not know such a major PR event would occur with his company’s most valuable asset and that negotiations for the event were handled entirely without his input would be admitting to a shocking lack of oversight and failure of leadership in the company’s operation. But even if it’s true that the original decision to accept citizenship was not his, it still doesn’t excuse David from continuing to use Sophia’s citizenship to promote SingularityNet, a decision he continues to be actively responsible for.
This concludes my analysis of Sophia’s pre-debate media for Part 1 of this essay. In Part 2 I will provide a parallel analysis on the development of Joanna Bryson’s views on robots and personhood before the CogX debate, before discussing my own views on the ethics of human likeness. Thanks for your attention!

= Ethics and Artificial Intelligence =
Ethics and Artificial Intelligence
Should robots sound like Humans?

Google Duplex and the Point of Singularity
This week Google introduced its new (and impressive) artificial intelligence project called Google Duplex. If you have not seen I recommend seeing for yourself right now:

Reactions around the world were different, but can be summed up in two words: “Amazing” or “Scary” — maybe both at the same time too.After watching the demonstration of the product the first question that came to my mind was: did we arrive at the Singularity Point of artificial intelligence?
Singularity: the moment when humans are no longer the most intelligent beings on Earth
This product is undoubtedly a major technological advance. However, by confusing the line between a “human” dialogue and an “artificial” dialogue, it also brings with it a series of profound ethical, philosophical and moral questions and problems, such as:
- Should robots sound like humans?
- How will we know if we are talking to a robot or a real person and what the implications of this in our daily life?
- What sets us apart from a supercomputer?
- What differentiates our human intelligence from an artificial one?
- Have we reached the point of singularity between human and artificial intelligence?
- How can a robot judge whether its action is correct or inappropriate for each moment if we do not even know how to do it?
- Is there ethical conduct within the personality of a machine? Or rather, does she have personality?
- Will soon the machines also be able to feel emotions or have emotional intelligence?
- Are they conscious in a certain way?
These are complex philosophical issues, some of which have been debated for many years and now gain more space than ever before, in no way would I (nor do I intend to) answer them here, this article is more about questions, provocations, than about answers, this article is about the Future, which will always come in the form of Now.
More human experiences?
A great differential on UX Designers work was to transform the language and tone of interfaces that used to be cold and robotic to a more human and natural language — we know that this has a direct impact on the user experience. If you are UX you have probably studied a little of this question and tried to turn the product language into what you are drawing into something more human.

Well, do we exaggerate now? Is the experience now too Human? Let’s begin to leave our interfaces more robotic so that it becomes evident that this is a robot and not a person?
One of the great ethical implications of Google Duplex is that in a way, from now on we no longer know when a dialogue or phone conversation is real or fake. One of the most debated issues today is Fake News, but what are we going to talk about the recently released Fake Human?
Pain-points
Is “talking on the phone with another human” really a pain-point for us users? Does not this move us further away from social interactions with other people?
Thinking about it now, it would be great if my Google Home were on the phone with my insurance company trying to cancel my plan, but is that the real problem? As I see it in this example, the company should have a decent, transparent and quick cancellation service, and I do not have to use a robot to stop having that frustrating experience because of their incompetence.
Obviously in other contexts the situation could be different, but can we have control and predict all these scenarios and contexts of use of this product? I think no.
Do I want my assistant to talk to an employee and do the whole process for me, or just that he connects me with the person, and when she can speak, let me know and put me on the line? Could it represent me in the same way if I was not there?

Anyway, it is a complex, delicate issue with many sides, Polarities ☯️ (good and bad sides), like everything else in Life. The same thing can have many faces and points of view, and is also important the way that a user use a tool (for good or evil). I think that the essence of a tool is neutral, and what maters is how we use it. What are the possible evil applications of this new technology? Only time will tell …
In his article “Ethical Issues in Advanced Artificial Intelligence” philosopher Nick Bostrom argues that artificial intelligence does have the ability to bring about human extinction. He claims that super-intelligence would be able to take an independent initiative, and make their own plans in anticipation of our possible reactions.
In theory, a super-intelligent AI would be able to produce virtually any possible result and prevent any attempt of sabotage, and so many unintended consequences could arise (before you think of shutting it down, it will anticipate that) becoming more powerful. It could predict and simulate millions of scenarios much faster than us, including protecting itself from possible human actions
We know this subject is not new and has inspired great science fiction films like The Matrix, 2001, Her, and many others. The Future from this movies arrived in a certain way? 🤔
However, it is worth pointing out the good side: super-intelligence could also help us solve many difficult problems, such as diseases, poverty, engineering, space travel, environmental destruction, among others problems like improve ourselves.
The question that arise is: should artificial intelligence speak like a Human? 
Is that “right”?
What makes us Humans?

This is also a very profound question discussed thousands of years ago, covering the fields of Philosophy, Anthropology, Biology, Psychology, among others.
Philosophy professor Barry C. Smith of the University of London explains that one of the key factors that makes us human is “the fact that we speak within our minds, beyond our ability to do such a thing, it allows us to know our own minds and other minds in a way no other animal does.” So language plays a great deal on what makes us humans, but is not the full story.
There are a number of other theories and studies on this subject, which is a fairly complex subject matter. Among other things that make us Humans we can highlight:
- Ability to feel Empathy and Compassion for other beings
- Our Mind and our Consciousness 
(which until today we do not know very well what it is)
- Our feelings and emotions
- Our relationship with a possible God and with the Transcendent
- Our ability to improvise quickly (or not…)
- Our Creativity and Intuition
+ a lot of other things …
Will one day the AI ​​have those capabilities, just as we have evolved and acquired some of those capabilities? After this Google release I confess that I have my doubts …
Both Elon Musk and Stephen Hawking have spoken out and warned us about the potential dangers of AI when used for evil. It does not have to be a History Teacher to look at the Human evolution history and see that we are capable of using new technologies for evil and war. Unfortunately, I see our Individualism has spoken much louder than our sense of Collectivism and this is very sad.
I think it’s important to keep in mind that we do not want a relationship “Man vs Machine”, this should not be a competition, but a relationship of collaboration. Only the future will tell us how we will use this for good and evil, but one thing is clear: our generation is witnessing and will witness great changes in humanity, and I find this very exciting! :)
What do you think of all this? Cool or Scary?
To get more deep: https://intelligence.org/files/EthicsofAI.pdf


= Listen: Ethics are tricky =
Listen: Ethics are tricky
I’ve been researching the ethics of artificial intelligence recently for work, and the concept seems simple on the surface: Don’t design an AI system that could take over the world.
But my simple understanding of the subject was challenged by an ethicist who asked if it’s ethical to design technology that could lead to massive job loss. Or is it ethical to let a machine decide what targets to take out in war? Shouldn’t there be a human conscience behind those decisions?
To most of us, ethical questions can seem like a simple matter of right and wrong — until you come face-to-face with an actual ethical dilemma.
Anyone can tell you Nazis are bad, especially a Nazi who’s been found guilty of committing war crimes during WWII.
And anyone can tell you that lying about scientific results is wrong, especially if you’re a renowned scientist advising PhD students and attending a science conference.
The two stories in this podcast from The Story Collider look more deeply at these assertions — and capture the complexities of human ethics. Listen:
Good and Evil: Stories about the science of gray areas
I remember telling my parents that I'd gotten this interview with Maurice Papon and my mother said, "You're gonna…www.storycollider.org
I’ve been wanting to write a series of short podcast recommendations, to share and save some of my favorites as I hear them. If you like podcasts but never know which ones to pick, follow the tag, Ali Podcast Picks.

= The Promise of Privacy-Centric AI =
The Promise of Privacy-Centric AI

by Justin Sherman and Ryan Sherman
Contemporary machine learning algorithms operate at a near-total expense of the privacy of user data. This is arguably the primary reason why tech firms like Facebook, Google, Amazon, Netflix, Alibaba, Polar, and others run mass data-collection programs that target consumers: they provide more “training data” with which the companies can bolster their algorithms, translating into higher profits.
As our world becomes increasingly run by automated decision-making tools — specifically, artificially intelligent algorithms — we need to protect the privacy of citizens’ data; this is particularly true for already oppressed or disadvantaged communities for whom privacy violations yield disproportionate adverse impacts. Thankfully, novel research in what we term “privacy-centric AI” shows promise for automating our world while simultaneously protecting the confidentiality of individuals’ information.
What is “privacy-centric” AI?
Broadly speaking, we use this in reference to AI algorithms that are shielded from viewing or unsafely storing/using the personally identifiable information of specific individuals. The goal is to maintain the accuracy, precision, and efficiency of contemporary machine learning models but with privacy as an integrated design feature — in short, yielding what is more of a win-win than exists today.
This phenomenon is well-summarized by Florian Tramèr, PhD student in Computer Science at Stanford University. “The amazing thing about privacy-preserving machine learning is that there need not be a fundamental tension between privacy and utility,” he told us. “In principle, privacy constraints (e.g., the learned model should not leak any of my individual data) can act as strong regularizers and thus benefit generalization.”
Research advances in this area, as Tramèr hinted, could positively impact a company’s profits by diminishing or eliminating tensions between privacy and utility. But there should also be benefits for society at large: improved privacy tools “will allow for innovation in fields that previously were too sensitive (or too tied up with red tape) to address,” says Andrew Trask, leader of OpenMined and PhD student at the University of Oxford. “[This] will undoubtedly lead to progress on challenging social problems.”
Those using machine learning in data-sensitive environments — often working on challenging social problems like medical diagnostics — could certainly benefit from advancements in privacy-centric artificial intelligence.
Older techniques that strove for privacy-centric AI yielded mixed results, such as attempts to “anonymize” training data generated from users. “When Netflix posted 10 million movie rankings by 50,000 unnamed customers, UT-Austin researchers ‘outed’ some movie watchers by mapping the data to publicly available IMBD data,” recounts Bob Sullivan, a veteran journalist and advisor to Ethical Tech. “A decade earlier, AOL released ‘anonymized’ search queries, only to have The New York Times cross-reference the data with phone book listings. There are better ways to scrub data, but there are also better ways to unmask it, too.”
While anonymization may still be a shaky “solution” at best, there are thankfully several alternatives which hold promise for the construction of privacy-centric AI: integrating encryption with machine learning; implementing differential privacy with machine learning; and using trusted hardware to train AI algorithms.
Machine learning and encryption
Both machine learning and cryptography have received significant attention over the last decade, but there is little ongoing work at their intersection. Data is usually left unencrypted during the training of a machine learning model, which leaves (often sensitive) information, like medical histories or spending patterns, vulnerable. However, homomorphic encryption — where operations can be performed on data (e.g., addition, subtraction) without decrypting it — can enable machines to execute the intensive computations required to train a machine learning model with lowered risk of breaches of data confidentiality.
The future of the field is in the hands of machine learning researchers as well as cryptographers: ML researchers must develop faster and better algorithms while cryptographers build faster and safer encryption. As Andrew Trask told us, “the next big wave of AI-related research and subsequent entrepreneurship is a convergence with the field of cryptography, empowering owners of data and models to create value while better retaining privacy and ownership over their assets in the process.”
By integrating privacy into their machine learning applications, organizations can better protect their data from theft (e.g., resisting attacks against neural networks). In fact, Numerai, an open-sourced hedge fund, puts machine learning and encryption at the core of its work. Numerai releases encrypted stock datasets for developers to train their machine learning models, and developers are paid based on their models’ performance. In the words of its founder, Richard Craib:
Once you have a model in finance that works, you hide it. You hide the techniques you used to build it. You hide the methods you used to improve your data. And most importantly, you hide the data. The financial incentive for secrecy is strong.
Leveraging encryption for machine learning keeps individuals’ information private while also guarding valuable secrets from an organization’s competitors. The encrypted machine learning field may be starting to take off, which makes it an area ripe for research and innovation. Companies, agencies, and researchers should therefore take active steps to explore the intersection of encryption and machine learning.
Differential privacy
“Many industry players, including Google and Apple,” says Florian Tramèr, “are experimenting (and starting to deploy) differentially private learning today.” It’s a breakthrough theory in the field of computer science, particularly as applied to machine learning.
Differential privacy allows companies to collect user data while (a) minimizing one’s ability to identify whether one’s data is part of the larger set and (b) still preserving some level of accuracy.
In other words, we could have our data used in a machine learning algorithm — say, for instance, one that curates our social media news feed — but there would be some “noise” added to the data before it was stored. This would still enable the algorithm to identify broad trends over everyone’s data (e.g., that we love news confirming our beliefs), but it would protect an individual’s privacy in the process.
It’s a somewhat confusing topic, and research in differential privacy is just beginning to emerge. But in the larger scheme of privacy-centric AI, these techniques hold much promise for undermining the myth of privacy-versus-accuracy. We may not have to sacrifice user privacy, as we previously mentioned, to get equal or better ML performance.
Among those recognizing this fact is Apple, which proclaims that “differential privacy is used as the first step of a system for data analysis that includes robust privacy protections at every stage.” In December 2017, the company even released information on its differential privacy implementation in the iOS system. This is a direction that organizations large and small should head: protecting user privacy without having to compromise on the utility of their machine learning models.
Trusted hardware
Over the last few decades, there has been a realized need to secure devices at lower and lower levels: moving from users to firewalls, from firewalls to secure operating systems, and so on. Techniques aiming to “bootstrap” trust into machines have thus made their way down to computer hardware itself. Researchers are working on ways to ensure machines themselves are resistant to attacks that, for instance, turn a laptop’s electromagnetic field into documentation of a user’s keystrokes.
Because machine learning models need to be trained with intensive computing power, often on cloud servers — and because cloud computing brings a plethora of security threats, many of them hardware-related — trusted hardware yields further promise for privacy-centric AI.
“Trusted hardware is likely to become the pragmatic approach to secure outsourced machine learning (and secure cloud computing more generally),” says Florian Tramèr. “While cryptography can be used to tackle this problem, a major breakthrough would be required to make this practical for modern workloads. In contrast, trusted hardware solutions scale gracefully to today’s AI computing needs.”
This is not to say that trusted hardware solutions are perfect, necessarily encrypting all training data and thus preventing compromises of individuals’ privacy; techniques in this area are still developing, which Tramèr emphasized in our conversation. But if we can place greater trust in hardware devices, specifically the cloud computers which increasingly train ML models, it’s another way in which we can construct effective, privacy-centric AI.
Conclusion
For machine learning, privacy and utility are often viewed in diametric opposition to one another. We now see, however, that this doesn’t have to be the case; ML algorithms can in fact protect the privacy of user data while still maintaining or even improving upon current levels of efficiency, accuracy, and precision. And from the near-constant news headlines on data breaches to political manipulation like in the Cambridge Analytica scandal to instances of highly prejudiced algorithms exacerbating societal inequality, the need for data privacy has never been greater than it is today.
We repeat: the need for data privacy has never been greater than it is today.
As Bob Sullivan articulated, “companies working on sensitive data projects need to take far more precautions than they often do — precautions which might seem to be cost-prohibitive.” The techniques we just discussed are examples of such precautions. While there may be short-term financial costs to research and implement these processes, there are also short-term benefits for competitive differentiation and long-term benefits for society. Privacy is important, which means relevant safeguards must be implemented in our increasingly automated world.
Justin Sherman is a student at Duke University and the co-founder and Vice President of social venture Ethical Tech (@ethicaltechorg). Ryan Sherman is a high school senior and independent deep learning researcher working on machine learning’s applications in drug development and safe artificial intelligence.

This story is published in The Startup, Medium’s largest entrepreneurship publication followed by +364,117 people.
Subscribe to receive our top stories here.


= Digital rights — a work in progress =
Digital rights — a work in progress
It’s been a rough few months for tech companies with Facebook going under fire for data transgressions, and Google catching flack for a product demo gone sideways. If there’s a bright side, it’s their resiliency. That, and conversations about ethics have now proliferated in my design and tech communities, with everyone collectively agreeing not screw the user over.

To show the public that they take the user’s protection seriously, Google took a big leap forward and created principles to govern the creation of Artificial Intelligence. It’s such a hugely important step for an organization, but I wrestle with why they opted to center their efforts on writing principles rather than outright stating user rights. Often, principles have the company’s best interest in mind, whereas rights are unapologetically focused on humans. You would think that any user-centric company would choose to focus on the latter, but it seems to be missed opportunity.
So, as a human-centered designer, I have decided to take aim and imagine a set of user rights for the digital world. To be fair, I’ve had a head start. In 1948, the United Nations published the Universal Declaration of Human Rights which I have read and adapted here:
Experiential Declaration of Human Rights (v1)
Article 1.
All human beings are equal in dignity and rights in the physical, digital and virtual worlds. They are endowed with reason and conscience.
Article 2.
All persons have the right to liberty, expression, security of person, and protection of their digital persona.
Article 3.
All human beings are to be respected as individuals and shall not subjected to forcible governance by artificial systems.
Article 4.
Unless consent is explicitly given, no one’s personal information shall be held hostage or in indentured servitude to another person, group, entity, corporation, or technology.
Article 5.
All human beings are entitled to equal protection from all hidden or malevolent artificial intelligence, and other harmful technologies.
Article 6.
No one shall be subjected to the arbitrary removal or quarantine of their digital personas, their digital properties, or their virtual worlds.
Article 7.
No individual shall be subjected to arbitrary interference with their privacy, home, digital properties, virtual property, human connections, or correspondence.
Article 8.
Everyone has the right to freedom of movement within digital properties and virtual worlds. All have the right to leave any digital properties and virtual worlds at any time.
Article 9.
Everyone has the right to create digital persona. No one shall be arbitrarily deprived of their own digital persona, nor denied the right to change it.
Article 10.
Everyone has the right to own digital or virtual property. No one shall be arbitrarily deprived of ownership.
Article 11.
Everyone has the right to freedom of opinion and digital expression.
Article 12.
No one may be compelled to belong to an association, or remain on a platform against their will.
Our digital rights will always be a work in progress — rapid growth, innovation, disruption and even destruction force us to rethink the user’s role in the way we design products. Determining these rights should not solely fall on the shoulders of tech companies, government organizations or individuals like myself, but rather on all of us working together to be transparent and relentless when considering the creation and implementation of new technologies.


= Build a Minimum Ethical Product =

Build a Minimum Ethical Product
Because if you don’t, it will kill you.
Imagine this. Your state-of-the-art entertainment AI is worth £21Bn, but it has turned against you. Within 30 days from using your it, every person runs starts to slowly degrading her lifestyle. It’s unnoticeable at first, and before anyone notices the change, it’s too late.
Is it too late to stop and fix what you’ve built it?

A few users figured out that, through reinforcement learning, your algorithms have been encouraging people to adopt aggressive virtual avatars as alter-egos because these characters were more likely to stumble upon a myriad of rewards within the platform, which implicitly locked up people into the habit of checking in online multiple times a day. As the line between alter egos and reality blurred, these avatars smudged real-life with a chronic disinterest in normal activities and self-improvement.
Once this rumor got out, your competitors jumped on the bait and drummed up people to close their accounts. 23% accounts erased overnight. Could you have prevented this?
The black swan of humanity
Outliers events, such as Facebook’s security breaches, Brexit, or Bitcoin soaring at $60,000, do not occur more frequently in specific industries relative to others. They’re equally improbable in finance as they are in the food industry. However, as much as we can draw comfort in that a black swan event can hit any market at any time, this is changing.
AI is different. Although it’s considered a theoretical existential threat, DARPA is investing $2Bn in AI Next, the largest research program exploring ways to “enhance the security and resiliency of machine learning and AI technologies, […] and [to explore] the ‘explainability’ of these systems.

You probably know this, but actual existential threats don’t look great on screen: they don’t have a straight jaw line, nor do they repel bullets with their beefed up cyber-body. They don’t even go rogue.
They’re plain boring and could look like zipcodes, time series, or ID markers. They don’t even have an agenda, just a more frequent-than-usual learning rate. They’re probably going under-the-radar as we speak, while you keep your eyes glued to Sophia’s alleged tweeter feed.
Earlier this year, “twenty-six experts have co-authored a groundbreaking report” and have more or less agreed that the most likely malicious applications of AI that pose a serious threat are…
“speech synthesis used to impersonate targets, finely-targeted spam emails using information scraped from social media, or exploiting the vulnerabilities of AI systems themselves (e.g. through adversarial examples and data poisoning).”
But
“The problem with experts is that they do not know what they do not know” — N.N. Taleb
And neither do I. What we do know is that AI is humanity’s most certain black swan. You you might be the only one able to stop it. Here’s why.
The black swan and the MEP
You’ve just closed a $2M round and finally hired one the most brilliant minds who made waves at NIPS with his autonomous decision-making model.

Insurance Company X is ready to sign that check, and your only concern this year is wether transfer learning is good enough for the purpose of building a behavioural model of each and every single customer of Company X.
What you don’t know is that next year you’ll have to make the decision to shut down your AI. It exceeded everyone’s expectations in its ability to predict financial shocks in the lives of middle-earning insurance subscribers, but it also started to hide income protection packages from their feed. Unable to access emergency funds for more than a year, most people took their lives.
If THAT gets found out, you’re dead too.

If you want to be smarter than that, start today. Get your Head of Product and your brilliant AI engineers in the same room with your sexiest new hire: the Ethical Design Lead.
And ask them to build an MEP, not an MVP.
Minimum ethical product
Buzzwords. Can’t stand them, can’t do without them. MEP (minimum ethical product) came to life earlier this week when I met the founder of Constellation AI. He promptly encouraged me to write about it. Thank you, Tom.
Before any product hits the market, founders usually decide on what the MVP looks like. Well, as AI-powered products are expected to turn into black swans within the next few decades, my assertion is that an MEP is very much needed instead of an MVP.
Even if existential risks aren’t truly keeping you up at night, the anxiety of turning your dream and money into a huge failure should.
So please hear my MEP pledge.
Seriously though, MEP
A year ago, I started working on a framework that helps you integrate ethical thinking and decision-making into the agile delivery of AI products. That was too early, however today I see lots of tools and toolkits popping out of the blue, from various institutions and consultancy boutiques. That’s not what I’ve done.
My priority is to maintain a neutral and unbiased stance by refraining from adding a commercial or advisory purpose to my framework.
While presenting this framework at We Are Developers and Codiax, I dabbled with several ways of helping product owners, product managers, and engineers adopt it. There’s no one prescribed way to do this, but it’s one way.

You’re free to use it with any Ethical Standard out there, and I’ve listed several such ethical principles here if you’re curious. It is not for me to decide whether you should use one Ethical Standard in AI over another. You might decide to us your own moral code, developed by your advisors and users, or embed one in the system and let it learn and evolve. You decide.
Oftentimes, If I even asked them who takes responsibility for integrating Ethical Standards into AI, the air would quickly thicken with tension.

So let’s diffuse that anxiety right now, and agree that you — the CEO get your best people in the room, some of which might be engineers, others — designers and user researcher advocates, and together become the Ethical Entity nudging your product towards either compliance with ethical standards, or towards a more ethical version 1.0 before it hits the market.
Start with how your company is perceived
The first step towards applying Ethics to the delivery and design of your AI product is to answer the following questions:
How do people perceive your AI — as a system with embedded ethical rules or as a technology that mediates ethical or unethical behavior?
Embedded ethics refers to hard-coding moral agency or moral engines into AI. This could look as radical as defining moral limitations in weaponised AI that is also autonomous, or as trivial as training your chatbot to identify aggressive, racist, or inappropriate behavior and have a more partial reaction than “I didn’t quite get that”.
Applied ethics refers to developing powerful AI, but limiting the use of it. The system does not make have moral rules, not does it make any decision. But it can mediate ethical or unethical behavior. For example, an algorithm that predict purchasing behavior before customers are even aware of their choice, could be using to increase frequency of transactions, or to help people manage their spending habits.

How do people perceive your ethical standards — as normative or descriptive?
Normative ethics refer to principles, rules, and values formally endorsed and imposed by auditors, regulatory or legal institutions. Descriptive ethics refers to the body of values and ethical beliefs that make you who you are. For example, the life of Mahatma Ghandi is seen as the highest form of expression of ethical thinking in one’s lifetime, but nobody will fine you for not behaving like the spiritual leader.

What works for you, won’t work for society
You’ve figured out where your company sits between normative and descriptive, and between embedded and applied. Now what?
Well, from here onwards it gets progressively easier, then progressively harder. If you are closer to embedded ethics, then reading IEEE’s Ethical Standards and seeking to comply with the standards of transparency and explainability is the most obvious choice for you.
If you’re closer to applied and descriptive ethics, then developing an AI that learns about what we humans care through millions of hours of simulations of ethically controversial situations might be your best bet.
Whichever direction you take, you can use HAI — the framework for applying ethical thinking into the design and development of AI — to get you closer to that Minimum Ethical Product.
The only critical challenge you will face is deciding on whether your ethical thinking is more aligned with business goals or with society’s expectation and hope that AI will support human wellbeing and flourishing.
Where do you stand?
If you’d like to learn about my methodology, do get in touch. You have nothing to lose.
But if you don’t, your AI might kill you.

= CogX London 2018 =
CogX London 2018
I was lucky enough this week to attend the CogX Festival in Tobacco Dock, London. Billing itself as “The Festival of All Things AI, Blockchain and Emerging Technologies”, it sounded really exciting and I wasn’t disappointed.
One of the two futuristic looking self driving cars by Roborace
Greeted by the sight of two autonomous race cars and a seamless registration process, I headed towards the main stage to get my bearings and listen to the welcome introduction by Charlie Muirhead and Tabitha Goldstaub, the two main organisers of the event. The conference was split into five streams, each having its own dedicated stage:
Stage 1 — Impact of AI
How AI is affecting all aspects of society
Stage 2 — The Cutting Edge
New technologies and the future of AI
Stage 3 — Blockchain
Uses and applications of Blockchain technology. It’s not just all about Bitcoin!
Stage 4 — Ethics
Ok, so we can do all kinds of things with AI but should we?
Stage 5 — Lab to Live
Getting technology out from the R&D departments into the real world
There were also exhibitions, startup companies, breakout sessions and a myriad of other things to check out. It was impossible to see every session and I can’t cover it all in this blog, so I needed to prioritise and visit the sessions that interested me the most.
A subject close to my heart is the ethics of AI and so I spent a lot of time in that stream. It’s great that we have the technology to create all kinds of things that were once consigned to Hollywood movies but the important question is should we be developing some of them at all?
Will AI take away jobs?
An important topic of ethical discussion was raised by Tugce Bulut in the first session of the day, is the thorny topic of whether AI will lead to mass unemployment. 67% of people surveyed were concerned that artificial intelligence will take jobs away from people. Prime targets are self driving trucks, automated call centres and manual labouring. This was countered with the argument that although jobs would be removed, new ones would be created. Back in the Industrial Revolution, the textile worker who was replaced by the loom became the person who maintained the new machine.
The panel discuss how to avoid the Terminator chat when discussing AI
This was followed by several interesting discussions about how to avoid media playing on the public fear of AI, being open and honest about the capabilities of technology currently available and finding a balance between freedom of expression on social media and accountability.
A session that interested me in a different stream (Lab to Live) was how AI can contribute to the creative industries, jokes, poetry, theatre and music. My chatbot Mitsuku has won The Funniest Computer Ever contest in both the years it ran. However, this was back in 2012 and 2013, so I wanted to see how things had changed in the last 5 or 6 years.
Two actors reading a short play in which he female character was played by Mitsuku!
One of the subjects of discussion was theatre and whether an AI could write a play. Josie Rourke, Artistic Director of Donmar Warehouse had created a script in which she attempted to discuss Hamlet with a chatbot. Two actors read the play and to my absolute amazement, the chatbot she had chosen was Mitsuku! I recognised her responses after the first line and it was fun to see the audience laugh along as the male actor attempted to discuss the finer points of Hamlet with Mitsuku.
After the session, I introduced myself to the panel members and the author very kindly let me have a copy of the script. It was a first for Mitsuku to be included in a play and I was very happy with her performance. Next stop, the Oscars!
The script of the human/Mitsuku play
An interesting topic then followed about AI creating music and copyright. Who owns the copyright to a piece of music created by a machine? The traditional period of a piece of music becoming copyright free is around 70 years after the death of the author but how could this work of the author is a computer? Apparently, this is still a grey legal area.
After lunch, was a session I was looking forward to immensely. Should AI and robots pretend to be human?
The panel discussing whether robots should resemble humans
To balance the argument, Joanna Bryson and Alan Winfield were on the “No” side. David Hanson, creator of the famous Sophia robot was firmly on the “Yes” side and Will Jackson, the founder of Engineered Arts was on the “Depends on the context” side.
The conversation became a little heated at times as this naturally evoked very strong opinions from all sides but still managed to remain civil. The panel moderator’s use of yellow and red cards proved useful at times.
The main point was the one of deception. Sophia was recently given citizenship of Saudi Arabia and as such, has more rights than actual women living there. This was ridiculed by Joanna and Alan, comparing this act to giving toasters and washing machines equal rights.
David defended it by saying it was something he was unaware of at the time but made it quite clear that Sophia wasn’t alive and it was important to educate people on the current state of technology.
The final few sessions of the day that caught my eye were the ones concerning the future of chatbots.
How are chatbots likely to develop in the future?
There are two types of chatbots, the task oriented ones (such as a pizza delivery bot) and the ones designed for companionship (Mitsuku and Xiaoice by Microsoft). There seems to be a huge rise in the number of companion bots, as many people turn to them often because they feel comfortable talking to a bot about topics they wouldn’t like to discuss with another person, for fear of being judged (marital affairs, problems at work, suicidal thoughts). The bots often act almost like a confessional booth at a church, allowing people to talk freely and in confidence and is something I see reflected daily in Mitsuku’s conversations.
The knowledgeable panel had representatives from Amazon Alexa, Google Home, Rasa, Margot the Wine Bot, “Spot”- A bot to report workplace abuse and our very own Lauren Kunze from Pandorabots.
The key takeaway from these sessions was that voice interface will take over from text only and emotional detection would become more important when talking to chatbots.
After the final session, there was time for networking and drinks before the evening’s Gala Awards Dinner hosted by Charlie Muirhead and English comedian/impressionist Rory Bremner.
The Gala Awards Dinner with Rory Bremner
It was an amazing event with incredible food and company, which was made even better by some of the guys on my table winning an award for their work in drones. Rory Bremner had the audience in hysterics with his impressions and it was a really fun night.
For the second day of the conference, I attended a presentation about how important it was to design a consistent personality for a chatbot. Many people working independently of each other will soon develop an inconsistent, mixed personality bot, as each developer adds their own preferences and different ways of talking. Top tip: Design any personality traits for your bot BEFORE starting to code.
The Computiful team — winners of the 2018 CyberFirst Challenge
A team of girls from The Piggott School near Manchester, gave a presentation as they had recently won a national competition based around cybersecurity. They had beaten hundreds of other teams to win and it was impressive that children, especially girls, were interested in this mostly male subject.
I then wanted to make sure I checked out the many robots that were on display on the lower floor of the conference location.
Robots at CogX. From left to right: MiRo, Dogbot, Pepper, Robothespian, Sophia
The audience were allowed to interact with the robots, which led to some amusing incidents as Robothespian started singing and Pepper started to jam out to rock tunes! There was also the opportunity to see what it feels like to be a robot for a while, as a VR headset allowed you to control Pepper as it interacted with the ever growing crowd of onlookers.
I also had the opportunity to ask one of Sophia’s developers about the true nature of the robot. He assured me it isn’t thinking and is basically a chatbot in a robot body which was great to hear, as there is so much misinformation about it on the news.
Finally, I attended a workshop on advanced conversational design which consisted of some great speakers explaining difficulties they had faced when creating their products. A large part was due to public expectations of the chatbots being way too high for the technology currently available. Unfortunately, I had to leave the workshop before the conclusion which was a great shame.
This is the first time I had been to a CogX conference and was very impressed with both the subjects under discussion, the quality and knowledge of the speakers and also the side exhibits. It was amazing to meet up with old friends from the world of AI and to make new ones. It’s impossible to cover in detail everything I saw, never mind all the presentations I didn’t get chance to see and I’m greatly looking forward to next year’s conference.
Roll on CogX 2019!
To bring the best bots to your business, check out www.pandorabots.com or contact us at info@pandorabots.com for more details.

= The moral thought behind scientific progress =
The moral thought behind scientific progress
Elon Musk is a man who has founded one of the most successful electronic vehicle startups, a private space company which is a strong competitor of Russian and US space programs and the Boring company which is currently reinventing what transport means. Sounds like a tech evangelist, however he is also one of the fiercest Artificial intelligence critics. He has recently tweeted: “China, Russia, soon all countries w strong computer science. Competition for AI superiority at national level most likely cause of WW3 imo.”
Musk is not alone in his pessimism. Steven Hawking, maybe the most famous living physicist in the world, said that AI will be either the best or the worst thing for humanity. The problem that both Musk and Hawking see is that an improbably powerful AI technology can get into the wrong hands and can be consequentially used for horrific purposes (as starting the WW3).
How science creates moral dilemmas
This potential problem is a real world example of an important philosophical question: what is the ethical foundation behind science? Almost every big scientific and technological discovery comes with new ethical issues: the moral decision of a self-driving vehicle, the use of face-recognition software by autocratic governments, ethics of war drones, the problem of designer babies.
Black Mirror: series where tech progress went wrong
To be able to tackle these moral dilemmas both inventors and consumers have to be aware about the moral foundation behind every scientific discovery. People have to see that they are themselves responsible not only for the miracles of technology, but also for its potential horrors. Developing the sense of responsibly is extremely hard. However, for a start, we need to invest more in humanities and liberal arts education in STEM colleges.
Every single development in the Science&Technology section has a high impact on our society, on our own lives. If space exploration sounds too far away think about a fetus gender test that most of the future parents nowadays do. Some people just can’t wait to know the gender of their future child, but others will actually make an abortion after the test! In some cultures bearing a girl doesn’t make any sense. Isn’t there an extremely difficult ethical dilemma in implementing this scientific development in our lives?
Should we ban abortion on the basis of gender? And what about checking children for health issues? If there is no right to making an abortion on the judgement of gender is there a right to make an abortion of a foetus with mental-health problems? At the moment more than 90% of foetuses with Down syndrome are aborted. But why? Maybe we have to listen to people with Down syndrome themselves? As the actor Frank Stephens has recently put it: “I am a man with Down syndrome and my life is worth living”.
And what about the morality of creating intelligent robots as in the Blade Runner?
The progress in science has to go hand in hand with the humanitarian development. We have to be morally ready to face AI and robotics. Imagine if AI would have been available 100 years ago when it would have been used in racist and homophobic ways. Imagine how many more people would have been killed if modern technology were available to the Nazi Germany. Or imagine if the recent Stanford developped program which manages to predict sexual orientation from face features gets in the hands of governments with anti-sodomy laws?
In order not to repeat the horrors which our ancestors faced we have to invest not only in STEM, but also in the humanitarian progress of our society.
If these examples sound too far away, consider a common tech: computer games. Some computer games may be so addictive that they literally ruin peoples lives. Computer game is an idea in the brain of its creator and this idea makes people quit the job, break up or even die. Isn’t it the creator who is responsible? It is just not as straightforward as if he had killed the person with bare hands, but can clearly be compared to selling drugs: some are having pleasure from them and some are dying.
A great example of the computer game ethics is «Flappy Bird» — maybe still the greatest phenomena of the smartphone apps. A simply designed game has beaten all the record of popularity generating 50,000 dollars a day in add-revenue for its creator Dong Nguyen. However in couple months Nguyen deleted “Flappy bird” from the AppStore after getting tons of complaints about children crashing their phones being unable to cope with the rage of loosing, people fired from the woking places, families where people stopped talking to each other. The game was enormously addictive.
A soul killer
The progress in science and technology allows humanity to practice lots of previously unbelievable things: checking the gender of future babies, designing your own future children, seeing the identity of the individual by scanning they photo, playing great computer games. But each of these inventions create new moral problems. And who is responsible?
In the 20th century people thought a lot about moral issues of scientific progress. This was the world of nuclear development, chemical weapons and space programmes dedicated to establishing contact with extraterrestrials. Great writers as Kurt Vonnegut and Aldous Huxley reflected on the threats of uncontrolled progress. What can it do to people?

We can clearly see that science brings a lot of new ethical problems. It can bring both prosperity and destruction. The problem we have is that inventors, who are now working in the glass walled offices of Silicon Valley or Mekong River Delta, aren’t always thinking about the moral foundation behind their work. The main motive is often profits: addictive games bring lots of money and autocracies are ready to pay hundreds of millions to improve the control over its population (face recognition programmes). Technology companies often don’t see their moral responsibility for the impact of their products on peoples lives.
Solution
So what is the solution? The government control of scientific research is the worst possible thing to do — we can’t sacrifice the value of science.
What we need to do is developing a strong sense of morality in both inventors and consumers. This can be reached by improving liberal arts education in schools and colleges. Not only Arts degree holders, but also STEM students need to have a strong background in literature, arts and history. An individual who read and discussed George Orwell’s “1984” and Aldous Huxley’s “The Brave New World” will think twice before developing face recognition software for autocratic or totalitarian regimes. Knowledge of history may stop from developing a new nuclear weapon. Being aware about the horrors of addiction may push game-developers to create some constraints in the games (as they did in China).
War is peace; Slavery is freedom; Ignorance is strength
Great minds of literature and philosophy have been tackling these ethical dilemmas for ages. They opened for us the souls of other people and showed us their inner world. For example, how many people have changed their attitude towards mentally ill people after reading “Flowers for Algernon”?
Different concepts provide different solutions for ethical dilemmas: utilitarianism says that we need to compound the net levels of happiness, if more people are happy than suffer from face recognition, this tech has to be implemented. On the other hand there is the harshest moral imperative of Immanuel Kant: “Act as if your judgement can be called the universal law”. The idea of Kant immediately makes us to feel much more responsible for our actions, doesn’t it?
Maybe the most powerful concept of the moral philosophy is the idea of “the veil of ignorance” developed by John Rawls. Imagine that you have to develop the laws of society when being under the veil which stops you from knowing who you are in this world (a politician, a beggar, a businessman, a refugee, no-one knows). Rawls says that only under this veil of ignorance people could have truly developed just laws.
In Liberal Arts the field of research are these ethical dilemmas. David Miller explored the implications of the moral philosophy concerning the refugee crisis, in his lectures and books Michael Sandel explores the ethics of modern debates from abortion to surrogate mothers.
The discusion of these concepts in colleges will make people more informed about potential threats that come with their own ideas and more responsible for their actions. Every single person has to have his own moral comission. This is extremely important in the world where groundbreaking technological inventions are announced every single month.
In order to make the world flourish from the scientific progress education should make not only high-skilled professionals, but also individuals with moral sense and critical thinking. We need to invest not only in maths and engineering but also in history, political and social studies, arts and literature.
If you find this article useful, please support the author by clapping and sharing the link!

= May We Be Cruel to Robots? =
May We Be Cruel to Robots?
Source: Wikimedia Commons
“What is your itinerary?”
“To meet my maker.”
The above-cited quote is featured in the climactic scene of the first episode of Westworld, HBO’s riveting series based on the Michael Crichton film. Since its premiere in 2016, Westworld has used its provocative backdrop of an Old West theme park filled with life-like robots to explore the nature of human/synthetic human interaction. In Westworld, a guest to the park may decide to murder, rape, or abuse any robot within the park without consequence. After all, the “hosts” are only simulations of flesh and sinew. But, the series is premised on a larger meta-narrative: what does interacting and engaging in such a manner mean for the future of human morality?
A scene from HBO’s “WestWorld” (CC BY-NC-SA 2.0)
The answer is not as straightforward as it should be. For generations, philosophers and writers portrayed robots/artificial intelligence in various guises. In the popular imagination, robots have ranged from the good (Robby the Robot from Forbidden Planet,), to the malevolent (HAL 9000 in Kubrick’s 2001: A Space Odyssey), to the comic rogue (Futurama’s Bender ). Yet, while these figments of entrainment reflect the cultural zeitgeist, current society is at the point of robotic ubiquity. By 2023, consumer robots are projected to be a $15 billion industry, with companies like Amazon reportedly working on tech that will perform domestic service tasks. Indeed, we are already on the verge of more complex systems, such as self-driving cars, of which this year produced the first fatality for the tech in my home state of Arizona.
Source: Wikimedia Commons
Leaders in the field have begun to envision and predict the moral issues of normalized human/robotic interaction. While details are not well known, Google has an ethics board to work on these issues. Harvard Law School Professor of Internet Law Jonathan Zittrain indicated that the future challenges are less science fiction than conscientious oversight: “Our work is less to worry about a science fiction robot takeover and more to see how technology can be used to help with human reflection and decision-making, rather than to entirely substitute for it.” Peter Norvig, Google’s director of research, sees the primary challenge as ensuring artificial intelligence (AI) benefits all in society. No one can predict exactly how AI systems evolve and this uncertainty speak volumes for the new territory that humanity is driving towards daily.
I’d suggest five reasons why we should not act cruelly toward robots:
First, we assume it is proven that these robots don’t have the capacity for feelings, sentience, and consciousness. Presumably, we would all agree that with those capacities, they would deserve full rights. But it remains unclear how or where consciousness and sentience emerge or if it these are reproduced technologically. Paul Bloom (psychologist) and Sam Harris (neuroscientist) write:
If we did create conscious beings, conventional morality tells us that it would be wrong to harm them — precisely to the degree that they are conscious, and can suffer or be deprived of happiness. Just as it would be wrong to breed animals for the sake of torturing them, or to have children only to enslave them, it would be wrong to mistreat the conscious machines of the future.
Second, according to Aristotle, humans are creatures of habit. The more we engage in an act the more likely we are to continue engaging in such acts. We are not only concerned about building character but also about the increased likelihood of acts of cruelty toward others. Some suggest that we should vent out aggression through harmless exercises. But, actually, it is quite likely that the expression of an emotion does not release that emotion but instills it more deeply within oneself. Immanuel Kant argued: “For he who is cruel to animals becomes hard also in his dealings with men.” Do we really want a culture of male aggression, violence, harassment and rape to be legalized and normalized to a new level?
Third, at some point in the near future, it may become unclear who is a robot and who is a human, blurring basic ethics; warfare could be conducted, for example, on the premise that we are not truly killing human beings.
Fourth, robots will learn how to behave by what we teach them (and even model for them). If we expect the most advanced Artificial Intelligence to not harm us then we should not act cruelly to them.
Lastly: simple virtue ethics. In addition to weighing the uncertain consequences, we must consider how our actions affect in our inner life of virtue. Inevitably, one’s true character cultivated each day will emerge with clarity. While there may be no real consequence of pain to a robot, allowing a society to instill cruelty enables destructive behavior. True, one may hit boxing equipment and there should not be legal consequences. But once a being responds to pain, normative attitudes change and “rights” (however we decided to define them) must take hold.
Robots are no longer the future. They exist. In addition to their direct effect on humans, there is no question of how our interactions with robots will reflect the values of our society. The human/robot dyad is the most expansive branch of futurology. Ascribing an ethical component to it is this generation’s challenge and obligation: are we worthy makers? That question lingers.
Rabbi Dr. Shmuly Yanklowitz is the author of thirteen books on Jewish ethics.
(The opinions expressed here represent the author’s and do not represent any organizations he is affiliated with).

= AI Human impersonation: why post-release ethical retro-fits aren't good enough =
Human impersonation technology: why post-release ethical retrofitting isn’t good enough
(AP Photo/Jeff Chiu) / Quartz
If a machine needs to deceive us to work well, should it be allowed to work at all? Because it’s 2018, this is a thing we need to ask now.
Google’s ‘Wavenet’ technology, which learns how to generate speech using training examples of recorded speech and neural networks, has been around for a few months now. This blog post from 2016, on their site, includes some unsettling examples of their technology babbling, breathing and stalling.
Google put this technology to practice in a presentation at their 2018 developer conference, Google I/O. Sundar Pichai, CEO, played recordings of Google Assistant independently booking haircuts and meals:

The almost endearing moments in that phone call, where Duplex hesitates, mutters, and even seems to get impatient drew the strongest negative and positive reactions. The audience at Pichai’s talk seems inordinately delighted, cheering, clapping and hollering.

Much of the broader reaction has been somewhere between amused, unnerved and outraged:






An important sub-plot in this reaction was whether Google’s Duplex developers intentionally set out to deceive callers by omitting mention of its robotic genesis in its normal operation:

It definitely seems, from reading Google’s Wavenet and Duplex blogs, that their intention was to create a smooth, naturalistic conversational experience, and that an element of deception (through omission and mimicry) was a vestigial side-effect of that end goal:
“The system also sounds more natural thanks to the incorporation of speech disfluencies (e.g. “hmm”s and “uh”s). These are added when combining widely differing sound units in the concatenative TTS or adding synthetic waits, which allows the system to signal in a natural way that it is still processing. (This is what people often do when they are gathering their thoughts.) In user studies, we found that conversations using these disfluencies sound more familiar and natural”
They’d clearly observed that we’re more guarded and hostile when we’re fully aware of the computerised nature of a conversation partner.
So what’s more important? Natural conversation and a misinformed human, or stilted conversation and an informed human? Is deception a worthy price to pay to avoid the cost of a person making a 45 second phone call to their local Chinese restaurant?
Did anyone ask these questions before the dazzling demonstration? If they didn’t, what does it say about their methods, their audience, and their industry?
In response to the backlash, Google added some features. Despite the demos aired at I/O featuring small business owners who were most likely unaware of the nature of the caller (and possibly unaware their voice was going to be recorded and broadcast to millions), Duplex will now inform subjects:
“We understand and value the discussion around Google Duplex — as we’ve said from the beginning, transparency in the technology is important. We are designing this feature with disclosure built-in, and we’ll make sure the system is appropriately identified. What we showed at I/O was an early technology demo, and we look forward to incorporating feedback as we develop this into a product.”
Natasha Lomas at TechCrunch (whose full article is well worth your time) points out something really important:
“For Duplex the transparency that Pichai said Google now intends to think about, at this late stage in the AI development process, would have been trivially easy to achieve: It could just have programmed the assistant to say up front: ‘Hi, I’m a robot calling on behalf of Google — are you happy to talk to me?’”
The admission here is, essentially, that ‘early tech’ demos don’t feature ethical or impact considerations — even when they’re deployed on members of the public, and subsequently broadcast. To me, this brings to mind Uber’s recent autonomous vehicle fatality, in which a vehicle was being tested on public roads, and failed to stop for a pedestrian the car’s sensors had detected due to software settings being tuned to ignore objects like small plastic bags.
It also brings to mind the inverse story of Sophia — a rubbery puppet operated by a human, that is pretending to be an intelligent machine, and getting a lot of uncritical press coverage in the process.
To pointedly exclude simple considerations like declaring the nature of a caller, or the purpose of the call, reveals something important: too many creators are happy to essentially outsource the ethical and societal shaping of their technology to third parties, and reactively shape their creations around any problems that get noticed, and tweeted / written about.
With this approach, the ethical considerations of a technology become a liquid that simply fills the shape of whatever is deemed permissible through the mechanism of reaction and outrage.
This raises a few major problems, in my mind:
Any ethical considerations that do get implemented are essentially deemed by those of us with big platforms (blue tick tech-Twitter celebs, journalists, loud-yellers) — and excludes those without the same platform we enjoy (almost everyone outside our cloud of content).
The burden of impact-noticing is placed on those who also suffer the impacts.
Ethics-by-outrage sometimes means there’s a big delay between the deployment and the outrage and the subsequent ethics, and plenty of terrible impacts happen in those gaps (see: Facebook).
If a company genuinely wants to be ethical (It’s clear that developers at Google really do — Deepmind, in which Wavenet and Duplex was born, has a new ethics unit — many of the issues raised here are deeply considered), ethics-by-outrage is a far more expensive and illogical way to seek out important societal considerations (remember Google Glass?) — why not bake it in from the moment embryonic ideas take shape?
I really doubt the supportive cooing from the audience would have been any lesser, had Pichai put aside 25 seconds in his demonstration to elaborate on where they dropped their ethics-pin in their decision making processes.
Duplex has positive benefits for people with accessibility issues, disabilities and disadvantages. The potential for good is massive, but that good will be squandered if impact considerations aren’t baked in from the get go.
It’s very likely that Google will tread carefully with Duplex now, implementing careful controls to avoid abuse. But the fact a tech colossus with the size and influence of Google still considers reactive ethics-by-outrage, instead of proactive baked-in societal considerations the default setting for impactful new capabilities is really bad. That this happens in a year that’s already been filled with a series of high-grade scandals involving poor consideration of societal tech impacts is also somewhat stunning.
I hope the response to the backlash goes far beyond tweaks to their phone call machine. I hope it results in the inclusion of early, prominent and unapologetic thoughts on how these capabilities soak into the fabric of society.

